<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431489</article-id><article-id pub-id-type="pmcid-ver">PMC12431489.1</article-id><article-id pub-id-type="pmcaid">12431489</article-id><article-id pub-id-type="pmcaiid">12431489</article-id><article-id pub-id-type="doi">10.3390/s25175357</article-id><article-id pub-id-type="publisher-id">sensors-25-05357</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Large Kernel Convolutional Neural Network with a Noise Transfer Mechanism for Real-Time Semantic Segmentation</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5509-0846</contrib-id><name name-style="western"><surname>Liu</surname><given-names initials="J">Jinhang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05357" ref-type="aff">1</xref><xref rid="af2-sensors-25-05357" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Du</surname><given-names initials="Y">Yuhe</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05357" ref-type="aff">1</xref><xref rid="af2-sensors-25-05357" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="J">Jing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05357" ref-type="aff">1</xref><xref rid="af2-sensors-25-05357" ref-type="aff">2</xref><xref rid="c1-sensors-25-05357" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4833-8604</contrib-id><name name-style="western"><surname>Tang</surname><given-names initials="X">Xing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af3-sensors-25-05357" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Deng</surname><given-names initials="LJ">Liang-Jian</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05357"><label>1</label>School of Computer Science, Hubei University of Technology, Wuhan 430070, China; <email>liujinhang@hbut.edu.cn</email> (J.L.); <email>102301202@hbut.edu.cn</email> (Y.D.)</aff><aff id="af2-sensors-25-05357"><label>2</label>Key Laboratory of Green Intelligent Computing Network in Hubei Province, Wuhan 430068, China</aff><aff id="af3-sensors-25-05357"><label>3</label>School of Computer Science and Artificial Intelligence, Wuhan University of Technology, Wuhan 430070, China; <email>tangxing@whut.edu.cn</email></aff><author-notes><corresp id="c1-sensors-25-05357"><label>*</label>Correspondence: <email>wangjing@hbut.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>29</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5357</elocation-id><history><date date-type="received"><day>28</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>27</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>29</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 14:25:13.570"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05357.pdf"/><abstract><p>In semantic segmentation tasks, large kernels and Atrous convolution have been utilized to increase the receptive field, enabling models to achieve competitive performance with fewer parameters. However, due to the fixed size of kernel functions, networks incorporating large convolutional kernels are limited in adaptively capturing multi-scale features and fail to effectively leverage global contextual information. To address this issue, we combine Atrous convolution with large kernel convolution, using different dilation rates to compensate for the single-scale receptive field limitation of large kernels. Simultaneously, we employ a dynamic selection mechanism to adaptively highlight the most important spatial features based on global information. Additionally, to enhance the model&#8217;s ability to fit the true label distribution, we propose a Multi-Scale Contextual Noise Transfer Matrix (NTM), which uses high-order consistency information from neighborhood representations to estimate NTM and correct supervision signals, thereby improving the model&#8217;s generalization capability. Extensive experiments conducted on Cityscapes, ADE20K, and COCO-Stuff-10K demonstrate that this approach achieves a new state-of-the-art balance between speed and accuracy. Specifically, LKNTNet achieves 80.05% mIoU on Cityscapes with an inference speed of 80.7 FPS and 42.7% mIoU on ADE20K with an inference speed of 143.6 FPS.</p></abstract><kwd-group><kwd>real-time semantic segmentation</kwd><kwd>large kernel convolution</kwd><kwd>noise transfer mechanism</kwd><kwd>position awareness</kwd><kwd>computer vision</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62472149</award-id><award-id>62302155</award-id></award-group><funding-statement>This work was supported by the National Natural Science Foundation of China [grant numbers 62472149, 62302155]. The authors gratefully acknowledge the financial support that made this research possible.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05357"><title>1. Introduction</title><p>Semantic segmentation is one of the core tasks in the field of computer vision, aiming to classify each pixel in an image [<xref rid="B1-sensors-25-05357" ref-type="bibr">1</xref>], thereby enabling the automatic recognition of objects and understanding their spatial distribution within the image. This technology holds significant application value in areas such as autonomous driving [<xref rid="B2-sensors-25-05357" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05357" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05357" ref-type="bibr">4</xref>], robotic navigation [<xref rid="B5-sensors-25-05357" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05357" ref-type="bibr">6</xref>], and intelligent transportation systems [<xref rid="B7-sensors-25-05357" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05357" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05357" ref-type="bibr">9</xref>].</p><p>In traditional semantic segmentation methods, Convolutional Neural Networks (CNNs) [<xref rid="B10-sensors-25-05357" ref-type="bibr">10</xref>] utilize locally receptive convolutional kernels and pooling operations to effectively extract spatial features from images, enabling pixel level predictions through convolutional layers. CNN-based methods have achieved remarkable progress in improving segmentation accuracy. However, due to their high computational requirements, these methods face challenges in real-time applications, particularly on embedded devices. With the introduction of the Transformer [<xref rid="B11-sensors-25-05357" ref-type="bibr">11</xref>] architecture, computational models for semantic segmentation have undergone a revolutionary transformation. Transformers leverage global self-attention mechanisms, overcoming the limitations of traditional CNNs, which focus only on local regions. This allows Transformers to better capture long-range dependencies, thereby enhancing segmentation performance. However, due to the computational complexity of their self-attention mechanisms, Transformers are often more time-consuming than CNNs when processing high-resolution images, resulting in a computational bottleneck.</p><p>In recent years, researchers have proposed various lightweight network architectures [<xref rid="B12-sensors-25-05357" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05357" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05357" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05357" ref-type="bibr">15</xref>]. But while optimizing accuracy and inference speed, we also observe that street scene images in semantic segmentation applications, such as those in Cityscapes [<xref rid="B16-sensors-25-05357" ref-type="bibr">16</xref>], contain a large amount of multi-scale data [<xref rid="B17-sensors-25-05357" ref-type="bibr">17</xref>], including cars, roads, and traffic lights. Existing state-of-the-art methods often struggle with blurry or incomplete recognition in fine regions such as road edges and traffic lights. These issues primarily stem from the following factors:</p><p>(1) Insufficient capture of receptive fields in current methods, limiting their ability to gather extensive contextual information across multiple scales, which negatively impacts performance in complex scenes.</p><p>(2) Inadequate noise-handling mechanisms in existing methods, which can lead to overfitting of neural networks and hinder the generalization of adaptive models.</p><p>To address the issue of insufficient receptive fields, RepLKNet [<xref rid="B18-sensors-25-05357" ref-type="bibr">18</xref>] proposed a novel method utilizing large kernels, enabling the model to achieve a larger effective receptive field. This improves the preservation of details and segmentation performance in complex scenes, effectively handling challenging elements such as trucks and traffic signs in street scenes, thereby reducing category confusion. Similarly, DSNet [<xref rid="B19-sensors-25-05357" ref-type="bibr">19</xref>] extends the large-kernel design concept to Atrous convolution, allowing the network to expand its receptive field and capture richer global information without increasing computational complexity, making it a lightweight alternative to large kernels. By effectively capturing information across different scales [<xref rid="B20-sensors-25-05357" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05357" ref-type="bibr">21</xref>], these approaches help improve the recognition and segmentation accuracy of models in multi-scale scenarios, ensuring better predictions for specific objects at their corresponding scales.</p><p>In terms of noise, PSPNet [<xref rid="B22-sensors-25-05357" ref-type="bibr">22</xref>] improves the segmentation accuracy of objects in complex scenes by capturing contextual information at different scales, but additional noise suppression strategies may be required in noisy scenes. Specifically, existing methods rely on manually setting thresholds to remove pseudo-labeled samples with low confidence [<xref rid="B23-sensors-25-05357" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05357" ref-type="bibr">24</xref>]; however, due to factors such as differences between different domains, the type of image noise, and the distribution of pixels in each category, the manual threshold setting is usually unable to adapt to all scenarios and it is difficult to ensure that the efficient performance can still be maintained in the presence of severe noise interference.</p><p><xref rid="sensors-25-05357-f001" ref-type="fig">Figure 1</xref>a illustrates the complexity of an urban street scene, in which there is a wide variety of elements and segmentation is difficult, especially the accurate segmentation of some detail parts is particularly difficult. The practice of simply removing potentially noisy labels by thresholding may lead to misclassification of the target domain, especially in high-confidence cases. Confusing pseudo-labels are usually found in ambiguous categories or poorly represented sub-categories. For example, as shown in <xref rid="sensors-25-05357-f001" ref-type="fig">Figure 1</xref>b, the &#8220;low wall&#8221; and &#8220;road surface&#8221;, which have little color difference, are prone to confusion. Additionally, fine objects such as &#8220;parking racks&#8221; and &#8220;street lamps&#8221; often exhibit blurred boundaries. These phenomena contribute to the generation of label noise. Simply removing these obfuscated pixels with high noise potential may affect the final segmentation of the model. Therefore, label noise needs to be handled more carefully in order to avoid deleting valid information by mistake and to improve the accuracy of semantic segmentation in the target domain.</p><p>To this end, we designed a core module in LKNTNet&#8212;Location-Aware Large Kernel (LALK) module to better expand the receptive field and capture multi-scale contextual information. First, the LALK module adopts a multi-level cascade structure of large convolutional kernels (with dilation rates of 1 and 3), effectively integrating spatial information at different scales. Meanwhile, a sigmoid gating mechanism is employed to adaptively regulate the feature responses of each branch. This dynamic control enhances flexibility and mitigates the over-smoothing effect caused by uniform context aggregation, thereby enabling more selective receptive field modeling. In addition, LALK incorporates the Multi-Scale Position-Aware Block (MSPA), which encodes semantic features across various receptive fields through atrous convolution branches. This design enhances the model&#8217;s ability to represent small objects and boundary details in complex scenes, significantly improving its sensitivity to edge regions. As a result, the model exhibits more robust performance in scenarios with occlusions, obstructions, and ambiguous boundaries, achieving finer-grained structure recognition. To further improve semantic consistency among features, MSPA innovatively introduces a Modality-Class Token mechanism, constructing a semantic alignment bridge across different modalities. This mechanism aligns features with semantic categories, guiding the network to focus on regions with higher semantic consistency. It effectively reduces classification bias caused by pseudo labels, ambiguous categories, or multi-modal fusion, thereby enhancing semantic discrimination. By fusing the class-level context information with the current features, it guides the aggregation of features to semantic categories, forming a closed-loop optimization link from the underlying features to the high-level semantics. Finally, LALK adopts a fusion design of multi-group convolution and channel attention mechanism. Through a parameter decoupling strategy based on channel grouping, it exponentially increases feature expression diversity while maintaining reduced FLOPs. This approach breaks down inter-group information barriers, building a dual advantage of high computational efficiency and strong representation capability without compromising real-time inference speed, thereby further enhancing the overall performance and generalization ability of the model.</p><p>In addition, this paper proposes an innovative structural module&#8212;Multi-Scale Contextual Noise Transfer (MSCNT) module. Unlike traditional context encoding or refinement blocks, MSCNT introduces a novel context-driven noise suppression and posterior correction paradigm, specifically designed to address the challenges posed by noisy labels and complex backgrounds. First, MSCNT leverages multi-scale pooling and channel dimensionality reduction to construct a consistent contextual representation. This enables the capture of both global and local semantic information across varying receptive fields, enhancing the model&#8217;s structural awareness of target regions. As a result, it effectively separates foreground from background, reduces misclassification rates, and significantly improves reasoning stability, especially in fine-grained target recognition and heavily occluded scenarios. Secondly, the module pioneeringly introduces Noise Transition Matrices (NTM) as a structural output correction mechanism, enabling explicit reconstruction and regulation of the posterior distribution in classification. Unlike fixed-structure attention mechanisms, this approach dynamically adjusts segmentation predictions based on the internal consistency of contextual information. This empowers the model with robust discriminative capability and stability in the face of complex structures, occlusions, background interference, and label noise. Therefore, MSCNT is not merely a feature processing module&#8212;it represents a paradigm breakthrough that integrates context modeling, noise recognition, and adaptive correction into a unified framework. It achieves end-to-end systemic innovation, spanning from feature representation to the final decision-making process at the output layer.</p><p>Building upon the two aforementioned modules, we further construct a lightweight semantic segmentation network&#8212;LKNTNet. This network achieves unprecedented receptive field expansion and fine-grained semantic discrimination, all while maintaining high inference efficiency. It can efficiently extract semantic information without incurring the heavy computational burden associated with bilateral networks. Specifically, we use a CNN architecture based on Transformer semantic information. This method can effectively extract semantic information and avoid the problem of high computational complexity caused by bilateral networks. LKNTNet learns remote context from training only the Transformer semantic branch to the CNN branch. To reduce the semantic gap between Transformer and CNN, a shared decoder head is used before alignment. During training, the CNN can jointly encode semantic information and spatial details using the aligned semantic information. Thus, LKNTNet can align semantic representations from the large effective receptive field of the Transformer structure while maintaining the efficient inference of the lightweight CNN structure. <xref rid="sensors-25-05357-f002" ref-type="fig">Figure 2</xref> visualizes the comparison between LKNTNet and other real-time segmentation methods on Cityscapes val. Specifically, the figure is based on the mIoU results of each comparison model on Cityscapes val, where the horizontal axis represents the inference speed (FPS) of the model, and the vertical axis represents the segmentation accuracy (mIoU). Different colors represent different categories of architectures, intuitively showing the trade-off between accuracy and latency for each method. Our method is shown in red, while other methods are represented in blue. The main contributions of this paper are summarized as follows:</p><p>(1) Location-Aware Large Kernel (LALK) module: We propose the LALK module, which combines multi-level large kernel convolutions with atrous convolutions, and employs a gating mechanism to achieve dynamic fusion of multi-scale features. This effectively expands the receptive field while suppressing excessive context smoothing. Additionally, a Modality-Class Token mechanism is introduced to facilitate semantic alignment, enhancing the model&#8217;s ability to distinguish small objects, boundaries, and ambiguous categories, thereby improving segmentation accuracy and semantic consistency.</p><p>(2) Multi-Scale Contextual Noise Transfer (MSCNT) module: The MSCNT module is designed to integrate multi-scale pooling and channel reduction to construct a consistent contextual representation, enhancing the model&#8217;s ability to distinguish between foreground and background. Meanwhile, a Noise Transition Matrix (NTM) is introduced to structurally reconstruct and dynamically refine the posterior distribution, effectively improving the model&#8217;s discriminative capability and output stability in the presence of noisy labels, occlusions, and complex backgrounds.</p><p>(3) Performance Evaluation: We conducted comprehensive evaluations of LKNTNet on multiple datasets, including Cityscapes, ADE20K [<xref rid="B25-sensors-25-05357" ref-type="bibr">25</xref>], and COCO-Stuff-10K [<xref rid="B26-sensors-25-05357" ref-type="bibr">26</xref>]. The experimental results demonstrate that LKNTNet outperforms existing real-time semantic segmentation methods, achieving a superior balance between accuracy and speed while maintaining real-time performance.</p><p>The remainder of this paper is organized as follows: <xref rid="sec2-sensors-25-05357" ref-type="sec">Section 2</xref> provides an overview of related work on the topic. <xref rid="sec3-sensors-25-05357" ref-type="sec">Section 3</xref> presents a detailed description of the LKNTNet architecture, focusing on the design principles of the large-kernel module and multi-scale Atrous convolutions. <xref rid="sec4-sensors-25-05357" ref-type="sec">Section 4</xref> outlines the experimental design and result analysis conducted on datasets such as Cityscapes, ADE20K, and COCO-Stuff-10K, demonstrating the advantages of LKNTNet in real-time semantic segmentation tasks. Finally, <xref rid="sec5-sensors-25-05357" ref-type="sec">Section 5</xref> concludes with a summary of the findings and contributions of this work.</p></sec><sec id="sec2-sensors-25-05357"><title>2. Materials and Methods</title><p>In this section, we discuss four types of methods most relevant to this work: traditional semantic segmentation methods, real-time semantic segmentation methods, Atrous convolutions and large-kernel convolutions, and noisy label processing with noise transition matrices.</p><sec id="sec2dot1-sensors-25-05357"><title>2.1. Traditional Semantic Segmentation Methods</title><p>Semantic segmentation techniques have made significant progress since the advent of convolutional neural network (CNN) models. Initially, research based on Fully Convolutional Networks (FCN) [<xref rid="B10-sensors-25-05357" ref-type="bibr">10</xref>] paved the way for pixel level segmentation of images. While FCN employed an end-to-end training approach for pixel-level predictions, its simplistic up-sampling design resulted in blurry edges and insufficient segmentation accuracy. To address these issues, UNet [<xref rid="B27-sensors-25-05357" ref-type="bibr">27</xref>] introduced an encoder&#8211;decoder structure combined with jump-connected stepwise up-sampling, which significantly improves the segmentation accuracy, and is especially widely used in medical image segmentation. Enhanced models like SegNet [<xref rid="B28-sensors-25-05357" ref-type="bibr">28</xref>] further optimized this approach by preserving max-pooling indices for precise up-sampling, reducing redundant features, and improving computational efficiency. The DeepLab series [<xref rid="B20-sensors-25-05357" ref-type="bibr">20</xref>] employs Atrous convolutions to expand the receptive field and introduces a fully connected Conditional Random Fields (CRFs) to enhance boundary information, thus improving the segmentation precision. Moreover, PSPNet [<xref rid="B22-sensors-25-05357" ref-type="bibr">22</xref>] integrated multi-scale contextual information through a pyramid pooling module, which further enhances the segmentation of complex scenes.</p><p>DMANet [<xref rid="B29-sensors-25-05357" ref-type="bibr">29</xref>] utilizes multi-scale external attention to process feature maps of different scales and employs a dual-branch cascaded attention mechanism to enable information exchange across branches. FDNet [<xref rid="B30-sensors-25-05357" ref-type="bibr">30</xref>] optimizes spectral feature learning by compressing redundant information while highlighting information bands, and can precisely distinguish and enhance high-frequency and low-frequency features. AMKBANet [<xref rid="B31-sensors-25-05357" ref-type="bibr">31</xref>] deeply explores and engages with multi-core contextual information containing high-frequency boundary information through the application of multi-core spatial attention and boundary-aware hybrid attention. Unimatch v2 [<xref rid="B32-sensors-25-05357" ref-type="bibr">32</xref>] further pushes the performance limits of semi-supervised semantic segmentation through improved matching strategies and consistency constraints, achieving more robust segmentation results in scenarios with limited labeled data. PoCo [<xref rid="B33-sensors-25-05357" ref-type="bibr">33</xref>] proposes a pixel-level metric learning paradigm for semantic segmentation by explicitly exploring the structure of labeled pixels.</p><p>Although CNN-based models have achieved promising results in many applications, they are generally limited by the locality of convolution operations, making it challenging to effectively capture long-range dependencies and global contextual information. To overcome these limitations, Transformers have been introduced into semantic segmentation in recent years as a complement to CNNs. Vision Transformer (ViT) [<xref rid="B11-sensors-25-05357" ref-type="bibr">11</xref>,<xref rid="B34-sensors-25-05357" ref-type="bibr">34</xref>] as one of the first works to bring the Transformer architecture into computer vision. By dividing images into fixed-sized patches and leveraging a global self-attention mechanism, ViT effectively captured long-range dependencies. However, its high computational complexity in processing high-resolution images constrained its practical applicability. To address this limitation, SegFormer [<xref rid="B35-sensors-25-05357" ref-type="bibr">35</xref>] introduced a lightweight Transformer architecture combined with multi-scale feature extraction methods, which improves the efficiency of semantic segmentation. Nevertheless, SegFormer struggles in complex scenarios, such as fine-grained boundaries or small-object segmentation, primarily due to its insufficient ability to model local details, and the fusion strategy of multi-scale features still has limitations in complex texture scenarios.</p></sec><sec id="sec2dot2-sensors-25-05357"><title>2.2. Lightweight Real-Time Semantic Segmentation Methods</title><p>Early research on real-time semantic segmentation primarily focused on the design of lightweight convolutional neural networks (CNNs) to reduce model parameters and computational complexity, thereby accelerating inference speed. Many methods [<xref rid="B36-sensors-25-05357" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05357" ref-type="bibr">37</xref>] have utilized pre-trained lightweight backbones to tackle real-time segmentation tasks. For instance, BiSeNet V2 [<xref rid="B38-sensors-25-05357" ref-type="bibr">38</xref>] employs a dual branch network architecture, extracting shallow detail features and deep semantic features separately, which ensures high inference speed while maintaining segmentation precision. STDC1 [<xref rid="B39-sensors-25-05357" ref-type="bibr">39</xref>] effectively reduces the number of parameters and improves the real-time performance through structured deep convolution and adaptive channel selection mechanism, especially in low-resolution scenes. However, when dealing with complex backgrounds and small targets, the accuracy performance may be inferior to that of some more delicate networks. DDRNet-23 [<xref rid="B5-sensors-25-05357" ref-type="bibr">5</xref>] incorporates residual modules and deformable convolutional modules, allowing for adaptive adjustment of feature map shapes to address variations across different scenes, which improves the robustness of the model. DMPNet [<xref rid="B40-sensors-25-05357" ref-type="bibr">40</xref>] extracts contextual information at different levels and scales in the form of a distributed multi-scale pyramid, enhancing the network&#8217;s representational capability. LCFNet [<xref rid="B41-sensors-25-05357" ref-type="bibr">41</xref>] introduces a compensation branch to design a three-branch structure for optimizing the network. It selectively extracts features from corresponding branches through two lightweight fusion modules, ultimately achieving multi-branch aggregation with relatively low complexity.</p><p>RTFormer [<xref rid="B42-sensors-25-05357" ref-type="bibr">42</xref>] leverages the Transformer architecture to capture global features, demonstrating particular advantages in segmenting objects within complex backgrounds and at long distances. AFFormer [<xref rid="B43-sensors-25-05357" ref-type="bibr">43</xref>] further improves segmentation accuracy by optimizing attention mechanisms, but its high computational overhead limits its suitability for low-latency applications. SeaFormer [<xref rid="B44-sensors-25-05357" ref-type="bibr">44</xref>] combines the strengths of CNN and Transformer architectures, which can better handle global context information. Other hybrid architectures, such as CSRNet [<xref rid="B45-sensors-25-05357" ref-type="bibr">45</xref>] and PIDNet [<xref rid="B46-sensors-25-05357" ref-type="bibr">46</xref>], enhance recognition and segmentation capabilities by optimizing the fusion of contextual information. However, despite these advances, hybrid architectures often introduce extra attention computation and memory access costs. These methods introduce dense fusion modules between the two branches to enhance the semantic information of the extracted features, leading to increased latency in real-time applications. Moreover, the reliance on Transformer branches for global modeling can be constrained under lightweight settings due to channel compression and reduced spatial resolution. Most existing methods also remain confined to shallow integration between CNN and Transformer components, which limits their ability to fully exploit both local and global representations. In summary, all of these bilateral methods have limitations in terms of inference speed and computational cost due to the additional branches and multiple fusion modules.</p></sec><sec id="sec2dot3-sensors-25-05357"><title>2.3. Receptive Field Expansion Techniques: Atrous Convolution and Large Kernel Convolution</title><p>Traditional convolution operations extract local features by sliding fixed size kernels over an image. However, in certain specific tasks, such as global semantic information modeling or capturing long-range dependencies, the fixed receptive field of traditional convolutions can limit the network&#8217;s performance. To overcome the fixed receptive field limitation, researchers have begun exploring convolution operations with dynamic adjustment capabilities. For example, Deformable Convolution Networks (DCN) [<xref rid="B47-sensors-25-05357" ref-type="bibr">47</xref>] achieve dynamic receptive field adjustment by learning the offsets of convolution kernels, enabling adaptation to the shapes and sizes of different objects. ACNet [<xref rid="B48-sensors-25-05357" ref-type="bibr">48</xref>] further combines the advantages of standard convolution and Atrous convolution, which significantly improves the ability to adapt to complex scenarios by aggregating multi-scale information under different expansion rates. By stacking Atrous blocks, ANet [<xref rid="B14-sensors-25-05357" ref-type="bibr">14</xref>] effectively expands the receptive field while maximising the retention of spatial information, thus achieving excellent performance while keeping the number of parameters low.</p><p>With advancements in hardware capabilities, Large Kernel Convolution (LKC) has gained significant attention for its applications in semantic segmentation and object detection tasks. ConvNeXt [<xref rid="B49-sensors-25-05357" ref-type="bibr">49</xref>] introduced convolutional kernels, substantially enhancing the ability to capture global semantic information while simplifying network structures and reducing reliance on complex attention mechanisms. Subsequently, RepLKNet [<xref rid="B18-sensors-25-05357" ref-type="bibr">18</xref>] employed convolutional kernels, demonstrating the potential of convolutional networks in capturing long-range dependency information. Taking this further, ConvFormer [<xref rid="B50-sensors-25-05357" ref-type="bibr">50</xref>] achieves a high balance of efficiency and accuracy on multi-tasks by combining the local feature modeling capability of convolution with the global semantic modeling capability of Transformer.</p><p>These studies collectively demonstrate that both Atrous convolution and appropriately designed large kernel convolution (LKC) can significantly enhance the extraction of contextual information in deep models. However, most existing approaches treat LKC and Atrous convolution as separate techniques, failing to fully exploit their complementary advantages. To address this limitation, we propose a novel method called LALK (Location-Aware Large Kernel Module), which, for the first time, introduces a position-aware Atrous mechanism within large kernel convolution. This allows the network to not only expand the receptive field but also effectively capture multi-scale semantic details. Unlike static large-kernel methods such as ConvNeXt or RepLKNet, LALK integrates convolutions with varying dilation rates, substantially improving the model&#8217;s adaptability to spatial variations and object structures in complex scenes.</p></sec><sec id="sec2dot4-sensors-25-05357"><title>2.4. Noise Label Processing and Noise Transition Matrix</title><p>The problem of learning with noisy labels during training can generally be divided into three classic strategies: label correction [<xref rid="B51-sensors-25-05357" ref-type="bibr">51</xref>], loss correction [<xref rid="B52-sensors-25-05357" ref-type="bibr">52</xref>,<xref rid="B53-sensors-25-05357" ref-type="bibr">53</xref>], and sample reweighting [<xref rid="B54-sensors-25-05357" ref-type="bibr">54</xref>]. However, these methods are primarily designed for image-level supervised classification tasks and cannot be directly applied to semantic segmentation. Since semantic segmentation requires predictions for each pixel, the impact of noise is more complex, especially in boundary regions and low-confidence areas. Therefore, improving semantic segmentation performance in the target domain requires careful handling of label noise.</p><p>Resampling strategies [<xref rid="B55-sensors-25-05357" ref-type="bibr">55</xref>] are another common approach for noise handling, often used to adjust data distribution to mitigate the effects of label noise. However, excessive adjustment through resampling may lead to data distribution bias [<xref rid="B56-sensors-25-05357" ref-type="bibr">56</xref>], causing the model to overfit to specific patterns during training and ultimately affecting generalization capability. In the target domain data, if low-confidence regions are excessively removed, it may result in an insufficient number of samples for certain important categories, thereby affecting the final segmentation performance of the model.</p><p>The Noise Transition Matrix (NTM) can be used to infer clean class posteriors from noisy class posteriors estimated based on noisy data [<xref rid="B57-sensors-25-05357" ref-type="bibr">57</xref>]. It is an essential tool for describing the relationship between clean labels and noisy labels. Early works primarily focused on modeling noisy labels using a known or estimated transition matrix. For example, ref. [<xref rid="B58-sensors-25-05357" ref-type="bibr">58</xref>] proposed a loss correction method to enhance the robustness of deep neural networks against noisy labels. However, this method relies on static loss correction and cannot adopt more flexible dynamic noise estimation or noise matrix learning approaches, which limits its performance in complex noisy scenarios.</p><p>To address the strong dependency of traditional NTM methods on the noise structure, researchers have proposed various optimization strategies. For example, ref. [<xref rid="B59-sensors-25-05357" ref-type="bibr">59</xref>] introduced a method based on maximum likelihood estimation that optimizes the noise matrix directly from the data, reducing the reliance on a prior knowledge. To further enhance the quality of NTM estimation, ref. [<xref rid="B60-sensors-25-05357" ref-type="bibr">60</xref>] proposed a framework that jointly optimizes data representation and the noise matrix. By refining the noise matrix to correct label noise, this approach enables the model to adaptively learn the noise distribution without relying on external noise matrix estimation.</p><p>These studies demonstrate that appropriate noise-handling strategies can significantly improve a network&#8217;s robustness to noisy labels, thereby enhancing segmentation accuracy. To this end, we propose the second original contribution&#8212;the Multi-Scale Contextual Noise Transfer (MSCNT) module. This module organically integrates contextual semantic encoding with noise modeling, enhancing the model&#8217;s robustness in challenging regions such as low-confidence boundaries. Unlike traditional methods based on static Noise Transition Matrices (NTM), MSCNT introduces a context-driven noise modulation mechanism, enabling flexible noise propagation along the spatial dimension. To the best of our knowledge, this is the first attempt to perform structured noise modeling and dynamic correction for semantic segmentation tasks.</p></sec></sec><sec id="sec3-sensors-25-05357"><title>3. Proposed Method</title><p>Removing the semantic branch of dual-branch networks can significantly improve inference speed. However, such simplifications result in shallow single-branch networks lacking sufficient long-range semantic information, leading to a decline in model accuracy. The traditional solution is to restore the accuracy by introducing deeper encoders, more powerful decoders, or complex feature enhancement modules. However, these approaches often substantially increase inference overhead, limiting the speed requirement in practical applications. To tackle this challenge, we propose an improved method to enrich semantic and spatial information without compromising inference speed. Specifically, we introduce LKNTNet, as illustrated in <xref rid="sensors-25-05357-f003" ref-type="fig">Figure 3</xref>. The network is divided into a CNN branch on the left and a training-only Transformer branch on the right, using location-aware big kernel convolution to replace the traditional CNN convolutional layer in the second half of the network, where the LALK Module is the core of the network, which extends the sensory field and captures information at all scales by means of a big convolutional kernel and multiscale context coding. The first two stages are ordinary residual blocks, and the last two stages are location-aware large kernel modules. Then, the features enter the multiscale contextual noise transfer module, where features are extracted using multiscale pooling with different convolutional kernels, and the output in the multiclassification task is adjusted using the transformation matrix, corrected by combining learned posterior distributions, and the inputs are adjusted for convex combinations using the weighting matrix. Finally, the features from network stages 2 and 4 are fed into the decoder, and operations such as Reshape are performed separately from the output of the Transformer decoder, followed by the computation of the alignment loss. The two branches are semantically aligned by the alignment loss in the middle part. In LALK Module, LK Block is the core part.</p><sec id="sec3dot1-sensors-25-05357"><title>3.1. Location-Aware Large Kernel Module</title><p>Traditional CNNs typically rely on small convolutional kernels. While increasing the number of convolutional layers can expand the receptive field (the region of the image that each neuron can &#8220;see&#8221;) to some extent, this stacking approach has inherent limitations: the receptive field expands slowly, and the computational and memory requirements grow significantly with the network depth. Moreover, convolutional layers at specific levels can only capture features on a single scale, which limits their ability to extract multi-scale features. This is particularly problematic in complex scenes with multi-scale objects, where CNNs often fail to capture small objects and fine details. Meanwhile, the convolutional operations build higher-level representations by aggregating local features, making CNNs adept at capturing local information. However, they often fall short in representing globally consistent features. Consequently, for tasks requiring global contextual information, such as semantic segmentation and scene understanding, traditional CNNs may encounter performance bottlenecks.</p><p>The Location-Aware Large Kernel Module (LALK) enhances the expansion of the receptive field by introducing the design of large kernel convolution. This module progressively extends the network&#8217;s receptive field through cascaded large kernels, so that the information is gradually aggregated in a larger spatial range to capture richer contextual features. The specific structure of this module is shown in <xref rid="sensors-25-05357-f004" ref-type="fig">Figure 4</xref>.</p><p>We introduce two Depthwise Convolution (DWConv) layers into the network, each utilizing kernels with different dilation rates. One convolution layer operates with a dilation rate of 1 and a kernel size of 5 &#215; 5 &#215; 5, while the other uses a dilation rate of 3 and a kernel size of 7 &#215; 7 &#215; 7. The chosen dilation rates allow the convolution kernels to achieve a larger receptive field without increasing computational costs. The outputs of these two large-kernel convolution layers are concatenated with the feature maps from the Multi-Scale Position-Aware (MSPA) block to integrate multi-scale information. This process can be expressed as Equation (1):<disp-formula id="FD1-sensors-25-05357"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>3</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>4</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Along the channel dimension, we apply Average Pooling (AVP) and Max Pooling (MAP) operations to the concatenated features to capture global information, respectively. Average Pooling is primarily used to extract smooth global features, while Max Pooling focuses on critical high-intensity features. The pooling process can be formulated as Equation (2):<disp-formula id="FD5-sensors-25-05357"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>AVP</mml:mi><mml:mfenced><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>3</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>4</mml:mn><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mfenced><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>3</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>4</mml:mn><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Next, the above concatenated features are fed into a convolutional layer with a kernel size of 7 &#215; 7 &#215; 7 followed by a sigmoid activation function to generate two dynamic selection coefficients Y1 and Y2. These dynamic selection coefficients are used to perform weighted selection on the features output by specific convolutional layers. The selection mechanism can be expressed as Equation (3):<disp-formula id="FD7-sensors-25-05357"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Using the generated dynamic selection coefficients Y1 and Y2, matrix multiplication operations are performed separately on the output of the convolutional layer with kernel size 5 &#215; 5 &#215; 5 and the output of the convolutional layer with kernel size 7 &#215; 7 &#215; 7. This allows for adaptive selection of features for different large kernels and their calibration. Subsequently, the weighted results of the two are summed and further added to the input features, resulting in a more enriched feature representation, the specific process is shown in Equation (4):<disp-formula id="FD8-sensors-25-05357"><label>(4)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfenced><mml:mrow><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>&#8855;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8853;</mml:mo><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo>&#8855;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>&#8853;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As the kernel size and dilation rate increase layer by layer, the network progressively expands its receptive field while maintaining a low computational cost, thereby capturing richer long-range dependency information. This approach efficiently extends the receptive field, making it particularly suitable for tasks in complex scenarios that require global contextual information.</p><p>In order to better fuse multi-scale information, we introduced the Multi-Scale Position-Aware Block (MSPA) within the large kernel module, as shown in <xref rid="sensors-25-05357-f005" ref-type="fig">Figure 5</xref>, which processes the input features through multi-layer convolution operations with different dilation rates to extract the feature information at different scales. Specifically, The MSPA block takes the input features and performs three convolution operations with different expansion rates as a precursor module for feature extraction, resulting in three intermediate feature maps. These intermediate feature maps have different receptive fields of 5 &#215; 5, 9 &#215; 9, and 13 &#215; 13, respectively, which effectively capture contextual information at different scales. The intermediate feature maps are concatenated along the channel dimension to form the final multi-scale feature representation. This multi-scale information fusion compensates for the limitation of a single-scale receptive field in large kernels, enhancing the richness of feature representation. The specific process can be formulated as Equation (5).<disp-formula id="FD9-sensors-25-05357"><label>(5)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>9</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>9</mml:mn></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>13</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>13</mml:mn></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>9</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>9</mml:mn></mml:mrow><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In semantic segmentation, especially in complex scenes with dense objects or intricate boundaries, features from different categories often overlap or interfere within the same spatial regions. Standard convolution operations may struggle to distinguish these categories, leading to ambiguous feature representations. To address this, we further introduce Modality-Class Tokens (MCT), which serve as the position-aware component. This module is used as a post module to further optimize the feature representation and can provide a unique identifier for each modality class (such as different regions or semantic categories within an image). These identifiers act as a form of inductive bias, helping the model better distinguish and learn the features of different modalities. We initialize the modality class identifier with a truncated cosine function and enable it to inherit gradient properties. Subsequently, the modal class identifiers are dimensionally matched to the feature map via matrix broadcast, thus ensuring that the dimensions of the feature map are aligned with the modal class markers. This combination enhances the feature capture and optimization capabilities of the model while being very adaptable and scalable.</p><p>To further enhance the diversity of feature extraction, we introduce Multi-Group Convolutions (MGCs), which divides the feature map into N independent groups. Each group is processed with different convolutional kernels, operating independently without interaction between groups. On this basis, the inter-group information is fused by pointwise convolution, so that the features between different groups can interact effectively. This design allows the network to learn more diverse feature representations, further improving its feature extraction capability. The output feature map of the multi-group convolution is represented as <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This structure provides finer characterization for each group of features, enhancing the network&#8217;s adaptability to diverse information.</p><p>In the final stage of feature fusion, in order to further enhance the effectiveness of the features, we adopt a channel attention mechanism to focus on high-frequency information. Specifically, the calculation process for the channel attention map <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is as Equation (6).<disp-formula id="FD12-sensors-25-05357"><label>(6)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>&#964;</mml:mi><mml:mfenced><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mfenced><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup><mml:mrow><mml:mi>Y</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi><mml:msup><mml:mo>(</mml:mo><mml:mo>&#8727;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the sigmoid function, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msup><mml:mo>(</mml:mo><mml:mo>&#8727;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the linear transformation, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mo>(</mml:mo><mml:mo>&#8727;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the activation function. Through the channel attention mechanism, the network can automatically focus on key channel features within the image, amplifying the features that contribute to the task. This enhances the overall representational power of the final features.</p><p>In summary, the Location-Aware Large Kernel Module leverages innovative designs such as multi-scale receptive fields, large kernel convolutions, and modality tokens. These enhancements not only expand the receptive field but also improve the expression of multi-scale features and strengthen the network&#8217;s ability to model global information effectively.</p></sec><sec id="sec3dot2-sensors-25-05357"><title>3.2. Multi-Scale Context Noise Transformation</title><p>In traditional decoder designs, feature map processing often relies on fixed convolutional kernels or single-scale feature extraction mechanisms, which pose significant limitations in capturing both local and global information. Specifically, when dealing with images containing complex structures and multi-scale features, single-scale feature extraction struggles to balance fine-grained local details and extensive contextual information, leading to suboptimal performance in handling complex visual scenes. Furthermore, when input images contain noise or background interference, traditional decoders lack effective mechanisms to differentiate between useful features and distracting information, which in turn affects the accuracy and robustness of classification. Additionally, traditional decoders typically lack regularization methods, making them prone to overfitting during training. Especially in the case of small datasets or large data diversity, the generalization ability of the model is poor, and it struggles to adapt to new data distributions. These challenges restrict the effectiveness of traditional decoders in real-world applications.</p><p>To address the aforementioned limitations, we propose the Multi-Scale Contextual Noise Transfer Module (MSCNT), as shown in <xref rid="sensors-25-05357-f006" ref-type="fig">Figure 6</xref>. The MSCNT module leverages multi-scale pooling and a combination of different convolutional kernels to extract multi-scale features, enabling the capture of information across various spatial scales. This design adapts more effectively to features of different resolutions while preserving rich contextual information. The noise transfer mechanism within the MSCNT module adjusts the outputs of multi-class tasks using a specific transformation matrix, thereby refining the learned posterior distributions to improve classification accuracy. Furthermore, in order to enhance the model&#8217;s robustness against noise, we introduce a weighted matrix for convex combination adjustments of input features. This ensures better inter-class differentiation while improving adaptability to complex backgrounds and noisy data, ultimately enhancing the stability of classification predictions.</p><p>To extract features more comprehensively, we adopt a multi-branch structure within the MSCNT module, enabling parallel processing of features across multiple scales. Each branch employs convolutional kernels of different sizes to process feature maps, thereby capturing spatial information at various scales. This multi-scale pyramid structure allows each branch to independently learn spatial information at its respective scale and facilitates feature fusion across different resolutions and depths through cross-channel interactions. Specifically, given an input feature map, each branch generates a multi-scale feature map, with the generation function defined as Equation (7).<disp-formula id="FD13-sensors-25-05357"><label>(7)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mfenced><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the height and width of the i-th convolution kernel, respectively. In this work, the set of convolution kernels used is <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mn>3</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>H</mml:mi><mml:mn>6</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mn>6</mml:mn></mml:mfrac></mml:mstyle><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where each kernel has a different size to extract features at multiple scales.</p><p>After the convolution operation, the multi-scale feature maps are further upsampled to ensure the consistency of the features at different scales. Subsequently, these contextual feature maps are concatenated to generate a demand coefficient <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#947;</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> that represents the overall contextual information. Demand coefficients are generated using Equation (8).<disp-formula id="FD14-sensors-25-05357"><label>(8)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#949;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>&#956;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#949; and &#181; represent the Sigmoid and ReLU functions, respectively, and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mfenced><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the concatenated upsampled pooling features. <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represent 1 &#215; 1 convolution layers. Next, each demand coefficient <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#947;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is multiplied with the contextual features <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to calibrate the context. Finally, the calibrated context is added element-wise to the original feature map. The specific process is described in Equation (9).<disp-formula id="FD15-sensors-25-05357"><label>(9)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mrow><mml:msub><mml:mi>&#947;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8901;</mml:mo></mml:mrow></mml:mstyle><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mfenced><mml:mi>X</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mfenced><mml:mo>&#8901;</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represents the Spatial Pyramid Pooling layer, and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the height (or width) of the output size from the pooling layer. By default, n is set to [<xref rid="B1-sensors-25-05357" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05357" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05357" ref-type="bibr">3</xref>,<xref rid="B6-sensors-25-05357" ref-type="bibr">6</xref>].</p><p>In addition, to further enhance the model&#8217;s ability to distinguish noisy data, we introduced a k-NN-based noisy label verification mechanism in the MSCNT module, as shown in <xref rid="sensors-25-05357-f007" ref-type="fig">Figure 7</xref>. This mechanism comparing the feature representations of noisy labels with their k-NNs to discriminate pixels with corrupted labels ensures discrimination between categories and also improves the robustness of classification predictions. It looks at the k nearest neighbors of the target feature in the dataset and uses their class information to determine the category of the new feature. Since we lack prior knowledge about whether the labels are clean, we set k to 2 for voting and use the majority class as the ground truth semantics. A similar idea has already been mentioned in [<xref rid="B61-sensors-25-05357" ref-type="bibr">61</xref>]. Suppose the input noisy label is <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, and its nearest and second-nearest neighbors among the k-NN are labeled as <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. If these two labels are inconsistent, it may indicate that the target pixel&#8217;s label is incorrect. Due to k-NN clustering, their ground truth labels are equal, i.e., <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>. For this purpose, we define <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, and then the probability equation as in Equation (10) always holds.<disp-formula id="FD16-sensors-25-05357"><label>(10)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mfenced close="" open="|"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>When this condition is satisfied, the target label is considered reliable; otherwise, it is marked as a noisy label. Further, we can utilize the second-order consistency equation as in Equation (11).<disp-formula id="FD17-sensors-25-05357"><label>(11)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced close="" open="|"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:munderover><mml:mrow><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced close="" open="|"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced close="" open="|"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Subsequently, the k-NN corresponding to the noisy label <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained. Specifically, the feature representation extracted by the backbone network is used to compute the cosine similarity to confirm the neighbor labels <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>&#8764;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>&#8764;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, the expected mean of a set of sampled instances R from the noisy dataset is used to estimate each higher-order consistency. The specific process is described in Equation (12).<disp-formula id="FD18-sensors-25-05357"><label>(12)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>&#951;</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>S</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mrow><mml:mi>&#958;</mml:mi><mml:mfenced close="}" open="{"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>&#951;</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>S</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mrow><mml:mi>&#958;</mml:mi><mml:mfenced close="}" open="{"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msubsup><mml:mi>&#951;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>S</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mrow><mml:mi>&#958;</mml:mi><mml:mfenced close="}" open="{"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>&#8764;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#958;</mml:mi><mml:msup><mml:mo>{</mml:mo><mml:mo>&#8727;</mml:mo></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the indicator function, which equals 1 if the condition is satisfied and 0 otherwise. S is randomly sampled following a uniform distribution, and performs consistency calculations multiple times, while |S| represents the sample size.</p><p>The Noise Transfer Matrix (NTM) corrects model predictions by explicitly modeling the noise distribution of labels in the training data. Specifically, NTM can smooth the model&#8217;s predictions on noisy labels, preventing the model from directly fitting incorrect labels, thereby effectively alleviating overfitting issues. In this way, NTM enables the model to consider the impact of label noise during the training phase, thereby learning more robust feature distributions that are closer to the true labels. This not only improves the model&#8217;s stability on noisy data but also significantly enhances its generalization performance on clean validation data.</p></sec><sec id="sec3dot3-sensors-25-05357"><title>3.3. Alignment Loss</title><p>To achieve effective semantic alignment between the Transformer branch and the CNN branch, we propose a specialized alignment loss function. This alignment loss focuses on preserving the consistency of semantic information, thereby enhancing the expressive capability of the model. Specifically, we employ a Channel-Wise Distillation Loss (CWDL) [<xref rid="B62-sensors-25-05357" ref-type="bibr">62</xref>] to achieve the alignment effect. The core of CWDL lies in unifying the channel dimensions of the features, enabling features from the Transformer branch and the CNN branch to be aligned and compared within the same feature space. This facilitates the sharing and calibration of semantic information across different network structures.</p><p>Before calculating the alignment loss, it is necessary to preprocess the features from the Transformer and CNN branches to ensure consistency in spatial resolution and channel dimensions. The specific operations include up-sampling and down-sampling. At the initial stage, the features from the Transformer branch generally have a higher density of semantic information but lower spatial resolution. In contrast, the features from the CNN branch retain more spatial detail but exhibit a lower level of semantic abstraction. Therefore, we first perform an up-sampling operation on the Transformer branch features to increase their spatial resolution, aligning them with the spatial scale of the CNN branch features. Similarly, for the CNN branch features, we apply down-sampling to reduce spatial information, making them better aligned with the high-level semantic features of the Transformer branch.</p><p>To preserve the original information of the CNN branch during feature fusion, we designed a feature projection module that projects the features of the CNN branch onto the dimensions of the Transformer branch to achieve channel alignment. Specifically, the representative features from the second and fourth stages of the CNN branch are selected, and these features are processed through pointwise convolution to expand their channel dimensions, ensuring consistency between the feature dimensions of the CNN and Transformer branches. Pointwise convolution not only adjusts the number of feature channels but also aligns the features while maintaining their content, thereby avoiding the loss of detail in the CNN branch features during the dimensional transformation process.</p><p>After completing the alignment of spatial and channel dimensions, the features from the CNN branch are semantically aligned with those from the Transformer branch. The high-dimensional features of the CNN branch are fed into the Transformer decoder to further enhance the semantic representation of the features and facilitate feature fusion in the high-dimensional feature space. During this process, the semantic alignment loss is used to measure the feature discrepancy between the Transformer branch and the CNN branch. Finally, semantic alignment loss is applied to the projected features to align their semantic representations. The aligned features are integrated into the Transformer decoder, participating in the decoding process and contributing to the final segmentation prediction. This design ensures that the advantages of alignment are not limited to intermediate representation learning, but also directly improve the quality of the model&#8217;s final output.</p><p>Specifically, the core design of the semantic alignment loss function focuses on aligning the feature maps generated by the two network architectures. Assuming the number of channels is <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the spatial dimensions are <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8901;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the feature maps from the Transformer branch and the CNN branch are denoted as <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>&#948;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. The t denotes a hyper-parameter called temperature. And the larger t is, the softer the probability distribution is. Then the alignment loss is calculated as shown in Equation (13):<disp-formula id="FD21-sensors-25-05357"><label>(13)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>&#968;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mfenced><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8901;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi><mml:mfenced><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>w</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>C</mml:mi></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8901;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>&#968;</mml:mi><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>&#8901;</mml:mo><mml:mi>log</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>&#968;</mml:mi><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>&#968;</mml:mi><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mi>&#948;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To further enhance the effectiveness of the alignment loss, we adopt different feature fusion strategies at various training stages. During the early training phase, the model&#8217;s ability to capture local features is still underdeveloped. Therefore, we primarily focus on aligning low-level features, using feedback from the loss function to guide the consistency between the CNN and Transformer branches in local feature representation. In the later training phase, as the semantic information becomes richer and the understanding of global context deepens, we gradually shift attention to aligning high-level semantic features to improve the model&#8217;s ability to capture global information. This progressive feature fusion strategy better aligns with the model&#8217;s learning process, ensuring that the alignment loss effectively optimizes the model at different stages of training.</p></sec><sec id="sec3dot4-sensors-25-05357"><title>3.4. Decoder Head</title><p>The decoder head is composed of a DAPPM module [<xref rid="B5-sensors-25-05357" ref-type="bibr">5</xref>] and a segmentation head. To further enhance contextual information, we introduce an additional DAPPM after the output of MSCNT module. The resulting feature map is then concatenated with the feature map from Stage 2. The fused feature is subsequently fed into the segmentation head, which consists of a 3 &#215; 3 convolution followed by batch normalization and a ReLU activation, and finally a 1 &#215; 1 convolution is applied for pixel-wise classification.</p></sec></sec><sec id="sec4-sensors-25-05357"><title>4. Experiments and Discussion</title><p>In this section, we first introduce the evaluation setup, including the datasets and methods used for comparison. Then, we compare the algorithm with state-of-the-art semantic segmentation methods on challenging benchmark datasets such as Cityscapes, ADE20K, and COCO-Stuff-10K.</p><sec id="sec4dot1-sensors-25-05357"><title>4.1. Datasets</title><p>(1) Cityscapes: This dataset is a high-quality urban street view dataset provided by Germany, consisting of 5000 finely annotated images and 20,000 coarsely annotated images, covering more than 50 European cities. The images have a resolution of 2048 &#215; 1024 pixels and include detailed pixel level annotations categorized into 30 classes, of which 19 are used for semantic segmentation tasks. These classes represent common urban scene elements, such as roads, pedestrians, vehicles, buildings, and trees. For training, we set the initial learning rate to 0.004 and a weight decay of 0.0125, using the AdamW [<xref rid="B63-sensors-25-05357" ref-type="bibr">63</xref>] optimizer. Specifically, we employ a polygon learning strategy with a power of 0.9 and implement a data expansion method that includes random cropping, random scaling, and random horizontal flipping. Random cropping of 1024 &#215; 1024 and scaling in the range of 0.5 to 2.0 were used. The model was trained over 160 k iterations with a batch size of 16.</p><p>(2) ADE20K: This dataset, provided by MIT, contains over 25,000 images of various scenes and scales, covering a wide range of environments, including indoor and outdoor settings. The image resolutions vary, but the average size is approximately 512 &#215; 512 pixels. ADE20K offers detailed pixel-level annotations spanning 150 categories, including buildings, roads, furniture, plants, animals, and more. We set the initial learning rate to 0.0005, with a random crop size of 512 &#215; 512, and applied scaling in the range of 0.5 to 2.0. The model was trained for 160k iterations with a batch size of 32. All other training details were the same as those used for the Cityscapes dataset.</p><p>(3) COCO-Stuff-10K: This dataset is an extended version of the COCO dataset, comprising 10,000 images that cover a wide range of scenes and object categories. Each image includes pixel-level annotations for 172 classes, which are divided into &#8220;stuff&#8221; categories (e.g., grass, sky, water) and &#8220;thing&#8221; categories (e.g., people, vehicles, furniture). The dataset provides detailed segmentation masks for each scene, annotating not only objects but also background and environmental elements. We set the initial learning rate to 0.01 and the weight decay to 0.00006, using the AdamW optimizer for training. Random cropping with a size of 640 &#215; 640 was applied, along with random scaling in the range of 0.5 to 2.0. All other training details were the same as those used for the Cityscapes dataset.</p></sec><sec id="sec4dot2-sensors-25-05357"><title>4.2. Evaluation Metrics</title><p>To evaluate the semantic segmentation performance of the data, we primarily use the mean Intersection over Union (mIoU) as the evaluation metric. mIoU is a widely used evaluation metric in semantic segmentation tasks. It quantifies the degree of overla between the predicted segmentation and the ground truth segmentation by computing the ratio of the intersection to the union of the two sets of pixels. The mIoU score is obtained by averaging the IoU scores across all classes in the dataset. It provides a measure of the overall accuracy of the segmentation model. In addition, the inference speed is reported in terms of frames per second (FPS), and the complexity of the model is evaluated based on the number of parameters (Params) and giga floating-point operations (GFLOPs). In this context, mIoU can effectively enable a fair comparison between the proposed LKNTNet and other state-of-the-art methods. The formula for this evaluation indicator is shown in Equation (14).<disp-formula id="FD23-sensors-25-05357"><label>(14)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mi>n</mml:mi><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the number of true positives, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of false positives, and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of false negatives, all of which are specifically related to each foreground category indexed by n.</p></sec><sec id="sec4dot3-sensors-25-05357"><title>4.3. Implementation Details</title><p>All the experiments in this study were conducted using the PyTorch framework (version 1.12.1) and Python 3.8.10. The code was trained and tested on a server equipped with a single NVIDIA GeForce RTX 4090 GPU (16 GB memory) (Nvidia, Santa Clara, CA, USA) and running the Ubuntu 20.04 LTS operating system, using the PyCharm IDE (Professional Edition 2023.1). Additional dependencies include PyTorch 1.11.0, TorchVision 0.12.1, and mmcv-full 1.6.0.</p></sec><sec id="sec4dot4-sensors-25-05357"><title>4.4. Performance Comparison</title><p>In this study, we conducted a comparative analysis of the method against several representative and state-of-the-art semantic segmentation approaches.</p><p>(1) Performance Comparison on the Cityscapes Dataset: Comparison of the synthesis and classification of LKNTNet with other methods is shown in <xref rid="sensors-25-05357-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-05357-t002" ref-type="table">Table 2</xref>. As shown in <xref rid="sensors-25-05357-t001" ref-type="table">Table 1</xref>, proposed LKNTNet achieves the best results in terms of mIoU, while also demonstrating the optimal speed accuracy trade-off in TensorRT and Torch implementations. We achieve a state-of-the-art real-time segmentation performance with 80.05% mIoU at 80.7 FPS. Compared to the second-best method, LKNTNet improves mIoU by 0.58%. Recent works, such as BiSeNet V2, utilize a dual-branch structure comprising a spatial branch and a context branch with lightweight convolutional operations. This approach enhances computational efficiency while maintaining segmentation accuracy. DDRNet employs dynamic residual modules that automatically adjust residual connections based on input features, significantly improving global feature extraction, particularly in multi-scale feature fusion and local detail processing. Transformer-based segmentation methods like RTFormer focus on global information extraction, effectively capturing long-range dependencies, making them particularly suited for complex scene segmentation tasks. AFFormer combines convolutional and Transformer, introducing an adaptive attention mechanism to fuse local and global information, which is advantageous in scenarios with ambiguous boundaries. PIDNet adopts staged feature fusion and depthwise separable convolution modules, optimizing the network&#8217;s performance in multi-scale feature handling. LCFNet incorporates a local context feature fusion mechanism along with a global context module, balancing global and local information capture to enhance segmentation performance. DSNet leverages dual-scale feature fusion and adaptive convolution modules to improve segmentation precision for fine details, particularly excelling in boundary handling.</p><p>To provide an intuitive comparison of the segmentation performance of different algorithms, we present the segmentation results on the Cityscapes dataset in <xref rid="sensors-25-05357-f008" ref-type="fig">Figure 8</xref>. LKNTNet demonstrates outstanding performance in handling texture details and boundaries, especially in areas with complex boundaries such as streetlights, poles, and road surfaces, achieving clearer and more accurate segmentation. This showcases its exceptional separation capability. Specifically, the LALK module in LKNTNet utilizes larger convolution kernels to provide the network with a broader receptive field, while the Atrous convolutions in the MSPA module capture multi-scale contextual information, effectively enhancing the network&#8217;s modeling capacity. Moreover, the MSCNT module extracts multi-scale features and employs a noise transition mechanism to distinguish between different categories, producing more complete and well-defined segmentation results. It can be seen that in the labeled region, LKNTNet has a better recognition effect with more complete and neat boundaries.</p><p>(2) Performance Comparison on ADE20K Dataset: The experimental results on the ADE20K dataset further validate the outstanding performance of LKNTNet, similar to the results on the Cityscapes dataset. As shown in <xref rid="sensors-25-05357-t003" ref-type="table">Table 3</xref>, LKNTNet achieves the best accuracy at the fastest speed, reaching an mIoU of 42.7% at 143.6 FPS. Considering the diverse range of images and semantic categories in ADE20K, this remarkable result further demonstrates the generalization capability of LKNTNet.</p><p>(3) Performance Comparison on the COCO-Stuff-10K Dataset: As shown in <xref rid="sensors-25-05357-t004" ref-type="table">Table 4</xref>, LKNTNet maintains the highest inference speed among real-time semantic segmentation methods on the COCO-Stuff-10K dataset. With an input size of 640 &#215; 640, LKNTNet achieves an mIoU of 35.4% at 143.6 FPS.</p></sec><sec id="sec4dot5-sensors-25-05357"><title>4.5. Ablation Study</title><sec id="sec4dot5dot1-sensors-25-05357"><title>4.5.1. Validate the Effectiveness of Different Modules</title><p>The proposed LKNTNet comprises three core components: the Location-Aware Large Kernel Convolution (LALK), the Multi-Scale Position-Aware Module (MSPA), and the Multi-Scale Contextual Noise Transfer Module (MSCNT). To validate the effectiveness of each component, we designed a series of ablation experiments by systematically removing or adding specific components for analysis. As shown in <xref rid="sensors-25-05357-t005" ref-type="table">Table 5</xref>, we conducted four ablation experiments to systematically evaluate the contribution of each module. In the first experiment, the complete LALK module was retained while the MSCNT module was removed. In the second experiment, the LALK module was replaced with a traditional CNN branch while retaining the MSCNT module to assess its impact on segmentation accuracy. The third experiment added the MSCNT module and removed the MSPA module based on the first experiment to further evaluate the contribution of the MSPA module to segmentation performance. As illustrated by the results in <xref rid="sensors-25-05357-t005" ref-type="table">Table 5</xref>, each core component is critical for enhancing LKNTNet&#8217;s performance. Specifically, the LALK module significantly improves the receptive field while capturing multi-scale semantic information. The MSCNT module enhances classification accuracy and reduces overfitting.</p><p>To further verify the improvements brought by NTM alone and the differences in improvements after integrating it with the MSCE module, we conducted additional ablation experiments. To ensure consistency in experimental conditions, we retained the complete LALK and MSPA modules as the baseline architecture. In the first experiment, we kept only the MSCE module while removing the NTM module to evaluate the independent performance of the MSCE module. In the second experiment, we retained only the NTM module while removing the MSCE module to assess the impact of the NTM module alone. Through these experiments, we were able to clearly compare the contributions of NTM and MSCE to segmentation accuracy.</p><p>From the results in <xref rid="sensors-25-05357-t006" ref-type="table">Table 6</xref>, it can be seen that removing either the MSCE module or the NTM module alone leads to a decrease in segmentation accuracy, indicating that both play complementary roles within the MSCNT module. Notably, compared to Experiment 4 in <xref rid="sensors-25-05357-t005" ref-type="table">Table 5</xref>, the synergy between the MSCE and NTM modules significantly improves segmentation performance. At the same time, we observed that although the gradual introduction of various modules into the network inevitably leads to a certain degree of decline in inference speed, this overhead is completely acceptable in comparison to the significant improvement in segmentation accuracy. In other words, while these modules increase the computational load, they significantly enhance the model&#8217;s ability to characterize complex scenes and fine-grained targets, thereby achieving a good overall performance in terms of accuracy&#8211;efficiency balance. The combination of NTM and MSCE not only enhances the model&#8217;s robustness to noise but also improves its overall stability. Specifically, the NTM module plays a crucial role in denoising and adaptive noise adjustment, while the MSCE module further optimizes segmentation results through multi-scale contextual information capture and noise transition. To more intuitively demonstrate the effectiveness of the proposed MSCNT module in suppressing prediction noise, we have added a visual comparison of the segmentation results before and after NTM correction in <xref rid="sensors-25-05357-f009" ref-type="fig">Figure 9</xref>. These visual results clearly reflect the role of this module in reducing prediction noise and optimizing boundary details.</p><p>To gain a deeper understanding of the independent contribution of the NTM module, we conducted additional experiments to explore its performance under different noise levels. Specifically, the low-noise setting was defined as adding Gaussian noise with a standard deviation of 0.01, while the high-noise setting used a standard deviation of 0.05. These values were selected to simulate mild and severe noise interference in practical scenarios. We evaluated the NTM module in both environments, and as shown in <xref rid="sensors-25-05357-t007" ref-type="table">Table 7</xref>, the results demonstrate that the NTM module effectively suppresses noise interference under high-noise conditions, significantly improving segmentation accuracy.</p><p>Combining the above experimental results, we can conclude that the NTM module and the MSCE module complement each other and jointly improve the performance of semantic segmentation. The organic combination of the two enables the MSCNT module to show stronger robustness and accuracy in complex noise environments.</p><p>To more intuitively demonstrate the advantages of DWConv in this model, we replaced it with the conventional standard convolution (Conv) for comparative experiments. This allows us to more clearly observe the differences between the two in terms of model performance. To ensure the consistency of experimental conditions, we kept all other modules and the underlying architecture unchanged, ensuring the accuracy and comparability of the results.</p><p>As shown in <xref rid="sensors-25-05357-t008" ref-type="table">Table 8</xref>, using DWConv can improve mIoU, making the model more efficient. Additionally, this optimization method preserves the feature representation capability while enhancing the model&#8217;s applicability in realtime scenarios, further validating its advantages in semantic segmentation tasks.</p></sec><sec id="sec4dot5dot2-sensors-25-05357"><title>4.5.2. Ablation Study on Semantic Alignment Loss</title><p>We conducted a systematic experimental analysis on the types, application positions, and weight configurations of semantic alignment losses. The proposed semantic alignment loss is designed to guide CNNs to more effectively learn long-range semantic information from Transformers. Therefore, loss functions that can effectively supervise contextual information learning are more suitable for use as alignment losses.</p><p>We selected Kullback&#8211;Leibler Divergence Loss (KL Loss) [<xref rid="B64-sensors-25-05357" ref-type="bibr">64</xref>], Mutual Information Loss (MI Loss) [<xref rid="B65-sensors-25-05357" ref-type="bibr">65</xref>], L2 Norm Loss (L2 Loss) [<xref rid="B66-sensors-25-05357" ref-type="bibr">66</xref>], and CWD Loss for these experiments. From the results in <xref rid="sensors-25-05357-t009" ref-type="table">Table 9</xref>, it can be seen that CWD Loss has a significant positive impact on model performance, whereas L2 Loss performs worse than the Baseline. Meanwhile, KL Loss exhibits almost the same performance as the Baseline before calibration. This may be because the advantage of CWD Loss lies in its adaptive alignment of channel distributions. Compared to L2 and KL Losses, which align feature maps in a per-element manner, CWD Loss transforms feature activations into channel-wise probability distributions, allowing for better alignment of overall channel-level information and capturing richer global semantic context. This approach not only avoids the information loss caused by the rigid alignment (i.e., hard alignment) of L2 Loss but also overcomes the alignment bias that KL Loss may introduce in an uncalibrated state. Additionally, CWD Loss maintains robustness across different scales of feature spaces, enabling long-range information from the Transformer to be more naturally transferred to the CNN branch, thereby enhancing the overall alignment effectiveness of the model.</p><p>Furthermore, the MI loss uses the variational estimation technique from the VID method [<xref rid="B67-sensors-25-05357" ref-type="bibr">67</xref>] to measure the correlation between the channel distributions, which performs similarly to the CWD loss. The commonality between the two lies in aligning channel distributions rather than local feature points, indicating that in LKNTNet, modeling information at the channel level is more critical than the specific form of similarity measurement. This is because the goal of LKNTNet is to align long-range contextual information in semantic branches, which is mainly reflected in the overall channel distribution rather than in local pixels or regions. Therefore, compared to traditional loss functions, CWD Loss can more effectively leverage global channel information, facilitating more efficient cross-modal feature alignment.</p><p>Additionally, <xref rid="sensors-25-05357-t010" ref-type="table">Table 10</xref> analyzes the impact of the application position of semantic alignment loss on performance. The experimental results indicate that applying semantic alignment loss to the second and fourth layers of features, the decoder, and the output logits achieves the optimal mIoU performance configuration. Notably, the application position of the alignment loss has no significant impact on inference FPS.</p><p><xref rid="sensors-25-05357-t011" ref-type="table">Table 11</xref> presents the experimental results on the weights of alignment loss. For output logits, a weight of 3 was identified as the optimal setting [<xref rid="B67-sensors-25-05357" ref-type="bibr">67</xref>]. For feature alignment, a weight of 15 yielded the best performance. Smaller weights fail to fully exploit the high-quality semantic information provided by the Transformer, while larger weights conflict with the CE loss, degrading model performance. Moreover, when the weight is excessively large (e.g., 30), the training loss fluctuates significantly during the process, leading to a substantial drop in model accuracy.</p></sec><sec id="sec4dot5dot3-sensors-25-05357"><title>4.5.3. Ablation Study on Convolution Kernel Size</title><p>To further analyze the impact of convolution kernel size on performance in the proposed module, we conducted ablation experiments on different convolution kernel configurations. <xref rid="sensors-25-05357-t012" ref-type="table">Table 12</xref> shows the experimental results for different convolution kernel sizes. As shown in the table, when the convolution kernel size is set to <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn><mml:mo>+</mml:mo><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the model achieves the highest mIoU on the benchmark dataset, improving by 0.8% compared to the <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn><mml:mo>+</mml:mo><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> kernel, but simultaneously introducing approximately 0.5% reduction in inference speed. Compared to the <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn><mml:mo>+</mml:mo><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> kernel, it improved by 1%, also increased inference speed by approximately 3%. When the convolution kernel size is further increased, performance improvements tend to plateau, while incurring unnecessary computational costs. Therefore, in all experiments in this paper, the convolution kernel size is fixed at <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn><mml:mo>+</mml:mo><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to achieve the optimal balance between accuracy and efficiency. It is worth noting that, despite being slightly slower in speed, the significant improvement in accuracy demonstrates the effectiveness of large receptive fields in modeling contextual relationships.</p></sec></sec><sec id="sec4dot6-sensors-25-05357"><title>4.6. Model Complexity Analysis</title><p>We evaluate the computational complexity of the proposed LKNTNet using the following metrics: the number of model parameters (Params) and frames per second (FPS). Params assesses the memory requirements, while FPS measures the execution speed of the model. Ideally, an efficient model should exhibit a smaller Params value while maintaining a higher FPS value.</p><p><xref rid="sensors-25-05357-t001" ref-type="table">Table 1</xref> demonstrates that LKNTNet achieved a segmentation accuracy of 80.05% mIoU, the best among all methods. This excellent performance is attributed to the fact that LKNTNet introduces a large kernel convolution module for extending the receptive field and uses a noise transfer mechanism to improve the model&#8217;s generalization ability, which results in accurate capture of contextual information. These designs provide the model with powerful feature representation and computational efficiency for processing complex scene data. In addition, the proposed LKNTNet exhibits lower Params than PIDNet and STDC2 and better performance than BMSeNet and DSNet in terms of FPS. LKNTNet embodies high computational efficiency and accuracy balancing ability, demonstrating the superiority of realizing high-precision real-time semantic segmentation under the condition of keeping low computational complexity.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05357"><title>5. Conclusions</title><p>This paper proposes a novel dual-branch network, LKNTNet, which expands the receptive field through multi-scale perception to enhance both global and local information modeling capabilities. Specifically, we integrate large kernel convolutions and Atrous convolutions to effectively expand the receptive field at different scales, thus capturing richer contextual information. Additionally, we estimate the NTM and correct the supervision signals using high-order consistency information from neighbor representations, eliminating the need for precisely defined anchors. Experimental results on multiple benchmark datasets demonstrate that LKNTNet achieves superior performance in both accuracy and computational efficiency. For example, on the Cityscapes dataset, LKNTNet achieves 80.05% mIoU; on the ADE20K dataset, it reaches 42.7% mIoU, outperforming several state-of-the-art methods; and on the COCO-Stuff-10K dataset, it also performs exceptionally well, achieving 35.4% mIoU. These results validate the effectiveness of this approach in complex scenes while highlighting the potential of LKNTNet for efficient semantic segmentation tasks.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors are thankful to the providers for all the datasets used in this study. They are also thankful to the anonymous reviewers and editors for their comments to improve this paper.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.L. and Y.D.; methodology, J.W.; validation, Y.D.; writing-original draft preparation, J.L. and Y.D.; writing-review and editing, J.W. and X.T.; visualization, Y.D. and J.W.; supervision, J.L.: project administration, J.W.: funding acquisition, J.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>This study uses publicly available datasets to support its findings. The Cityscapes dataset is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.cityscapes-dataset.com">https://www.cityscapes-dataset.com</uri> (accessed on 1 October 2023), the ADE20K dataset can be accessed at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://groups.csail.mit.edu/vision/datasets/ADE20K/">https://groups.csail.mit.edu/vision/datasets/ADE20K/</uri> (accessed on 1 October 2023), and the COCO-Stuff 10K dataset is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/nightrome/cocostuff10k">https://github.com/nightrome/cocostuff10k</uri> (accessed on 1 October 2023). All datasets are freely accessible for academic research under their respective licenses.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest. All authors affirm that there are no personal or financial relationships that could be perceived as influencing the research presented in this manuscript.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">LKNTNet</td><td align="left" valign="middle" rowspan="1" colspan="1">Large Kernel CNN Network Including a Noise Transfer Mechanism</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LALK</td><td align="left" valign="middle" rowspan="1" colspan="1">Location-Aware Large Kernel</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MSPA</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-Scale Position-Aware Block</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MSCNT</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-Scale Contextual Noise Transfer</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NTM</td><td align="left" valign="middle" rowspan="1" colspan="1">Noise Transition Matrix</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05357"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jing</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name></person-group><article-title>Coarse-to-fine semantic segmentation from image-level labels</article-title><source>IEEE Trans. Image Process.</source><year>2019</year><volume>29</volume><fpage>225</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1109/TIP.2019.2926748</pub-id><pub-id pub-id-type="pmid">31329556</pub-id></element-citation></ref><ref id="B2-sensors-25-05357"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Haase-Sch&#252;tz</surname><given-names>C.</given-names></name><name name-style="western"><surname>Rosenbaum</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hertlein</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gl&#228;ser</surname><given-names>C.</given-names></name><name name-style="western"><surname>Timm</surname><given-names>F.</given-names></name></person-group><article-title>Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2020</year><volume>22</volume><fpage>1341</fpage><lpage>1360</lpage><pub-id pub-id-type="doi">10.1109/TITS.2020.2972974</pub-id></element-citation></ref><ref id="B3-sensors-25-05357"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>A.</given-names></name><name name-style="western"><surname>Weng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title>Multi-scale fusion with matching attention model: A novel decoding network cooperated with NAS for real-time semantic segmentation</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2021</year><volume>23</volume><fpage>12622</fpage><lpage>12632</lpage><pub-id pub-id-type="doi">10.1109/TITS.2021.3115705</pub-id></element-citation></ref><ref id="B4-sensors-25-05357"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>L.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.</given-names></name></person-group><article-title>MFNet: Multi-feature fusion network for real-time semantic segmentation in road scenes</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>20991</fpage><lpage>21003</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3182311</pub-id></element-citation></ref><ref id="B5-sensors-25-05357"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Y.</given-names></name></person-group><article-title>Deep dual-resolution networks for real-time and accurate semantic segmentation of traffic scenes</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>24</volume><fpage>3448</fpage><lpage>3460</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3228042</pub-id></element-citation></ref><ref id="B6-sensors-25-05357"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bartolozzi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.H.</given-names></name><name name-style="western"><surname>Nawrocki</surname><given-names>R.A.</given-names></name></person-group><article-title>Neuromorphic electronics for robotic perception, navigation and control: A survey</article-title><source>Eng. Appl. Artif. Intell.</source><year>2023</year><volume>126</volume><fpage>106838</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.106838</pub-id></element-citation></ref><ref id="B7-sensors-25-05357"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ling</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name></person-group><article-title>Center-point-pair detection and context-aware re-identification for end-to-end multi-object tracking</article-title><source>Neurocomputing</source><year>2023</year><volume>524</volume><fpage>17</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2022.11.094</pub-id></element-citation></ref><ref id="B8-sensors-25-05357"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Braz&#225;lez</surname><given-names>E.</given-names></name><name name-style="western"><surname>Maci&#224;</surname><given-names>H.</given-names></name><name name-style="western"><surname>D&#237;az</surname><given-names>G.</given-names></name><name name-style="western"><surname>Valero</surname><given-names>V.</given-names></name><name name-style="western"><surname>Boubeta-Puig</surname><given-names>J.</given-names></name></person-group><article-title>PITS: An intelligent transportation system in pandemic times</article-title><source>Eng. Appl. Artif. Intell.</source><year>2022</year><volume>114</volume><fpage>105154</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2022.105154</pub-id><pub-id pub-id-type="pmid">35821739</pub-id><pub-id pub-id-type="pmcid">PMC9264037</pub-id></element-citation></ref><ref id="B9-sensors-25-05357"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sood</surname><given-names>S.K.</given-names></name></person-group><article-title>A scientometric analysis of quantum driven innovations in intelligent transportation systems</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>138</volume><fpage>109258</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.109258</pub-id></element-citation></ref><ref id="B10-sensors-25-05357"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shelhamer</surname><given-names>E.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully convolutional networks for semantic segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="B11-sensors-25-05357"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>30</volume><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="B12-sensors-25-05357"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Du</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>T.</given-names></name></person-group><article-title>LAANet: Lightweight attention-guided asymmetric network for real-time semantic segmentation</article-title><source>Neural Comput. Appl.</source><year>2022</year><volume>34</volume><fpage>3573</fpage><lpage>3587</lpage><pub-id pub-id-type="doi">10.1007/s00521-022-06932-z</pub-id></element-citation></ref><ref id="B13-sensors-25-05357"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name></person-group><article-title>TSD-CAM: Transformer-based self distillation with CAM similarity for weakly supervised semantic segmentation</article-title><source>J. Electron. Imaging</source><year>2024</year><volume>33</volume><fpage>023029</fpage><pub-id pub-id-type="doi">10.1117/1.JEI.33.2.023029</pub-id></element-citation></ref><ref id="B14-sensors-25-05357"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>J.</given-names></name></person-group><article-title>A lightweight dual-branch semantic segmentation network for enhanced obstacle detection in ship navigation</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>136</volume><fpage>108982</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.108982</pub-id></element-citation></ref><ref id="B15-sensors-25-05357"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Song</surname><given-names>T.</given-names></name></person-group><article-title>Parallel segmentation network for real-time semantic segmentation</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>148</volume><fpage>110487</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2025.110487</pub-id></element-citation></ref><ref id="B16-sensors-25-05357"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cordts</surname><given-names>M.</given-names></name><name name-style="western"><surname>Omran</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rehfeld</surname><given-names>T.</given-names></name><name name-style="western"><surname>Enzweiler</surname><given-names>M.</given-names></name><name name-style="western"><surname>Benenson</surname><given-names>R.</given-names></name><name name-style="western"><surname>Franke</surname><given-names>U.</given-names></name><name name-style="western"><surname>Roth</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name></person-group><article-title>The cityscapes dataset for semantic urban scene understanding</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, Nevada, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2016</year><fpage>3213</fpage><lpage>3223</lpage></element-citation></ref><ref id="B17-sensors-25-05357"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name></person-group><article-title>HoloSeg: An efficient holographic segmentation network for real-time scene parsing</article-title><source>Proceedings of the 2022 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>23&#8211;27 May 2022</conf-date><fpage>2395</fpage><lpage>2402</lpage></element-citation></ref><ref id="B18-sensors-25-05357"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name></person-group><article-title>Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date></element-citation></ref><ref id="B19-sensors-25-05357"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Bian</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ni</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name></person-group><article-title>DSNet: A Novel Way to Use Atrous Convolutions in Semantic Segmentation</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>35</volume><fpage>3679</fpage><lpage>3692</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2024.3509504</pub-id></element-citation></ref><ref id="B20-sensors-25-05357"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.-C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Schroff</surname><given-names>F.</given-names></name><name name-style="western"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>Encoder&#8211;decoder with atrous separable convolution for semantic image segmentation</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>801</fpage><lpage>818</lpage></element-citation></ref><ref id="B21-sensors-25-05357"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>N.</given-names></name></person-group><article-title>An Intelligent Weighted Object Detector for Feature Extraction to Enrich Global Image Information</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>7825</elocation-id><pub-id pub-id-type="doi">10.3390/app12157825</pub-id></element-citation></ref><ref id="B22-sensors-25-05357"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Pyramid scene parsing network</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>6230</fpage><lpage>6239</lpage></element-citation></ref><ref id="B23-sensors-25-05357"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lian</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>F.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>B.</given-names></name></person-group><article-title>Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A non-adversarial approach</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>6758</fpage><lpage>6767</lpage></element-citation></ref><ref id="B24-sensors-25-05357"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>B.V.K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>289</fpage><lpage>305</lpage></element-citation></ref><ref id="B25-sensors-25-05357"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Puig</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fidler</surname><given-names>S.</given-names></name><name name-style="western"><surname>Barriuso</surname><given-names>A.</given-names></name><name name-style="western"><surname>Torralba</surname><given-names>A.</given-names></name></person-group><article-title>Scene parsing through ADE20K dataset</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>633</fpage><lpage>641</lpage></element-citation></ref><ref id="B26-sensors-25-05357"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Caesar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Uijlings</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ferrari</surname><given-names>V.</given-names></name></person-group><article-title>Coco-stuff: Thing and stuff classes in context</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>1209</fpage><lpage>1218</lpage></element-citation></ref><ref id="B27-sensors-25-05357"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-net: Convolutional networks for biomedical image segmentation</article-title><source>Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2015, Proceedings of the 18th International Conference, Munich, Germany, 5&#8211;9 October 2015</source><series>Part III</series><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2015</year></element-citation></ref><ref id="B28-sensors-25-05357"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kendall</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cipolla</surname><given-names>R.</given-names></name></person-group><article-title>Segnet: A deep convolutional encoder-decoder architecture for image segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>39</volume><fpage>2481</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type="pmid">28060704</pub-id></element-citation></ref><ref id="B29-sensors-25-05357"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Q.</given-names></name></person-group><article-title>DMANet: Dual-branch multiscale attention network for real-time semantic segmentation</article-title><source>Neurocomputing</source><year>2025</year><volume>617</volume><fpage>128991</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2024.128991</pub-id></element-citation></ref><ref id="B30-sensors-25-05357"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>Attention-based multi-kernelized and boundary-aware network for image semantic segmentation</article-title><source>Neurocomputing</source><year>2024</year><volume>597</volume><fpage>127988</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2024.127988</pub-id></element-citation></ref><ref id="B31-sensors-25-05357"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lyu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name></person-group><article-title>A frequency decoupling network for semantic segmentation of remote sensing images</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2025</year><volume>63</volume><fpage>5607921</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2025.3594760</pub-id></element-citation></ref><ref id="B32-sensors-25-05357"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Unimatch v2: Pushing the limit of semi-supervised semantic segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2025</year><volume>47</volume><fpage>3031</fpage><lpage>3048</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2025.3528453</pub-id><pub-id pub-id-type="pmid">40031040</pub-id></element-citation></ref><ref id="B33-sensors-25-05357"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name></person-group><article-title>Cross-image pixel contrasting for semantic segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>5398</fpage><lpage>5412</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2024.3367952</pub-id><pub-id pub-id-type="pmid">38386572</pub-id></element-citation></ref><ref id="B34-sensors-25-05357"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thisanke</surname><given-names>H.</given-names></name><name name-style="western"><surname>Deshan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chamith</surname><given-names>K.</given-names></name><name name-style="western"><surname>Seneviratne</surname><given-names>S.</given-names></name><name name-style="western"><surname>Vidanaarachchi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Herath</surname><given-names>D.</given-names></name></person-group><article-title>Semantic segmentation using Vision Transformers: A survey</article-title><source>Eng. Appl. Artif. Intell.</source><year>2023</year><volume>126</volume><fpage>106669</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.106669</pub-id></element-citation></ref><ref id="B35-sensors-25-05357"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Anandkumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Alvarez</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>P.</given-names></name></person-group><article-title>SegFormer: Simple and efficient design for semantic segmentation with transformers</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>12077</fpage><lpage>12090</lpage></element-citation></ref><ref id="B36-sensors-25-05357"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>G.</given-names></name><name name-style="western"><surname>Shu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Deep multi-branch aggregation network for real-time semantic segmentation in street scenes</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>17224</fpage><lpage>17240</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3150350</pub-id></element-citation></ref><ref id="B37-sensors-25-05357"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Or&#353;i&#263;</surname><given-names>M.</given-names></name><name name-style="western"><surname>&#352;egvi&#263;</surname><given-names>S.</given-names></name></person-group><article-title>Efficient semantic segmentation with pyramidal fusion</article-title><source>Pattern Recognit.</source><year>2021</year><volume>110</volume><fpage>107611</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2020.107611</pub-id></element-citation></ref><ref id="B38-sensors-25-05357"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sang</surname><given-names>N.</given-names></name></person-group><article-title>Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</article-title><source>Int. J. Comput. Vis.</source><year>2021</year><volume>129</volume><fpage>3051</fpage><lpage>3068</lpage><pub-id pub-id-type="doi">10.1007/s11263-021-01515-2</pub-id></element-citation></ref><ref id="B39-sensors-25-05357"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chai</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name></person-group><article-title>Rethinking bisenet for real-time semantic segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>19&#8211;25 June 2021</conf-date></element-citation></ref><ref id="B40-sensors-25-05357"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Atif</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mazhar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ahamed</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Bhuyan</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Alfarhood</surname><given-names>S.</given-names></name><name name-style="western"><surname>Safran</surname><given-names>M.</given-names></name></person-group><article-title>DMPNet: Distributed Multi-Scale Pyramid Network for Real-Time Semantic Segmentation</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>16573</fpage><lpage>16585</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3359425</pub-id></element-citation></ref><ref id="B41-sensors-25-05357"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name></person-group><article-title>Lcfnets: Compensation Strategy for RealTime Semantic Segmentation of Autonomous Driving</article-title><source>IEEE Trans. Intell. Veh.</source><year>2024</year><volume>9</volume><fpage>4715</fpage><lpage>4729</lpage><pub-id pub-id-type="doi">10.1109/TIV.2024.3363830</pub-id></element-citation></ref><ref id="B42-sensors-25-05357"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gou</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>RTFormer: Efficient design for real-time semantic segmentation with transformer</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>7423</fpage><lpage>7436</lpage></element-citation></ref><ref id="B43-sensors-25-05357"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name></person-group><article-title>Head-free lightweight semantic segmentation with linear transformer</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>516</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i1.25126</pub-id></element-citation></ref><ref id="B44-sensors-25-05357"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wan</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Seaformer: Squeeze-enhanced axial transformer for mobile semantic segmentation</article-title><source>Proceedings of the The Eleventh International Conference on Learning Representations</source><conf-loc>Kigali, Rwanda</conf-loc><conf-date>1&#8211;5 May 2023</conf-date></element-citation></ref><ref id="B45-sensors-25-05357"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Po</surname><given-names>L.M.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>W.Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xian</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ou</surname><given-names>W.</given-names></name></person-group><article-title>CSRNet: Cascaded Selective Resolution Network for real-time semantic segmentation</article-title><source>Expert Syst. Appl.</source><year>2023</year><volume>211</volume><fpage>118537</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2022.118537</pub-id></element-citation></ref><ref id="B46-sensors-25-05357"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Bhattacharyya</surname><given-names>S.P.</given-names></name></person-group><article-title>PIDNet: A real-time semantic segmentation network inspired by PID controllers</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date></element-citation></ref><ref id="B47-sensors-25-05357"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name></person-group><article-title>Deformable convolutional networks</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date></element-citation></ref><ref id="B48-sensors-25-05357"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name></person-group><article-title>Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>1911</fpage><lpage>1920</lpage></element-citation></ref><ref id="B49-sensors-25-05357"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Feichtenhofer</surname><given-names>C.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>S.</given-names></name></person-group><article-title>A convnet for the 2020s</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date></element-citation></ref><ref id="B50-sensors-25-05357"><label>50.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>L.</given-names></name></person-group><article-title>ConvFormer: Plug-and-play CNN-style transformers for improving medical image segmentation</article-title><source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source><publisher-name>Springer Nature Switzerland</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2023</year></element-citation></ref><ref id="B51-sensors-25-05357"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Arik</surname><given-names>S.O.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pfister</surname><given-names>T.</given-names></name></person-group><article-title>Distilling effective supervision from severe label noise</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>9294</fpage><lpage>9303</lpage></element-citation></ref><ref id="B52-sensors-25-05357"><label>52.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name></person-group><article-title>Simt: Handling open-set noise for domain adaptive semantic segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>7032</fpage><lpage>7041</lpage></element-citation></ref><ref id="B53-sensors-25-05357"><label>53.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Q.</given-names></name></person-group><article-title>Training noise-robust deep neural networks via meta-learning</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>4524</fpage><lpage>4533</lpage></element-citation></ref><ref id="B54-sensors-25-05357"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Diao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.Y.</given-names></name></person-group><article-title>AST: Adaptive Self-supervised Transformer for optical remote sensing representation</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2023</year><volume>200</volume><fpage>41</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2023.04.003</pub-id></element-citation></ref><ref id="B55-sensors-25-05357"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krawczyk</surname><given-names>B.</given-names></name></person-group><article-title>Learning from Imbalanced Data: Open Challenges and Future Directions</article-title><source>Prog. Artif. Intell.</source><year>2016</year><volume>5</volume><fpage>221</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1007/s13748-016-0094-0</pub-id></element-citation></ref><ref id="B56-sensors-25-05357"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name></person-group><article-title>SPGAN-DA: Semantic-preserved generative adversarial network for domain adaptive remote sensing image semantic segmentation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>5406717</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2023.3313883</pub-id></element-citation></ref><ref id="B57-sensors-25-05357"><label>57.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ramamohanarao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>D.</given-names></name></person-group><article-title>Learning with bounded instance and label-dependent label noise</article-title><source>International Conference on Machine Learning</source><publisher-name>PMLR</publisher-name><publisher-loc>New York City, NY, USA</publisher-loc><fpage>1789</fpage><lpage>1799</lpage></element-citation></ref><ref id="B58-sensors-25-05357"><label>58.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Patrini</surname><given-names>G.</given-names></name><name name-style="western"><surname>Rozza</surname><given-names>A.</given-names></name><name name-style="western"><surname>Menon</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Nock</surname><given-names>R.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>L.</given-names></name></person-group><article-title>Making deep neural networks robust to label noise: A loss correction approach</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date></element-citation></ref><ref id="B59-sensors-25-05357"><label>59.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Learning from massive noisy labeled data for image classification</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date></element-citation></ref><ref id="B60-sensors-25-05357"><label>60.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tanaka</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ikami</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yamasaki</surname><given-names>T.</given-names></name><name name-style="western"><surname>Aizawa</surname><given-names>K.</given-names></name></person-group><article-title>Joint optimization framework for learning with noisy labels</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date></element-citation></ref><ref id="B61-sensors-25-05357"><label>61.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>Clusterability as an alternative to anchor points when learning with noisy labels</article-title><source>International Conference on Machine Learning</source><publisher-name>PMLR</publisher-name><publisher-loc>New York City, NY, USA</publisher-loc><year>2021</year><fpage>12912</fpage><lpage>12923</lpage></element-citation></ref><ref id="B62-sensors-25-05357"><label>62.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name></person-group><article-title>Channel-wise knowledge distillation for dense prediction</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date></element-citation></ref><ref id="B63-sensors-25-05357"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Loshchilov</surname><given-names>I.</given-names></name><name name-style="western"><surname>Hutter</surname><given-names>F.</given-names></name></person-group><article-title>Decoupled weight decay regularization</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1711.05101</pub-id></element-citation></ref><ref id="B64-sensors-25-05357"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kullback</surname><given-names>S.</given-names></name><name name-style="western"><surname>Leibler</surname><given-names>R.A.</given-names></name></person-group><article-title>On information and sufficiency</article-title><source>Ann. Math. Stat.</source><year>1951</year><volume>22</volume><fpage>79</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177729694</pub-id></element-citation></ref><ref id="B65-sensors-25-05357"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>Structure-aware feature fusion for unsupervised domain adaptation</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2020</year><volume>34</volume><fpage>10567</fpage><lpage>10574</lpage><pub-id pub-id-type="doi">10.1609/aaai.v34i07.6629</pub-id></element-citation></ref><ref id="B66-sensors-25-05357"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="B67-sensors-25-05357"><label>67.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ahn</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Damianou</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lawrence</surname><given-names>N.D.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>Z.</given-names></name></person-group><article-title>Variational information distillation for knowledge transfer</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>16&#8211;17 June 2019</conf-date></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05357-f001" orientation="portrait"><label>Figure 1</label><caption><p>(<bold>a</bold>) Unlabeled target domain data. (<bold>b</bold>) Generated pseudo-labels. (<bold>c</bold>) Ground-truth labels.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g001.jpg"/></fig><fig position="float" id="sensors-25-05357-f002" orientation="portrait"><label>Figure 2</label><caption><p>Speed and accuracy performance of different methods on the Cityscapes validation set.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g002.jpg"/></fig><fig position="float" id="sensors-25-05357-f003" orientation="portrait"><label>Figure 3</label><caption><p>Overview of the LKNTNet architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g003.jpg"/></fig><fig position="float" id="sensors-25-05357-f004" orientation="portrait"><label>Figure 4</label><caption><p>Proposed Location-Aware Large Kernel Module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g004.jpg"/></fig><fig position="float" id="sensors-25-05357-f005" orientation="portrait"><label>Figure 5</label><caption><p>Proposed Multi-Scale Position-Aware Block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g005.jpg"/></fig><fig position="float" id="sensors-25-05357-f006" orientation="portrait"><label>Figure 6</label><caption><p>Proposed Multi-Scale Context Noise Transformation Module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g006.jpg"/></fig><fig position="float" id="sensors-25-05357-f007" orientation="portrait"><label>Figure 7</label><caption><p>Proposed Anchor-Free NTM Estimation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g007.jpg"/></fig><fig position="float" id="sensors-25-05357-f008" orientation="portrait"><label>Figure 8</label><caption><p>Visualization results on the Cityscapes validation set. Compared to other methods, the generated masks exhibit finer details, as highlighted in the light blue boxes.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g008.jpg"/></fig><fig position="float" id="sensors-25-05357-f009" orientation="portrait"><label>Figure 9</label><caption><p>Visualization of NTM module ablation experiments.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05357-g009.jpg"/></fig><table-wrap position="float" id="sensors-25-05357-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of accuracy and speed with other state-of-the-art real-time methods on the Cityscapes validation set. &#8220;-&#8221; indicates that the corresponding results are not provided by the method. Our results are highlighted in red. # Params refers to the number of parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Reference</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"># Params</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Resolution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">BiSeNet V2</td><td align="center" valign="middle" rowspan="1" colspan="1">CVPR2020</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">118.5</td><td align="center" valign="middle" rowspan="1" colspan="1">47.3</td><td align="center" valign="middle" rowspan="1" colspan="1">75.23</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">STDC2-Seg75</td><td align="center" valign="middle" rowspan="1" colspan="1">CVPR2021</td><td align="center" valign="middle" rowspan="1" colspan="1">22.2 M</td><td align="center" valign="middle" rowspan="1" colspan="1">1536 &#215; 768</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">84.3</td><td align="center" valign="middle" rowspan="1" colspan="1">77.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegNet-T-Seg100</td><td align="center" valign="middle" rowspan="1" colspan="1">NeurIPS2022b</td><td align="center" valign="middle" rowspan="1" colspan="1">4.3 M</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">28.1</td><td align="center" valign="middle" rowspan="1" colspan="1">79.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDRNet-23</td><td align="center" valign="middle" rowspan="1" colspan="1">TIP2022</td><td align="center" valign="middle" rowspan="1" colspan="1">20.1 M</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">143.1</td><td align="center" valign="middle" rowspan="1" colspan="1">56.7</td><td align="center" valign="middle" rowspan="1" colspan="1">79.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RTFormer-B</td><td align="center" valign="middle" rowspan="1" colspan="1">NeurIPS2022</td><td align="center" valign="middle" rowspan="1" colspan="1">16.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">50.2</td><td align="center" valign="middle" rowspan="1" colspan="1">79.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AFFormer-B-Seg</td><td align="center" valign="middle" rowspan="1" colspan="1">AAAI2023</td><td align="center" valign="middle" rowspan="1" colspan="1">3.0 M</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">28.4</td><td align="center" valign="middle" rowspan="1" colspan="1">78.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SeaFormer-B-Seg100</td><td align="center" valign="middle" rowspan="1" colspan="1">ICLR2023</td><td align="center" valign="middle" rowspan="1" colspan="1">8.6 M</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">37.5</td><td align="center" valign="middle" rowspan="1" colspan="1">77.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CSRNet-heavy</td><td align="center" valign="middle" rowspan="1" colspan="1">ESWA2023</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">768 &#215; 768</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">36.3</td><td align="center" valign="middle" rowspan="1" colspan="1">77.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PIDNet-M</td><td align="center" valign="middle" rowspan="1" colspan="1">CVPR2023</td><td align="center" valign="middle" rowspan="1" colspan="1">42.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">197.4</td><td align="center" valign="middle" rowspan="1" colspan="1">33.7</td><td align="center" valign="middle" rowspan="1" colspan="1">78.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LCFNet</td><td align="center" valign="middle" rowspan="1" colspan="1">TransIntellVeh2024</td><td align="center" valign="middle" rowspan="1" colspan="1">8.3 M</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">77.92</td><td align="center" valign="middle" rowspan="1" colspan="1">87.6</td><td align="center" valign="middle" rowspan="1" colspan="1">68.67</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">CVPR2024</td><td align="center" valign="middle" rowspan="1" colspan="1">7.6 M</td><td align="center" valign="middle" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" rowspan="1" colspan="1">226.6</td><td align="center" valign="middle" rowspan="1" colspan="1">79.6</td><td align="center" valign="middle" rowspan="1" colspan="1">78.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.5 M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2048 &#215; 1024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">158.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">80.05</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t002_Table 2</object-id><label>Table 2</label><caption><p>Classification results on the Cityscapes validation set (IoU scores). The category list (from left to right): road (Roa), sidewalk (Sid), building (Bui), wall (Wal), fence (Fen), pole (Pol), traffic light (Tli), traffic sign (TSi), vegetation (Veg), terrain (Ter), sky (Sky), person (Per), rider (Rid), car (Car), truck (Tru), bus (Bus), train (Tra), motorcycle (Mot), and bicycle (Bic). The best results in each category are highlighted in red.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Roa</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sid</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bui</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Wal</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Fen</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pol</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TLi</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TSi</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Veg</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ter</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">BiSeNet V2</td><td align="center" valign="middle" rowspan="1" colspan="1">97.95</td><td align="center" valign="middle" rowspan="1" colspan="1">83.36</td><td align="center" valign="middle" rowspan="1" colspan="1">91.77</td><td align="center" valign="middle" rowspan="1" colspan="1">50.16</td><td align="center" valign="middle" rowspan="1" colspan="1">57.27</td><td align="center" valign="middle" rowspan="1" colspan="1">62.02</td><td align="center" valign="middle" rowspan="1" colspan="1">68.62</td><td align="center" valign="middle" rowspan="1" colspan="1">76.41</td><td align="center" valign="middle" rowspan="1" colspan="1">92.24</td><td align="center" valign="middle" rowspan="1" colspan="1">62.18</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PIDNet</td><td align="center" valign="middle" rowspan="1" colspan="1">98.10</td><td align="center" valign="middle" rowspan="1" colspan="1">84.91</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color:red">92.90</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color:red">56.63</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">63.53</td><td align="center" valign="middle" rowspan="1" colspan="1">65.48</td><td align="center" valign="middle" rowspan="1" colspan="1">71.98</td><td align="center" valign="middle" rowspan="1" colspan="1">78.49</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color:red">92.61</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">64.91</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LCFNet</td><td align="center" valign="middle" rowspan="1" colspan="1">97.63</td><td align="center" valign="middle" rowspan="1" colspan="1">81.71</td><td align="center" valign="middle" rowspan="1" colspan="1">90.54</td><td align="center" valign="middle" rowspan="1" colspan="1">53.76</td><td align="center" valign="middle" rowspan="1" colspan="1">53.82</td><td align="center" valign="middle" rowspan="1" colspan="1">59.15</td><td align="center" valign="middle" rowspan="1" colspan="1">56.76</td><td align="center" valign="middle" rowspan="1" colspan="1">69.72</td><td align="center" valign="middle" rowspan="1" colspan="1">91.52</td><td align="center" valign="middle" rowspan="1" colspan="1">59.13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">98.54</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">87.68</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.83</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">65.34</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">66.83</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">73.42</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">80.84</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">65.74</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sky</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Per</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rid</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Car</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tru</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bus</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tra</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mot</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bic</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIoU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BiSeNet V2</td><td align="center" valign="middle" rowspan="1" colspan="1">94.57</td><td align="center" valign="middle" rowspan="1" colspan="1">81.12</td><td align="center" valign="middle" rowspan="1" colspan="1">58.32</td><td align="center" valign="middle" rowspan="1" colspan="1">94.65</td><td align="center" valign="middle" rowspan="1" colspan="1">72.07</td><td align="center" valign="middle" rowspan="1" colspan="1">79.86</td><td align="center" valign="middle" rowspan="1" colspan="1">75.03</td><td align="center" valign="middle" rowspan="1" colspan="1">55.25</td><td align="center" valign="middle" rowspan="1" colspan="1">76.52</td><td align="center" valign="middle" rowspan="1" colspan="1">75.23</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PIDNet</td><td align="center" valign="middle" rowspan="1" colspan="1">94.66</td><td align="center" valign="middle" rowspan="1" colspan="1">82.62</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color:red">65.49</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">95.30</td><td align="center" valign="middle" rowspan="1" colspan="1">80.80</td><td align="center" valign="middle" rowspan="1" colspan="1">88.44</td><td align="center" valign="middle" rowspan="1" colspan="1">79.75</td><td align="center" valign="middle" rowspan="1" colspan="1">62.73</td><td align="center" valign="middle" rowspan="1" colspan="1">77.07</td><td align="center" valign="middle" rowspan="1" colspan="1">78.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LCFNet</td><td align="center" valign="middle" rowspan="1" colspan="1">93.50</td><td align="center" valign="middle" rowspan="1" colspan="1">76.42</td><td align="center" valign="middle" rowspan="1" colspan="1">51.25</td><td align="center" valign="middle" rowspan="1" colspan="1">92.54</td><td align="center" valign="middle" rowspan="1" colspan="1">49.59</td><td align="center" valign="middle" rowspan="1" colspan="1">65.37</td><td align="center" valign="middle" rowspan="1" colspan="1">45.45</td><td align="center" valign="middle" rowspan="1" colspan="1">45.62</td><td align="center" valign="middle" rowspan="1" colspan="1">71.23</td><td align="center" valign="middle" rowspan="1" colspan="1">68.67</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">94.84</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">83.17</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">95.67</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">86.05</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">89.75</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">84.22</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">67.04</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">79.09</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">80.05</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t003_Table 3</object-id><label>Table 3</label><caption><p>Accuracy and Speed Comparison with Other Advanced Real-Time Methods on the ADE20K Dataset. FPS is measured at a resolution of 512 &#215; 512. Our results are highlighted in red.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GFLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">BiSeNet V2</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">118.5</td><td align="center" valign="middle" rowspan="1" colspan="1">44.9</td><td align="center" valign="middle" rowspan="1" colspan="1">30.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RTFormer-S</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">95.2</td><td align="center" valign="middle" rowspan="1" colspan="1">36.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RTFormer-B</td><td align="center" valign="middle" rowspan="1" colspan="1">16.8 M</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">93.4</td><td align="center" valign="middle" rowspan="1" colspan="1">42.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SeaFormer-B-Seg100</td><td align="center" valign="middle" rowspan="1" colspan="1">8.6 M</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">44.5</td><td align="center" valign="middle" rowspan="1" colspan="1">41.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PIDNet-M</td><td align="center" valign="middle" rowspan="1" colspan="1">37.4 M</td><td align="center" valign="middle" rowspan="1" colspan="1">197.4</td><td align="center" valign="middle" rowspan="1" colspan="1">28.6</td><td align="center" valign="middle" rowspan="1" colspan="1">39.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AFFormer-B-Seg</td><td align="center" valign="middle" rowspan="1" colspan="1">3.0 M</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">49.6</td><td align="center" valign="middle" rowspan="1" colspan="1">41.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DMPNet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.74 M</td><td align="center" valign="middle" rowspan="1" colspan="1">259.7</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">35.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">6.5 M</td><td align="center" valign="middle" rowspan="1" colspan="1">226.6</td><td align="center" valign="middle" rowspan="1" colspan="1">72.3</td><td align="center" valign="middle" rowspan="1" colspan="1">40.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LCFNet</td><td align="center" valign="middle" rowspan="1" colspan="1">7.3 M</td><td align="center" valign="middle" rowspan="1" colspan="1">77.92</td><td align="center" valign="middle" rowspan="1" colspan="1">59.3</td><td align="center" valign="middle" rowspan="1" colspan="1">37.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.8 M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">158.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">143.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">42.7</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison with other state-of-the-art real-time methods on the COCO-Stuff-10K test set. FPS is measured at a resolution of 640 &#215; 640. Our results are highlighted in red.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GFLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">BiSeNet V2</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">118.5</td><td align="center" valign="middle" rowspan="1" colspan="1">38.7</td><td align="center" valign="middle" rowspan="1" colspan="1">25.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDRNet-23</td><td align="center" valign="middle" rowspan="1" colspan="1">20.1 M</td><td align="center" valign="middle" rowspan="1" colspan="1">143.1</td><td align="center" valign="middle" rowspan="1" colspan="1">108.8</td><td align="center" valign="middle" rowspan="1" colspan="1">32.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SeaFormer-B-Seg100</td><td align="center" valign="middle" rowspan="1" colspan="1">8.6 M</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">41.9</td><td align="center" valign="middle" rowspan="1" colspan="1">34.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AFFormer-B-Seg</td><td align="center" valign="middle" rowspan="1" colspan="1">3.0 M</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">46.5</td><td align="center" valign="middle" rowspan="1" colspan="1">35.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DSNet</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3 M</td><td align="center" valign="middle" rowspan="1" colspan="1">77.92</td><td align="center" valign="middle" rowspan="1" colspan="1">57.9</td><td align="center" valign="middle" rowspan="1" colspan="1">31.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LCFNet</td><td align="center" valign="middle" rowspan="1" colspan="1">5.9 M</td><td align="center" valign="middle" rowspan="1" colspan="1">226.6</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">33.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.3 M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">158.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">133.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color:red">35.4</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation Study of Each Component.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">LALK</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSPA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSCNT</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">79.61</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">78.60</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">79.39</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.05</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t006_Table 6</object-id><label>Table 6</label><caption><p>Ablation study of MSCNT modules.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">LALK</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSPA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSCE</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NTM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">78.1</td><td align="center" valign="middle" rowspan="1" colspan="1">79.68</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.81</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t007_Table 7</object-id><label>Table 7</label><caption><p>Performance of NTM module under different noise scenarios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Noise</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">LALK</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSPA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSCE</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NTM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">79.91</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.87</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t008_Table 8</object-id><label>Table 8</label><caption><p>Ablation study of DWConv.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSPA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSCNT</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">DWConv</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">79.93</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Conv</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.67</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t009_Table 9</object-id><label>Table 9</label><caption><p>Ablation on different types of loss.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Loss Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Baseline</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">KL Loss</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MI Loss</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">L2 Loss</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CWD Loss</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIoU (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t010_Table 10</object-id><label>Table 10</label><caption><p>Ablation on the location of alignment loss.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Logits</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Stage 2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Stage 4</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Decoder</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">78.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">78.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">79.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">79.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t011" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t011_Table 11</object-id><label>Table 11</label><caption><p>Ablation on the weight of alignment loss.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Loss Weight</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">0,0,0,0</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">3,0,0,0</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">3,5,5,5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">3,10,10,10</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">3,15,15,15</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIoU (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.3</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05357-t012" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05357-t012_Table 12</object-id><label>Table 12</label><caption><p>Ablation on the convolution kernel size.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Kernel Size</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">nIoU (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#215; 1 &#215; 1 + 1 &#215; 1 &#215; 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#215; 1 &#215; 1 + 5 &#215; 5 &#215; 5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5 &#215; 5 &#215; 5 + 5 &#215; 5 &#215; 5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5 &#215; 5 &#215; 5 + 7 &#215; 7 &#215; 7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#215; 1 &#215; 1 + 7 &#215; 7 &#215; 7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7 &#215; 7 &#215; 7 + 7 &#215; 7 &#215; 7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.8</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>