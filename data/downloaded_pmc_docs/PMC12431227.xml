<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431227</article-id><article-id pub-id-type="pmcid-ver">PMC12431227.1</article-id><article-id pub-id-type="pmcaid">12431227</article-id><article-id pub-id-type="pmcaiid">12431227</article-id><article-id pub-id-type="doi">10.3390/s25175448</article-id><article-id pub-id-type="publisher-id">sensors-25-05448</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MINI-DROID-SLAM: Improving Monocular Visual SLAM Using MINI-GRU RNN Network</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-7064-5922</contrib-id><name name-style="western"><surname>Albukhari</surname><given-names initials="I">Ismaiel</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05448" ref-type="aff">1</xref><xref rid="fn1-sensors-25-05448" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4746-9095</contrib-id><name name-style="western"><surname>El-Sayed</surname><given-names initials="A">Ahmed</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-05448" ref-type="aff">2</xref><xref rid="c1-sensors-25-05448" ref-type="corresp">*</xref><xref rid="fn1-sensors-25-05448" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6066-5045</contrib-id><name name-style="western"><surname>Alshibli</surname><given-names initials="M">Mohammad</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af3-sensors-25-05448" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Ledezma Espino</surname><given-names initials="A">Agapito</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>de Miguel</surname><given-names initials="AS">Araceli Sanchis</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05448"><label>1</label>Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, CT 06604, USA; <email>ialbukha@my.bridgeport.edu</email></aff><aff id="af2-sensors-25-05448"><label>2</label>Department of Electrical and Computer Engineering, University of Bridgeport, Bridgeport, CT 06604, USA</aff><aff id="af3-sensors-25-05448"><label>3</label>Department of Computer Systems, SUNY Farmingdale State College, Farmingdale, NY 11735, USA; <email>alshibm@farmingdale.edu</email></aff><author-notes><corresp id="c1-sensors-25-05448"><label>*</label>Correspondence: <email>aelsayed@bridgeport.edu</email>; Tel.: +1-203-576-4571</corresp><fn id="fn1-sensors-25-05448"><label>&#8224;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>03</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5448</elocation-id><history><date date-type="received"><day>15</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>26</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>28</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>03</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05448.pdf"/><abstract><p>Recently, visual odometry and SLAM (Simultaneous Localization and Mapping) have shown tremendous performance improvements compared to LiDAR and 3D sensor techniques. Unfortunately, attempts to achieve these improvements always face numerous challenges due to their complexity and insufficient compatibility for real-time environments. This paper presents an enhanced deep-learning-based SLAM system, primarily for Monocular Visual SLAM, by utilizing a Mini-GRU (gated recurrent unit). The proposed system, MINI-DROID-SLAM, demonstrates significant improvements and robustness through persistent iteration of the camera position. Similar to the original DROID SLAM, the system calculates pixel-wise depth mapping and enhances it using the BA (Bundle Adjustment) technique. The architecture introduced in this research reduces the time used and computation complexity compared to the original DROID-SLAM network. The introduced model is trained locally on a single GPU using monocular camera images from the TartanAir datasets. The training time and reconstruction metric, assessed using ATE (Absolute Trajectory Error), show robustness and high performance compared to the original DROID-SLAM.</p></abstract><kwd-group><kwd>SLAM</kwd><kwd>Deep Learning</kwd><kwd>Monocular-SLAM</kwd><kwd>Bundle Adjustment</kwd><kwd>Mini-GRU</kwd><kwd>CONV-GRU</kwd><kwd>DROID-SLAM</kwd><kwd>Visual-SLAM</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05448"><title>1. Introduction</title><p>Simultaneous Localization and Mapping (SLAM) is an indispensable technology that enables autonomous systems to concurrently construct a map of an unknown environment while simultaneously determining their own position within that map. This dual capability is fundamental for navigation and understanding new environments. SLAM systems commonly integrate data from multiple sensor modalities to enhance map quality and robustness. These sensors typically include cameras (monocular and stereo), depth sensors, and Light Detection and Ranging (LiDAR). The incorporation of advanced algorithmic techniques, such as those that support dense reconstruction or feature visualization, significantly improves the system&#8217;s ability to recognize and adapt to novel environments. In essence, SLAM empowers autonomous agents to navigate and comprehend their surroundings by concurrently localizing themselves within a newly generated map [<xref rid="B1-sensors-25-05448" ref-type="bibr">1</xref>]. A prominent variant of SLAM is Visual SLAM (VSLAM), which heavily relies on visual sensors for its operations. VSLAM systems typically utilize data from monocular cameras, stereo cameras, and, in some cases, LiDAR, to achieve robust performance. A key strength of VSLAM lies in its ability to generate real-time maps by leveraging distinctive visual features extracted from the environment. This characteristic makes VSLAM particularly well-suited for applications demanding high fidelity and real-time environment discovery. Both general SLAM and VSLAM methodologies are critically dependent on their abilities to construct new maps robustly, a vital capability for applications such as autonomous vehicles and drones [<xref rid="B2-sensors-25-05448" ref-type="bibr">2</xref>]. This research highlights a clear trend towards more efficient, accurate, and robust SLAM systems powered by deep learning. <xref rid="sensors-25-05448-f001" ref-type="fig">Figure 1</xref> shows the SLAM output for a real-time sample where DROID-SLAM failed to converge to the correct environment map, while MINI-DROID-SLAM was used to build an accurate map for the new environment. The main contributions of the paper can be summarized as follows:<list list-type="bullet"><list-item><p>Introduction of a new DROID-SLAM architecture based on the Mini-GRU RNN network.</p></list-item><list-item><p>Improvement on the training performance on the newly proposed model.</p></list-item><list-item><p>Reduction in the minimum hardware requirements from 4 GPUs to 1 GPU.</p></list-item><list-item><p>Introduction of MINI-DROID-SLAM, a lightweight DROID-based Monocular-SLAM system that is suitable for real-time requirements.</p></list-item></list></p><fig position="anchor" id="sensors-25-05448-f001" orientation="portrait"><label>Figure 1</label><caption><p>Result of DROID-SLAM (to the <bold>left</bold>), and MINI-DROID-SLAM (to the <bold>right</bold>) on a real-time example.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g001.jpg"/></fig><p>This proposed work maintains the robustness of deep SLAM frameworks while making them more accessible for real-time systems and embedded applications. For validation, this paper evaluates MINI-DROID-SLAM on various datasets using a monocular camera, as shown in the real-world example in <xref rid="sensors-25-05448-f001" ref-type="fig">Figure 1</xref>, the TUM-RGB dataset, the EuRoC dataset, and finally on the synthetic TartanAir dataset, demonstrating competitive performance and significantly reduced computational overhead.</p></sec><sec id="sec2-sensors-25-05448"><title>2. Related Works</title><p>Recent research has significantly advanced Simultaneous Localization and Mapping (SLAM) systems through the integration of deep learning methodologies, yielding enhanced performance. A notable example is the application of Visual SLAM (VSLAM) techniques for constructing dense 3D maps, as demonstrated with datasets like TartanAir [<xref rid="B3-sensors-25-05448" ref-type="bibr">3</xref>].</p><p>In 2022, Zachary Teed and Jia Deng introduced DROID-SLAM [<xref rid="B4-sensors-25-05448" ref-type="bibr">4</xref>], a pioneering framework that fuses deep learning with traditional SLAM paradigms to achieve highly accurate and robust VSLAM. DROID-SLAM&#8217;s architecture is built upon a recurrent iterative update mechanism, inspired by the RAFT model [<xref rid="B5-sensors-25-05448" ref-type="bibr">5</xref>], which refines camera poses and pixel-wise depth estimations. The system then employs Dense Bundle Adjustment (DBA), enabling it to process a diverse range of input types, including monocular, RGB-D, and stereo data. To manage inter-frame relationships and dependencies within video streams, DROID-SLAM utilizes a Frame Graph Representation, which facilitates efficient loop closure and optimization. A core innovation lies in its deep learning model, which extracts updates for poses and depth by minimizing geometric errors through state-of-the-art optical flow estimation, to refine predictions iteratively. The model&#8217;s efficacy was validated across multiple datasets, including TartanAir, EuRoC, TUM-RGBD, and ETH3D-SLAM, demonstrating robust performance without requiring any retraining. Although it was initially trained on four NVIDIA 3090 GPUs for approximately one week using the TartanAir dataset, the model&#8217;s efficacy was demonstrated across various datasets.</p><p>In late 2024, the same authors presented DPV-SLAM, an evolution of their DROID-SLAM approach, with a primary focus on improving the efficiency of the loop closure mechanism, while DROID-SLAM incorporated both proximity-based and classical loop closure techniques (image retrieval and pose graph optimization for scale drift correction), DPVO&#8217;s redesigned architecture significantly enhances efficiency. This new design enables DPV-SLAM to operate effectively on a single GPU, supporting high frame rates and minimizing memory consumption. Comparative analyses against DROID-SLAM revealed that DPV-SLAM achieves comparable accuracy while being 2.5 times faster, with no observed failures in either indoor or outdoor environments. It is worth noting that DPV-SLAM is an extended version of the DPVO visual odometry system [<xref rid="B6-sensors-25-05448" ref-type="bibr">6</xref>].</p><p>To continue, several other deep-learning-based SLAM systems have emerged, each with unique contributions, including the following types: iMAP, which utilizes a multi-layer perceptron (MLP) for inferring scene representation and rendering [<xref rid="B7-sensors-25-05448" ref-type="bibr">7</xref>]. This system employs RGB-D cameras for real-time training, eliminating the need for prior data to construct dense 3D occupancy and color models. iMAP integrates key frame structures, multi-processing, and dynamic pixel sampling to achieve efficient tracking and global map updates. It excels in scene representation, detail control, and filling unobserved areas, often surpassing traditional SLAM systems in completeness and memory efficiency. Its real-time training capability marks a significant step towards dense real-time SLAM with incremental training and implicit neural representation. Volume-DROID is a real-time SLAM system that combines volumetric mapping with the DROID-SLAM framework [<xref rid="B8-sensors-25-05448" ref-type="bibr">8</xref>]. It processes both RGB-D and monocular camera inputs to create 3D maps and accurately track robot localization. A key innovation is the real-time integration of DROID-SLAM with the ConvBKI algorithm, which optimizes data processing efficiency. The system leverages optimized camera positions and point clouds from RGB-D frames to enhance mapping accuracy for autonomous navigation, while evaluations on the TartanAir dataset showed promising performance, challenges with semantic segmentation were observed due to label mismatches. DVI-SLAM enhances accuracy and reliability by integrating visual and inertial (IMU) data, a feature shared with prior deep learning methods, such as DROID-SLAM [<xref rid="B9-sensors-25-05448" ref-type="bibr">9</xref>]. This hybrid model uniquely blends various visual data types, adjusting their impact based on confidence levels, resulting in a significant reduction in tracking errors for moving objects within datasets such as TartanAir and EuRoC. DVI-SLAM demonstrates flexibility across different sensor configurations and has been successfully tested on both datasets and real-time data. However, identified limitations include processing speed, memory usage, and sensor integration complexities.</p><p>Further enhancements to the DROID-SLAM framework for visual odometry have involved integrating Global Self-Attention and Atrous Spatial Pyramid Pooling (ASPP) into its Conv-GRU model [<xref rid="B10-sensors-25-05448" ref-type="bibr">10</xref>]. This modification expands the receptive field, enabling improved optical flow estimation in challenging environments, while the original DROID-SLAM occasionally outperforms these modifications on specific datasets, these enhancements generally improve DROID-SLAM&#8217;s accuracy and memory efficiency. GO-SLAM is a deep-learning-based real-time SLAM system designed for reliable 3D reconstruction using RGB-D, monocular, or stereo camera inputs [<xref rid="B11-sensors-25-05448" ref-type="bibr">11</xref>]. It reduces trajectory errors through online loop closing and full Bundle Adjustment, achieving superior performance compared to earlier techniques such as DROID-SLAM and iMAP. Operating at 8 FPS with 18 GB of GPU RAM, GO-SLAM demonstrates strong performance in large-scale environments. Optimal performance is achieved through careful key frame selection and loss configurations, leading to cutting-edge results in position estimation and 3D reconstruction across various datasets. Rover-SLAM is a real-time visual-SLAM framework that incorporates deep learning to enhance performance in challenging conditions [<xref rid="B12-sensors-25-05448" ref-type="bibr">12</xref>]. It supports diverse camera configurations, including monocular, stereo, monocular&#8211;inertial, and stereo&#8211;inertial data. The framework utilizes advanced feature extraction and matching algorithms, specifically SuperPoint and LightGlue, to enhance adaptability in dynamic lighting conditions and weakly structured environments. Rover-SLAM achieves high localization accuracy and robust tracking performance comparable to existing SLAM systems. SPAQ-DL-SLAM (Structured Pruning and Quantization) is an optimization framework developed for deep learning SLAM models, particularly DROID-SLAM, to enable their deployment on resource-constrained devices [<xref rid="B13-sensors-25-05448" ref-type="bibr">13</xref>]. This optimized version achieves a 20% reduction in model size and an 18.9% decrease in computational complexity, while improving accuracy by 10.5% on the TUM-RGBD dataset. For the SPAQ-DL-SLAM, the enhancements stem from two main steps: structured pruning, which reduces computational demands, and post-training quantization (PTQ), which converts the model&#8217;s data from 32-bit to 8-bit integers, maintaining accuracy while improving hardware efficiency; while it is effective across various datasets, this approach struggles in environments with high angular velocity. On the other hand, several newer versions of the GRU modules have been recently introduced, as in [<xref rid="B14-sensors-25-05448" ref-type="bibr">14</xref>], where the authors claim a lightweight architecture, yet they still utilize activation functions like Tanh in the input state. Additionally, the proposed models in their research are also used for other applications, rather than SLAM systems [<xref rid="B14-sensors-25-05448" ref-type="bibr">14</xref>]. Moreover, in 2025, another research group used a multiscale GRU. For that model, the authors are still using a large number of parameters [<xref rid="B15-sensors-25-05448" ref-type="bibr">15</xref>], even more than the original GRU. However, as will be introduced in the next section, the MINI-GRU model is a lightweight RNN network designed for sequential data, such as in Visual SLAM systems. The main concept of MINI-GRU is to reduce the parameters from the original GRU blocks. As a result, MINI-CONV-SLAM, the convolutional version of the MINI-GRU, achieves high performance and accuracy while reducing computational complexity.</p></sec><sec id="sec3-sensors-25-05448"><title>3. Background</title><sec id="sec3dot1-sensors-25-05448"><title>3.1. RNN</title><p>Recurrent Neural Networks (RNNs) [<xref rid="B16-sensors-25-05448" ref-type="bibr">16</xref>] are a specialized type of neural network (NN) [<xref rid="B17-sensors-25-05448" ref-type="bibr">17</xref>] architecture designed to process sequential data by retaining a portion of information from previous inputs. So, this technique is received via many cycles in the network to allow information to be fed back into itself (hidden layers). This feedback mechanism characterizes the RNNs from feed-forward neural networks, which process inputs in a single pass without cycles. Moreover, RNNs are specialized and potent in applications that involve sequence prediction and temporal patterns, such as language interpretation, speech recognition, image descriptions, and video labeling. The architecture of RNNs allows them to consider both the current input and the historical context, which is crucial for understanding sequences where the order of data points is considerable. <xref rid="sensors-25-05448-f002" ref-type="fig">Figure 2</xref> shows the basic architecture for the vanilla RNN network for input sequence processing.</p></sec><sec id="sec3dot2-sensors-25-05448"><title>3.2. LSTM</title><p>In 1997, Sepp and Schmidhuber presented LSTM (Long Short-Term Memory) [<xref rid="B18-sensors-25-05448" ref-type="bibr">18</xref>], a new development of RNN at the time [<xref rid="B16-sensors-25-05448" ref-type="bibr">16</xref>]. Their method was designed to overcome the limitations of vanilla RNN, such as the gradient vanishing or explosion problems, by integrating the input and bypassing the gates. These gates were designed to capture long-term dependencies in sequence data, making them particularly effective for tasks that integrate long sequences of text. <xref rid="sensors-25-05448-f003" ref-type="fig">Figure 3</xref> presents the internal architecture and gates of the LSTM module. In the figure, the arrow connection is a linear fully connected network architecture.</p></sec><sec id="sec3dot3-sensors-25-05448"><title>3.3. GRU</title><p>A gated recurrent unit (GRU) is an advanced version of the RNN architecture, designed to handle data sequences such as speech or text translation. However, it addresses some limitations of the original RNN, such as vanishing gradients and long-term dependency problems. In addition, GRUs provide a gate model to control the flow of information to all networks, to learn what to memorize, what to forget, and what to update. The last main advantage of GRU is its support for long-term dependence, which can be utilized more effectively compared to RNNs and LSTMs [<xref rid="B20-sensors-25-05448" ref-type="bibr">20</xref>]. <xref rid="sensors-25-05448-f004" ref-type="fig">Figure 4</xref> shows the basic architecture and gates of the GRU module. CONV-GRU is an expanded version of the original GRU that incorporates the convolutional process into the module for computer vision applications.</p><sec><title>Traditional CONV-GRU (Modified from the Original GRU Module)</title><disp-formula id="FD1-sensors-25-05448"><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mlabeledtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>Conv</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mfenced><mml:mspace width="1.em"/><mml:mo>#</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>Input</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>and</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>hidden</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>state</mml:mi></mml:mrow></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>Conv</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mfenced><mml:mspace width="1.em"/><mml:mo>#</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>Reset</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>gate</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>(</mml:mo><mml:mi>same</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>as</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>previous</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>step</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">tanh</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mi>Conv</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mfenced></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:math></disp-formula></sec></sec><sec id="sec3dot4-sensors-25-05448"><title>3.4. MINI-GRU</title><p>Mini-GRU [<xref rid="B21-sensors-25-05448" ref-type="bibr">21</xref>] is a light version of the regular GRU, where the reset gate is removed and the other steps are simplified by eliminating the hyperbolic tangent (Tanh) activation function [<xref rid="B16-sensors-25-05448" ref-type="bibr">16</xref>]. Moreover, it is also used to process sequences, such as time series data and language translation, among others. Notably, MINI-GRU is lighter and more efficient for real-time applications, featuring fewer gates and parameters, which results in a lower memory footprint. As a result, MINI-GRU achieves much faster training and execution time and is efficient on most datasets. <xref rid="sensors-25-05448-f005" ref-type="fig">Figure 5</xref> presents the block diagram of the state-of-the-art MINI-GRU RNN module and shows the effective connections after removing the reset gate. A comparison between the original GRU and MINI-GRU modules from different computational and performance perspectives is shown in <xref rid="sensors-25-05448-t001" ref-type="table">Table 1</xref>. Similar to CONV-GRU, MINI-CONV-GRU is a modified version of the MINI-GRU module that is introduced in this paper for computer vision applications. The following equations explain the details of the proposed module.</p><sec><title>MINI-CONV-GRU (Modified)</title><p>Reset gate removed with the Tanh activation function.</p><disp-formula id="FD5-sensors-25-05448"><mml:math id="mm924" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mlabeledtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mn>6</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>Conv</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mfenced><mml:mspace width="1.em"/><mml:mo>#</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>Input</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>and</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>hidden</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>state</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mn>7</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>Conv</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">d</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mlabeledtr></mml:mtable></mml:mrow></mml:math></disp-formula></sec></sec></sec><sec sec-type="methods" id="sec4-sensors-25-05448"><title>4. Methodology</title><p>Considering the advantages of the MIN-GRU presented in the previous section, this paper proposes enhancements and modifications to the DROID-SLAM deep learning Simultaneous Localization and Mapping (SLAM) system. Specifically, it replaces the utilized CONV-GRU module with a modified version of the state-of-the-art MINI-GRU module, known as MINI-CONV-GRU. <xref rid="sensors-25-05448-t002" ref-type="table">Table 2</xref> presents a comparison between fully connected neural networks (Basic NN), CONV-GRU, and MINI-CONV-GRU modules, focusing on their suitability for real-time applications. The proposed system utilizes the BA (Bundle Adjustment) for pixel-wise depth, precisely as the DROID-SLAM technique does. The proposed system architecture is shown in <xref rid="sensors-25-05448-f006" ref-type="fig">Figure 6</xref>. The proposed system targets monocular camera data; therefore, only monocular Tartanair datasets will be used in the training process. The process starts by extracting features from the input image. The features are extracted using a network of six residual blocks and three downsampling layers similar to the original DROID-SLAM architecture to produce a dense feature map that is used to build the <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, input to the update process, as shown in <xref rid="sensors-25-05448-f007" ref-type="fig">Figure 7</xref>. Those correlation features are indexed using the <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, correlation lookup operator. Another context network works on the input image to produce context features that are also used in the output update step. Similar to the original technique, the dense corresponding field <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated and used in both the indexing process and the BA step. <xref rid="sensors-25-05448-f008" ref-type="fig">Figure 8</xref> illustrates the process in algorithmic steps.</p><sec><title>Update Operator in MINI-DROID-SLAM</title><p>After the input sequences are processed for feature extraction and indexing, the data are applied to the MINI-CONV-GRU to produce updated information for the output level. <xref rid="sensors-25-05448-f007" ref-type="fig">Figure 7</xref> illustrates the <italic toggle="yes">update operator</italic>, which is the central core of MINI-DROID-SLAM, utilizing the MINI-CONV-GRU module, and is responsible for refining camera poses and dense depth maps iteratively. Unlike the original MINI-GRU, the MINI-CONV-GRU concatenates and uses the input of the previous layer <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to calculate the <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> gate. The update operator acts on edges of the frame graph, where each edge connects two frames <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that have overlapping views.</p><p>As explained earlier, correlation volumes are computed from dense feature maps of the two frames, representing similarity scores between all pairs of pixels. Context features extracted from the images provide additional information to guide updates. The hidden state <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the recurrent unit carries the previous memory across iterations. The update operator is implemented mainly as a <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> MINI-CONV-GRU. At each iteration <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, it updates its hidden state based on the inputs and outputs flow revisions, which are corrections to the current optical flow estimates. The process inside the MINI-CONV-GRU can be stated as follows:<list list-type="bullet"><list-item><p><bold>Step 1:</bold> Concatenate the new input indexed correlation features <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> sequence along the second dimension (<monospace>dim = 1</monospace>).</p></list-item><list-item><p><bold>Step 2:</bold> Concatenate the <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the indexed <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> tensors along the second dimension to form <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><bold>Step 3:</bold> Obtain the shape of the <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> tensor, denoted as <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><bold>Step 4:</bold> Apply a sigmoid activation function to the output of a weighted layer <italic toggle="yes">w</italic> applied to <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, then perform element-wise multiplication with <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which results an intermediate variable <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><bold>Step 5:</bold> Reshape the resulting tensor <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to a 3D tensor, compute the mean along the last dimension, and reshape it back to a 4D tensor.</p></list-item><list-item><p><bold>Step 6:</bold> Apply a sigmoid activation function to the output of a convolutional layer applied to <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the output of this step is the <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><bold>Step 7:</bold> Compute an intermediate step variable <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by summing the outputs of two parallel convolutional layers:</p><list list-type="simple"><list-item><label>&#8211;</label><p><monospace>convq</monospace>, by appling <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional filters to the concatenated <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> tensor.</p></list-item><list-item><label>&#8211;</label><p><monospace>convq_glo</monospace>, by applying <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional process for channel adjustments to the <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> tensor.</p></list-item></list></list-item><list-item><p><bold>Step 8:</bold> Update the <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> tensor using the calculated <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as follows:<disp-formula><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><bold>Step 9:</bold> Return the updated <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> variable tensor, and repeat <bold>Step 2</bold> again with the new input indexed correlation.</p></list-item></list></p><p>The predicted flow revisions outputs from the update operator, along with the dense corresponding field <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, are passed to the Dense Bundle Adjustment (DBA) layer. The DBA performs a differentiable Gauss&#8211;Newton optimization that jointly updates camera poses and dense depth maps. This process tightly couples pose and depth refinement, enforcing geometric consistency across frames. At each iteration, the current pose and depth estimates are used to compute dense correspondences between frames, which inform the next update. Lastly, the operator works on the edges of a frame graph that encodes co-visibility between frames.</p></sec></sec><sec sec-type="results" id="sec5-sensors-25-05448"><title>5. Results</title><p>As stated earlier, the proposed MINI-DROID-SLAM system is trained on monocular images from the TartanAiR dataset, with a batch size of 1250 steps, a resolution of 384 &#215; 512, 7-frame clips, and 12 iterations for BA, rather than the original DROID-SLAM. The machine used for this work has the following specifications: an Intel Core i9 processor, 32 GB of RAM, and a single GPU, the RTX 3090 with 24 GB of memory. The structure of applying input data sequence to the proposed architecture is shown in <xref rid="sensors-25-05448-f009" ref-type="fig">Figure 9</xref>. <xref rid="sensors-25-05448-t003" ref-type="table">Table 3</xref> shows the testing results of DROID-SLAM against the proposed MINI-DROID-SLAM model on the monocular benchmark TarTanAir dataset. The data utilized for the benchmark are not the same as those used for training.</p><p><xref rid="sensors-25-05448-f010" ref-type="fig">Figure 10</xref> presents the ablation experiment on TartanAir validation split dataset to show the advantage of the proposed MINI-DROID-SLAM over the original DROID-SLAM.</p><p>For further verification, the proposed model is tested on the same dataset used for testing in the original DROID-SLAM paper, specifically the EuRoC and the TUM datasets. A sample from the EuRoC dataset with our output SLAM result is shown in <xref rid="sensors-25-05448-f011" ref-type="fig">Figure 11</xref> and <xref rid="sensors-25-05448-f012" ref-type="fig">Figure 12</xref>, respectively.</p><p>The complete comparison between generated maps and localization between DROID-SLAM, ORB-SLAM3, and MINI-DROID-SLAM on EuRoC and TUM monocular RGB datasets is shown in <xref rid="sensors-25-05448-t004" ref-type="table">Table 4</xref> and <xref rid="sensors-25-05448-t005" ref-type="table">Table 5</xref>. These results demonstrate that MINI-DROID-SLAM generates maps and trajectories for the EuRoC and TUM datasets with the same accuracy as DROID-SLAM, but with improved execution time.</p><p>Moreover, we also applied the MINI-DROID-SLAM model to real-time data collected from the local campus of the University of Bridgeport to test its real-time capabilities, map building, and trajectory generation. The generated map and trajectory matched the results collected from the original DROID-SLAM technique, but better frame rates per second were achieved on one GPU compared to the original DROID-SLAM. <xref rid="sensors-25-05448-f013" ref-type="fig">Figure 13</xref> and <xref rid="sensors-25-05448-f014" ref-type="fig">Figure 14</xref> show the map built for the campus building. As shown earlier in <xref rid="sensors-25-05448-f001" ref-type="fig">Figure 1</xref>, in some cases, MINI-DROID-SLAM shows more robust and better performance compared to the original DROID-SLAM.</p><p>Finally, to demonstrate the scalability of the trained model, the TartanAir validation dataset is tested on a single lower-end GPU, specifically an RTX 3070 with 8 GB of memory. The results show that the proposed model runs on the GPU without encountering an out-of-memory error, which is a common issue with the original DROID-SLAM model. <xref rid="sensors-25-05448-f015" ref-type="fig">Figure 15</xref> shows the collected results from the RTX 3070 GPU on samples from both TartanAiR and TUM datasets. For this demonstration, the input sequence has been processed at a rate of 7.75 iterations per second, with a memory utilization of 6.2 GB.</p></sec><sec sec-type="discussion" id="sec6-sensors-25-05448"><title>6. Discussion</title><p>The MINI-DROID-SLAM system represents a significant advancement over conventional SLAM techniques, particularly in comparison to its predecessor, DROID-SLAM. This progress is attributed to the integration of a MINI-CONV-GRU module within the feature map and camera pose update processes. This architectural optimization yields substantial improvements in both computational efficiency and mapping accuracy. Evaluations conducted on the TartanAir synthetic dataset and real-world &#8220;campus building&#8221; data demonstrate MINI-DROID-SLAM&#8217;s capability to construct highly accurate 3D maps with reduced trajectory error, and improved real-time performance. The system exhibits performance comparable to established SLAM frameworks, such as ORB-SLAM and DROID-SLAM, particularly in RMSE-based ATE evaluations across various sequences. The robustness and sustained performance of MINI-DROID-SLAM are maintained through the application of BA and a lightweight recurrent module. This design facilitates efficient training and real-time inference on a single high-end RTX GPU. The successful implementation of MINI-DROID-SLAM underscores the potential for architectural optimizations, such as the MINI-GRU, to achieve efficient and accurate SLAM solutions, even with limited hardware resources, thereby rivaling state-of-the-art frameworks trained with significantly greater computational power. This evidence supports the hypothesis that innovative architectural designs can lead to robust SLAM solutions for real-world applications. <xref rid="sensors-25-05448-t006" ref-type="table">Table 6</xref> presents a comprehensive comparison of DROID-SLAM, DPV-SLAM, and the proposed MINI-DROID-SLAM algorithm. This analysis highlights the advancements of the proposed technique over existing state-of-the-art algorithms, as evidenced by various performance metrics.</p><p>Although the MINI-DROID-SLAM shows improved results over DROID-SLAM, the proposed model still requires execution on a GPU to enhance quality and performance in both training and testing times. On the other hand, compared to the original DROID-SLAM, the proposed technique requires a lower GPU footprint and can be executed on a lower-latency GPU, such as the RTX 3070. For outdoor environments, although the mapping process can sometimes be a significant challenge and requires further improvement, the camera localization task still performs comparably. To examine the performance of the MINI-DROID-SLAM in outdoor environments, it has been tested against ORB-SLAM3 on one of the common Kitti&#8217;s dataset sequences (01). As shown in <xref rid="sensors-25-05448-f016" ref-type="fig">Figure 16</xref>, the ORB-SLAM3 [<xref rid="B26-sensors-25-05448" ref-type="bibr">26</xref>] failed to detect the correct camera trajectory due to lighting and challenging environmental conditions. On the other hand, MINI-DROID-SLAM successfully generates the camera trajectory with comparable performance, as shown in <xref rid="sensors-25-05448-f017" ref-type="fig">Figure 17</xref>. Moreover, as stated earlier, <xref rid="sensors-25-05448-f001" ref-type="fig">Figure 1</xref> shows a comparison between the original DROID-SLAM and MINI-DROID-SLAM, where the original version failed to build a map or find the camera trajectory, compared to the MINI version that worked better regardless of the challenging light conditions (the sun was facing the camera). On the other hand, since MINI-DROID-SLAM inherits the same BA technique from the old DROID-SLAM, it will suffer from the same drawbacks, such as the loop-closure problem. This problem necessitates a reevaluation of the BA system to enhance its quality and further improve its performance.</p></sec><sec sec-type="conclusions" id="sec7-sensors-25-05448"><title>7. Conclusions</title><p>This paper has presented MINI-DROID-SLAM, an enhanced VSLAM system that refines the deep learning architecture of DROID-SLAM by integrating a compact and efficient MINI-GRU module. Results indicate that the proposed methodology significantly improves training speed and reduces computational complexity, all while maintaining or surpassing the accuracy of the original DROID-SLAM. The system demonstrates robust performance across diverse environments using monocular input, thereby confirming the viability of lightweight models for real-time SLAM applications. Overall, MINI-DROID-SLAM offers a more accessible and efficient solution for Simultaneous Localization and Mapping, particularly for real-world deployments. Future research can extend the current MINI-DROID-SLAM methodology in several key areas. Expanding its support to include multiple sensor modalities and sensor fusion, such as RGB-D and stereo vision, to enhance its versatility. Furthermore, increasing the diversity of training data to encompass a broader range of real-world environments could significantly improve the model&#8217;s generalization capabilities. Moreover, optimizing the model for lower-latency devices and embedded systems by reducing GPU memory consumption is also a crucial direction. Additionally, future work could involve implementing an adaptive learning mechanism for dynamic environments and rigorously evaluating performance under uncertainty and low-light conditions, which would be essential for broader deployment in autonomous vehicle systems. Moreover, explainable AI techniques can be applied to expand the intended flow of the proposed modules.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.E.-S.; Methodology, A.E.-S.; Software, I.A., A.E.-S. and M.A.; Validation, I.A. and M.A.; Investigation, I.A.; Resources, M.A.; Data curation, I.A. and M.A.; Writing&#8212;original draft, I.A.; Writing&#8212;review and editing, A.E.-S.; Visualization, M.A.; Supervision, A.E.-S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original data presented in this study are included in the article. Further inquiries can be directed to the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">MINI-DROID</td><td align="left" valign="middle" rowspan="1" colspan="1">Minimal-Differentiable Recurrent Optimization-Inspired Design</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DL</td><td align="left" valign="middle" rowspan="1" colspan="1">deep learning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NN</td><td align="left" valign="middle" rowspan="1" colspan="1">Neural Networks</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BA</td><td align="left" valign="middle" rowspan="1" colspan="1">Bundle Adjustment</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SLAM</td><td align="left" valign="middle" rowspan="1" colspan="1">Simultaneous Localization and Mapping</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ATE</td><td align="left" valign="middle" rowspan="1" colspan="1">Absolute Trajectory Error</td></tr></tbody></array></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05448"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Engel</surname><given-names>J.</given-names></name><name name-style="western"><surname>Koltun</surname><given-names>V.</given-names></name><name name-style="western"><surname>Cremers</surname><given-names>D.</given-names></name></person-group><article-title>Direct Sparse Odometry</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1607.02565</pub-id><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2658577</pub-id><pub-id pub-id-type="pmid">28422651</pub-id></element-citation></ref><ref id="B2-sensors-25-05448"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>K.</given-names></name></person-group><article-title>An Overview on Visual SLAM: From Tradition to Semantic</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>3010</elocation-id><pub-id pub-id-type="doi">10.3390/rs14133010</pub-id></element-citation></ref><ref id="B3-sensors-25-05448"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kapoor</surname><given-names>A.</given-names></name><name name-style="western"><surname>Scherer</surname><given-names>S.</given-names></name></person-group><article-title>TartanAir: A Dataset to Push the Limits of Visual SLAM</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2003.14338</pub-id><pub-id pub-id-type="arxiv">2003.14338</pub-id></element-citation></ref><ref id="B4-sensors-25-05448"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teed</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name></person-group><article-title>DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2108.10869</pub-id></element-citation></ref><ref id="B5-sensors-25-05448"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teed</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name></person-group><article-title>RAFT: Recurrent All-Pairs Field Transforms for Optical Flow</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2003.12039</pub-id><pub-id pub-id-type="arxiv">2003.12039</pub-id></element-citation></ref><ref id="B6-sensors-25-05448"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lipson</surname><given-names>L.</given-names></name><name name-style="western"><surname>Teed</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name></person-group><article-title>Deep Patch Visual SLAM</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2408.01654</pub-id><pub-id pub-id-type="arxiv">2408.01654</pub-id></element-citation></ref><ref id="B7-sensors-25-05448"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sucar</surname><given-names>E.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ortiz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Davison</surname><given-names>A.J.</given-names></name></person-group><article-title>iMAP: Implicit Mapping and Positioning in Real-Time</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2103.12352</pub-id><pub-id pub-id-type="arxiv">2103.12352</pub-id></element-citation></ref><ref id="B8-sensors-25-05448"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stratton</surname><given-names>P.</given-names></name><name name-style="western"><surname>Garimella</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Saxena</surname><given-names>A.</given-names></name><name name-style="western"><surname>Amutha</surname><given-names>N.</given-names></name><name name-style="western"><surname>Gerami</surname><given-names>E.</given-names></name></person-group><article-title>Volume-DROID: A Real-Time Implementation of Volumetric Mapping with DROID-SLAM</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2306.06850</pub-id><pub-id pub-id-type="doi">10.54364/AAIML.2023.1173</pub-id></element-citation></ref><ref id="B9-sensors-25-05448"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>S.Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name></person-group><article-title>DVI-SLAM: A Dual Visual Inertial SLAM Network</article-title><source>Proceedings of the 2024 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>13&#8211;17 May 2024</conf-date><fpage>12020</fpage><lpage>12026</lpage><pub-id pub-id-type="doi">10.1109/ICRA57147.2024.10610042</pub-id></element-citation></ref><ref id="B10-sensors-25-05448"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bangunharcana</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>K.S.</given-names></name></person-group><article-title>Revisiting the Receptive Field of Conv-GRU in DROID-SLAM</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;20 June 2022</conf-date><fpage>1905</fpage><lpage>1915</lpage><pub-id pub-id-type="doi">10.1109/CVPRW56347.2022.00207</pub-id></element-citation></ref><ref id="B11-sensors-25-05448"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tosi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Mattoccia</surname><given-names>S.</given-names></name><name name-style="western"><surname>Poggi</surname><given-names>M.</given-names></name></person-group><article-title>GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction</article-title><source>Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>3704</fpage><lpage>3714</lpage><pub-id pub-id-type="doi">10.1109/ICCV51070.2023.00345</pub-id></element-citation></ref><ref id="B12-sensors-25-05448"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name></person-group><article-title>A real-time, robust and versatile visual-SLAM framework based on deep learning networks</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2405.03413</pub-id></element-citation></ref><ref id="B13-sensors-25-05448"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pudasaini</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hanif</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Shafique</surname><given-names>M.</given-names></name></person-group><article-title>SPAQ-DL-SLAM: Towards Optimizing Deep Learning-based SLAM for Resource-Constrained Embedded Platforms</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2409.14515</pub-id></element-citation></ref><ref id="B14-sensors-25-05448"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ni</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name></person-group><article-title>A lightweight GRU-based gesture recognition model for skeleton dynamic graphs</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>70545</fpage><lpage>70570</lpage><pub-id pub-id-type="doi">10.1007/s11042-024-18313-w</pub-id></element-citation></ref><ref id="B15-sensors-25-05448"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Luan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name></person-group><article-title>Quality prediction of a fermentation process using multi-scale GRU with hybrid modeling strategy</article-title><source>Control Eng. Pract.</source><year>2025</year><volume>164</volume><fpage>106408</fpage><pub-id pub-id-type="doi">10.1016/j.conengprac.2025.106408</pub-id></element-citation></ref><ref id="B16-sensors-25-05448"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schmidt</surname><given-names>R.M.</given-names></name></person-group><article-title>Recurrent Neural Networks (RNNs): A gentle Introduction and Overview</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1912.05911</pub-id><pub-id pub-id-type="arxiv">1912.05911</pub-id></element-citation></ref><ref id="B17-sensors-25-05448"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maind</surname><given-names>S.B.</given-names></name><name name-style="western"><surname>Wankar</surname><given-names>P.</given-names></name></person-group><article-title>Research paper on basic of artificial neural network</article-title><source>Int. J. Recent Innov. Trends Comput. Commun.</source><year>2014</year><volume>2</volume><fpage>96</fpage><lpage>100</lpage></element-citation></ref><ref id="B18-sensors-25-05448"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group><article-title>LSTM and GRU Neural Network Performance Comparison Study: Taking Yelp Review Dataset as an Example</article-title><source>Proceedings of the 2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI)</source><conf-loc>Shanghai, China</conf-loc><conf-date>12&#8211;14 June 2020</conf-date><fpage>98</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1109/IWECAI50956.2020.00027</pub-id></element-citation></ref><ref id="B19-sensors-25-05448"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bhatt</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>I.</given-names></name><name name-style="western"><surname>Patidar</surname><given-names>P.</given-names></name></person-group><article-title>Leveraging LSTM for Predictive Modeling of Satellite Clock Bias</article-title><source>Proceedings of the 2025 8th International Conference on Data Science and Machine Learning Applications (CDMA)</source><conf-loc>Riyadh, Saudi Arabia</conf-loc><conf-date>16&#8211;17 February 2025</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Riyadh, Saudi Arabia</publisher-loc><year>2025</year><fpage>25</fpage><lpage>30</lpage></element-citation></ref><ref id="B20-sensors-25-05448"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tsantekidis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Passalis</surname><given-names>N.</given-names></name><name name-style="western"><surname>Tefas</surname><given-names>A.</given-names></name></person-group><article-title>Chapter 5&#8212;Recurrent neural networks</article-title><source>Deep Learning for Robot Perception and Cognition</source><person-group person-group-type="editor"><name name-style="western"><surname>Iosifidis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tefas</surname><given-names>A.</given-names></name></person-group><publisher-name>Academic Press</publisher-name><publisher-loc>Orlando, FL, USA</publisher-loc><year>2022</year><fpage>101</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/B978-0-32-385787-1.00010-5</pub-id></element-citation></ref><ref id="B21-sensors-25-05448"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tung</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>M.O.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hajimirsadeghi</surname><given-names>H.</given-names></name></person-group><article-title>Were RNNs All We Needed?</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2410.01201</pub-id><pub-id pub-id-type="arxiv">2410.01201</pub-id></element-citation></ref><ref id="B22-sensors-25-05448"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gulcehre</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>K.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="doi">10.48550/arXiv.1412.3555</pub-id><pub-id pub-id-type="arxiv">1412.3555</pub-id></element-citation></ref><ref id="B23-sensors-25-05448"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mur-Artal</surname><given-names>R.</given-names></name><name name-style="western"><surname>Montiel</surname><given-names>J.M.M.</given-names></name><name name-style="western"><surname>Tard&#243;s</surname><given-names>J.D.</given-names></name></person-group><article-title>ORB-SLAM: A Versatile and Accurate Monocular SLAM System</article-title><source>IEEE Trans. Robot.</source><year>2015</year><volume>31</volume><fpage>1147</fpage><lpage>1163</lpage><pub-id pub-id-type="doi">10.1109/TRO.2015.2463671</pub-id></element-citation></ref><ref id="B24-sensors-25-05448"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Forster</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gassner</surname><given-names>M.</given-names></name><name name-style="western"><surname>Werlberger</surname><given-names>M.</given-names></name><name name-style="western"><surname>Scaramuzza</surname><given-names>D.</given-names></name></person-group><article-title>SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems</article-title><source>IEEE Trans. Robot.</source><year>2017</year><volume>33</volume><fpage>249</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1109/TRO.2016.2623335</pub-id></element-citation></ref><ref id="B25-sensors-25-05448"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zubizarreta</surname><given-names>J.</given-names></name><name name-style="western"><surname>Aguinaga</surname><given-names>I.</given-names></name><name name-style="western"><surname>Montiel</surname><given-names>J.M.M.</given-names></name></person-group><article-title>Direct Sparse Mapping</article-title><source>IEEE Trans. Robot.</source><year>2020</year><volume>36</volume><fpage>1363</fpage><lpage>1370</lpage><pub-id pub-id-type="doi">10.1109/TRO.2020.2991614</pub-id></element-citation></ref><ref id="B26-sensors-25-05448"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Campos</surname><given-names>C.</given-names></name><name name-style="western"><surname>Elvira</surname><given-names>R.</given-names></name><name name-style="western"><surname>Rodriguez</surname><given-names>J.J.G.</given-names></name><name name-style="western"><surname>Montiel</surname><given-names>J.M.M.</given-names></name><name name-style="western"><surname>Tardos</surname><given-names>J.D.</given-names></name></person-group><article-title>ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual&#8211;Inertial, and Multimap SLAM</article-title><source>IEEE Trans. Robot.</source><year>2021</year><volume>37</volume><fpage>1874</fpage><lpage>1890</lpage><pub-id pub-id-type="doi">10.1109/TRO.2021.3075644</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05448-f002" orientation="portrait"><label>Figure 2</label><caption><p>Basic RNN block diagram with the sequence input (X), hidden state (H), output sequence (Y), and weight (W) [<xref rid="B16-sensors-25-05448" ref-type="bibr">16</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g002.jpg"/></fig><fig position="float" id="sensors-25-05448-f003" orientation="portrait"><label>Figure 3</label><caption><p>Single LSTM block diagram with the three utilized gates, input gate, forget gate, and output gate [<xref rid="B19-sensors-25-05448" ref-type="bibr">19</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g003.jpg"/></fig><fig position="float" id="sensors-25-05448-f004" orientation="portrait"><label>Figure 4</label><caption><p>Basic GRU block diagram showing the reset and update gates&#8217; connections.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g004.jpg"/></fig><fig position="float" id="sensors-25-05448-f005" orientation="portrait"><label>Figure 5</label><caption><p>Basic MINI-GRU block diagram that shows the hidden state, input, and output.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g005.jpg"/></fig><fig position="float" id="sensors-25-05448-f006" orientation="portrait"><label>Figure 6</label><caption><p>MINI-DROID-SLAM system with the usage of MINI-CONV-GRU models.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g006.jpg"/></fig><fig position="float" id="sensors-25-05448-f007" orientation="portrait"><label>Figure 7</label><caption><p>Operation update. Where Lr is correlation lookup, MINI-CONV-GRU has no reset gate (r), Pij is projection, and DBA is Dense Bundle Adjustment.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g007.jpg"/></fig><fig position="float" id="sensors-25-05448-f008" orientation="portrait"><label>Figure 8</label><caption><p>MINI-DROID-SLAM system workflow.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g008.jpg"/></fig><fig position="float" id="sensors-25-05448-f009" orientation="portrait"><label>Figure 9</label><caption><p>Building a dense 3D map of the unknown environment and simultaneously localizing using estimated camera positions. The generated trajectory is presented in red.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g009.jpg"/></fig><fig position="float" id="sensors-25-05448-f010" orientation="portrait"><label>Figure 10</label><caption><p>The number of successful trajectories as a function of the Absolute Trajectory Error (ATE) for the output of the trained model on the validation group of the TartanAir dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g010.jpg"/></fig><fig position="float" id="sensors-25-05448-f011" orientation="portrait"><label>Figure 11</label><caption><p>Sample scene from the MAV0 sequence of the EUROC dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g011.jpg"/></fig><fig position="float" id="sensors-25-05448-f012" orientation="portrait"><label>Figure 12</label><caption><p>Generated 3D map and trajectory for the EuRoC MAV0 sequence using MINI-DROID-SLAM model. The generated trajectory is presented in red.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g012.jpg"/></fig><fig position="float" id="sensors-25-05448-f013" orientation="portrait"><label>Figure 13</label><caption><p>Engineering and Technology Building of the University of Bridgeport map from Google Maps.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g013.jpg"/></fig><fig position="float" id="sensors-25-05448-f014" orientation="portrait"><label>Figure 14</label><caption><p>Three-dimensional construction of the Engineering and Technology Building using MINI-DROID-SLAM trained model. The generated trajectory is presented in red.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g014.jpg"/></fig><fig position="float" id="sensors-25-05448-f015" orientation="portrait"><label>Figure 15</label><caption><p>Examples of collected results from several datasets utilized for SLAM tasks. For these samples, a single GPU 3070 RTX running 12 BA iterations has been used: (<bold>a</bold>) Cabinet TUM-RGB stream dataset [<xref rid="B25-sensors-25-05448" ref-type="bibr">25</xref>]. (<bold>b</bold>) Results show the map and trajectory collected from the trained model for the Cabinet example. The generated trajectory is presented in red. (<bold>c</bold>) Abandoned Factory scenario from TartanAir [<xref rid="B3-sensors-25-05448" ref-type="bibr">3</xref>] dataset. (<bold>d</bold>) The result of map building and localization for the Abandoned Factory scenario. The generated trajectory is presented in red.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g015.jpg"/></fig><fig position="float" id="sensors-25-05448-f016" orientation="portrait"><label>Figure 16</label><caption><p>ORB-SLAM3 ATE on Kitti 01 sequence = 661.87.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g016.jpg"/></fig><fig position="float" id="sensors-25-05448-f017" orientation="portrait"><label>Figure 17</label><caption><p>MINI-DROID-SLAM ATE on Kitti 01 sequence = 56.24.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05448-g017.jpg"/></fig><table-wrap position="float" id="sensors-25-05448-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05448-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison between GRU and MINI-GRU.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feature</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">GRU [<xref rid="B22-sensors-25-05448" ref-type="bibr">22</xref>]</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MINI-GRU</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Gates in single block</td><td align="left" valign="middle" rowspan="1" colspan="1">Update gate (<inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and reset gate (<italic toggle="yes">r</italic>)</td><td align="left" valign="middle" rowspan="1" colspan="1">Only update gate, reset gate removed</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Parameters and gates</td><td align="left" valign="middle" rowspan="1" colspan="1">Has many parameters</td><td align="left" valign="middle" rowspan="1" colspan="1">Fewer parameters</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Processing</td><td align="left" valign="middle" rowspan="1" colspan="1">Slow</td><td align="left" valign="middle" rowspan="1" colspan="1">Fast</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Memory usage</td><td align="left" valign="middle" rowspan="1" colspan="1">Uses much memory during training</td><td align="left" valign="middle" rowspan="1" colspan="1">Uses less memory during training</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cases usage</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep learning (DL) methods, DL-SLAM in regular methods</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep learning (DL) methods, DL-SLAM with low weights methods</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05448-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05448-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of different deep learning techniques used in VSLAM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Pros</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cons</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Realtime</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CONV-GRU</td><td align="center" valign="middle" rowspan="1" colspan="1">Excellent accuracy, less speed</td><td align="center" valign="middle" rowspan="1" colspan="1">high Complexity</td><td align="center" valign="middle" rowspan="1" colspan="1">Better time complexity than LSTM</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MINI-CONV-GRU</td><td align="center" valign="middle" rowspan="1" colspan="1">Excellent accuracy, high speed</td><td align="center" valign="middle" rowspan="1" colspan="1">Less complexity</td><td align="center" valign="middle" rowspan="1" colspan="1">Better on real-time applications</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Basic NN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fastest with fine accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lower accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Best for real-time applications</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05448-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05448-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of trajectory errors (ATE in meters) across sequences for different scenarios in monocular SLAM methods. Results are collected from the TartanAir monocular benchmark dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH000</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH001</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH002</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH003</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH004</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH005</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH006</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH007</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>(Classic)</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ORB-SLAM [<xref rid="B23-sensors-25-05448" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.73</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>(DL-Method)</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DROID-SLAM3 [<xref rid="B4-sensors-25-05448" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">1.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MINI-DROID-SLAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05448-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05448-t004_Table 4</object-id><label>Table 4</label><caption><p>Absolute Trajectory Error (ATE) comparison on the EuRoC dataset (lower is better).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH01</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH02</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH03</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH04</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MH05</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">V101</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">V102</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">V103</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">V201</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">V202</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">V203</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>(Classic)</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSO [<xref rid="B1-sensors-25-05448" ref-type="bibr">1</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.046</td><td align="center" valign="middle" rowspan="1" colspan="1">0.046</td><td align="center" valign="middle" rowspan="1" colspan="1">0.172</td><td align="center" valign="middle" rowspan="1" colspan="1">3.810</td><td align="center" valign="middle" rowspan="1" colspan="1">0.110</td><td align="center" valign="middle" rowspan="1" colspan="1">0.089</td><td align="center" valign="middle" rowspan="1" colspan="1">0.107</td><td align="center" valign="middle" rowspan="1" colspan="1">0.903</td><td align="center" valign="middle" rowspan="1" colspan="1">0.044</td><td align="center" valign="middle" rowspan="1" colspan="1">0.132</td><td align="center" valign="middle" rowspan="1" colspan="1">1.152</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SVO [<xref rid="B24-sensors-25-05448" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.100</td><td align="center" valign="middle" rowspan="1" colspan="1">0.120</td><td align="center" valign="middle" rowspan="1" colspan="1">0.410</td><td align="center" valign="middle" rowspan="1" colspan="1">0.430</td><td align="center" valign="middle" rowspan="1" colspan="1">0.300</td><td align="center" valign="middle" rowspan="1" colspan="1">0.070</td><td align="center" valign="middle" rowspan="1" colspan="1">0.210</td><td align="center" valign="middle" rowspan="1" colspan="1">X</td><td align="center" valign="middle" rowspan="1" colspan="1">0.110</td><td align="center" valign="middle" rowspan="1" colspan="1">0.110</td><td align="center" valign="middle" rowspan="1" colspan="1">1.080</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSM [<xref rid="B25-sensors-25-05448" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.036</td><td align="center" valign="middle" rowspan="1" colspan="1">0.055</td><td align="center" valign="middle" rowspan="1" colspan="1">0.057</td><td align="center" valign="middle" rowspan="1" colspan="1">0.067</td><td align="center" valign="middle" rowspan="1" colspan="1">0.067</td><td align="center" valign="middle" rowspan="1" colspan="1">0.095</td><td align="center" valign="middle" rowspan="1" colspan="1">0.059</td><td align="center" valign="middle" rowspan="1" colspan="1">0.076</td><td align="center" valign="middle" rowspan="1" colspan="1">0.056</td><td align="center" valign="middle" rowspan="1" colspan="1">0.057</td><td align="center" valign="middle" rowspan="1" colspan="1">0.784</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ORB-SLAM3 [<xref rid="B26-sensors-25-05448" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.027</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.028</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.138</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.072</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.033</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.015</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.033</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.023</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.029</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>(DL-Method)</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DROID-SLAM [<xref rid="B4-sensors-25-05448" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.013
</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.014</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.022</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.043</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.043</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.037</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.012</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.020</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.017</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.013</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.014</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MINI-DROID-SLAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.013</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.014</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.022</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.043</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.043</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.037</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.012</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.020</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.017</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.013</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.014</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05448-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05448-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of different SLAM techniques applied to the monocular TUM-RGB dataset. The numbers represent ATE (m) for various sequences (lower is better).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">360</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Desk</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Desk2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Floor</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Plant</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Room</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Rpy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Teddy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Xyz</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>(Classic)</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ORB-SLAM3 [<xref rid="B26-sensors-25-05448" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.017
</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.210</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.034</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">X</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.009</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>(DL-Method)</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DROID-SLAM [<xref rid="B4-sensors-25-05448" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.111</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.018</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.042</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.021</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.016</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.049</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.026</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.048</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.012</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MINI-DROID-SLAM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.111</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.018</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.042</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.021</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.016</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.049</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.026</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.048</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.012</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05448-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05448-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of DROID-SLAM, DPV-SLAM, and MINI-DROID-SLAM (ours).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feature</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DROID-SLAM [<xref rid="B4-sensors-25-05448" ref-type="bibr">4</xref>]</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DPV-SLAM [<xref rid="B6-sensors-25-05448" ref-type="bibr">6</xref>]</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MINI-DROID-SLAM (Ours)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>Method type</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Full SLAM</td><td align="left" valign="middle" rowspan="1" colspan="1">Visual odometry (VO)</td><td align="left" valign="middle" rowspan="1" colspan="1">Full SLAM</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>Computational cost</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">High GPU usage</td><td align="left" valign="middle" rowspan="1" colspan="1">Lower GPU usage</td><td align="left" valign="middle" rowspan="1" colspan="1">Lower GPU usage</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>Accuracy</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">High accuracy</td><td align="left" valign="middle" rowspan="1" colspan="1">High in VO</td><td align="left" valign="middle" rowspan="1" colspan="1">High and efficient</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>Scalability</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Not real-time application</td><td align="left" valign="middle" rowspan="1" colspan="1">Suitable for real-time</td><td align="left" valign="middle" rowspan="1" colspan="1">Better in real-time</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>Primary goal</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Full SLAM achieved</td><td align="left" valign="middle" rowspan="1" colspan="1">Efficient VO</td><td align="left" valign="middle" rowspan="1" colspan="1">Full SLAM achieved</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<bold>Machine setup</bold>
</td><td align="left" valign="middle" rowspan="1" colspan="1">4&#215; GPU 3090</td><td align="left" valign="middle" rowspan="1" colspan="1">1&#215; GPU 3090</td><td align="left" valign="middle" rowspan="1" colspan="1">1&#215; GPU 3090</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Training time</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8764;7 days</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Depends (avg 1&#8211;3 days)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8764;5 days</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>