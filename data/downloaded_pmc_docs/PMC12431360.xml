<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431360</article-id><article-id pub-id-type="pmcid-ver">PMC12431360.1</article-id><article-id pub-id-type="pmcaid">12431360</article-id><article-id pub-id-type="pmcaiid">12431360</article-id><article-id pub-id-type="doi">10.3390/s25175493</article-id><article-id pub-id-type="publisher-id">sensors-25-05493</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Center-of-Gravity-Aware Graph Convolution for Unsafe Behavior Recognition of Construction Workers</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9411-2721</contrib-id><name name-style="western"><surname>Jin</surname><given-names initials="P">Peijian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-8531-3198</contrib-id><name name-style="western"><surname>Guo</surname><given-names initials="S">Shihao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="c1-sensors-25-05493" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-6363-1140</contrib-id><name name-style="western"><surname>Li</surname><given-names initials="C">Chaoqun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Yitzhaky</surname><given-names initials="Y">Yitzhak</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05493">School of Emergency Science and Engineering, Jilin Jianzhu University, 5088 Xincheng Avenue, Nanguan District, Changchun 130119, China</aff><author-notes><corresp id="c1-sensors-25-05493"><label>*</label>Correspondence: <email>a1913944184@outlook.com</email></corresp></author-notes><pub-date pub-type="epub"><day>04</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5493</elocation-id><history><date date-type="received"><day>25</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>06</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 13:25:28.783"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05493.pdf"/><abstract><p>Falls from height are a critical safety concern in the construction industry, underscoring the need for effective identification of high-risk worker behaviors near hazardous edges for proactive accident prevention. This study aimed to address this challenge by developing an improved action recognition model. We propose a novel dynamic spatio-temporal graph convolutional network (CoG-STGCN) that incorporates a center of gravity (CoG)-aware mechanism. The method computes global and local CoG using anthropometric priors and extracts four key dynamic CoG features, which a Multi-Layer Perceptron (MLP) then uses to generate modulation weights that dynamically adjust the skeleton graph&#8217;s adjacency matrix, enhancing sensitivity to stability changes. On a self-constructed dataset of eight typical edge-related hazardous behaviors, CoG-STGCN achieved a Top-1 accuracy of 95.83% (baseline ST-GCN: 93.75%) and an average accuracy of 94.17% in fivefold cross-validation (baseline ST-GCN: 92.91%), with significant improvements in recognizing actions involving rapid CoG shifts. The CoG-STGCN provides a more effective and physically informed approach for intelligent unsafe behavior recognition and early warning in built environments.</p></abstract><kwd-group><kwd>human action recognition</kwd><kwd>construction safety</kwd><kwd>spatio-temporal graph convolutional network</kwd></kwd-group><funding-group><award-group><funding-source>Early Warning Research on Multi-source Data Fusion of Gas-containing Coal and Rock Dynamic Hazards</funding-source><award-id>YDZJ202101ZYTS146</award-id></award-group><funding-statement>This research was funded by Early Warning Research on Multi-source Data Fusion of Gas-containing Coal and Rock Dynamic Hazards, grant number YDZJ202101ZYTS146. The APC was also funded by this same project.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05493"><title>1. Introduction</title><p>The construction industry suffers from a high incidence of accidents, with falls from height being particularly frequent and lethal [<xref rid="B1-sensors-25-05493" ref-type="bibr">1</xref>]. Beyond inadequate safety facilities, a primary cause is the unsafe behavior of workers. Current reliance on manual safety inspections is insufficient, hindered by limited coverage, poor real-time performance, and subjectivity. Therefore, developing technologies for real-time identification and intervention of such hazardous behaviors is crucial for improving construction safety.</p><p>Early technological solutions relied on traditional methods. For instance, Lim et al. [<xref rid="B2-sensors-25-05493" ref-type="bibr">2</xref>] used BLE and RFID sensors to monitor worker location and movement, while Zhu et al. [<xref rid="B3-sensors-25-05493" ref-type="bibr">3</xref>] employed visual detection with HOG features for tracking. While effective for specific tasks, these early approaches often lacked real-time processing capabilities and suffered from poor generalization. Recent advances in deep learning offer new solutions through video-based human action recognition (HAR). HAR primarily utilizes two data modalities: RGB images and skeleton data. Although information-rich, RGB-based methods lack robustness in complex environments like construction sites and are computationally intensive, hindering real-time performance [<xref rid="B4-sensors-25-05493" ref-type="bibr">4</xref>]. In contrast, skeleton-based recognition focuses on human motion, making it resilient to background and lighting variations. This modality effectively captures temporal dynamics and naturally represents the human body as a topological graph, with joints as nodes and bones as edges. The pioneering ST-GCN model by Yan et al. [<xref rid="B5-sensors-25-05493" ref-type="bibr">5</xref>] successfully applied Graph Convolutional Networks (GCNs) to this domain, inspiring much subsequent research and demonstrating strong performance in HAR tasks, including applications in building safety [<xref rid="B6-sensors-25-05493" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05493" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05493" ref-type="bibr">8</xref>].</p><p>However, existing GCN-based methods have key limitations when applied to fall risk detection. Falls are intrinsically linked to a loss of balance and shifts in the body&#8217;s center of gravity (CoG). Most models rely on predefined skeletal connections or learn feature-level correlations, failing to explicitly model the dynamics of the CoG. This oversight hinders their ability to detect subtle motion patterns indicative of instability. To address this limitation, we propose the center-of-gravity-aware spatio-temporal graph convolutional network (CoG-STGCN). Its core innovation is a dynamic adjacency matrix that adjusts joint connection strengths based on real-time estimates of global and local CoG positions and velocities. This allows CoG-STGCN to more accurately capture motion features related to equilibrium changes, thus improving the identification of unsafe behaviors that may lead to falls.</p><p>The main contributions of this paper include the following:<list list-type="order"><list-item><p>We propose a novel center-of-gravity-aware spatio-temporal graph convolutional network (CoG-STGCN). The model addresses the shortcomings of existing methods by explicitly incorporating CoG dynamics into the graph construction process.</p></list-item><list-item><p>We conduct a systematic study to identify unsafe behaviors that lead to fall accidents in high-risk areas of construction sites (e.g., floor edges, openings).</p></list-item><list-item><p>We identify, define, and categorize a set of high-risk unsafe behaviors based on their impact on human balance. This provides a specific basis for risk assessment in such scenarios.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05493"><title>2. Related Works</title><sec id="sec2dot1-sensors-25-05493"><title>2.1. Human Pose Estimation Model</title><p>Human pose estimation (HPE) methodologies are predominantly categorized into two main paradigms: bottom-up and top-down. The bottom-up approach begins by detecting all human keypoints across the entire image. Subsequently, techniques like Part Affinity Fields (PAFs) are employed to associate these keypoints, grouping them to form complete individual skeletons. Representative models adopting this strategy include OpenPose [<xref rid="B9-sensors-25-05493" ref-type="bibr">9</xref>], HigherHRNet [<xref rid="B10-sensors-25-05493" ref-type="bibr">10</xref>], and PersonLab [<xref rid="B11-sensors-25-05493" ref-type="bibr">11</xref>]. While this approach offers advantages in speed for crowded, multi-person scenes, its accuracy and performance can be suboptimal in scenarios with fewer individuals compared to top-down methods.</p><p>In contrast, the top-down strategy first detects persons in the image using an object detector (e.g., Faster R-CNN [<xref rid="B12-sensors-25-05493" ref-type="bibr">12</xref>], YOLO [<xref rid="B13-sensors-25-05493" ref-type="bibr">13</xref>], SSD [<xref rid="B14-sensors-25-05493" ref-type="bibr">14</xref>]) to generate bounding boxes for each individual. A single-person pose estimator is then applied within each bounding box to predict the keypoints. Notable algorithms in this category include AlphaPose [<xref rid="B15-sensors-25-05493" ref-type="bibr">15</xref>], HRNet [<xref rid="B16-sensors-25-05493" ref-type="bibr">16</xref>], and Mask R-CNN [<xref rid="B17-sensors-25-05493" ref-type="bibr">17</xref>]. This two-stage process generally yields high accuracy, but it suffers from slower inference speeds in multi-person settings, and its performance is inherently dependent on the initial human detection stage.</p><p>To overcome the trade-offs between speed and accuracy inherent in these two paradigms, researchers have sought to develop more streamlined, end-to-end solutions. Inspired by the success of single-stage object detectors like the YOLO (You Only Look Once) family, models such as YOLO-Pose [<xref rid="B18-sensors-25-05493" ref-type="bibr">18</xref>] have emerged. The core concept is to unify human detection and keypoint prediction into a single, deep neural network. This network performs a single forward pass to directly output bounding boxes and their corresponding pose information for all detected persons. This end-to-end architecture eliminates the bottlenecks of multi-stage pipelines. Specifically, YOLOv8-Pose has demonstrated performance that matches or exceeds previous methods in both accuracy and speed, particularly in multi-person scenarios. Its simplicity and efficiency make it a state-of-the-art front-end solution for applications requiring real-time, accurate skeleton data extraction, such as the behavioral analysis on construction sites targeted in our research.</p></sec><sec id="sec2dot2-sensors-25-05493"><title>2.2. Skeleton-Based Human Action Recognition</title><p>Early research in skeleton-based action recognition heavily relied on hand-crafted features. These features, such as the relative positions, angles, and velocities of joints, were manually designed and fed into traditional machine learning classifiers. However, these methods required significant domain expertise, exhibited limited generalization capabilities, and struggled to capture the complex spatio-temporal dynamics embedded in human actions.</p><p>With the proliferation of deep learning, research shifted towards methods that could automatically learn features from raw skeleton data. Initial attempts utilized Recurrent Neural Networks (RNNs) and variants like LSTM and GRU to model the temporal dynamics of action sequences [<xref rid="B19-sensors-25-05493" ref-type="bibr">19</xref>]. While effective at capturing temporal dependencies, RNNs are deficient in modeling the inherent spatial graph structure of the human skeleton. Another approach involved transforming skeleton data into pseudo-images, allowing for feature extraction with Convolutional Neural Networks (CNNs) [<xref rid="B20-sensors-25-05493" ref-type="bibr">20</xref>]. Although CNNs are powerful for image processing, forcing skeleton data&#8212;which have a natural graph structure&#8212;into a grid-like format can disrupt or discard the intrinsic physical and kinematic relationships between joints, leading to information loss.</p><p>To more effectively process the non-Euclidean structure of skeleton data, Graph Convolutional Networks (GCNs) were introduced and have since become the dominant paradigm. The seminal ST-GCN model innovatively represents a skeleton sequence as a spatio-temporal graph, where nodes correspond to joints and edges represent both spatial connections within a frame and temporal connections across frames. By alternately applying spatial and temporal convolutions, the model learns both spatial configuration and temporal dynamics simultaneously.</p><p>The success of ST-GCN has spurred extensive follow-up research aimed at enhancing its performance, with a significant focus on optimizing the graph structure. The original ST-GCN uses a fixed, predefined adjacency matrix, which limits its ability to capture dynamic, action-specific inter-joint relationships. To address this, subsequent works have proposed data-driven adaptive graph structures. For example, 2s-AGCN [<xref rid="B21-sensors-25-05493" ref-type="bibr">21</xref>] learns an adaptive adjacency matrix to complement the physical skeleton graph, enabling the model to discover latent joint correlations. MS-G3D [<xref rid="B22-sensors-25-05493" ref-type="bibr">22</xref>] introduced a multi-scale graph convolution module to capture dependencies at different ranges, while CTR-GCN [<xref rid="B23-sensors-25-05493" ref-type="bibr">23</xref>] proposed a channel-wise topology modeling module for finer-grained interactions.</p><p>Despite achieving excellent performance, these advanced adaptive graph methods often overlook fundamental physical principles governing human motion. Specifically, they lack an explicit mechanism to account for the body&#8217;s center of gravity, a critical factor for maintaining balance and stability. This omission is a notable limitation, especially for recognizing high-risk behaviors like falls. Therefore, this paper aims to improve upon the ST-GCN framework by introducing a center-of-gravity-aware mechanism, designed to more effectively identify potential fall-risk behaviors.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05493"><title>3. Methods</title><sec id="sec3dot1-sensors-25-05493"><title>3.1. Overall Framework</title><p>The comprehensive architecture of our proposed system, designed to process video input through to final behavior classification, is illustrated in <xref rid="sensors-25-05493-f001" ref-type="fig">Figure 1</xref>. The workflow begins with raw video clips from on-site surveillance cameras, which are fed into the YOLOv8-Pose front-end module. This module efficiently generates a precise 2D human skeleton sequence for each frame. To capture the temporal dynamics of actions, the sequential skeleton data are then passed to our improved CoG-STGCN model. Herein lies our core innovation: the model first calculates the center of gravity (CoG) from the input skeletons and extracts CoG-related features. These features are processed by a Multi-Layer Perceptron (MLP) to generate a CoG-aware weight matrix, which is then combined with the base graph structure to create a dynamic adjacency matrix. This dynamic graph is subsequently used within a series of ST-GCN blocks to learn spatio-temporal features that are highly sensitive to body stability. The framework concludes with a Softmax classifier that outputs the final action classification.</p></sec><sec id="sec3dot2-sensors-25-05493"><title>3.2. YOLOv8-Pose</title><p>In order to efficiently and accurately extract the key points of the human skeleton from the input video stream, this paper chooses to use a single-stage pose estimation algorithm, YOLOv8-Pose, whose processing flow of the image is shown in <xref rid="sensors-25-05493-f002" ref-type="fig">Figure 2</xref>.</p><p>Ultralytics provides pre-training models with different computational complexities and prediction accuracies for YOLOv8-Pose, and we choose a pre-training model for YOLOv8-Pose, taking into account the possibility of future deployment on resource-constrained edge devices and the priority of ensuring real-time processing. Considering the potential for future deployment on resource-constrained edge devices and prioritizing real-time processing, we chose the YOLOv8s-pose.pt pre-training model. The model is able to directly output the bounding box of each detected human instance as well as the 2D coordinates and confidence level of the standard 17 keypoints of COCO. In order to adapt to the input of the ST-GCN model, which usually requires 18 keypoints of the human skeleton, we generate the 18th keypoint, i.e., the joints of the neck, based on the keypoints of the left and right shoulders in the COCO format, and the distribution of the joints of the skeleton in the model is shown in <xref rid="sensors-25-05493-f003" ref-type="fig">Figure 3</xref>. To process continuous video frames and generate individual motion sequences, we utilized the integrated tracking functionality provided by the Ultralytics YOLOv8 framework and chose ByteTrack as the underlying tracking algorithm. <xref rid="sensors-25-05493-t001" ref-type="table">Table 1</xref> compares the performance of some mainstream pose estimation models, and YOLOv8s-pose is a more lightweight model while maintaining a high AP@50 (85.8%). It is worth noting that the total system overhead of AlphaPose as a Top-Down method also needs to account for the additional human body detector, whereas YOLOv8s-pose is an all-in-one design.</p></sec><sec id="sec3dot3-sensors-25-05493"><title>3.3. The Proposed COG-STGCN Model</title><sec id="sec3dot3dot1-sensors-25-05493"><title>3.3.1. ST-GCN Model</title><p>In this study, we improve the ST-GCN model to enhance the recognition of dangerous behaviors in the adjacent area of a construction site. ST-GCN models the human skeleton sequence as a spatio-temporal graph G = (V, E), where node V represents the human body joints, and the edge E represents the skeletal connections and the interframe connections. ST-GCN learns spatial-temporal features by alternately applying spatial and temporal graph convolutions. The working principle and flow of the model are shown in <xref rid="sensors-25-05493-f004" ref-type="fig">Figure 4</xref>, which receives the time series data of the skeleton joint points and constructs them into a spatio-temporal graph, then learns the spatial structural features and temporal dynamic features of the actions alternately through multiple spatio-temporal graph convolution modules, and finally aggregates the information through global pooling and outputs the final action categories using classifiers.</p><p>The spatial graph convolution operation of this model can be formulated as<disp-formula id="FD1-sensors-25-05493"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#923;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#923;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the input and output features, respectively. <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the kernel size in the spatial dimension, which is set to 3, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the learnable weight matrix, and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the predefined adjacency matrix representing the graph&#8217;s connectivity structure. <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#923;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the corresponding degree matrix, where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 0.001 to avoid empty rows in <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is an attention map used to represent the importance of each vertex. As can be seen from Equation (1), the ST-GCN model utilizes a fixed, predefined adjacency matrix <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. This matrix is constructed based on the physical connections of the human body and a spatial partitioning strategy, and it remains static for all input samples. This makes it difficult to adaptively adjust the graph structure and connection strengths according to the dynamic characteristics of the actions themselves. Consequently, a key limitation of ST-GCN in capturing specific complex actions lies in its use of a fixed, unchanging graph structure.</p></sec><sec id="sec3dot3dot2-sensors-25-05493"><title>3.3.2. Center-of-Gravity-Based Improvement to the ST-GCN Model</title><p>In order to overcome the limitations of the ST-GCN model described in the previous section and enable it to capture real-time motion information more adequately, a dynamic adaptive graph structure updating rule needs to be found. In the field of biomechanics, the center of gravity of the human body is widely regarded as one of the core parameters for analyzing postural control and movement dynamics [<xref rid="B24-sensors-25-05493" ref-type="bibr">24</xref>], which not only represents the centralized point of body mass distribution, but also the dynamic changes of its position, velocity, and acceleration are a high degree of generalization of the individual&#8217;s overall movement state and postural stability. For example, as shown in <xref rid="sensors-25-05493-f005" ref-type="fig">Figure 5</xref>, the center of gravity changes with the body posture, and the center of gravity of the character falls continuously in vertical height. The trajectory pattern of the CoG thus contains rich discriminative information. Therefore, we leverage the CoG, a physically meaningful metric, as the basis for dynamically adjusting the graph structure. Considering the close relationship between the position of the center of gravity and the position of each node of the human body, we adopt the a priori knowledge of anthropometry and weighted average of each part of the human body to introduce this information of CoG into the model. Specifically, we calculate the global center of gravity at each frame of the video sequence as follows:<disp-formula id="FD2-sensors-25-05493"><label>(2)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the 2D coordinate of the <italic toggle="yes">v</italic>-th keypoint at frame <italic toggle="yes">t</italic>. <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the estimated CoG at frame <italic toggle="yes">t</italic>, calculated as a weighted sum of the keypoint coordinates. <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the mass weight assigned to the <italic toggle="yes">v</italic>-th keypoint. The weights were selected with reference to the percentage of mass of human segments determined in anthropometric and biomechanical studies (<xref rid="sensors-25-05493-t002" ref-type="table">Table 2</xref>), and we approximated the mass share of segments such as head, torso, left/right arm, left/right leg, etc., by assigning them to the articulation points of the skeleton that best represent these segments, which is also reflected in <xref rid="sensors-25-05493-f005" ref-type="fig">Figure 5</xref>. These specific body mass distribution data references are derived from the segmental inertia parameters proposed by de Leva et al. [<xref rid="B25-sensors-25-05493" ref-type="bibr">25</xref>] after adapting and summarizing classical anthropometric data, which are now widely used in the field of biomechanics.</p><p>In addition to calculating the global center of gravity, which represents the mass distribution of the whole body, in order to capture the motion state of each core region of the human body, we also designed a localized core center of gravity calculation method, and this localized center of gravity can help decouple the motion contribution of the torso and limbs and provide richer information. Considering the relatively even distribution of mass in the core region, its specific calculation formula is as follows:<disp-formula id="FD3-sensors-25-05493"><label>(3)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munder><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the 2D coordinate of the <italic toggle="yes">v</italic>-th keypoint belonging to the core keypoint subset <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> at frame <italic toggle="yes">t</italic>, and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of keypoints in the core region; for instance, the left arm region consists of three keypoints. <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the 2D coordinate of the local CoG at frame <italic toggle="yes">t</italic>, which is obtained by calculating the arithmetic mean of all keypoints within the subset <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The specific distribution of local centroids and the corresponding keypoint selections are detailed in <xref rid="sensors-25-05493-t003" ref-type="table">Table 3</xref>.</p><p>With a formula that can approximately represent the position of the body&#8217;s center of gravity, in order to maximize the extraction of the information carried by the CoG, we performed feature construction based on the CoG, and obtained a total of four features: the distance of the node to the global center of gravity (f1), the rate of movement of the global center of gravity (f2), the distance of the node to the local center of gravity (f3), and the rate of movement of the local center of gravity (f4). Two distance features (f1, f3) are used to describe the spatial relationship between the node and the global and local regions, and two rate features (f2, f4) are used to describe the motion intensity of the body as a whole and the local region. In order to provide the subsequent dynamic graph modulation mechanism with more stable motion information over a certain time scale and to avoid the noise effect of transient frames, we adopt a feature extraction strategy similar to that of a time window. Specifically, we set a time window of length T frames, and we set T = 30 frames, and the center-of-gravity-related features described subsequently are all based on the aggregated features computed from the skeleton data within such a time window. The formulas for each feature are shown below:<disp-formula id="FD4-sensors-25-05493"><label>(4)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Dist</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-05493"><label>(5)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mi mathvariant="italic">Dist</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (4) defines a method for calculating the Euclidean distance between two points: <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Equation (5) calculates the average Euclidean distance from the <italic toggle="yes">v</italic>-th joint point <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> to the global center of gravity <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> of the same frame within the time window of the current T-frame, where <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents a certain frame index within the time window.<disp-formula id="FD6-sensors-25-05493"><label>(6)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05493"><label>(7)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo></mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (6) calculates the average Euclidean distance from the <italic toggle="yes">v</italic>-th joint point <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> of each core region to the local center of gravity <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> of that core region throughout the segment to capture the average relative spatial relationship between the parts within the core region of the body. <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, shown in Equation (7), is a combination of the average distance feature <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> from the joints of each core region within the window to its corresponding local center of gravity.<disp-formula id="FD8-sensors-25-05493"><label>(8)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (8) calculates the average rate of movement of the global center of gravity <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, throughout the action segment, i.e., the magnitude of the global center of gravity displacements between each frame is summed and time averaged to measure the average degree of fastness of the overall body movement.<disp-formula id="FD9-sensors-25-05493"><label>(9)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05493"><label>(10)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>4,1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>4,2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>)</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo></mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>l</mml:mi><mml:mrow><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (9) calculates the magnitude of the average relative velocity of the local center of gravity <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of a particular core region moving with respect to the global center of gravity <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> throughout the entire action segment to capture the average internal rate of motion of that core region with respect to the whole body overall motion. <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, shown in Equation (10), consists of a combination of different core region local center of gravity relative rate features <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Finally, for each joint, the feature vector of its input MLP is formed by stitching the four sets of previously computed features sequentially in feature dimensions. Thus, <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a composite feature vector of dimension 2 + 2L, which can be expressed by Equation (11):<disp-formula id="FD11-sensors-25-05493"><label>(11)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This feature vector enables the MLP to perceive the overall body dynamics and spatial configuration of the input sample. Through backpropagation, the optimizer fine-tunes the network to understand the complex, non-linear relationships within the CoG features. The final output of the MLP is a tensor of shape (N, V, M), which generates a specific weight, w, for each node v. This weight signifies the degree to which the connections associated with that node should be preserved or weakened during the dynamic graph adjustment. This computation is performed independently for each person and sample, ensuring its applicability in multi-person scenarios.</p><p>The process of generating this dynamic adjacency matrix is detailed in <xref rid="sensors-25-05493-f006" ref-type="fig">Figure 6</xref>. As illustrated in the figure, we define and compute two types of CoG from the input skeleton data: a global center of gravity (GlobalCoG), which represents the overall body balance, and a core center of gravity (CoreCoG), which focuses on the stability of the torso and trunk. Based on these two reference points, we extract four key features for each node: (f1) the distance to the GlobalCoG, (f2) the velocity of the GlobalCoG, (f3) the distance to the CoreCoG, and (f4) the velocity of the CoreCoG. These features form the input to the MLP, which then learns to generate the node-specific modulation weights, <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The final dynamic adjacency matrix, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is then obtained by an element-wise product (Hadamard product) of the k-th subset of the base adjacency matrix, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and the learned modulation weights:<disp-formula id="FD12-sensors-25-05493"><label>(12)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This dynamically generated, sample-specific adjacency matrix is then used in the improved spatial graph convolution operation, which can be expressed as follows:<disp-formula id="FD13-sensors-25-05493"><label>(13)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#923;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#923;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>)</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8857;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>It is worth noting that the overall architecture of CoG-STGCN follows the standard stacking pattern of multiple ST-GCN blocks. Our core improvement is the dynamic processing of the graph structure within each block. In scenarios where one or several keypoints required for CoG computation are occluded, the system degrades to using only the underlying fixed graph structure. This ensures a baseline level of performance and maintains the model&#8217;s robustness, preventing complete failure when faced with partial input data.</p><p>The specific process is shown in <xref rid="sensors-25-05493-f007" ref-type="fig">Figure 7</xref>. The global center of gravity and each core center of gravity are computed from the input data, i.e., the 18-node skeleton sequence (N, C, T, V, M). Based on this center of gravity, we further extract four key CoG-related features), which together describe the overall stability and movement characteristics of the human body. The four features are fed into a multilayer perceptron (MLP) to learn the modulation weights W<sub>mlp</sub> of the output nodes, which are expanded and shaped as (N, 1, V, 1, M) for tuning each node. The learned W<sub>mlp</sub> is multiplied element-by-element with the underlying adjacency matrix Ak. This step generates the sample specific and dynamically tuned adjacency matrix A<sub>cog,k</sub> simplified and shaped as (V, V). This dynamically generated A<sub>cog,k</sub> is then used directly in the spatial graph convolution (GCN) module within the ST-GCN cell, which performs a graph convolution operation with the data input to the GCN module, followed by a temporal convolutional network module, and the result is summed up with the residual features, and finally passes through an activation function that constitutes the complete output of an ST-GCN cell. After the features have been extracted and abstracted layer by layer by such multiple dynamically tuned ST-GCN cells, the node dimensional information is aggregated by Global Average Pooling and finally fed into the Fully Connected Layer and Softmax function for action classification.</p><p>In order to verify the validity of the improvements of the CoG-STGCN model with respect to the baseline approach, we will detail the experimental setup, the dataset used, and the specific evaluation results in the next section.</p></sec></sec></sec><sec id="sec4-sensors-25-05493"><title>4. Experiments and Result Analysis</title><sec id="sec4dot1-sensors-25-05493"><title>4.1. Data Collection</title><p>Given the scarcity of public datasets for worker behavior on construction sites, we constructed a custom dataset focused on high-risk activities performed near edges to validate our proposed model. Drawing from the literature review [<xref rid="B26-sensors-25-05493" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05493" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05493" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05493" ref-type="bibr">29</xref>] and field observations, we categorized high-frequency unsafe behaviors into three main types: movements with significant center of gravity (CoG) displacement, unstable postures, and distracted attention. Consequently, our dataset comprises six unsafe actions: running, jumping, quick squat, getting up quickly, squatting (long-time), and phone-walking. For contrast, two common safe behaviors, standing and walking, were also included. A detailed description of each action is provided in <xref rid="sensors-25-05493-t004" ref-type="table">Table 4</xref>.</p><p>To create the dataset, we recruited 10 volunteers, each performing 6 repetitions of the 8 specified actions. The actions were recorded using a standard mobile phone camera (1080 &#215; 1920 resolution, 30 fps, OnePlus Ace 2, OnePlus, Shenzhen, China) and subsequently segmented into individual clips. This process yielded a total of 480 raw video clips, with an average duration of 3.42 s per clip. Care was taken to ensure that the subject&#8217;s keypoints remained unobstructed throughout each video. Illustrative examples of the collected action samples are shown in <xref rid="sensors-25-05493-f008" ref-type="fig">Figure 8</xref>. The clips for each action class were organized into separate folders to facilitate balanced data splitting.</p><p>To ensure a robust evaluation of our model&#8217;s generalization capability, we employed two distinct experimental protocols:<list list-type="order"><list-item><p>Fixed Train-Validation-Test Split: This protocol was used for initial model development and hyperparameter tuning. The 480 video clips were first partitioned into a training set (384 clips, 80%), a validation set (48 clips, 10%), and a test set (48 clips, 10%). Subsequently, data augmentation techniques, including horizontal flipping and random scaling, were applied exclusively to the training set, expanding it from 384 to 1152 samples. This strategy provides a consistent and static test bed for comparing model performance during the development phase.</p></list-item><list-item><p>Five-Fold Cross-Validation: To obtain a more reliable and generalized assessment of the final model&#8217;s performance, we employed a five-fold cross-validation scheme. The entire dataset of 480 clips was randomly partitioned into five equal, non-overlapping folds. In each of the five iterations, four folds (384 clips) were used for training, while the remaining fold (96 clips) served as the validation set. Similar to the fixed split, the training data in each iteration were augmented to 1152 samples. The final performance metrics were then averaged across all five folds. This approach minimizes the potential bias from a single, arbitrary data split.</p></list-item></list></p><p>For both protocols, the YOLOv8-Pose model was utilized to extract 2D skeleton sequences from the video frames. After processing, the skeleton data were saved as NumPy array files, and the corresponding labels were stored in pickle files, creating the required input format for the CoG-STGCN and ST-GCN models.</p></sec><sec id="sec4dot2-sensors-25-05493"><title>4.2. Validation of CoG Calculation Effectiveness</title><p>A core component of the proposed CoG-STGCN model is the computation of the body&#8217;s center of gravity from 2D keypoints. Therefore, validating the effectiveness of our weighted-average computation method is crucial. Considering that it is difficult to directly quantify the &#8220;real&#8221; center of gravity, this paper chooses to indirectly verify the validity of the method from the following three aspects, taking into account the feasibility and readability:<list list-type="order"><list-item><p>Comparison of CoG position with biomechanical reference points in static standardized posture</p></list-item></list></p><p>To initially assess the reasonableness of our weighted average CoG calculation method, we first examined the location of the center of gravity calculated in a standard static upright posture. According to widely accepted human biomechanical studies, the whole-body center of gravity of an adult in natural standing is located roughly within the pelvic region, approximately anterior to the second sacral vertebrae (S2). As shown in <xref rid="sensors-25-05493-f009" ref-type="fig">Figure 9</xref>, the location of the center of gravity computed by our weighted-average method is illustrated in a standard standing posture (red dots). As can be visually observed from the figure, the calculated center of gravity point indeed falls near the lower part of the torso of the skeleton, near the center region of the line connecting the two hip joints. Although the calculation based on 2D skeleton points and average segmental weights could not be precisely localized to the S2 vertebrae, its projected position in the image is consistent and as expected with the approximate region of the center of gravity described in the biomechanical literature. This provides a preliminary rationalization to support our CoG calculation methodology.</p><list list-type="simple"><list-item><label>2.</label><p>Stability observation of CoG trajectories in dynamic processes</p></list-item></list><p>We validate the reasonableness and dynamic stability of the CoG computation strategy through a skeleton data visualization model that includes the center of gravity computation. We visualize a process of fast squatting to standing up to observe the dynamic trajectory of CoG, as shown in <xref rid="sensors-25-05493-f010" ref-type="fig">Figure 10</xref>. Frames 10 to 50 show the process of fast squatting, and it can be seen that the CoG point (the red origin) moves smoothly down along a near-vertical path, which is always kept in the double-legged position. The 60 to 90 frames show the process of squatting to standing, and the CoG point moves smoothly upward along a similar path until it returns to the initial standing height. Throughout the dynamics, the trajectory of the CoG is continuous and consistent with the biomechanical expectations of the maneuver, with no unreasonably violent oscillations. The reasonableness of this dynamic trajectory further strengthens our confidence in the validity of the adopted CoG calculation method.</p><list list-type="simple"><list-item><label>3.</label><p>Positive Impact on Downstream Behavior Recognition Task Performance</p></list-item></list><p>In addition to the above observations at the static and dynamic levels, the ultimate effectiveness of the center of gravity computation and its dynamic graph mechanism will be demonstrated by its practical contribution to the performance of downstream behavior recognition tasks. We expect that by introducing this biomechanically based center of gravity sensing capability, the CoG-STGCN model should be able to more accurately capture balance- and stability-related motion features compared to the original ST-GCN model with a static graph structure, and thus exhibit superior performance in recognizing specific risky behaviors. A detailed performance comparison and in-depth analysis of CoG-STGCN versus ST-GCN on our constructed proximity hazardous behavior dataset will be fully elaborated in <xref rid="sec4dot4-sensors-25-05493" ref-type="sec">Section 4.4</xref>.</p></sec><sec id="sec4dot3-sensors-25-05493"><title>4.3. Experimental Setup and Model Training</title><p>Validating the performance of the proposed model, this paper presents detailed training and testing validation of the CoG-STGCN model. The experiments are performed on a GPU equipped with NVIDIA GeForce RTX 4060 Ti, 16 GB video memory (NVIDIA Corporation, Santa Clara, CA, USA). The platform is powered by Ubuntu 20.04 LTS with a kernel version 5.4.0-155-generic operating system, and is configured with an Intel(R) Xeon(R) CPU E5-2683 v4 processor (Intel Corporation, Santa Clara, CA, USA) and 20 GB of system RAM. The development, training, and evaluation of the models were carried out in the Python 3.8.10 environment using the PyTorch 1.12.0 + CUDA 11.3 deep learning framework, which provides stable and efficient computational support for the whole experimental process.</p><p>For the selection of training parameters, we choose the stochastic gradient descent (SGD) optimizer to optimize the parameters of the proposed CoG-STGCN model. The initial learning rate (base_lr) is set to 0.01, and a learning rate decay strategy is configured, i.e., the learning rate is adjusted at the 60th and 100th cycles (epochs) of the training process. The weight_decay coefficient was set to 0.0005, and the batch_size was 16. The model was trained for a total of 120 epochs.</p></sec><sec id="sec4dot4-sensors-25-05493"><title>4.4. Experimental Results and Analysis</title><p>In order to verify the effectiveness of our proposed improved model, we trained the CoG-STGCN model obtained after the improvement with the baseline ST-GCN model under the same experimental platform and conditions, and tested its performance with the test set. Five main evaluation metrics are chosen for the model: the precision rate measures the proportion of samples predicted by the model to be positively classified that are actually also positively classified; the recall rate measures the proportion of all samples that are actually positively classified that are successfully predicted by the model to be positively classified; F1 value, as the reconciled average of precision rate and recall rate, combines these two metrics and is commonly used to evaluate the model&#8217;s overall performance on unbalanced datasets; Top-1 value, as the reconciliation mean of precision rate and recall rate, is commonly used to evaluate the model&#8217;s performance on unbalanced datasets and the overall performance of the model on an unbalanced dataset; Top-1 accuracy measures the proportion of samples for which the action category with the highest probability of prediction by the model agrees with the true action label. The formulas for precision, recall, F1-score and Top-1 accuracy are shown in (14)&#8211;(17):<disp-formula id="FD14-sensors-25-05493"><label>(14)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD15-sensors-25-05493"><label>(15)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Recall</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-05493"><label>(16)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD17-sensors-25-05493"><label>(17)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Top</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo><mml:mi>Accuracy</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this paper, the ST-GCN model and the improved CoG-STGCN model are used to train on the self-constructed behavioral dataset, and <xref rid="sensors-25-05493-f011" ref-type="fig">Figure 11</xref> shows the accuracy curves of ST-GCN and CoG-STGCN on the validation set and the loss curves on the training set, with the accuracy and validation loss values being evaluated and recorded every five rounds (epochs), and the training loss values being recorded every round. Experimentally, the accuracy of the ST-GCN model (orange curve) shows rapid growth in the early training period (0&#8211;40 epochs), and then slows down and stabilizes at around 93% after about 80 epochs; in contrast, our improved CoG-STGCN generally outperforms the ST-GCN in terms of accuracy throughout the entire training cycle; specifically, the accuracy of CoG-STGCN is about 35% at the initial stage, the accuracy of CoG-STGCN is about 30% at the initial stage, and the accuracy of CoG-STGCN is about 30% at the initial stage. Specifically, the accuracy of CoG-STGCN rapidly climbs to over 90% in the first 35 epochs. In the 35&#8211;55 epoch phase, the accuracy rate enters a relatively flat growth plateau, which may be attributed to the fact that the model needs to additionally learn how to efficiently extract the four defined CoG features from the skeleton sequences while learning the basic spatio-temporal features, and the accuracy rate basically stabilizes around 95% after 70 epochs, indicating that the model has reached convergence.</p><p>Following the training phase, we evaluated the model performance on the test set. The results confirmed the superiority of our proposed model, which achieved an overall accuracy of 95.83%, surpassing the 93.75% accuracy of the original ST-GCN. To understand the contribution of each CoG-based feature, we conducted the ablation study shown in <xref rid="sensors-25-05493-t005" ref-type="table">Table 5</xref>, revealing that all four features are beneficial since removing any single component decreases performance. The velocity-based features (f2 and f4) prove most critical, with the removal of global velocity (f2) causing the largest accuracy drop to 93.96%, indicating that dynamic motion intensity is the key discriminator our model learns. While spatial distance features (f1 and f3) also contribute, these results confirm that our method&#8217;s strength lies in leveraging dynamic cues from the CoG.</p><p>An in-depth analysis of the classification performance for each action category was conducted, and confusion matrices were plotted to visually demonstrate the improvements over the baseline for both models, as shown in <xref rid="sensors-25-05493-f012" ref-type="fig">Figure 12</xref>. A detailed analysis of the confusion matrices reveals the specific advantages of our CoG-aware mechanism. For actions characterized by rapid and significant changes in the center of gravity, our CoG-STGCN model shows marked improvement. Specifically, for the &#8216;quick squat&#8217; and &#8216;getting up quickly&#8217; categories, CoG-STGCN achieves near-perfect recognition, whereas the baseline ST-GCN exhibits considerable confusion, misclassifying &#8216;getting up quickly&#8217; as &#8216;jumping&#8217; 33.33% of the time. This highlights that our CoG-based graph adjustment mechanism effectively captures the critical dynamic signatures of these high-risk movements, validating its design significance. However, the analysis also highlights certain limitations. For static actions like &#8216;standing&#8217;, the CoG features exhibit minimal variation, making it challenging for the MLP to learn effective adjustments. This leads to some confusion with &#8216;jumping&#8217; in the CoG-STGCN model. Similarly, for periodic movements like &#8216;walking&#8217;, the subtle dynamic differences between it and &#8216;running&#8217; are not fully captured by the current CoG features, resulting in some misclassification. This indicates that while our method excels in scenarios where CoG dynamics are a key discriminant, its universality is constrained by the expressive power of the selected features. Therefore, our model is particularly well-suited for applications focused on identifying movements with pronounced instability.</p><p>To further validate the robustness of the performance of the proposed CoG-STGCN model and reduce the chance of a single data division, we implemented a five-fold cross-validation of the CoG-STGCN and baseline ST-GCN models. Specifically, we randomly disrupted the overall dataset and divided it equally into five disjoint subsets. In each fold, one subset was selected as the validation set in turn, and the remaining four subsets were merged and processed with data enhancement to match the size of the training set in the previous experiments as the training set. Finally, we calculate and compare the mean and standard deviation of the key evaluation metrics of the two models in the five-fold cross-validation, and the results are shown in <xref rid="sensors-25-05493-t006" ref-type="table">Table 6</xref>. In terms of the average accuracy, CoG-STGCN reaches 94.17%, which is about 1.26 percentage points higher than that of ST-GCN at 92.91%. Similarly, CoG-STGCN (94.27%) outperforms ST-GCN (93.03%) in terms of macro-averaged F1 scores for evaluating comprehensive multi-class categorization performance. Notably, the low standard deviation for both models indicates that their performance is not overly sensitive to the specific data partition, demonstrating good stability. Crucially, CoG-STGCN maintains this stability while achieving a higher level of average performance. Together, these results suggest that the introduction of the center-of-gravity-aware dynamic graph mechanism not only improves the average recognition ability of the model, but that this improvement is robust and credible.</p><p>To further evaluate the generalization ability of the CoG-STGCN model, we chose to test our model on the publicly available dataset Kinetics 400 dataset [<xref rid="B30-sensors-25-05493" ref-type="bibr">30</xref>], which is a large-scale, high-quality human behavior recognition dataset published by DeepMind, containing approximately 300,000 video clips covering 400 different human action categories. We use the Kinetics-400 skeleton data available in the community, which was extracted from the original videos using OpenPose. The skeleton data contain 18 human body joints with node definitions consistent with our model format. <xref rid="sensors-25-05493-t007" ref-type="table">Table 7</xref> demonstrates the comparison of our model with other models. Our model achieves 33.1% in Top-1 accuracy, which is a 2.4% improvement compared to the 30.7% of ST-GCN, and this performance improvement is achieved while the number of parameters and computational effort of the model remain basically unchanged, as our model does not make any modifications to the convolutional network; compared to the MS-G3D model, our model has approximately 49.1% of its number of parameters and 69.2% of its computational effort. It can be seen that our improvement improves the performance of the benchmark model while maintaining the lightness of the model better.</p></sec><sec id="sec4dot5-sensors-25-05493"><title>4.5. Application Demonstration and Discussion</title><p>In this study, a simple real-time behavior recognition system is designed, which is able to acquire real-time video streaming data from an RTSP protocol camera, extract the skeleton data sequence from continuous video frames via YOLOv8-POSE, and transmit the skeleton sequence to CoG-STGCN for behavior recognition and display the behavioral categories on top of the detection frames, and <xref rid="sensors-25-05493-f013" ref-type="fig">Figure 13</xref> illustrates the final visualized output of the system, which contains the original video with the skeleton and detection frame superimposed. This system not only validates the feasibility of our proposed complete technological process from video input to behavioral categorization, but also provides an initial prototype for subsequent deployment and testing in real building environments.</p><p>To address the practical challenge of imperfect data, our CoG-STGCN model incorporates a robustness mechanism for handling partial occlusions or noisy detections. During inference, the system evaluates the confidence scores of keypoints required for CoG calculation. If an insufficient number of keypoints meet a predefined confidence threshold (e.g., 0.3), indicating unreliable input, the CoG-aware dynamic module is temporarily bypassed. In these instances, the model defaults to using only its base, fixed adjacency matrix, effectively reverting to the behavior of the original ST-GCN. The standard ST-GCN architecture implicitly handles missing joints by treating them as zero-value inputs and leveraging graph convolutions to aggregate features from visible neighboring joints. This trained ability to extract robust patterns from partial data ensures our system does not fail. By adopting this fallback strategy, our model maintains a baseline performance and enhances its overall robustness for real-world deployment where occlusions are common.</p><p>Furthermore, to address the practical concern of computational efficiency for potential real-world deployment, we conducted a preliminary performance analysis of our end-to-end system. Our benchmark tests on the experimental platform (NVIDIA GeForce RTX 4060 Ti) revealed that the proposed CoG-aware module introduces a minimal inference overhead to the overall pipeline. Specifically, a baseline system using YOLOv8s-Pose and the standard ST-GCN achieved an average processing speed of approximately 23.5 FPS. Our complete pipeline incorporating the CoG-STGCN model was found to run at approximately 23.1 FPS. This indicates that the added latency for CoG computation and dynamic graph modulation is only around 0.7 ms per inference cycle. While a comprehensive evaluation on actual edge devices is a crucial direction for future work, this result confirms that our method improves recognition accuracy at a negligible computational cost&#8212;a throughput reduction of less than 2%&#8212;thereby demonstrating its strong potential for real-time applications.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05493"><title>5. Conclusions</title><p>In this study, we addressed the critical challenge of identifying fall-risk behaviors on construction sites by proposing a novel center-of-gravity-aware spatio-temporal graph convolutional network, termed CoG-STGCN. Our core contribution is the explicit integration of center of gravity (CoG) dynamics&#8212;a key physical prior for human stability&#8212;into the construction of the graph, enabling the model to dynamically adapt to stability changes in real time. Experimental results on our self-constructed dataset of hazardous edge-related behaviors demonstrate the superiority of our approach. The CoG-STGCN achieved a Top-1 accuracy of 95.83% on the test set, outperforming the baseline ST-GCN (93.75%). Furthermore, it yielded a robust average accuracy of 94.17% in a five-fold cross-validation, a significant improvement over the baseline&#8217;s 92.91%. Notably, the performance gains were most pronounced in recognizing actions characterized by rapid CoG shifts, such as &#8220;quick squat&#8221; and &#8220;getting up quickly&#8221;.</p><p>These findings lead to a key insight: explicitly modeling the physical principles of human balance, rather than solely relying on learned feature-level correlations, is a more effective strategy for recognizing instability and potential fall risks. The success of CoG-STGCN validates that a physically informed model is more sensitive and interpretable, providing a new paradigm for developing more intelligent and proactive safety monitoring systems in the construction industry. By combining this approach with an efficient front-end like YOLOv8-POSE, our work presents a lightweight yet powerful scheme suitable for practical deployment.</p><p>Despite the promising results, this study has several limitations that open avenues for future research. A primary limitation is the scale and diversity of our self-constructed dataset. While sufficient to validate the core concept of CoG-STGCN, its generalization capability to complex, real-world construction environments with varied participants and conditions remains to be further verified. Consequently, the model&#8217;s robustness against severe occlusions and noisy detections, which are prevalent on actual sites, was not exhaustively evaluated. Future work will therefore prioritize the expansion and diversification of the dataset by recruiting more participants, collecting data from live construction sites to include naturalistic occlusions and noise, and employing advanced data augmentation techniques. Additionally, we plan to further enhance the model by exploring the fusion of multimodal information (e.g., 3D pose and environmental data) and validating its performance on edge devices.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to express their sincere gratitude to Long Zhan and Dali Hao of Shanghai Baoye Group Corp., LTD., for their invaluable assistance during the data collection phase and for providing crucial insights into the practical application scenarios of this research.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, P.J.; Formal analysis, C.L.; Data curation, S.G.; Writing&#8212;original draft, S.G.; Writing&#8212;review &amp; editing, P.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05493"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>X.</given-names></name></person-group><article-title>A Correlation Analysis of Construction Site Fall Accidents Based on Text Mining</article-title><source>Front. Built Environ.</source><year>2021</year><volume>7</volume><fpage>690071</fpage><pub-id pub-id-type="doi">10.3389/fbuil.2021.690071</pub-id></element-citation></ref><ref id="B2-sensors-25-05493"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lim</surname><given-names>J.-S.</given-names></name><name name-style="western"><surname>Song</surname><given-names>K.-I.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>H.-L.</given-names></name></person-group><article-title>Real-Time Location Tracking of Multiple Construction Laborers</article-title><source>Sensors</source><year>2016</year><volume>16</volume><elocation-id>1869</elocation-id><pub-id pub-id-type="doi">10.3390/s16111869</pub-id><pub-id pub-id-type="pmid">27827973</pub-id><pub-id pub-id-type="pmcid">PMC5134528</pub-id></element-citation></ref><ref id="B3-sensors-25-05493"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name></person-group><article-title>Integrated detection and tracking of workforce and equipment from construction jobsite videos</article-title><source>Autom. Constr.</source><year>2017</year><volume>81</volume><fpage>161</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2017.05.005</pub-id></element-citation></ref><ref id="B4-sensors-25-05493"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shaikh</surname><given-names>M.B.</given-names></name><name name-style="western"><surname>Chai</surname><given-names>D.</given-names></name></person-group><article-title>RGB-D Data-Based Action Recognition: A Review</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>4246</elocation-id><pub-id pub-id-type="doi">10.3390/s21124246</pub-id><pub-id pub-id-type="pmid">34205782</pub-id><pub-id pub-id-type="pmcid">PMC8234200</pub-id></element-citation></ref><ref id="B5-sensors-25-05493"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>D.</given-names></name></person-group><article-title>Spatial temporal graph convolutional networks for skeleton-based action recognition</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>2&#8211;7 February 2018</conf-date></element-citation></ref><ref id="B6-sensors-25-05493"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Han</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Worker Abnormal Behavior Recognition Based on Spatio-Temporal Graph Convolution and Attention Model</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>2915</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12132915</pub-id></element-citation></ref><ref id="B7-sensors-25-05493"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name></person-group><article-title>Study on the Interaction Behaviors Identification of Construction Workers Based on ST-GCN and YOLO</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>6318</elocation-id><pub-id pub-id-type="doi">10.3390/s23146318</pub-id><pub-id pub-id-type="pmid">37514613</pub-id><pub-id pub-id-type="pmcid">PMC10384721</pub-id></element-citation></ref><ref id="B8-sensors-25-05493"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>H.</given-names></name></person-group><article-title>Determination of workers&#8217; compliance to safety regulations using a spatio-temporal graph convolution network</article-title><source>Adv. Eng. Inform.</source><year>2023</year><volume>56</volume><fpage>101942</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2023.101942</pub-id></element-citation></ref><ref id="B9-sensors-25-05493"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hidalgo</surname><given-names>G.</given-names></name><name name-style="western"><surname>Simon</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>S.-E.</given-names></name><name name-style="western"><surname>Sheikh</surname><given-names>Y.</given-names></name></person-group><article-title>OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>43</volume><fpage>172</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2929257</pub-id><pub-id pub-id-type="pmid">31331883</pub-id></element-citation></ref><ref id="B10-sensors-25-05493"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>5385</fpage><lpage>5394</lpage></element-citation></ref><ref id="B11-sensors-25-05493"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.-C.</given-names></name><name name-style="western"><surname>Gidaris</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kokkinos</surname><given-names>I.</given-names></name><name name-style="western"><surname>Murphy</surname><given-names>K.</given-names></name></person-group><article-title>PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2018</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>282</fpage><lpage>299</lpage></element-citation></ref><ref id="B12-sensors-25-05493"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1506.01497</pub-id><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id></element-citation></ref><ref id="B13-sensors-25-05493"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You Only Look Once: Unified, Real-Time Object Detection</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1506.02640</pub-id></element-citation></ref><ref id="B14-sensors-25-05493"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Reed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name></person-group><article-title>SSD: Single Shot MultiBox Detector</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2016</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>8&#8211;16 October 2016</conf-date><fpage>21</fpage><lpage>37</lpage></element-citation></ref><ref id="B15-sensors-25-05493"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>H.S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>C.</given-names></name></person-group><article-title>AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>7157</fpage><lpage>7173</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3222784</pub-id><pub-id pub-id-type="pmid">37145952</pub-id></element-citation></ref><ref id="B16-sensors-25-05493"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Deep High-Resolution Representation Learning for Human Pose Estimation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>5686</fpage><lpage>5696</lpage></element-citation></ref><ref id="B17-sensors-25-05493"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gkioxari</surname><given-names>G.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><article-title>Mask R-CNN</article-title><source>Proceedings of the 2017 IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2961</fpage><lpage>2969</lpage></element-citation></ref><ref id="B18-sensors-25-05493"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Maji</surname><given-names>D.</given-names></name><name name-style="western"><surname>Nagori</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mathew</surname><given-names>M.</given-names></name><name name-style="western"><surname>Poddar</surname><given-names>D.</given-names></name></person-group><article-title>Yolo-Pose: Enhancing Yolo for Multi Person Pose Estimation Using Object Keypoint Similarity Loss</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;24 June 2022</conf-date><fpage>2637</fpage><lpage>2646</lpage></element-citation></ref><ref id="B19-sensors-25-05493"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Du</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Hierarchical recurrent neural network for skeleton based action recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>1110</fpage><lpage>1118</lpage></element-citation></ref><ref id="B20-sensors-25-05493"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Du</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Skeleton based action recognition with convolutional neural network</article-title><source>Proceedings of the ACCV 2016 Workshops</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>20&#8211;24 November 2016</conf-date><fpage>105</fpage><lpage>119</lpage></element-citation></ref><ref id="B21-sensors-25-05493"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name></person-group><article-title>Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>12026</fpage><lpage>12035</lpage></element-citation></ref><ref id="B22-sensors-25-05493"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name></person-group><article-title>Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>143</fpage><lpage>152</lpage></element-citation></ref><ref id="B23-sensors-25-05493"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>W.</given-names></name></person-group><article-title>Channel-Wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>13359</fpage><lpage>13368</lpage></element-citation></ref><ref id="B24-sensors-25-05493"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Winter</surname><given-names>D.A.</given-names></name></person-group><source>Biomechanics and Motor Control of Human Movement</source><edition>4th ed.</edition><publisher-name>John Wiley &amp; Sons</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2009</year><fpage>86</fpage></element-citation></ref><ref id="B25-sensors-25-05493"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Leva</surname><given-names>P.</given-names></name></person-group><article-title>Adjustments to Zatsiorsky-Seluyanov&#8217;s segment inertia parameters</article-title><source>J. Biomech.</source><year>1996</year><volume>29</volume><fpage>1223</fpage><lpage>1230</lpage><pub-id pub-id-type="doi">10.1016/0021-9290(95)00178-6</pub-id><pub-id pub-id-type="pmid">8872282</pub-id></element-citation></ref><ref id="B26-sensors-25-05493"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Winter</surname><given-names>D.A.</given-names></name></person-group><article-title>Human balance and posture control during standing and walking</article-title><source>Gait Posture</source><year>1995</year><volume>3</volume><fpage>193</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/0966-6362(96)82849-9</pub-id></element-citation></ref><ref id="B27-sensors-25-05493"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Freeman</surname><given-names>R.</given-names></name></person-group><article-title>Neurogenic orthostatic hypotension</article-title><source>N. Engl. J. Med.</source><year>2008</year><volume>358</volume><fpage>615</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1056/NEJMcp074189</pub-id><pub-id pub-id-type="pmid">18256396</pub-id></element-citation></ref><ref id="B28-sensors-25-05493"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>van der Beek</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>van Gaalen</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Frings-Dresen</surname><given-names>M.H.</given-names></name></person-group><article-title>Working postures and activities of lorry drivers: A reliability study of on-site observation and recording on a laptop computer</article-title><source>Appl. Ergon.</source><year>1992</year><volume>23</volume><fpage>331</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1016/0003-6870(92)90294-6</pub-id><pub-id pub-id-type="pmid">15676879</pub-id></element-citation></ref><ref id="B29-sensors-25-05493"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stavrinos</surname><given-names>D.</given-names></name><name name-style="western"><surname>Byington</surname><given-names>K.W.</given-names></name><name name-style="western"><surname>Schwebel</surname><given-names>D.C.</given-names></name></person-group><article-title>Distracted walking: Cell phones increase injury risk for pedestrians</article-title><source>Accid. Anal. Prev.</source><year>2011</year><volume>43</volume><fpage>150</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.jsr.2011.01.004</pub-id><pub-id pub-id-type="pmid">21569892</pub-id></element-citation></ref><ref id="B30-sensors-25-05493"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kay</surname><given-names>W.</given-names></name><name name-style="western"><surname>Carreira</surname><given-names>J.</given-names></name><name name-style="western"><surname>Simonyan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hillier</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vijayanarasimhan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Viola</surname><given-names>F.</given-names></name><name name-style="western"><surname>Green</surname><given-names>T.</given-names></name><name name-style="western"><surname>Back</surname><given-names>T.</given-names></name><name name-style="western"><surname>Natsev</surname><given-names>P.</given-names></name><etal/></person-group><article-title>The Kinetics Human Action Video Dataset</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1705.06950</pub-id><pub-id pub-id-type="doi">10.48550/arXiv.1705.06950</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05493-f001" orientation="portrait"><label>Figure 1</label><caption><p>Flowchart of the system for detecting dangerous behavior in edge areas. The asterisk (*) denotes the element-wise multiplication.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g001.jpg"/></fig><fig position="float" id="sensors-25-05493-f002" orientation="portrait"><label>Figure 2</label><caption><p>The overall pipeline for YOLOv8-Pose.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g002.jpg"/></fig><fig position="float" id="sensors-25-05493-f003" orientation="portrait"><label>Figure 3</label><caption><p>The definition of the 18-keypoint human skeleton.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g003.jpg"/></fig><fig position="float" id="sensors-25-05493-f004" orientation="portrait"><label>Figure 4</label><caption><p>Workflow diagram of ST-GCN behavior recognition model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g004.jpg"/></fig><fig position="float" id="sensors-25-05493-f005" orientation="portrait"><label>Figure 5</label><caption><p>Schematic diagram of the change in center of gravity with the squatting movement.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g005.jpg"/></fig><fig position="float" id="sensors-25-05493-f006" orientation="portrait"><label>Figure 6</label><caption><p>Flowchart of dynamic adjacency matrix generation based on center of gravity (CoG) feature extraction and MLP learning.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g006.jpg"/></fig><fig position="float" id="sensors-25-05493-f007" orientation="portrait"><label>Figure 7</label><caption><p>Architecture of the CoG-aware dynamic graph convolutional unit in CoG-STGCN.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g007.jpg"/></fig><fig position="float" id="sensors-25-05493-f008" orientation="portrait"><label>Figure 8</label><caption><p>Illustrative examples of action samples. (<bold>a</bold>) phone-walking; (<bold>b</bold>) running; (<bold>c</bold>) squatting; (<bold>d</bold>) Getting up Quickly; (<bold>e</bold>) standing; (<bold>f</bold>) jumping; (<bold>g</bold>) walking; (<bold>h</bold>) quick squat.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g008.jpg"/></fig><fig position="float" id="sensors-25-05493-f009" orientation="portrait"><label>Figure 9</label><caption><p>Example of calculated center of gravity position in standard standing position. The blue dots represent the detected skeleton keypoints, while the red dot indicates the computed center of gravity (CoG).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g009.jpg"/></fig><fig position="float" id="sensors-25-05493-f010" orientation="portrait"><label>Figure 10</label><caption><p>Schematic diagram of the change in the center of gravity position in different key frames during the rapid squat-to-stand process.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g010.jpg"/></fig><fig position="float" id="sensors-25-05493-f011" orientation="portrait"><label>Figure 11</label><caption><p>Top-1 accuracy and loss curves of ST-GCN and CoG-STGCN on the training and validation sets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g011.jpg"/></fig><fig position="float" id="sensors-25-05493-f012" orientation="portrait"><label>Figure 12</label><caption><p>Mixing matrix of (<bold>a</bold>) ST-GCN and (<bold>b</bold>) CoG-STGCN on the test set.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g012.jpg"/></fig><fig position="float" id="sensors-25-05493-f013" orientation="portrait"><label>Figure 13</label><caption><p>Visualization examples of recognition results. (<bold>a</bold>) Phone-walking; (<bold>b</bold>) running; (<bold>c</bold>) squatting; (<bold>d</bold>) getting up quickly; (<bold>e</bold>) standing; (<bold>f</bold>) jumping; (<bold>g</bold>) walking; (<bold>h</bold>) quick squat. Please note that the apparent low resolution of the visualized outputs is a result of the real-time processing pipeline, which prioritizes computational efficiency and speed over graphical quality.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05493-g013.jpg"/></fig><table-wrap position="float" id="sensors-25-05493-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05493-t001_Table 1</object-id><label>Table 1</label><caption><p>Performance comparison with human pose estimation models on the COCO val2017 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Input Size (Pixels)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AP (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AP@50 (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th></tr></thead><tbody><tr><td colspan="6" align="center" valign="middle" rowspan="1">Bottom-Up</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">OpenPose<break/>(VGG19) [<xref rid="B9-sensors-25-05493" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">61.8</td><td align="center" valign="middle" rowspan="1" colspan="1">84.9</td><td align="center" valign="middle" rowspan="1" colspan="1">49.5</td><td align="center" valign="middle" rowspan="1" colspan="1">221.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HigherHRNet (HRNet-W32) [<xref rid="B10-sensors-25-05493" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">512 &#215; 512</td><td align="center" valign="middle" rowspan="1" colspan="1">67.1</td><td align="center" valign="middle" rowspan="1" colspan="1">86.2</td><td align="center" valign="middle" rowspan="1" colspan="1">28.6</td><td align="center" valign="middle" rowspan="1" colspan="1">47.9</td></tr><tr><td colspan="6" align="center" valign="middle" rowspan="1">Top-Down</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AlphaPose<break/>(ResNet-50) [<xref rid="B15-sensors-25-05493" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">320 &#215; 256</td><td align="center" valign="middle" rowspan="1" colspan="1">73.3</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td><td align="center" valign="middle" rowspan="1" colspan="1">28.1</td><td align="center" valign="middle" rowspan="1" colspan="1">26.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mask R-CNN<break/>(ResNet-50-FPN) [<xref rid="B17-sensors-25-05493" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">67.0</td><td align="center" valign="middle" rowspan="1" colspan="1">87.3</td><td align="center" valign="middle" rowspan="1" colspan="1">42.3</td><td align="center" valign="middle" rowspan="1" colspan="1">260</td></tr><tr><td colspan="6" align="center" valign="middle" rowspan="1">One-stage</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv8s-pose</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">640 &#215; 640</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05493-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05493-t002_Table 2</object-id><label>Table 2</label><caption><p>Global center of gravity node selection and weight assignment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Body Segment</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Keypoint Index</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Weight</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Head</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Trunk</td><td align="center" valign="middle" rowspan="1" colspan="1">1, 11, 8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Left Arm</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Right Arm</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Left Leg</td><td align="center" valign="middle" rowspan="1" colspan="1">11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Right Leg</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.17</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05493-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05493-t003_Table 3</object-id><label>Table 3</label><caption><p>Core area delineation and node selection.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ID</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Core Region</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Keypoint Index</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">l</italic>
<sub>1</sub>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Head</td><td align="center" valign="middle" rowspan="1" colspan="1">0, 14, 15, 16, 17</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">l</italic>
<sub>2</sub>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Trunk</td><td align="center" valign="middle" rowspan="1" colspan="1">1, 8, 11</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">l</italic>
<sub>3</sub>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Left Arm</td><td align="center" valign="middle" rowspan="1" colspan="1">5, 6, 7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">l</italic>
<sub>4</sub>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Right Arm</td><td align="center" valign="middle" rowspan="1" colspan="1">2, 3, 4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">l</italic>
<sub>5</sub>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Left Leg</td><td align="center" valign="middle" rowspan="1" colspan="1">11, 12, 13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">l</italic>
<sub>6</sub>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Right Leg</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8, 9, 10</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05493-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05493-t004_Table 4</object-id><label>Table 4</label><caption><p>Descriptions of the eight edge-related action categories selected in the experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Category</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Action</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" colspan="1">Significant CoG Displacement</td><td align="center" valign="middle" rowspan="1" colspan="1">Running</td><td align="center" valign="middle" rowspan="1" colspan="1">Large inertia, difficult to stop or turn quickly.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jumping</td><td align="center" valign="middle" rowspan="1" colspan="1">High impact upon landing, poor stability.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Quick Squat</td><td align="center" valign="middle" rowspan="1" colspan="1">Rapid CoG drop, prone to dizziness or instability.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Getting up Quickly</td><td align="center" valign="middle" rowspan="1" colspan="1">Rapid CoG rise, prone to dizziness or instability.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Unstable Posture</td><td align="center" valign="middle" rowspan="1" colspan="1">Squatting</td><td align="center" valign="middle" rowspan="1" colspan="1">Legs prone to fatigue, reduced coordination when standing up.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Distracted Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">Phone-walking</td><td align="center" valign="middle" rowspan="1" colspan="1">Complete disregard for frontal and lateral road conditions.</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Safe Behaviors</td><td align="center" valign="middle" rowspan="1" colspan="1">Standing</td><td align="center" valign="middle" rowspan="1" colspan="1">CoG stable, strong stability.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walking</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small CoG displacement, wide field of view.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05493-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05493-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation study results on CoG-STGCN components.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (Top-1)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Comparison with Baseline</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ST-GCN(Baseline)</td><td align="center" valign="middle" rowspan="1" colspan="1">93.75%</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CoG-STGCN (f1 + f2 + f3 + f4)</td><td align="center" valign="middle" rowspan="1" colspan="1">95.83%</td><td align="center" valign="middle" rowspan="1" colspan="1">+2.08%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">w/o Dist_GlobalCoG (f1)</td><td align="center" valign="middle" rowspan="1" colspan="1">95.21%</td><td align="center" valign="middle" rowspan="1" colspan="1">+1.46%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">w/o Vel_GlobalCoG (f2)</td><td align="center" valign="middle" rowspan="1" colspan="1">93.96%</td><td align="center" valign="middle" rowspan="1" colspan="1">+0.21%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">w/o Dist_CoreCoG (f3)</td><td align="center" valign="middle" rowspan="1" colspan="1">94.79%</td><td align="center" valign="middle" rowspan="1" colspan="1">+1.04%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/o Vel_CoreCoG (f4)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.17%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+0.42%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05493-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05493-t006_Table 6</object-id><label>Table 6</label><caption><p>Performance comparison of ST-GCN and CoG-STGCN using five-fold cross-validation on the self-collected dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Metric</th><th colspan="5" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Cross Validation</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Average &#177; Std Dev </th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ST-GCN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9270</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9479</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9062</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9375</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9270</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9291 &#177; 0.0146</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Macro-F1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9283</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9491</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9075</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9384</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9281</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9303 &#177; 0.0146</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">CoG-STGCN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9375</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9583</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9167</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9583</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9375</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9417 &#177; 0.0169</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Macro-F1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9387</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9594</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9177</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9592</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9384</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9427 &#177; 0.0169</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05493-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05493-t007_Table 7</object-id><label>Table 7</label><caption><p>Performance and efficiency comparison of different action recognition models on the Kinetics-400 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Models</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (Top-1)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (Top-5)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ST-GCN [<xref rid="B5-sensors-25-05493" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">30.7%</td><td align="center" valign="middle" rowspan="1" colspan="1">52.8%</td><td align="center" valign="middle" rowspan="1" colspan="1">3.10</td><td align="center" valign="middle" rowspan="1" colspan="1">16.30</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2s-AGCN [<xref rid="B21-sensors-25-05493" ref-type="bibr">21</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">36.1%</td><td align="center" valign="middle" rowspan="1" colspan="1">58.7%</td><td align="center" valign="middle" rowspan="1" colspan="1">6.94</td><td align="center" valign="middle" rowspan="1" colspan="1">37.30</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep LSTM [<xref rid="B5-sensors-25-05493" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">16.4%</td><td align="center" valign="middle" rowspan="1" colspan="1">35.3%</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MS-G3D [<xref rid="B22-sensors-25-05493" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">38.0%</td><td align="center" valign="middle" rowspan="1" colspan="1">60.9%</td><td align="center" valign="middle" rowspan="1" colspan="1">6.44</td><td align="center" valign="middle" rowspan="1" colspan="1">24.50</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CTR-GCN [<xref rid="B23-sensors-25-05493" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">5.84</td><td align="center" valign="middle" rowspan="1" colspan="1">7.88</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CoG-STGCN (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.1%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.95</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>