<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431283</article-id><article-id pub-id-type="pmcid-ver">PMC12431283.1</article-id><article-id pub-id-type="pmcaid">12431283</article-id><article-id pub-id-type="pmcaiid">12431283</article-id><article-id pub-id-type="doi">10.3390/s25175324</article-id><article-id pub-id-type="publisher-id">sensors-25-05324</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MBFILNet: A Multi-Branch Detection Network for Autonomous Mining Trucks in Dusty Environments</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2734-1924</contrib-id><name name-style="western"><surname>Xu</surname><given-names initials="FX">Fei-Xiang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05324" ref-type="aff">1</xref><xref rid="af2-sensors-25-05324" ref-type="aff">2</xref><xref rid="af3-sensors-25-05324" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhu</surname><given-names initials="DL">Di-Long</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05324" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Hu</surname><given-names initials="YP">Yu-Peng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05324" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="R">Rui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05324" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4489-6799</contrib-id><name name-style="western"><surname>Zhou</surname><given-names initials="C">Chen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05324" ref-type="aff">1</xref><xref rid="af2-sensors-25-05324" ref-type="aff">2</xref><xref rid="c1-sensors-25-05324" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Kollias</surname><given-names initials="S">Stefanos</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05324"><label>1</label>School of Information and Control Engineering, China University of Mining and Technology, Xuzhou 221116, China; <email>xufx92@cumt.edu.cn</email> (F.-X.X.); <email>zdl0130@cumt.edu.cn</email> (D.-L.Z.); <email>hyphyp_18@cumt.edu.cn</email> (Y.-P.H.); <email>zrcumt@cumt.edu.cn</email> (R.Z.)</aff><aff id="af2-sensors-25-05324"><label>2</label>State Key Laboratory of Fluid Power &amp; Mechatronic Systems, Zhejiang University, Hangzhou 310027, China</aff><aff id="af3-sensors-25-05324"><label>3</label>Guangdong Institute of Electronic Information Engineering, University of Electronic Science and Technology of China, Dongguan 523950, China</aff><author-notes><corresp id="c1-sensors-25-05324"><label>*</label>Correspondence: <email>zc111@cumt.edu.cn</email>; Tel.: +86-130-6627-6127</corresp></author-notes><pub-date pub-type="epub"><day>27</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5324</elocation-id><history><date date-type="received"><day>08</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>10</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>20</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>27</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05324.pdf"/><abstract><p>As a critical technology of autonomous mining trucks, object detection directly determines system safety and operational reliability. However, autonomous mining trucks often work in dusty open-pit environments, in which dusty interference significantly degrades the accuracy of object detection. To overcome the problem mentioned above, a multi-branch feature interaction and location detection network (MBFILNet) is proposed in this study, consisting of multi-branch feature interaction with differential operation (MBFI-DO) and depthwise separable convolution-enhanced non-local attention (DSC-NLA). On one hand, MBFI-DO not only strengthens the extraction of channel-wise semantic features but also improves the representation of salient features of images with dusty interference. On the other hand, DSC-NLA is used to capture long-range spatial dependencies to focus on target-object structural information. Furthermore, a custom dataset called Dusty Open-pit Mining (DOM) is constructed, which is augmented using a cycle-consistent generative adversarial network (CycleGAN). Finally, a large number of experiments based on DOM are conducted to evaluate the performance of MBFILNet in dusty open-pit environments. The results show that MBFILNet achieves a mean Average Precision (mAP) of 72.0% based on the DOM dataset, representing a 1.3% increase compared to the Featenhancer model. Moreover, in comparison with YOLOv8, there is an astounding 2% increase in the mAP based on MBFILNet, demonstrating detection accuracy in dusty open-pit environments can be effectively improved with the method proposed in this paper.</p></abstract><kwd-group><kwd>autonomous mining truck</kwd><kwd>object detection</kwd><kwd>dusty weather</kwd><kwd>multi-branch feature interaction</kwd></kwd-group><funding-group><award-group><funding-source>National Key Research and Development Program</funding-source><award-id>2023YFC2907600</award-id></award-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>52475143</award-id></award-group><award-group><funding-source>Jiangsu Provincial Natural Science Foundation</funding-source><award-id>BK20241638</award-id></award-group><award-group><funding-source>Open Foundation of the State Key Laboratory of Fluid Power and Mechatronic Systems</funding-source><award-id>GZKF-202421</award-id></award-group><award-group><funding-source>Guangdong Basic and Applied Basic Research Foundation</funding-source><award-id>2024A1515110216</award-id></award-group><funding-statement>This work was supported in part by the National Key Research and Development Program under Grant 2023YFC2907600, in part by the National Natural Science Foundation of China under Grant 52475143, in part by the Jiangsu Provincial Natural Science Foundation (BK20241638), in part by the Open Foundation of the State Key Laboratory of Fluid Power and Mechatronic Systems under Grant GZKF-202421, and in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2024A1515110216.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05324"><title>1. Introduction</title><p>With the rapid development of intelligent mine construction, autonomous mining trucks (AMTs) have been widely applied in mining, owing to their advantages in fatigue driving risk mitigation and operational cost reduction [<xref rid="B1-sensors-25-05324" ref-type="bibr">1</xref>]. It is important for AMTs to perceive surrounding environmental information, which is the foundation of decision-making and control. As a common evaluation indicator of perception, high object detection accuracy plays a vital role for AMTs in complex mining environments [<xref rid="B2-sensors-25-05324" ref-type="bibr">2</xref>]. Although object detection for urban road scenarios has reached maturity, its application in unstructured mining environments remains remarkably limited.</p><p>Object detection is tough in open-pit mines due to the complexity of associated environments due to, e.g., significant scale variation, severe mutual occlusion, and the camouflage effect. To address the problem of significant scale variation, Song et al. [<xref rid="B3-sensors-25-05324" ref-type="bibr">3</xref>] built MSFANet to capture rich context features, enhancing the feature saliency of objects with different scales. Simultaneously, detection accuracy is also decreased, owing to the loss of critical feature information, which is caused by mutual occlusion between objects in open-pit mines. Therefore, Bo et al. [<xref rid="B4-sensors-25-05324" ref-type="bibr">4</xref>] combined the guidance module of contextual information with the efficient squeeze&#8211;excitation attention mechanism, ensuring the model focuses on channels with important feature information. In addition, due to interference caused by the high similarity between the target and background, missed and misused results are increased during detection. Hence, Ren et al. [<xref rid="B5-sensors-25-05324" ref-type="bibr">5</xref>] constructed a multi-scales fusion and attention-based model to improve the performance of object detection for camouflaged obstacles in mining.</p><p>Apart from the aforementioned characteristics in unstructured open-pit mine environments, variable weather is also a important factor affecting the accuracy of object detection. Currently, research on object detection for AMTs primarily focuses on normal weather conditions, but AMTs often operate in dusty environments [<xref rid="B6-sensors-25-05324" ref-type="bibr">6</xref>], as shown in <xref rid="sensors-25-05324-f001" ref-type="fig">Figure 1</xref>. Compared with normal weather conditions, visual characteristics of images such as color balance, fine-grained details, and luminance are severely distorted by dust interference, complicating the extraction of critical channel-wise semantic features and the representation of salient object information [<xref rid="B7-sensors-25-05324" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05324" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05324" ref-type="bibr">9</xref>]. Ordinary bad weather (such as rain, fog, etc.) is evenly distributed in the form of droplets and introduces interference based on Mie scattering. In contrast with ordinary bad weather, mineral dust is composed of irregularly shaped mineral particles with an uneven spatial distribution, causing local occlusion that disrupts object feature continuity and induces edge confusion in detection tasks. Additionally, current detection of adverse weather conditions is based on the principle of uniform distribution of rain, fog, and water droplets in the atmosphere, aiming to enhance detection performance by reducing optical interference. However, the distribution of dust particles in dusty environments violates this principle, and at the same time, dust particles can block the texture and edge details of objects, making existing detection algorithms inapplicable. Subsequently, inaccurate positioningand false detection may occur due to such complications, directly impairing the detection accuracy of AMTs [<xref rid="B10-sensors-25-05324" ref-type="bibr">10</xref>].</p><p>In this article, an efficient detector networkfor AMTs is presented to specifically addresses the robust extraction of salient target features and accurate identification of structural characteristics, thereby improving detection accuracy for unmanned mining trucks in dusty environments. The main contributions of this work are as described as follows:</p><p>(1) To improve the accuracy of object detection for AMTs in dusty weather, a novel object detection method based on YOLOv8 and named multi-branch feature interaction and location detection net (MBFILNet) is presented.</p><p>(2) Aiming at the challenge of discriminating salient target information in dust-laden environments, the dust interference is filtered out based on MBFI-DO, which integrates multi-branch information interaction and employs differential information guidance.</p><p>(3) To enhance the discrimination of structural features in dust-obscured environments, DSC-NLA is designed to capture global spatial long-range dependencies and enhance cross-channel information interaction. This design can augment the recognition robustness of detection models across different dust levels.</p><p>The remainder of this research is organized as follows. Related works on detection under adverse weather conditions are introduced in <xref rid="sec2-sensors-25-05324" ref-type="sec">Section 2</xref>. The proposed MBFILNet is elaborated upon in <xref rid="sec3-sensors-25-05324" ref-type="sec">Section 3</xref>. In <xref rid="sec4-sensors-25-05324" ref-type="sec">Section 4</xref>, a large number of experiments on DOM are conducted to evaluate the performance of MBFILNet. Finally, conclusions are presented in <xref rid="sec5-sensors-25-05324" ref-type="sec">Section 5</xref>.</p></sec><sec id="sec2-sensors-25-05324"><title>2. Related Works</title><p>In this section, relevant works about object detection under adverse weather, as well as feature enhancement and global context feature representation, are analyzed briefly.</p><sec id="sec2dot1-sensors-25-05324"><title>2.1. Object Detection Under Adverse Weather</title><p>Although the YOLO series [<xref rid="B11-sensors-25-05324" ref-type="bibr">11</xref>] and Faster R-CNN [<xref rid="B12-sensors-25-05324" ref-type="bibr">12</xref>] methods achieve high detection accuracy in optimal weather conditions, their performance proves less satisfactory under adverse weather phenomena [<xref rid="B13-sensors-25-05324" ref-type="bibr">13</xref>]. To overcome this problem, research in the last few years has primarily focused on three approaches: (1) image restoration networks, (2) enhanced detection algorithms, and (3) domain adaptation. Restoring an image is conducive to improving the image quality, enhancing the detection accuracy. Huang et al. [<xref rid="B14-sensors-25-05324" ref-type="bibr">14</xref>] developed DSNet, a dual-branch network sharing feature extraction for joint image restoration and detection. IA-YOLO [<xref rid="B15-sensors-25-05324" ref-type="bibr">15</xref>] comprises restoration and detection networks with a learnable image processing module, which is trained in an end-to-end manner using detection loss alone. However, the approaches mentioned above inevitably lose critical image information during the recovery process. To mitigate information loss in restoration networks, enhanced detection algorithms with scene feature extraction capabilities have been proposed. A lightweight object detection network with a novel plug-and-play Cross-Fusion (CF) block was proposed by Ding et al. [<xref rid="B16-sensors-25-05324" ref-type="bibr">16</xref>], combining the advantages of FPN and PAN in a more flexible architecture. Conventional detection algorithms require a substantial amount of training data, yet datasets under adverse weather conditions are particularly limited. To address these limitations, domain adaptation techniques have attracted widespread attention in the field of image detection. Wang et al. [<xref rid="B17-sensors-25-05324" ref-type="bibr">17</xref>] proposed the image Quality Translation Network (QTNet) and Feature Calibration Network (FCNet), enabling models to progressively generalize from clear-weather to adverse-weather domains based on domain adaptation. Nevertheless, cross-domain learning often loses critical channel dimension information, resulting in poor accuracy performance under adverse weather conditions.</p><p>Furthermore, dust storms occur less frequently compared to traditional adverse weather conditions, and they are mostly found in open-pit mines, resulting in a scarcity of related research. Currently, there are two kinds of approaches for object detection in dusty mines: image restoration methods and enhanced detection algorithms. For example, TIAN et al. [<xref rid="B18-sensors-25-05324" ref-type="bibr">18</xref>] proposed a coal mine image enhancement algorithm based on dual-domain decomposition, which achieved image defogging in low-frequency images to eliminate the influence of dust. However, this method ignores the color-shift effect caused by coal dust, resulting in color distortion of the restored image and an unsatisfactory dust removal effect. In terms of enhanced detection algorithms, Yang et al. [<xref rid="B19-sensors-25-05324" ref-type="bibr">19</xref>] proposed a multi-scale edge enhancement (MSEE) module and fused it with the C2f module, which enhanced the extraction of the personnel feature under high-dust conditions. Nevertheless, this approach failed to address the background interference of dust without significant improvement of detection accuracy. In summary, there is a significant gap in the research on dusty open-pit mines, so we need to conduct research on the detection of mines in dusty environments.</p><p>Motivated by enhanced detection algorithms, in this paper, crucial latent features are preserved by analyzing images corrupted by dust, thereby avoiding the information loss inherent in preprocessing stages.</p></sec><sec id="sec2dot2-sensors-25-05324"><title>2.2. Feature Enhancement</title><p>Deep learning-based object detection methods extract high-dimensional features through backbone networks, but the feature extraction abilities are reduced in dust-obscured images due to interference from backgrounds. To enhance multi-scale representation, Lin et al. [<xref rid="B20-sensors-25-05324" ref-type="bibr">20</xref>] introduced the Feature Pyramid Network (FPN) by aggregating high-level and low-level features across different resolutions. However, this approach failed to address feature degradation in adverse weather conditions, limiting its robustness in adverse environments. To enhance feature representation in weather-degraded scenarios, Chen et al. [<xref rid="B21-sensors-25-05324" ref-type="bibr">21</xref>] proposed detail-enhanced convolution (DEConv) and content-guided attention (CGA), boosting dehazing performance significantly. But the methods mentioned above are sufficient for specific tasks only an cannot achieve detection in dynamic and changing environments. To adapt to changing environments, Li et al. [<xref rid="B22-sensors-25-05324" ref-type="bibr">22</xref>] developed a change detection difference enhancement module to extract critical features from difference maps. Based on the above analysis, it can be seen that existing methods primarily focus on detailed feature extraction, neglecting salient feature enhancement through background interference suppression.</p><p>In order to address this limitation, in this paper, we propose MBFI-DO, which guides discriminative feature extraction in target regions based on the the integration of high-level semantic information, suppressing dust interference while enhancing salient feature representation.</p></sec><sec id="sec2dot3-sensors-25-05324"><title>2.3. Global Context Feature Representation</title><p>Enhancing the feature representation in dusty environments [<xref rid="B23-sensors-25-05324" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05324" ref-type="bibr">24</xref>] is crucial for target localization based on global receptive fields and contextual information, which can be obtained by modeling the global relationship between targets and the background [<xref rid="B25-sensors-25-05324" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05324" ref-type="bibr">26</xref>]. Noman M et al. [<xref rid="B27-sensors-25-05324" ref-type="bibr">27</xref>] constructed an efficient local&#8211;global context aggregator (ELGCA) module within the encoder to capture enhanced global context and local spatial information. However, oversimplification of long-range dependencies may occur due to its fixed pooling strategy. To address the challenge of feature refinement, Hu et al. [<xref rid="B28-sensors-25-05324" ref-type="bibr">28</xref>] designed a guided refinement decoder (GRD) to extract context information and further refine prediction results. Nevertheless, dust-induced noise is propagated from high-level features to finer scales due to the lack of adaptive filtering. In contrast, the non-local neural network (NLNet) proposed in ref. [<xref rid="B29-sensors-25-05324" ref-type="bibr">29</xref>] established reliable global context feature representations by calculating the pairwise correlations between spatial pixels, in which noise interference is reduced at the same time.</p><p>Inspired by the theoretical insights in ref. [<xref rid="B29-sensors-25-05324" ref-type="bibr">29</xref>], DSC-NLA is proposed in this paper. First, the multi-scale receptive field feature from feature maps is extracted by means of depthwise separable convolution. Subsequently, the representations of the heterogeneous receptive field are aggregated based on feature fusion strategies. Finally, pixel-wise correlation modeling is employed to construct spatial long-range dependencies, and target localization is finished.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05324"><title>3. Methods</title><p>In this section, the overall architecture of MBFILNet is introduced. <xref rid="sec3dot2-sensors-25-05324" ref-type="sec">Section 3.2</xref> introduces the salient feature representation branch in detail, while <xref rid="sec3dot3-sensors-25-05324" ref-type="sec">Section 3.3</xref> discusses the attention mechanism for target structural features.</p><sec id="sec3dot1-sensors-25-05324"><title>3.1. The Overall Architecture of MBFILNet</title><p>To address the challenges of obscured salient features and lost positional information caused by background interference, MBFILNet is proposed in this paper, as illustrated in <xref rid="sensors-25-05324-f002" ref-type="fig">Figure 2</xref>. The proposed MBFILNet maintains the architectural framework of YOLOv8, with novel enhancements through the MBFI-DO and DSC-NLA modules. MBFI-DO enhances target feature extraction across different feature scales by implementing multi-branch cross-fusion. Then, differential operation is introduced into the model, which dynamically focuses on the extraction of discriminative target features and the mining of channel semantic information. Meanwhile, CFDSA extends the original feature fusion module in YOLOv8 C2f by adding DSC-NLA and a channel shuffle operation (CSO) [<xref rid="B30-sensors-25-05324" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05324" ref-type="bibr">31</xref>]. Notably, DSC-NLA captures spatial contextual information to establish semantic correlations across regions, thereby enhancing the model&#8217;s perception of structural location features. A residual structure is also introduced to enrich the diversity of feature information in MBFILNet, which helps to alleviate the vanishing gradient problem and enables the model to learn more discriminative features.</p></sec><sec id="sec3dot2-sensors-25-05324"><title>3.2. Multi-Branch Feature Interaction with Differential Operation (MBFI-DO)</title><p>Substantial dust is generated during coal mining, not only hindering target recognition by even leading to missed detections [<xref rid="B32-sensors-25-05324" ref-type="bibr">32</xref>]. To overcome this problem, a multi-branch feature interaction (MBFI) module is designed in an AMT detection system, which expands the receptive field of the backbone network and multi-branch cross-fusion. MBFI enables the model to capture multi-scale features and enhance channel-wise interaction across different feature scales. Moreover, based on the integration of differential operations (DOs), the attention of the model is dynamically guided to the extraction of discriminative target features and the mining of channel semantic information. As a result, the interference from image distortion and suspended particulate in dust-obscured environments is decoupled effectively.</p><p>MBFI-DO not only incorporates multi-scale target feature extraction capabilities but also emphasizes channel-wise semantic information interaction, making it particularly suitable for target detection in complex, dusty open-pit mines, as illustrated in <xref rid="sensors-25-05324-f003" ref-type="fig">Figure 3</xref>.</p><p>A multi-path parallel architecture is designed in the MBFI module to extract rich feature information through multi-scale convolutions. Moreover, both global average pooling (GAP) and global maximum pooling (GMP) are integrated to compress features and capture channel-wise global context [<xref rid="B32-sensors-25-05324" ref-type="bibr">32</xref>]. The mathematical expressions of GAP and GMP can be written as follows:<disp-formula id="FD1-sensors-25-05324"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05324"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the input feature map and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the feature map after maximum pooling and average pooling, respectively.</p><p>Subsequently, the multi-branch fusion mechanism enhances cross-channel interaction between feature maps from diverse receptive fields, exploring inter-channel semantic correlations deeply, which can be expressed by the following formulas:<disp-formula id="FD3-sensors-25-05324"><label>(3)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo form="prefix">Softmax</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup></mml:mfenced><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi><mml:mspace width="4.pt"/></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi><mml:mspace width="4.pt"/></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05324"><label>(4)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo form="prefix">Softmax</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mfenced><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi><mml:mspace width="4.pt"/></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi><mml:mspace width="4.pt"/></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the normalized exponential function and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represent standard convolution operations with kernel sizes of 1 &#215; 1, 3 &#215; 3, 5 &#215; 5, and 7 &#215; 7, respectively.</p><p>Furthermore, the feature maps of rich semantic information are multiplied by the feature maps extracted from diverse receptive fields, which enhances the local salient features effectively under semantic guidance. Matrix multiplication is applied to global feature fusion, thereby jointly enhancing both spatial and channel-wise correlations across feature maps. Subsequently, a sigmoid activation function is utilized to generate the refined feature vector, as shown in Formula (5):<disp-formula id="FD5-sensors-25-05324"><label>(5)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>Z</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mfenced></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>Z</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are the resultant features of cross-multiplication. <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the sigmoid activation function.</p><p>Finally, the output formula for MBFI can be written as follows:<disp-formula id="FD6-sensors-25-05324"><label>(6)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mfenced><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The MBFI module demonstrates insufficiency for AMT detection, where simultaneous attention to both spatial information and channel-wise semantics is required. In particular, some minor but useful features are filtered out when features from different dimensions are weighted by learned attention weights.</p><p>To address the problem mentioned above, MBFI-DO is designed as a feature reuse strategy in cross-fusion feature extraction, in which MBFI is located after the concatenated max-pooling layers of SPPF; then, the feature is enhanced by differential operations (DOs). The proposed DO serves as a feature refinement mechanism that explicitly extracts and amplifies the learned feature enhancements. As a result, the representation of semantic information is strengthened through semantics-guided feature enhancement based on the MBFI-DO strategy. In detail, the feature difference between the original input feature map (<italic toggle="yes">X</italic>) and the output feature map (<inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) is calculated by the residual branch of SPPF, which is shown in Formula (7). By computing <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, we obtain the residual features that represent the actual improvements brought about by the MBFI module, including both channel-wise semantic reinforcements and spatial saliency information. Subsequently, multi-scale spatial&#8211;channel semantic features are extracted by multiplying the normalized difference result and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, while dust interference is suppressed due to the enhancement of target discriminablity guided by semantic features.<disp-formula id="FD7-sensors-25-05324"><label>(7)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">F</italic> denotes the SPPF incorporated in MBFI; <italic toggle="yes">X</italic> represents the input feature map; and <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is composed of the four parts, namely Conv, the maximum pooling layer, concatenation layer, and MBFI.</p></sec><sec id="sec3dot3-sensors-25-05324"><title>3.3. Depthwise Separable Convolution-Enhanced Non-Local Attention (DSC-NLA)</title><p>Dust in open-pit environments not only occludes target objects but also hinders the extraction of surface texture information [<xref rid="B33-sensors-25-05324" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05324" ref-type="bibr">34</xref>]. As a result, structural features are impaired, owing to the loss of edge information and target&#8211;background boundary blurring [<xref rid="B35-sensors-25-05324" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05324" ref-type="bibr">36</xref>]. To address these challenges, an advanced fusion mechanism called DSC-NLA is proposed in this paper, as illustrated in <xref rid="sensors-25-05324-f004" ref-type="fig">Figure 4</xref>. To better capture global spatial dependencies, non-local attention and channel shuffling are combined, which facilitates cross-dimensional feature interaction and calibration.</p><p>The corresponding query, key, and value vectors are generated by DSC-NLA, where the concatenated input feature maps are processed by means of depthwise separable convolution. The Q, K, and V in DSC-NLA can be calculated as follows:<disp-formula id="FD8-sensors-25-05324"><label>(8)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">Q</mml:mi><mml:mo>=</mml:mo><mml:mi>&#948;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the rectified linear unit (ReLU) activation function; <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the transposition operation; <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>conv</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the depthwise convolution operations with kernel sizes of 3 &#215; 3, 5 &#215; 5, and 7 &#215; 7, respectively.</p><p>Then, a similarity matrix is established by applying the softmax activation function to the dot product between the query and key vectors. Subsequently, the global spatial dependencies within the feature maps are captured via the multiplication of the matrix by value vectors. As a result, structural features are enhanced through dynamic suppression of dust background noise based on dependencies. The feature map with enhanced structural features (<italic toggle="yes">Y</italic>) can be formulated as follows:<disp-formula id="FD9-sensors-25-05324"><label>(9)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mi>Q</mml:mi><mml:mspace width="0.166667em"/><mml:mi>&#183;</mml:mi><mml:mspace width="0.166667em"/><mml:mi>K</mml:mi></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>However, the non-local attention mechanism in Formula (9) only focuses on structured information, leading to smaller yet useful features being filtered out during the feature weighting processing. Thus, a feature reuse structure is applied to compensate for the limitation of the non-local attention mechanism. The output of the feature reuse structure is described in Formula (10), integrating the bottleneck of C2f into DSC-NLA for the implementation of feature reuse.<disp-formula id="FD10-sensors-25-05324"><label>(10)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> denotes the bottleneck after the introduction of DSC-NLA; <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the input feature map; and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is composed of a 1 &#215; 1 convolution, a 3 &#215; 3 convolution, and the integrated DSC-NLA.</p><p>Finally, the CSO is introduced to enhance inter-channel information flow, and a bottleneck block with two groups is divided into several subgroups. Then, subgroups are fed to the next layer&#8217;s corresponding groups in a shuffled manner to enable efficient cross-group channel interaction.</p></sec></sec><sec sec-type="results" id="sec4-sensors-25-05324"><title>4. Results and Discussion</title><p>In this section, lots of comparative and ablation experiments are conducted on the DAWN public dusty dataset and the self-made DOM dataset. All experiments are performed on an Ubuntu 20.04, using the PyTorch 1.10.0 deep learning framework and CUDA 11.3 for computational acceleration, with models trained and validated on an NVIDIA RTX 3080 GPU.</p><sec id="sec4dot1-sensors-25-05324"><title>4.1. Construction of Dusty Open-Pit Mine Datasets</title><p>The Dusty Open-pit Mine (DOM) data is constructed in this paper for dust-obscured open-pit mining environments. It is composed of field data acquisition, selective sampling from the AutoMine database, and CycleGAN-based data augmentation. To validate a robust object detection algorithm for autonomous mining trucks in dusty conditions, this paper constructs the DOM dataset, which contains 7371 dusty images across four categories and is divided into training and testing sets in an 8:2 ratio. The four categories are Bulldozer, Mining-Truck, Excavators, and Loader. Among these, the sample count for &#8220;Bulldozer&#8221; is 623, and the numbers of &#8220;Mining-Truck&#8221;, &#8220;Excavators&#8221;, and &#8220;Loader&#8221; instances are 6447, 5075, and 1020, respectively. The scale and distribution of different categories of objects are shown in <xref rid="sensors-25-05324-f005" ref-type="fig">Figure 5</xref>.</p><p>The CycleGAN model architecture consists of a ResNet-based generator with six residual blocks and two stride-2 convolutions for downsampling, paired with a 70 &#215; 70 PatchGAN discriminator, implementing three key loss functions: cycle-consistency loss (<inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#955;</mml:mi></mml:mrow></mml:math></inline-formula> = 10), LSGAN adversarial loss, and identity loss (<inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#955;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.5). Training was conducted on an unpaired dataset of 7371 real dusty and 7371 synthetic clean images using the Adam optimizer (lr = 0.0002, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#946;</mml:mi></mml:mrow></mml:math></inline-formula>1 = 0.5) for 300 epochs with a fixed batch size of 8, maintaining a 1:1 synthetic-to-real data ratio during detector training to preserve CycleGAN output diversity while strictly adhering to the original CycleGAN methodology without architectural modifications or pretraining.</p><p>Our framework creates a closed-loop system where CycleGAN generates DOM training data with physically accurate dust patterns. These synthetic data directly shape MBFILNet&#8217;s architectural design; specifically, its MBFI-DO and DSC-NLA modules are optimized to handle the characteristic interference patterns present in CycleGAN-generated data. To rule out data leakage, we perform forced bundling of adjacent time-series data, selecting data samples from different time periods as the training set and test set so as to ensure complete independence between the training set and the test set.</p></sec><sec id="sec4dot2-sensors-25-05324"><title>4.2. Evaluation Metrics</title><p>In the object detection experiments on the DOM and the DAWN public dusty cityscape datasets, the models were pretrained for 100 epochs on the COCO dataset. Specifically, the number of epochs and learning rate were set to 200 and 0.01 in the training of the models, and the batch size was set to 16. In addition, mean Average Precision (mAP) is adopted as the evaluation criterion, with a confidence threshold of 0.5. The formulas for precision, recall, AP, and mAP are expressed as follows:<disp-formula id="FD11-sensors-25-05324"><label>(11)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05324"><label>(12)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05324"><label>(13)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD14-sensors-25-05324"><label>(14)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of true-positive bounding boxes with an IoU &gt; 0.5, <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of false-positive bounding boxes with an IoU &#8804; 0.5, <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is represents false negatives, and <italic toggle="yes">N</italic> refers to the number of object classes.</p></sec><sec id="sec4dot3-sensors-25-05324"><title>4.3. Experiments on DOM and DAWN</title><p>Comparison experiments with several classic object detection models are conducted in this section, demonstrating the excellent performance of MBFILNet in dusty open-pit mines. Comparison models include the YOLO series, dusty image restoration before detection (DIR), domain adaptation detection (DAD) methods, and enhanced detection algorithms (EDG). All models are trained and tested based on DOM and DAWN, and training is stopped when the model reaches convergence.</p><p>The mAP and FPS of the object detection models in different dust conditions are shown in <xref rid="sensors-25-05324-f006" ref-type="fig">Figure 6</xref>. As can be seen from <xref rid="sensors-25-05324-f006" ref-type="fig">Figure 6</xref>, compared with mainstream detection methods for adverse weather conditions, MBFILNet achieved the highest detection accuracy. More importantly, while delivering this exceptional precision, MBFILNet also maintains competitive FPS performance, surpassing most existing models. This dual advantage highlights that MBFILNet has achieved an optimal balance between detection accuracy and computational efficiency.</p><p>As can be seen from <xref rid="sensors-25-05324-t001" ref-type="table">Table 1</xref>, compared with DIR, a mAP value of 72.0% is obtained by MBFILNet, which is higher than that of DSNet, IA-YOLO, and BAD-Net by 3.8%, 2.7%, and 0.6%, respectively. Simultaneously, in comparison with the DAD MIC, the mAP of the proposed model is enhanced by 0.9% and 1.1% on the DOM and DAWN datasets, respectively. Furthermore, MBFILNet surpasses the advanced Featenhancer EDG by 1.3% on DOM and 2.6% on DAWN. According to the FPS indicators in <xref rid="sensors-25-05324-t001" ref-type="table">Table 1</xref>, the FPS values of DIR, DAD, and EDG are substantially lower, rendering them unsuitable for object detection of AMTs. In addition, compared with the base YOLOv8 model, the mAP values of MBFILNet are improved by 2.0% and 3.7% on the DOM and DAWN datasets. MDFILNet achieves best accuracy performance compared to the original models while maintaining efficient operation, with only a modest 17.8 FPS increase in computational cost. The optimal balance between accuracy and processing speed fully meets the real-time requirements of AMTs. Notably, MBFILNet also outperforms the newer YOLO11, which increased mAP by 1.8% on the DOM dataset and 1.7% on the DAWN dataset.</p><p>The detection results of the above models on the DOM datasets are visualized in <xref rid="sensors-25-05324-f007" ref-type="fig">Figure 7</xref>. Although BAD-Net adaptively enhances the input images by eliminating weather-specific information, some relevant target features are inevitably lost during the process. In comparison, YOLO11 effectively enhances multi-scale feature fusion through cross-scale connections and deformable convolutional modules. However, false detections in complex dusty environments occur due to inadequate structural information. Moreover, while multi-scale feature are extracted by the multiple layers of YOLOv8, it cannot overcome severe dust interference. In contrast, MBFILNet achieves better performance by precisely capturing important object details and structural information in dusty environments.</p><p>It can be concluded that MBFILNet performs best in various experiments compared with various classic algorithms on the DOM and DAWN datasets. The comparison shows that the introduction of MBFI-DO and DSC-NLA in MBFILNet not only improves detection accuracy but also significantly reduces both false and missed detections, exhibiting stronger overall robustness.</p></sec><sec id="sec4dot4-sensors-25-05324"><title>4.4. Ablation Study</title><p>This section investigates the robustness of each component of the detection method proposed in this paper. All experiments are conducted on the DOM dataset, and the baseline model is built based on YOLOv8, with results shown in <xref rid="sensors-25-05324-t002" ref-type="table">Table 2</xref>.</p><p>As can be seen from <xref rid="sensors-25-05324-t002" ref-type="table">Table 2</xref>, YOLOv8-MBFI-DO improved mAP by 1.4% compared with YOLOv8. As illustrated in <xref rid="sensors-25-05324-f008" ref-type="fig">Figure 8</xref>, the interference from the dust background is reduced by MBFI-DO under the guidance of semantic information, thereby enhancing the salient feature representation of target objects. This demonstrates that MBFI-DO can improve detection accuracy by focusing on the essential characteristics of objects in open-pit mines affected by dust.</p><p>In addition, mAP values based on YOLOv8-DSC-NLA are improved by 1.6% relative to YOLOv8. As a result, in comparison to standard NLA, DSC-NLA demonstrates better suitability for target detection of AMTs in dusty conditions in comparative ablation studies. <xref rid="sensors-25-05324-f009" ref-type="fig">Figure 9</xref> verifies that target structural information is enhanced by modeling global spatial long-range dependencies based on DSC-NLA, thereby emphasizing contour boundary features between objects.</p><p>Notably, the mAP is increased by 2.0% using the combination of MBFI-DO and DSC-NLA, where local semantic information is enriched to reduce dusty background interference. Furthermore, compared with YOLOv8, the MBFILNet proposed in this paper improves accuracy at the cost of a slightly increased computational load, ensuing the suitability for the mobile deployment of AMTs in dusty environments.</p><sec id="sec4dot4dot1-sensors-25-05324"><title>4.4.1. Ablation Experiments on MBFI-DO</title><p>In this section, ablation experiments on MBFI-DO are performed to explore the effects of MBFI and DO, the results of which are shown in <xref rid="sensors-25-05324-t003" ref-type="table">Table 3</xref>.</p><p>As can be seen from <xref rid="sensors-25-05324-t003" ref-type="table">Table 3</xref>, detection performance is enhanced by each component of MBFI-DO. Compared to YOLOv8, MBFI and DO significantly enhance mAP by 1.0% and 0.7% on DOM and 0.7% and 0.8% on DAWN, respectively. In particular, compared with the baseline model with YOLOv8, 1.0% and 2.0% mAP improvements are achieved on the DOM and DAWN datasets due to MBFI. Moreover, the salient feature representations of the targets are better focused under DO guidance, which also reduces the number of parameters.</p><p>Through the combined experiments on different pooling operations of GAP and GMP in feature extraction, it can be found that the accuracy improvement is maximized by locating GAP with larger kernel convolution after feature extraction. The reason is that context space information is better focused based on GAP with a larger kernel convolution. Meanwhile, the salient information of the target is more the focus of GMP, which employs gradient-guided feature amplification to suppress non-critical regions. In addition, the GAP and GMP effects of the exchange order achieve the worst performance, with the map decreasing by 2.1% and 3.8% on DOM and DAWN datasets, respectively (<xref rid="sensors-25-05324-t004" ref-type="table">Table 4</xref>).</p></sec><sec id="sec4dot4dot2-sensors-25-05324"><title>4.4.2. Ablation Experiments on DSC-NLA</title><p>The experimental results for SE, CA, DAM, and NLA are presented in <xref rid="sensors-25-05324-t005" ref-type="table">Table 5</xref>. Compared to DAM, NLA significantly enhances mAP by 0.6% and 1.2% on DOM and DAWN, respectively. The increased performance is attributed to the NLA modeling of dependencies between different object directions under the interference of dusty environments.</p><p>As shown in the <xref rid="sensors-25-05324-t006" ref-type="table">Table 6</xref>, compared with STDConv and GhostConv, the depth-separable (DW) convolution applied in this paper not only achieves a reduction in parameters but also improves the accuracy of the mAP on the DOM dataset by 0.8% and 0.4%, respectively, which is attributable to the fact that DW convolution achieves spatial filtering of each input channel separately, avoiding the weight coupling between channels observed in standard convolution.</p></sec></sec><sec id="sec4dot5-sensors-25-05324"><title>4.5. Robustness Evaluation</title><p>In the DOM dataset, we categorized the test sets into three levels (clear, light dust, and heavy dust) for dust conditions in open-pit mining environments. Clear conditions with visibility exceeding 80 m represent normal operations. Light dust conditions occur when visibility ranges between 30 and 80 m, indicating moderately challenging working environments. Heavy dust conditions emerge when visibility drops below 30 m. Based on the above standards, the test proportions of clear weather, light dust, and heavy dust are 37%, 52%, and 11%, respectively.</p><p>The mAP curves in different dust scenarios are shown in <xref rid="sensors-25-05324-f010" ref-type="fig">Figure 10</xref>. As can be found from <xref rid="sensors-25-05324-f010" ref-type="fig">Figure 10</xref>, MBFILNet demonstrates the most outstanding detection performance under heavy dust conditions, fully demonstrating the robustness of MBFILNet against interference in dusty environments. It is worth noting that as the dust becomes more severe, the detection performance of existing algorithms shows a significant downward trend. This phenomenon further verifies the fact that dust conditions could exacerbate the difficulty of AMT target detection.</p><p>Additionally, the experimental results demonstrate MBFILNet&#8217;s superior robustness under challenging dust conditions. As indicated in <xref rid="sensors-25-05324-t007" ref-type="table">Table 7</xref>, MBFILNet achieves the highest detection accuracy of 68.7% mAP in heavy dust environments, outperforming all comparable real-time methods. Notably, it maintains a significant 0.5% mAP improvement over R-YOLO while delivering comparable processing speeds of 185.6 FPS versus 113.7 FPS. While conventional YOLO-series detectors exhibit excellent computational efficiency, they show notable performance degradation in heavy dust conditions, with mAP values of YOLOv9 approximately 1.6% lower than those of MBFILNet. Compared with YOLO-series models, the proposed MBFILNet model successfully addresses the common trade-off between speed and accuracy in dust-obscured environments, achieving both real-time processing capabilities and superior detection robustness.</p><p><xref rid="sensors-25-05324-t008" ref-type="table">Table 8</xref> presents the detailed performance metrics of MBFILNet over five independent training runs. The results show minimal fluctuation across runs, with run 3 achieving the highest mAP value of 72.2% on DOM and run 4 showing the best performance of 56.1% on DAWN. The calculated average mAP values of 72.02% and 55.88% confirm the stability of our approach, while the narrow standard deviations of &#177;0.2% and &#177;0.3% further substantiate the reproducibility of these improvements. These comprehensive measurements address potential concerns about performance variance and validate the reliability of our reported results.</p><p>Furthermore, in order to verify the generalization ability of MBFILNet, we also conducted experimental comparisons on the public non-dust KITTI dataset. A comparison of the performance of object detection methods on the KITTI dataset is shown in <xref rid="sensors-25-05324-t009" ref-type="table">Table 9</xref>. In <xref rid="sensors-25-05324-t009" ref-type="table">Table 9</xref>. With a precision of 94.2% and a recall of 88.6%, MBFILNet achieves the highest mAP value of 93.4% among all evaluated methods, surpassing Faster R-CNN at 5.2%, YOLOv8 at 0.6%, and YOLOv9 at 1.0%. In terms of processing speed, MBFILNet maintains excellent efficiency at 185.6 FPS, significantly outperforming Transformer-based approaches like RT-DETRv2-R18, achieving an mAP value of 90.4% at only 28.9 FPS. This comprehensive evaluation demonstrates MBFILNet as a particularly effective solution for autonomous driving applications where both detection accuracy and real-time performance are critical requirements. The comparative analysis of object detection methods on the KITTI dataset demonstrates MBFILNet&#8217;s superior generalization across different scenarios.</p><p>As shown in <xref rid="sensors-25-05324-t009" ref-type="table">Table 9</xref>, the comparative analysis of object detection methods on the KITTI dataset demonstrates MBFILNet&#8217;s superior performance across multiple metrics. With a precision of 94.2% and recall of 88.6%, MBFILNet achieves the highest mAP value of 93.4% among all evaluated methods, surpassing Faster R-CNN at 5.2%, YOLOv8 at 0.6%, and YOLOv9 at 1.0%. In terms of processing speed, MBFILNet maintains excellent efficiency at 185.6 FPS, significantly outperforming Transformer-based approaches like RT-DETRv2-R18, which achieves an mAP value of 90.4% at only 28.9 FPS. This comprehensive evaluation positions MBFILNet as a particularly effective solution for autonomous driving applications where both detection accuracy and real-time performance are critical requirements. The above comparative experiments also demonstrate the outstanding performance of MBFILNet on a non-dusty dataset, highlighting the generalization ability of MBFILNet.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05324"><title>5. Conclusions</title><p>To improve detection accuracy in dusty environments, an efficient object detector for AMTs in dusty environments called MBFILNet is proposed in this paper, which incorporates MBFI-DO and DSC-NLA modules. MBFI-DO enhances discriminative features in target regions and integrated semantic information across multiple levels. DSC-NLA captures global spatial long-range dependencies based on pixel correlations. Meanwhile, a feature fusion strategy is implemented to aggregate diverse receptive-field representations, enhancing multi-scale object detection capability.</p><p>According to abundant experiments and comparison with the latest methods on self-made and public datasets, complemented by extensive validation and data analysis, the proposed MBFILNet performs better in AMT object detection under dusty conditions. Notably, MBFILNet achieved an mAP of 72.0% on our self-made DOM dataset and 55.8% on the public DAWN dataset, representing significant improvements of 2.0% and 3.7%, respectively, over the baseline YOLOv8 model. The superior capability of MBFILNet is demonstrated by these gains, addressing the challenges of low object detection accuracy caused by hard extraction of salient represent feature and edge information in dusty backgrounds.</p><p>Although the proposed MBFILNet performs robust detection in dusty environments, it performance in extremely dust-laden environments is unsatisfied. Extremely dust-laden environments include environments with extremely high dust concentrations and visibility lower than 5 m, as well as sandstorm conditions. Furthermore, the unique working environment of open-pit mines is not only associated with significant dust problems but also frequent complex adverse conditions such as low light, rain, and snow. When these environmental factors occur simultaneously with dust, more extreme multimodal interference scenarios are formed, posing even greater challenges to the perception systems of unmanned mining trucks. As extremely dust-laden conditions severely degrade image quality and cause significant information loss, target discriminability should be enhanced by means of multi-sensor fusion in the future.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, C.Z.; methodology, C.Z.; software, D.-L.Z. and Y.-P.H.; validation, R.Z.; formal analysis, F.-X.X.; investigation, Y.-P.H.; resources, R.Z.; writing&#8212;original draft preparation, D.-L.Z.; writing&#8212;review and editing, F.-X.X.; visualization, D.-L.Z.; supervision, F.-X.X.; project administration, F.-X.X.; funding acquisition, F.-X.X. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available from the corresponding author upon request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05324"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sangiovanni-Vincentelli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>C.</given-names></name></person-group><article-title>Fear-Neuro-Inspired Reinforcement Learning for Safe Autonomous Driving</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>267</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3322426</pub-id><pub-id pub-id-type="pmid">37801378</pub-id></element-citation></ref><ref id="B2-sensors-25-05324"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name></person-group><article-title>FusionPlanner: A multi-task motion planner for mining trucks via multi-sensor fusion</article-title><source>Mech. Syst. Signal Process.</source><year>2024</year><volume>208</volume><fpage>111051</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2023.111051</pub-id></element-citation></ref><ref id="B3-sensors-25-05324"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>F.</given-names></name></person-group><article-title>MSFANet: A Light Weight Object Detector Based on Context Aggregation and Attention Mechanism for Autonomous Mining Truck</article-title><source>IEEE Trans. Intell. Veh.</source><year>2023</year><volume>8</volume><fpage>2285</fpage><lpage>2295</lpage><pub-id pub-id-type="doi">10.1109/TIV.2022.3221767</pub-id></element-citation></ref><ref id="B4-sensors-25-05324"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name></person-group><article-title>Research on Lightweight Open-Pit Mine Driving Obstacle Detection Algorithm Based on Improved YOLOv8s</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>11741</elocation-id><pub-id pub-id-type="doi">10.3390/app142411741</pub-id></element-citation></ref><ref id="B5-sensors-25-05324"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Song</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>Y.</given-names></name></person-group><article-title>An Feature Fusion Object Detector for Autonomous Driving in Mining Area</article-title><source>Proceedings of the 2021 International Conference on Cyber-Physical Social Intelligence (ICCSI)</source><conf-loc>Beijing, China</conf-loc><conf-date>18&#8211;20 December 2021</conf-date><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/ICCSI53130.2021.9736214</pub-id></element-citation></ref><ref id="B6-sensors-25-05324"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Song</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>C.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.Y.</given-names></name></person-group><article-title>PMScenes: A Parallel Mine Dataset for Autonomous Driving in Surface Mines</article-title><source>IEEE Intell. Transp. Syst. Mag.</source><year>2025</year><volume>17</volume><fpage>30</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1109/MITS.2024.3454275</pub-id></element-citation></ref><ref id="B7-sensors-25-05324"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Teng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xuanyuan</surname><given-names>Z.</given-names></name><etal/></person-group><article-title>AutoMine: An Unmanned Mine Dataset</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;24 June 2022</conf-date><fpage>21276</fpage><lpage>21285</lpage><pub-id pub-id-type="doi">10.1109/CVPR52688.2022.02062</pub-id></element-citation></ref><ref id="B8-sensors-25-05324"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Deep Feature Enhancement Method for Land Cover With Irregular and Sparse Spatial Distribution Features: A Case Study on Open-Pit Mining</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>4401220</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2023.3241331</pub-id></element-citation></ref><ref id="B9-sensors-25-05324"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name></person-group><article-title>A method of mining truck loading volume detection based on deep learning and image recognition</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>635</elocation-id><pub-id pub-id-type="doi">10.3390/s21020635</pub-id><pub-id pub-id-type="pmid">33477512</pub-id><pub-id pub-id-type="pmcid">PMC7831092</pub-id></element-citation></ref><ref id="B10-sensors-25-05324"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>L.</given-names></name></person-group><article-title>A scheme to detect the intensity of dusty weather by applying microwave radars and lidar</article-title><source>Sci. Total Environ.</source><year>2023</year><volume>859</volume><fpage>160248</fpage><pub-id pub-id-type="doi">10.1016/j.scitotenv.2022.160248</pub-id><pub-id pub-id-type="pmid">36403835</pub-id></element-citation></ref><ref id="B11-sensors-25-05324"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>F.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Circle-YOLO: An anchor-free lung nodule detection algorithm using bounding circle representation</article-title><source>Pattern Recognit.</source><year>2025</year><volume>161</volume><fpage>111294</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2024.111294</pub-id></element-citation></ref><ref id="B12-sensors-25-05324"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Djenouri</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Belhadi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Srivastava</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.C.W.</given-names></name></person-group><article-title>Hybrid graph convolution neural network and branch-and-bound optimization for traffic flow forecasting</article-title><source>Future Gener. Comput. Syst.</source><year>2023</year><volume>139</volume><fpage>100</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.future.2022.09.018</pub-id></element-citation></ref><ref id="B13-sensors-25-05324"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jeon</surname><given-names>M.</given-names></name><name name-style="western"><surname>Seo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Min</surname><given-names>J.</given-names></name></person-group><article-title>DA-RAW: Domain Adaptive Object Detection for Real-World Adverse Weather Conditions</article-title><source>Proceedings of the 2024 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>13&#8211;17 May 2024</conf-date><fpage>2013</fpage><lpage>2020</lpage><pub-id pub-id-type="doi">10.1109/ICRA57147.2024.10611219</pub-id></element-citation></ref><ref id="B14-sensors-25-05324"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Le</surname><given-names>T.H.</given-names></name><name name-style="western"><surname>Jaw</surname><given-names>D.W.</given-names></name></person-group><article-title>DSNet: Joint Semantic Learning for Object Detection in Inclement Weather Conditions</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>43</volume><fpage>2623</fpage><lpage>2633</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.2977911</pub-id><pub-id pub-id-type="pmid">32149681</pub-id></element-citation></ref><ref id="B15-sensors-25-05324"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Image-adaptive YOLO for Object Detection in Adverse Weather Conditions</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Virtual</conf-loc><conf-date>22 February&#8211;1 March 2022</conf-date><publisher-name>AAAI Press</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2022</year><volume>Volume 36</volume><fpage>1792</fpage><lpage>1800</lpage></element-citation></ref><ref id="B16-sensors-25-05324"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>M.</given-names></name></person-group><article-title>CF-YOLO: Cross Fusion YOLO for Object Detection in Adverse Weather With a High-Quality Real Snow Dataset</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2023</year><volume>24</volume><fpage>10749</fpage><lpage>10759</lpage><pub-id pub-id-type="doi">10.1109/TITS.2023.3285035</pub-id></element-citation></ref><ref id="B17-sensors-25-05324"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>R-YOLO: A Robust Object Detector in Adverse Weather</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>72</volume><fpage>5000511</fpage><pub-id pub-id-type="doi">10.1109/TIM.2022.3229717</pub-id></element-citation></ref><ref id="B18-sensors-25-05324"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Z.J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.L.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gui</surname><given-names>W.F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.Q.</given-names></name></person-group><article-title>Mine image enhancement algorithm based on dual domaindecomposition</article-title><source>Acta Photonica Sin.</source><year>2019</year><volume>48</volume><fpage>107</fpage><lpage>119</lpage></element-citation></ref><ref id="B19-sensors-25-05324"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name></person-group><article-title>MSS-YOLO: Multi-Scale Edge-Enhanced Lightweight Network for Personnel Detection and Location in Coal Mines</article-title><source>Appl. Sci.</source><year>2025</year><volume>15</volume><elocation-id>3238</elocation-id><pub-id pub-id-type="doi">10.3390/app15063238</pub-id></element-citation></ref><ref id="B20-sensors-25-05324"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name></person-group><article-title>Feature Pyramid Networks for Object Detection</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>936</fpage><lpage>944</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.106</pub-id></element-citation></ref><ref id="B21-sensors-25-05324"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>He</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Z.M.</given-names></name></person-group><article-title>DEA-Net: Single Image Dehazing Based on Detail-Enhanced Convolution and Content-Guided Attention</article-title><source>IEEE Trans. Image Process.</source><year>2024</year><volume>33</volume><fpage>1002</fpage><lpage>1015</lpage><pub-id pub-id-type="doi">10.1109/TIP.2024.3354108</pub-id><pub-id pub-id-type="pmid">38252568</pub-id></element-citation></ref><ref id="B22-sensors-25-05324"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>Z.</given-names></name></person-group><article-title>STADE-CDNet: Spatial&#8211;Temporal Attention With Difference Enhancement-Based Network for Remote Sensing Image Change Detection</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>5611617</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3367948</pub-id></element-citation></ref><ref id="B23-sensors-25-05324"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sakaridis</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gool</surname><given-names>L.V.</given-names></name></person-group><article-title>Scale-Aware Domain Adaptive Faster R-CNN</article-title><source>Int. J. Comput. Vis.</source><year>2021</year><volume>129</volume><fpage>2223</fpage><lpage>2243</lpage><pub-id pub-id-type="doi">10.1007/s11263-021-01447-x</pub-id></element-citation></ref><ref id="B24-sensors-25-05324"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name></person-group><article-title>A Modality Alignment and Fusion-Based Method for Around-the-Clock Remote Sensing Object Detection</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>4964</elocation-id><pub-id pub-id-type="doi">10.3390/s25164964</pub-id><pub-id pub-id-type="pmid">40871828</pub-id><pub-id pub-id-type="pmcid">PMC12390532</pub-id></element-citation></ref><ref id="B25-sensors-25-05324"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Dehghan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jagersand</surname><given-names>M.</given-names></name></person-group><article-title>U2-Net: Going deeper with nested U-structure for salient object detection</article-title><source>Pattern Recognit.</source><year>2020</year><volume>106</volume><fpage>107404</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2020.107404</pub-id></element-citation></ref><ref id="B26-sensors-25-05324"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.W.</given-names></name></person-group><article-title>Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2025</year><volume>36</volume><fpage>595</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2023.3336563</pub-id><pub-id pub-id-type="pmid">38261502</pub-id></element-citation></ref><ref id="B27-sensors-25-05324"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Noman</surname><given-names>M.</given-names></name><name name-style="western"><surname>Fiaz</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cholakkal</surname><given-names>H.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>F.S.</given-names></name></person-group><article-title>ELGC-Net: Efficient Local&#8211;Global Context Aggregation for Remote Sensing Change Detection</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>4701611</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3362914</pub-id></element-citation></ref><ref id="B28-sensors-25-05324"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>F.</given-names></name></person-group><article-title>Efficient Camouflaged Object Detection Network Based on Global Localization Perception and Local Guidance Refinement</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>34</volume><fpage>5452</fpage><lpage>5465</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2023.3349209</pub-id></element-citation></ref><ref id="B29-sensors-25-05324"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>A.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><article-title>Non-local Neural Networks</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City</source><conf-loc>UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>7794</fpage><lpage>7803</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00813</pub-id></element-citation></ref><ref id="B30-sensors-25-05324"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City</source><conf-loc>UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>6848</fpage><lpage>6856</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00716</pub-id></element-citation></ref><ref id="B31-sensors-25-05324"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ruan</surname><given-names>J.</given-names></name></person-group><article-title>YOLO-CJ: A Lightweight Network for Compound Jamming Signal Detection</article-title><source>IEEE Trans. Aerosp. Electron. Syst.</source><year>2024</year><volume>60</volume><fpage>6807</fpage><lpage>6821</lpage><pub-id pub-id-type="doi">10.1109/TAES.2024.3406491</pub-id></element-citation></ref><ref id="B32-sensors-25-05324"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>M.</given-names></name></person-group><article-title>Attention guided neural network models for occluded pedestrian detection</article-title><source>Pattern Recognit. Lett.</source><year>2020</year><volume>131</volume><fpage>91</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2019.12.010</pub-id></element-citation></ref><ref id="B33-sensors-25-05324"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>M.M.</given-names></name></person-group><article-title>YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time Object Detection</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2025</year><volume>47</volume><fpage>4240</fpage><lpage>4252</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2025.3538473</pub-id><pub-id pub-id-type="pmid">40031746</pub-id></element-citation></ref><ref id="B34-sensors-25-05324"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>G.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>C.</given-names></name></person-group><article-title>GSTNet: Global Spatial-Temporal Network for Traffic Flow Prediction</article-title><source>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence IJCAI-19</source><conf-loc>Macao, China</conf-loc><conf-date>10&#8211;16 August 2019</conf-date><fpage>2286</fpage><lpage>2293</lpage></element-citation></ref><ref id="B35-sensors-25-05324"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qiao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>R.</given-names></name></person-group><article-title>BGI-Net: Bilayer Graph Inference Network for Low Light Image Enhancement</article-title><source>Pattern Recognit. Lett.</source><year>2025</year><volume>190</volume><fpage>29</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2025.02.001</pub-id></element-citation></ref><ref id="B36-sensors-25-05324"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>K.</given-names></name></person-group><article-title>Mine-DW-Fusion: BEV Multiscale-Enhanced Fusion Object-Detection Model for Underground Coal Mine Based on Dynamic Weight Adjustment</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>5185</elocation-id><pub-id pub-id-type="doi">10.3390/s25165185</pub-id><pub-id pub-id-type="pmid">40872048</pub-id><pub-id pub-id-type="pmcid">PMC12390041</pub-id></element-citation></ref><ref id="B37-sensors-25-05324"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name></person-group><article-title>Detection-Friendly Dehazing: Object Detection in Real-World Hazy Scenes</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>8284</fpage><lpage>8295</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3234976</pub-id><pub-id pub-id-type="pmid">37018582</pub-id></element-citation></ref><ref id="B38-sensors-25-05324"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style="western"><surname>He</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name></person-group><article-title>MDAM-DRNet: Dual Channel Residual Network With Multi-Directional Attention Mechanism in Strawberry Leaf Diseases Detection</article-title><source>Front. Plant Sci.</source><year>2022</year><volume>45</volume><fpage>8284</fpage><lpage>8295</lpage><pub-id pub-id-type="doi">10.3389/fpls.2022.869524</pub-id><pub-id pub-id-type="pmcid">PMC9305473</pub-id><pub-id pub-id-type="pmid">35874000</pub-id></element-citation></ref><ref id="B39-sensors-25-05324"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hoyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date><fpage>11721</fpage><lpage>11732</lpage></element-citation></ref><ref id="B40-sensors-25-05324"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>W.</given-names></name></person-group><article-title>PP-YOLOE: An evolved version of YOLO</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;24 June 2022</conf-date><fpage>16250</fpage><lpage>16257</lpage></element-citation></ref><ref id="B41-sensors-25-05324"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name></person-group><article-title>DeNet: Detection-Driven Enhancement Network for Object Detection Under Adverse Weather Conditions</article-title><source>Proceedings of the Asian Conference on Computer Vision (ACCV)</source><conf-loc>Macao, China</conf-loc><conf-date>4&#8211;8 December 2022</conf-date><fpage>2813</fpage><lpage>2829</lpage></element-citation></ref><ref id="B42-sensors-25-05324"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>He</surname><given-names>W.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Han</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Gold-YOLO: Efficient object detector via gather-and-distribute mechanism</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2023</year><volume>36</volume><fpage>51094</fpage><lpage>51112</lpage></element-citation></ref><ref id="B43-sensors-25-05324"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hashmi</surname><given-names>K.A.</given-names></name><name name-style="western"><surname>Kallempudi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Stricker</surname><given-names>D.</given-names></name><name name-style="western"><surname>Afzal</surname><given-names>M.Z.</given-names></name></person-group><article-title>FeatEnhancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>6725</fpage><lpage>6735</lpage></element-citation></ref><ref id="B44-sensors-25-05324"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>DETRs Beat YOLOs on Real-time Object Detection</article-title><source>Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;24 June 2024</conf-date><fpage>16965</fpage><lpage>16974</lpage><pub-id pub-id-type="doi">10.1109/CVPR52733.2024.01605</pub-id></element-citation></ref><ref id="B45-sensors-25-05324"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>YOLOv8-QSD: An Improved Small Object Detection Algorithm for Autonomous Vehicles Based on YOLOv8</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2024</year><volume>73</volume><fpage>2513916</fpage><pub-id pub-id-type="doi">10.1109/TIM.2024.3379090</pub-id></element-citation></ref><ref id="B46-sensors-25-05324"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>I.H.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.Y.M.</given-names></name></person-group><article-title>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2025</year><fpage>1</fpage><lpage>21</lpage></element-citation></ref><ref id="B47-sensors-25-05324"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name></person-group><article-title>Yolov10: Real-time end-to-end object detection</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2405.14458</pub-id></element-citation></ref><ref id="B48-sensors-25-05324"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khanam</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>M.</given-names></name></person-group><article-title>Yolov11: An overview of the key architectural enhancements</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2410.17725</pub-id><pub-id pub-id-type="arxiv">2410.17725</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05324-f001" orientation="portrait"><label>Figure 1</label><caption><p>Dusty images in open-pit mine.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g001.jpg"/></fig><fig position="float" id="sensors-25-05324-f002" orientation="portrait"><label>Figure 2</label><caption><p>Model architecture diagram of MBFILNet.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g002.jpg"/></fig><fig position="float" id="sensors-25-05324-f003" orientation="portrait"><label>Figure 3</label><caption><p>Schematic representation of MBFI-DO.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g003.jpg"/></fig><fig position="float" id="sensors-25-05324-f004" orientation="portrait"><label>Figure 4</label><caption><p>Schematic of DSC-NLA.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g004.jpg"/></fig><fig position="float" id="sensors-25-05324-f005" orientation="portrait"><label>Figure 5</label><caption><p>Distribution of different objects in DOM.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g005.jpg"/></fig><fig position="float" id="sensors-25-05324-f006" orientation="portrait"><label>Figure 6</label><caption><p>FPS and mAP in different dust conditions. (<bold>a</bold>) mAP; (<bold>b</bold>) FPS.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g006.jpg"/></fig><fig position="float" id="sensors-25-05324-f007" orientation="portrait"><label>Figure 7</label><caption><p>Object detection results with different methods on DOM. (<bold>a</bold>) Ground truth; (<bold>b</bold>) BAD-Net, (<bold>c</bold>) YOLO11; (<bold>d</bold>) YOLOv8; (<bold>e</bold>) MBFILNet.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g007.jpg"/></fig><fig position="float" id="sensors-25-05324-f008" orientation="portrait"><label>Figure 8</label><caption><p>Heatmap visualization comparison with different methods. (<bold>a</bold>) Original image; (<bold>b</bold>) feature Heatmap produced by YOLOv8; (<bold>c</bold>) feature heatmap produced by YOLOv8-MBFI-DO.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g008.jpg"/></fig><fig position="float" id="sensors-25-05324-f009" orientation="portrait"><label>Figure 9</label><caption><p>Heatmap visualization comparison with different methods. (<bold>a</bold>) The original image; (<bold>b</bold>) feature heatmap produced using YOLOv8; (<bold>c</bold>) feature heatmap produced by YOLOv8-DSC-NLA.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g009.jpg"/></fig><fig position="float" id="sensors-25-05324-f010" orientation="portrait"><label>Figure 10</label><caption><p>mAP under different dust conditions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05324-g010.jpg"/></fig><table-wrap position="float" id="sensors-25-05324-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t001_Table 1</object-id><label>Table 1</label><caption><p>Quantitative comparison of results with those of mainstream detection methods on the DOM and DAWN datasets. Bold values indicate the best performance for each metric.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Method</th><th align="center" rowspan="3" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">FPS (f s<sup>&#8722;1</sup>)</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Datasets</th></tr><tr><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">DOM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DAWN</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bulldozer</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mining-Truck</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Excavators</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Loader</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP (%)</th></tr></thead><tbody><tr><td align="center" rowspan="3" valign="middle" style="border-bottom:solid thin" colspan="1">DIR</td><td align="center" valign="middle" rowspan="1" colspan="1">DSNet [<xref rid="B14-sensors-25-05324" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">83.4</td><td align="center" valign="middle" rowspan="1" colspan="1">60.8</td><td align="center" valign="middle" rowspan="1" colspan="1">75.3</td><td align="center" valign="middle" rowspan="1" colspan="1">70.1</td><td align="center" valign="middle" rowspan="1" colspan="1">66.6</td><td align="center" valign="middle" rowspan="1" colspan="1">68.2</td><td align="center" valign="middle" rowspan="1" colspan="1">51.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">IA-YOLO [<xref rid="B15-sensors-25-05324" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">76.2</td><td align="center" valign="middle" rowspan="1" colspan="1">61.9</td><td align="center" valign="middle" rowspan="1" colspan="1">76.7</td><td align="center" valign="middle" rowspan="1" colspan="1">71.2</td><td align="center" valign="middle" rowspan="1" colspan="1">67.4</td><td align="center" valign="middle" rowspan="1" colspan="1">69.3</td><td align="center" valign="middle" rowspan="1" colspan="1">53.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BAD-Net [<xref rid="B37-sensors-25-05324" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>69.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.6</td></tr><tr><td align="center" rowspan="3" valign="middle" style="border-bottom:solid thin" colspan="1">DAD</td><td align="center" valign="middle" rowspan="1" colspan="1">MDAM [<xref rid="B38-sensors-25-05324" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">52.3</td><td align="center" valign="middle" rowspan="1" colspan="1">62.9</td><td align="center" valign="middle" rowspan="1" colspan="1">77.8</td><td align="center" valign="middle" rowspan="1" colspan="1">72.3</td><td align="center" valign="middle" rowspan="1" colspan="1">69.0</td><td align="center" valign="middle" rowspan="1" colspan="1">70.5</td><td align="center" valign="middle" rowspan="1" colspan="1">52.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">R-YOLO [<xref rid="B17-sensors-25-05324" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">113.7</td><td align="center" valign="middle" rowspan="1" colspan="1">63.2</td><td align="center" valign="middle" rowspan="1" colspan="1">78.3</td><td align="center" valign="middle" rowspan="1" colspan="1">72.7</td><td align="center" valign="middle" rowspan="1" colspan="1">69.4</td><td align="center" valign="middle" rowspan="1" colspan="1">70.9</td><td align="center" valign="middle" rowspan="1" colspan="1">55.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIC [<xref rid="B39-sensors-25-05324" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.7</td></tr><tr><td align="center" rowspan="5" valign="middle" style="border-bottom:solid thin" colspan="1">EDG</td><td align="center" valign="middle" rowspan="1" colspan="1">PPYOLOE [<xref rid="B40-sensors-25-05324" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">149.1</td><td align="center" valign="middle" rowspan="1" colspan="1">58.7</td><td align="center" valign="middle" rowspan="1" colspan="1">72.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.8</td><td align="center" valign="middle" rowspan="1" colspan="1">64.5</td><td align="center" valign="middle" rowspan="1" colspan="1">65.9</td><td align="center" valign="middle" rowspan="1" colspan="1">48.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DENet [<xref rid="B41-sensors-25-05324" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">69.9</td><td align="center" valign="middle" rowspan="1" colspan="1">60.2</td><td align="center" valign="middle" rowspan="1" colspan="1">74.5</td><td align="center" valign="middle" rowspan="1" colspan="1">69.3</td><td align="center" valign="middle" rowspan="1" colspan="1">66.0</td><td align="center" valign="middle" rowspan="1" colspan="1">67.5</td><td align="center" valign="middle" rowspan="1" colspan="1">49.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gold-YOLO [<xref rid="B42-sensors-25-05324" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">132.9</td><td align="center" valign="middle" rowspan="1" colspan="1">62.1</td><td align="center" valign="middle" rowspan="1" colspan="1">76.8</td><td align="center" valign="middle" rowspan="1" colspan="1">71.5</td><td align="center" valign="middle" rowspan="1" colspan="1">68.0</td><td align="center" valign="middle" rowspan="1" colspan="1">69.6</td><td align="center" valign="middle" rowspan="1" colspan="1">51.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Featenhancer [<xref rid="B43-sensors-25-05324" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">69.5</td><td align="center" valign="middle" rowspan="1" colspan="1">63.1</td><td align="center" valign="middle" rowspan="1" colspan="1">77.9</td><td align="center" valign="middle" rowspan="1" colspan="1">72.6</td><td align="center" valign="middle" rowspan="1" colspan="1">69.2</td><td align="center" valign="middle" rowspan="1" colspan="1">70.7</td><td align="center" valign="middle" rowspan="1" colspan="1">53.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RT-DETR [<xref rid="B44-sensors-25-05324" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">114.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.2</td></tr><tr><td align="center" rowspan="4" valign="middle" style="border-bottom:solid thin" colspan="1">YOLO Series</td><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8 [<xref rid="B45-sensors-25-05324" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">232.1</td><td align="center" valign="middle" rowspan="1" colspan="1">62.4</td><td align="center" valign="middle" rowspan="1" colspan="1">77.1</td><td align="center" valign="middle" rowspan="1" colspan="1">71.9</td><td align="center" valign="middle" rowspan="1" colspan="1">68.6</td><td align="center" valign="middle" rowspan="1" colspan="1">70.0</td><td align="center" valign="middle" rowspan="1" colspan="1">52.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv9 [<xref rid="B46-sensors-25-05324" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">75.5</td><td align="center" valign="middle" rowspan="1" colspan="1">63.8</td><td align="center" valign="middle" rowspan="1" colspan="1">78.7</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>73.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">69.9</td><td align="center" valign="middle" rowspan="1" colspan="1">71.5</td><td align="center" valign="middle" rowspan="1" colspan="1">54.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLO10 [<xref rid="B47-sensors-25-05324" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">168.4</td><td align="center" valign="middle" rowspan="1" colspan="1">59.4</td><td align="center" valign="middle" rowspan="1" colspan="1">73.4</td><td align="center" valign="middle" rowspan="1" colspan="1">68.5</td><td align="center" valign="middle" rowspan="1" colspan="1">65.1</td><td align="center" valign="middle" rowspan="1" colspan="1">66.6</td><td align="center" valign="middle" rowspan="1" colspan="1">53.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO11 [<xref rid="B48-sensors-25-05324" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>271.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MBFILNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">185.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>64.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>79.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>55.8</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05324-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t002_Table 2</object-id><label>Table 2</label><caption><p>Ablation experiments on different module combinations. Bold values indicate the best performance for each metric.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="2" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Module</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Metrics</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MBFI-DO</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DSC-NLA</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Para (M)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DOM mAP (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="3" colspan="1">YOLOv8</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>3.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>8.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">70.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">3.6</td><td align="center" valign="middle" rowspan="1" colspan="1">9.5</td><td align="center" valign="middle" rowspan="1" colspan="1">71.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>72.0</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05324-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison and verification of MBFI-DO performance. Bold values indicate the best performance for each metric.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Module</th><th colspan="4" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Metrics</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MBFI</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DO</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ADD</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Para (M)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DOM mAP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DAWN mAP (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">3.4</td><td align="center" valign="middle" rowspan="1" colspan="1">9.2</td><td align="center" valign="middle" rowspan="1" colspan="1">71.0</td><td align="center" valign="middle" rowspan="1" colspan="1">54.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>3.3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>8.8</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">70.7</td><td align="center" valign="middle" rowspan="1" colspan="1">52.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">3.7</td><td align="center" valign="middle" rowspan="1" colspan="1">10.2</td><td align="center" valign="middle" rowspan="1" colspan="1">67.7</td><td align="center" valign="middle" rowspan="1" colspan="1">50.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">4.2</td><td align="center" valign="middle" rowspan="1" colspan="1">11.7</td><td align="center" valign="middle" rowspan="1" colspan="1">68.1</td><td align="center" valign="middle" rowspan="1" colspan="1">51.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>55.0</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05324-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison and verification of MBFI performance. Bold values indicate the best performance for each metric.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Module</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Metrics</th></tr><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Branch1</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Branch2</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DOM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DAWN</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GMP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GMP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">70.8</td><td align="center" valign="middle" rowspan="1" colspan="1">53.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">70.6</td><td align="center" valign="middle" rowspan="1" colspan="1">51.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">68.9</td><td align="center" valign="middle" rowspan="1" colspan="1">50.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>54.1</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05324-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison and verification of DSC-NLA performance. Bold values indicate the best performance for each metric.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Module</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Metrics</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SE</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CA</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DAM</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NLA</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DOM mAP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DAWN mAP (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">68.3</td><td align="center" valign="middle" rowspan="1" colspan="1">50.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">69.8</td><td align="center" valign="middle" rowspan="1" colspan="1">51.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">70.2</td><td align="center" valign="middle" rowspan="1" colspan="1">52.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>70.8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>53.3</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05324-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison and verification of DWConv performance. Bold values indicate the best performance for each metric.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Module</th><th colspan="4" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Metrics</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">STDConv</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ghost</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DW</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Para (M)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DOM mAP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DAWNmAP (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">4.9</td><td align="center" valign="middle" rowspan="1" colspan="1">10.7</td><td align="center" valign="middle" rowspan="1" colspan="1">70.8</td><td align="center" valign="middle" rowspan="1" colspan="1">53.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">4.2</td><td align="center" valign="middle" rowspan="1" colspan="1">10.1</td><td align="center" valign="middle" rowspan="1" colspan="1">71.2</td><td align="center" valign="middle" rowspan="1" colspan="1">53.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>54.1</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05324-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t007_Table 7</object-id><label>Table 7</label><caption><p>Quantified object detection performance under graded dust condition on DOM with mainstream detection methods. Bold font indicates the optimal value.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" rowspan="2" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Category</th><th align="left" valign="middle" rowspan="2" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th align="center" valign="middle" rowspan="2" style="border-top:solid thin;border-bottom:solid thin" colspan="1">FPS (f s<sup>&#8722;1</sup>)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin" rowspan="1">DOM mAP(%)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Clear</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Light</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Heavy</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">DIR</td><td align="left" valign="middle" rowspan="1" colspan="1">DSNet [<xref rid="B14-sensors-25-05324" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">83.4</td><td align="center" valign="middle" rowspan="1" colspan="1">75.1</td><td align="center" valign="middle" rowspan="1" colspan="1">70.3</td><td align="center" valign="middle" rowspan="1" colspan="1">63.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IA-YOLO [<xref rid="B15-sensors-25-05324" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">76.2</td><td align="center" valign="middle" rowspan="1" colspan="1">76.2</td><td align="center" valign="middle" rowspan="1" colspan="1">71.3</td><td align="center" valign="middle" rowspan="1" colspan="1">63.1</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BAD-Net [<xref rid="B37-sensors-25-05324" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>78.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.5</td></tr><tr><td align="left" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">DAD</td><td align="left" valign="middle" rowspan="1" colspan="1">MDAM [<xref rid="B38-sensors-25-05324" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">52.3</td><td align="center" valign="middle" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" rowspan="1" colspan="1">72.1</td><td align="center" valign="middle" rowspan="1" colspan="1">65.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">R-YOLO [<xref rid="B17-sensors-25-05324" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">113.7</td><td align="center" valign="middle" rowspan="1" colspan="1">77.8</td><td align="center" valign="middle" rowspan="1" colspan="1">72.7</td><td align="center" valign="middle" rowspan="1" colspan="1">66.2</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIC [<xref rid="B39-sensors-25-05324" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.6</td></tr><tr><td align="left" valign="middle" rowspan="5" style="border-bottom:solid thin" colspan="1">EDG</td><td align="left" valign="middle" rowspan="1" colspan="1">PYYOLOE [<xref rid="B40-sensors-25-05324" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">149.1</td><td align="center" valign="middle" rowspan="1" colspan="1">73.5</td><td align="center" valign="middle" rowspan="1" colspan="1">68.8</td><td align="center" valign="middle" rowspan="1" colspan="1">61.4</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DENet [<xref rid="B41-sensors-25-05324" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">69.9</td><td align="center" valign="middle" rowspan="1" colspan="1">74.8</td><td align="center" valign="middle" rowspan="1" colspan="1">70.0</td><td align="center" valign="middle" rowspan="1" colspan="1">63.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Gold-YOLO [<xref rid="B42-sensors-25-05324" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">132.9</td><td align="center" valign="middle" rowspan="1" colspan="1">76.5</td><td align="center" valign="middle" rowspan="1" colspan="1">71.5</td><td align="center" valign="middle" rowspan="1" colspan="1">64.8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Federalancer [<xref rid="B43-sensors-25-05324" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">69.5</td><td align="center" valign="middle" rowspan="1" colspan="1">77.6</td><td align="center" valign="middle" rowspan="1" colspan="1">72.5</td><td align="center" valign="middle" rowspan="1" colspan="1">66.0</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RT-DETR [<xref rid="B44-sensors-25-05324" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">114.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.6</td></tr><tr><td align="left" valign="middle" rowspan="4" style="border-bottom:solid thin" colspan="1">YOLO Series</td><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8 [<xref rid="B45-sensors-25-05324" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">232.1</td><td align="center" valign="middle" rowspan="1" colspan="1">77.0</td><td align="center" valign="middle" rowspan="1" colspan="1">71.9</td><td align="center" valign="middle" rowspan="1" colspan="1">65.1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv9 [<xref rid="B46-sensors-25-05324" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">75.5</td><td align="center" valign="middle" rowspan="1" colspan="1">70.7</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>73.4</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">67.1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv10 [<xref rid="B47-sensors-25-05324" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">168.4</td><td align="center" valign="middle" rowspan="1" colspan="1">74.0</td><td align="center" valign="middle" rowspan="1" colspan="1">69.2</td><td align="center" valign="middle" rowspan="1" colspan="1">62.6</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO11 [<xref rid="B48-sensors-25-05324" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>271.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.5</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MBFTLNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">185.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>66.7</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05324-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t008_Table 8</object-id><label>Table 8</label><caption><p>Robustness evaluation across multiple runs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="2" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Runs</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Metrics</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DOM mAP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DAWN mAP (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">71.8</td><td align="center" valign="middle" rowspan="1" colspan="1">55.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">72.0</td><td align="center" valign="middle" rowspan="1" colspan="1">55.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" rowspan="1" colspan="1">55.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">71.8</td><td align="center" valign="middle" rowspan="1" colspan="1">56.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">72.3</td><td align="center" valign="middle" rowspan="1" colspan="1">55.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.88</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05324-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05324-t009_Table 9</object-id><label>Table 9</label><caption><p>Comparative Performance Evaluation of Object Detection Methods on the KITTI dataset. Bold font indicates the optimal value.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Module</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SSD</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">64.4</td><td align="center" valign="middle" rowspan="1" colspan="1">72.4</td><td align="center" valign="middle" rowspan="1" colspan="1">59</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster R-CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">86.5</td><td align="center" valign="middle" rowspan="1" colspan="1">74.3</td><td align="center" valign="middle" rowspan="1" colspan="1">88.2</td><td align="center" valign="middle" rowspan="1" colspan="1">70.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RT-DETR-R18</td><td align="center" valign="middle" rowspan="1" colspan="1">84.7</td><td align="center" valign="middle" rowspan="1" colspan="1">80.5</td><td align="center" valign="middle" rowspan="1" colspan="1">88.2</td><td align="center" valign="middle" rowspan="1" colspan="1">39.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RT-DETRv2-R18</td><td align="center" valign="middle" rowspan="1" colspan="1">87</td><td align="center" valign="middle" rowspan="1" colspan="1">85.1</td><td align="center" valign="middle" rowspan="1" colspan="1">90.4</td><td align="center" valign="middle" rowspan="1" colspan="1">28.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LKStar-YOLO</td><td align="center" valign="middle" rowspan="1" colspan="1">84.6</td><td align="center" valign="middle" rowspan="1" colspan="1">78.1</td><td align="center" valign="middle" rowspan="1" colspan="1">85.4</td><td align="center" valign="middle" rowspan="1" colspan="1">164.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOV8</td><td align="center" valign="middle" rowspan="1" colspan="1">92.9</td><td align="center" valign="middle" rowspan="1" colspan="1">88.1</td><td align="center" valign="middle" rowspan="1" colspan="1">92.8</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>217.4</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOV9</td><td align="center" valign="middle" rowspan="1" colspan="1">93.1</td><td align="center" valign="middle" rowspan="1" colspan="1">88.4</td><td align="center" valign="middle" rowspan="1" colspan="1">92.4</td><td align="center" valign="middle" rowspan="1" colspan="1">70.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOV10</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td><td align="center" valign="middle" rowspan="1" colspan="1">85.1</td><td align="center" valign="middle" rowspan="1" colspan="1">92.2</td><td align="center" valign="middle" rowspan="1" colspan="1">157.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLO11</td><td align="center" valign="middle" rowspan="1" colspan="1">92.6</td><td align="center" valign="middle" rowspan="1" colspan="1">87.3</td><td align="center" valign="middle" rowspan="1" colspan="1">92.4</td><td align="center" valign="middle" rowspan="1" colspan="1">90.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CR-YOLO</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>94.7</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">86.1</td><td align="center" valign="middle" rowspan="1" colspan="1">92.2</td><td align="center" valign="middle" rowspan="1" colspan="1">78.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BML-YOLO</td><td align="center" valign="middle" rowspan="1" colspan="1">92.6</td><td align="center" valign="middle" rowspan="1" colspan="1">85.1</td><td align="center" valign="middle" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" rowspan="1" colspan="1">168.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MBFILNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>93.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">185.6</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>