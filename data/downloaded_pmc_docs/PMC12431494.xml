<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431494</article-id><article-id pub-id-type="pmcid-ver">PMC12431494.1</article-id><article-id pub-id-type="pmcaid">12431494</article-id><article-id pub-id-type="pmcaiid">12431494</article-id><article-id pub-id-type="doi">10.3390/s25175411</article-id><article-id pub-id-type="publisher-id">sensors-25-05411</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Redesigning Multimodal Interaction: Adaptive Signal Processing and Cross-Modal Interaction for Hands-Free Computer Interaction</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Quan</surname><given-names initials="BH">Bui Hong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05411" ref-type="aff">1</xref><xref rid="fn1-sensors-25-05411" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Anh</surname><given-names initials="NDT">Nguyen Dinh Tuan</given-names></name><xref rid="af1-sensors-25-05411" ref-type="aff">1</xref><xref rid="fn1-sensors-25-05411" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Phi</surname><given-names initials="HV">Hoang Van</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05411" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-5822-2675</contrib-id><name name-style="western"><surname>Thanh</surname><given-names initials="BT">Bui Trung</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05411" ref-type="aff">2</xref><xref rid="c1-sensors-25-05411" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Mercaldo</surname><given-names initials="F">Francesco</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05411"><label>1</label>Faculty of Information Technology, VNU-University of Engineering and Technology (VNU-UET), Hanoi 10000, Vietnam; <email>22028016@vnu.edu.vn</email> (B.H.Q.); <email>22028136@vnu.edu.vn</email> (N.D.T.A.); <email>22028167@vnu.edu.vn</email> (H.V.P.)</aff><aff id="af2-sensors-25-05411"><label>2</label>Faculty of Mechanical Engineering, Hung Yen University of Technology and Education, Hungyen 16000, Vietnam</aff><author-notes><corresp id="c1-sensors-25-05411"><label>*</label>Correspondence: <email>buitrungthanh@utehy.edu.vn</email></corresp><fn id="fn1-sensors-25-05411"><label>&#8224;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>02</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5411</elocation-id><history><date date-type="received"><day>16</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>28</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>29</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05411.pdf"/><abstract><p>Hands-free computer interaction is a key topic in assistive technology, with camera-based and voice-based systems being the most common methods. Recent camera-based solutions leverage facial expressions or head movements to simulate mouse clicks or key presses, while voice-based systems enable control via speech commands, wake-word detection, and vocal gestures. However, existing systems often suffer from limitations in responsiveness and accuracy, especially under real-world conditions. In this paper, we present 3-Modal Human-Computer Interaction (3M-HCI), a novel interaction system that dynamically integrates facial, vocal, and eye-based inputs through a new signal processing pipeline and a cross-modal coordination mechanism. This approach not only enhances recognition accuracy but also reduces interaction latency. Experimental results demonstrate that 3M-HCI outperforms several recent hands-free interaction solutions in both speed and precision, highlighting its potential as a robust assistive interface.</p></abstract><kwd-group><kwd>human-computer interaction</kwd><kwd>hands-free interaction</kwd><kwd>vision/camera-based sensors</kwd><kwd>adaptive signal processing</kwd><kwd>multimodal interaction</kwd><kwd>assistive technology</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05411"><title>1. Introduction</title><p>Advances in artificial intelligence (AI) are rapidly improving machines&#8217; ability to process and understand visual data [<xref rid="B1-sensors-25-05411" ref-type="bibr">1</xref>]. In addition, AI also promotes progress in fields such as robotics and education [<xref rid="B2-sensors-25-05411" ref-type="bibr">2</xref>]. These developments are creating new opportunities to support accessible human-computer interaction, especially for individuals with disabilities [<xref rid="B3-sensors-25-05411" ref-type="bibr">3</xref>]. One of the main objectives of assistive technology is supporting individuals with upper-limb impairments [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05411" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05411" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>]. This group includes people with limb amputations, neuromuscular disorders such as amyotrophic lateral sclerosis (ALS), cerebral palsy, muscular dystrophy, spinal cord injuries, and congenital limb differences, all of which can significantly hinder the use of conventional input devices such as the mouse or keyboard.</p><p>According to [<xref rid="B9-sensors-25-05411" ref-type="bibr">9</xref>], as of 2019, there were approximately 552.45 million people living with traumatic amputations. Additionally, nearly 33,000 people in the U.S. are currently living with ALS, and that number is projected to reach 36,000 by 2030 [<xref rid="B10-sensors-25-05411" ref-type="bibr">10</xref>]. These conditions make it difficult or nearly impossible for individuals to use a computer mouse. However, the ability to move the head and eyes is often retained, even in individuals living with severe disabilities; therefore, computer interfaces based on head or eye movement are commonly employed as alternative input methods.</p><p>Recognizing this issue, many researchers have proposed solutions to support people with disabilities in accessing and interacting with computers effectively. Existing approaches can be broadly divided into two categories. Sensor-based systems (e.g., tilt sensors, accelerometers, gyroscopes) provide fast and accurate control but are costly, require technical setup, and may cause discomfort when worn for long periods [<xref rid="B11-sensors-25-05411" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05411" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05411" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05411" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05411" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05411" ref-type="bibr">16</xref>]. Camera-based systems using standard RGB webcams are more affordable and convenient, ranging from early vision-based techniques [<xref rid="B17-sensors-25-05411" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05411" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05411" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05411" ref-type="bibr">20</xref>] (template matching, color tracking) to more recent deep learning methods for head pose estimation, facial expression recognition, and gaze estimation [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>,<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>,<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>,<xref rid="B21-sensors-25-05411" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05411" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05411" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05411" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05411" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05411" ref-type="bibr">26</xref>].</p><p>While various camera-based approaches have been proposed to help individuals with disabilities control computers, most still suffer from latency caused by noisy input and inefficient filtering. Recent systems also rely on limited modalities and a small set of facial gestures (for example, smiling, mouth opening, eyebrow raising), which reduces flexibility and may interfere with precise cursor control due to involuntary movements. Moreover, input modalities are often treated in isolation, with little use of cross-modal integration. A promising direction is to exploit contextual cues, for example, validating voice commands with mouth opening detection, but this remains underexplored.</p><p>To address these challenges, we propose a novel 3-Modal Human-Computer Interaction (3M-HCI) system that integrates three complementary input modalities, namely head movement with facial expressions, voice commands, and eye gaze. Our contributions are threefold:</p><list list-type="bullet"><list-item><p>We introduce an adaptive filtering mechanism to suppress signal noise while maintaining low-latency responsiveness.</p></list-item><list-item><p>We redefine the mapping strategy from input signals to cursor movements for more precise control.</p></list-item><list-item><p>We incorporate cross-modal coordination to improve reliability and reduce false activations.</p></list-item></list><p>To evaluate the system, two types of tests were conducted: (a) functional tests under various technical and environmental conditions; (b) user evaluations to assess usability, responsiveness, and perceived effectiveness.</p><p>The remainder of this paper is organized as follows. <xref rid="sec2-sensors-25-05411" ref-type="sec">Section 2</xref> reviews related work on sensor-based and camera-based approaches for hands-free computer interaction. <xref rid="sec3-sensors-25-05411" ref-type="sec">Section 3</xref> introduces the system architecture and input modalities. <xref rid="sec4-sensors-25-05411" ref-type="sec">Section 4</xref> outlines the materials used and the evaluation methodology. <xref rid="sec5-sensors-25-05411" ref-type="sec">Section 5</xref> presents experimental results and discussions. Finally, <xref rid="sec6-sensors-25-05411" ref-type="sec">Section 6</xref> concludes the paper and suggests future research.</p></sec><sec id="sec2-sensors-25-05411"><title>2. Related Work</title><p>Research on alternative computer input methods for individuals with upper-limb disabilities has evolved over the past two decades, moving from wearable sensor-based devices to camera-based solutions, and more recently toward vision models enhanced by deep learning. These approaches can be broadly categorized into two main directions: sensor-based and camera-based.</p><sec id="sec2dot1-sensors-25-05411"><title>2.1. Sensor-Based Approaches</title><p>Early studies primarily focused on wearable sensor devices to capture head or body movements. A head-controlled computer mouse using tilt sensors was developed for people with disabilities, where one sensor detects horizontal movement and another detects vertical movement, and a touch switch allows click actions [<xref rid="B11-sensors-25-05411" ref-type="bibr">11</xref>]. Some studies [<xref rid="B12-sensors-25-05411" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05411" ref-type="bibr">13</xref>] employed a dual-axis accelerometer to control the mouse. Another approach used a combination of a gyro sensor and an optical sensor to perform clicking actions [<xref rid="B14-sensors-25-05411" ref-type="bibr">14</xref>]. Additionally, a system for controlling a computer mouse using a camera and software mounted on a cap worn by the user has been developed [<xref rid="B15-sensors-25-05411" ref-type="bibr">15</xref>]. Other studies [<xref rid="B16-sensors-25-05411" ref-type="bibr">16</xref>] combined eye tracking and head gestures, using a light source mounted on the user. These methods have demonstrated particularly fast and accurate results. However, they all rely on sensor devices. This increases deployment costs, making it difficult for users in low-income areas or those without access to advanced technology. Furthermore, installing and calibrating sensors often requires a certain level of technical skill, which not all users may possess. Additionally, wearing sensor devices on the head for extended periods can cause discomfort, neck fatigue, or a sense of heaviness, negatively affecting the user experience.</p></sec><sec id="sec2dot2-sensors-25-05411"><title>2.2. Camera-Based Approaches</title><p>To reduce cost and improve accessibility, researchers gradually shifted to camera-based approaches, typically relying on standard built-in RGB webcams available on consumer devices. Early methods relied on template matching [<xref rid="B17-sensors-25-05411" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05411" ref-type="bibr">18</xref>] or color-based segmentation [<xref rid="B19-sensors-25-05411" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05411" ref-type="bibr">20</xref>] to track head or facial features. Interaction mechanisms included dwell clicking (cursor dwell time to simulate left-click) or eye blinks for right clicks. Some systems combined head movements with voice commands [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>,<xref rid="B21-sensors-25-05411" ref-type="bibr">21</xref>], while others integrated head orientation with eye blinks [<xref rid="B22-sensors-25-05411" ref-type="bibr">22</xref>].</p><p>With the advent of deep learning, camera-based methods gained robustness and precision. Some studies directly mapped visual input to screen coordinates [<xref rid="B23-sensors-25-05411" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05411" ref-type="bibr">24</xref>], while others extracted facial landmarks and expressions for interaction [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>,<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>,<xref rid="B25-sensors-25-05411" ref-type="bibr">25</xref>]. Despite these advances, most systems still relied on a limited set of gestures (e.g., smiling, eyebrow raising, mouth opening), restricting user flexibility and sometimes interfering with natural expressions. Another common limitation is latency, which often arises from noisy input signals and insufficient filtering mechanisms, leading to slower system responses and reduced usability in real-world scenarios. Moreover, input modalities were typically handled in isolation without cross-modal coordination. While some studies have employed multiple modalities [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>,<xref rid="B26-sensors-25-05411" ref-type="bibr">26</xref>], they typically process inputs independently without cross-modal coordination. These limitations motivate the need for a more flexible, responsive, and integrated interaction framework, which we address in this work.</p></sec></sec><sec id="sec3-sensors-25-05411"><title>3. System Architecture</title><p>The 3-Modal Human Computer Interaction (3M-HCI) system employs a callback-based architecture that fundamentally separates processing logic from user interface components, ensuring optimal performance, maintainability, and scalability. The core processing pipeline operates independently in separate threads, with results delivered to the GUI for display (<xref rid="sensors-25-05411-f001" ref-type="fig">Figure 1</xref>). To achieve maximum efficiency, the system implements a multi-threaded architecture within a single process, with mechanisms for safe thread coordination and termination. By using primarily I/O-bound operations, the system maintains significantly lower resource overhead compared to existing solutions.</p><p><xref rid="sensors-25-05411-f001" ref-type="fig">Figure 1</xref> illustrates the overall system architecture and its integrated modules. Within the camera thread, OpenCV is adopted as the standard library for capturing and processing input images. Implemented in optimized C/C++ with multi-core support, it delivers high performance and a comprehensive set of tools for efficient real-time image processing [<xref rid="B27-sensors-25-05411" ref-type="bibr">27</xref>]. Additional libraries are incorporated for specific modules, which will be described in detail in the following sections. The cross-modal coordination mechanism enhances interaction among modules: the face processor validates input for the voice processor, while the voice processor can adjust profile settings and influence the behavior of the face processor. Both modules interface with the operating system through PyAutoGUI, which enables seamless control of mouse and keyboard actions.</p><p>Building upon this modular architecture, the main processing pipeline operates through synchronous and asynchronous callbacks as illustrated in <xref rid="sensors-25-05411-f002" ref-type="fig">Figure 2</xref>. The computer vision thread continuously captures frames from the camera input. Through the callback mechanism, each frame is delivered to the face processor module. The face processor provides two processing modes: the IMAGE mode processes each frame separately and synchronously, while the smooth mode handles frames asynchronously. Upon successful processing, the extracted facial landmarks and blendshape data are forwarded through the pipeline to compute and execute mouse movements andkeyboard actions.</p><p>The voice processor functions as an independent module running in its own dedicated thread. It captures the microphone input and processes it through its recognition engine to identify pre-configured voice commands. To enhance user experiences and prevent false activations from external audio sources, the system incorporates an intelligent gating mechanism that cross-references facial expression data from the face processor module before executing voice commands.</p><p>This architecture demonstrates significant optimization advantages compared with existing solutions such as Google Project GameFace [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>], which utilizes a busy-waiting pipeline that tightly couples GUI and processing components. Google&#8217;s implementation continuously captures and processes images at extremely high frequencies (sub-millisecond intervals), resulting in substantial resource consumption and redundant frame processing. In contrast, our event-driven approach with controlled frame rates delivers superior resource efficiency while maintaining real-time responsiveness. The detailed implementation of each module is described in the subsequent sections.</p><sec id="sec3dot1-sensors-25-05411"><title>3.1. Video Processing Module</title><sec id="sec3dot1dot1-sensors-25-05411"><title>3.1.1. Face Processing</title><p>The face processing modules&#8217; main function is to detect and track the user&#8217;s face and return facial landmarks, eye gaze, and expressions, which makes them the most critical component in the entire system pipeline. The face processing algorithm must satisfy several requirements: it should be fast, deliver high accuracy, and ideally operate efficiently without requiring a GPU.</p><p>Previous systems [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>] have typically used the Dlib library [<xref rid="B28-sensors-25-05411" ref-type="bibr">28</xref>], the Haar Cascade algorithm [<xref rid="B29-sensors-25-05411" ref-type="bibr">29</xref>], or custom-built CNNs for face detection and tracking [<xref rid="B23-sensors-25-05411" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05411" ref-type="bibr">24</xref>,<xref rid="B30-sensors-25-05411" ref-type="bibr">30</xref>]. Recent systems [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>,<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>] have increasingly adopted MediaPipe [<xref rid="B31-sensors-25-05411" ref-type="bibr">31</xref>] due to its superior performance and the wide range of built-in features it provides. We compare several lightweight face processing algorithms. All algorithms below were evaluated on a single laptop with a Ryzen 5 5500U CPU, 12 GB RAM, and AMD Radeon Vega 7 integrated graphics to ensure consistent performance comparisons. The result are presented in <xref rid="sensors-25-05411-t001" ref-type="table">Table 1</xref>.</p><p>Based on the comparison results shown in the table above, we decided to use MediaPipe Face Landmarker as the core tool for developing our system. MediaPipe [<xref rid="B31-sensors-25-05411" ref-type="bibr">31</xref>] provides 478 facial landmarks (<xref rid="sensors-25-05411-f003" ref-type="fig">Figure 3</xref>), including key regions such as the eyes, eyebrows, mouth, nose, and jawline, which are essential for precise facial expression analyses.</p></sec><sec id="sec3dot1dot2-sensors-25-05411"><title>3.1.2. Mapping Features to Mouse Coordinate</title><p>Camera-based head-mouse systems typically use landmarks such as the nose tip [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>,<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>], forehead [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>], or mouth [<xref rid="B19-sensors-25-05411" ref-type="bibr">19</xref>] as anchors. However, landmark instability during facial expressions can introduce unwanted cursor drift. For example, raising an eyebrow, as seen in Google&#8217;s Project GameFace, can shift the forehead anchor, while actions such as nose sneer or performing gestures such as &#8220;mouse left&#8221; and &#8220;mouse right&#8221; can alter the nose tip position, leading to unintentional pointer movement. A simple way to mitigate this issue is to avoid using facial expressions that can distort the anchor points altogether. Therefore, our solution is to use the two inner eye corners (medial canthi) as anchor points instead. After thorough testing, we found that the medial canthi remained stable across various facial expressions. Therefore, we chose them as reliable reference points for pointer movements (<xref rid="sensors-25-05411-f004" ref-type="fig">Figure 4</xref>). The movement vector formed by tracking these two inner eye corners is then converted into mouse pointer movement signals.</p><p>Formally, let <italic toggle="yes">P<sub>L</sub></italic> = (<italic toggle="yes">X<sub>L</sub></italic>, <italic toggle="yes">Y<sub>L</sub></italic>) and <italic toggle="yes">P<sub>R</sub></italic> = (<italic toggle="yes">X<sub>R</sub></italic>, <italic toggle="yes">Y<sub>R</sub></italic>) be the coordinates of the left and right medial canthus, respectively. We computed the midpoint at frame t:<disp-formula id="FD1-sensors-25-05411"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, we have cursor displacement vector <italic toggle="yes">V<sub>t</sub></italic>:<disp-formula id="FD2-sensors-25-05411"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#916;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#916;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Displacement vector <italic toggle="yes">V<sub>t</sub></italic> serves as the raw cursor moving signal in our system.</p></sec><sec id="sec3dot1dot3-sensors-25-05411"><title>3.1.3. Adaptive Movement Signal Filtering and Acceleration</title><p>With signals measured from sensors or cameras (since a camera itself is a type of sensor), there is always some noise present in the data. To eliminate this noise, we apply filtering techniques. Depending on the characteristics of the signal, different filtering methods can be used (i) for signals with significant &#8220;salt-and-pepper&#8221; noise, a median filter can be applied to remove outliers; (ii) for signals affected by Gaussian noise, a moving average, low-pass, or Gaussian filter may be applied.</p><p>In our specific case, the movement signals extracted from facial landmarks often contain Gaussian noise. This causes the cursor movement to appear jittery. To address this issue, some systems use a simple region-based technique, where a virtual window is overlaid on the user&#8217;s face and the cursor only moves when the tracked landmark crosses the boundary of this window [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>]. Additionally, previous systems have applied low-pass filters as a basic solution to smooth the pointer motion [<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>,<xref rid="B17-sensors-25-05411" ref-type="bibr">17</xref>]. A more optimized approach is to use a Hamming filter, which offers better frequency response characteristics [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>].</p><p>Although these techniques help smooth the cursor movement, they can negatively impact the pointer speed and responsiveness. Conventional low-pass filters have a fixed cutoff frequency, which creates a trade-off between smoothness and responsiveness. If the cutoff is too low, the cursor becomes stable but sluggish; if it is too high, unwanted jitter may persist.</p><p>To increase responsiveness, we employed the 1&#8364; filter [<xref rid="B34-sensors-25-05411" ref-type="bibr">34</xref>], an adaptive filter that dynamically adjusts its cutoff frequency according to the signal&#8217;s speed. This allows the system to remain smooth during slow movements while still being responsive during rapid changes. Note that, instead of applying the 1&#8364; filter directly to the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> coordinates separately, we apply the 1&#8364; filter to the cursor displacement magnitude to get the smoothing factor <italic toggle="yes">&#945;</italic>. This approach avoids the issue of having different cutoff frequencies on each axis, which could cause asynchronous pointer behavior.</p><p>We define <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mi mathvariant="sans-serif">&#916;</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="sans-serif">&#916;</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> to be the cursor displacement magnitude, using the 1&#8364; filter to get the smoothing factor:<disp-formula id="FD3-sensors-25-05411"><label>(3)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi mathvariant="normal">&#8364;</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Next, we calculate the filtered displacement vector, using adaptive smoothing factor <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD4-sensors-25-05411"><label>(4)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, we apply a pointer acceleration function to improve the responsiveness and usability of the system. This allows small head movements to result in fine cursor control, while larger or faster movements produce quicker pointer displacement, enhancing both the precision and efficiency. Pointer acceleration is typically based on sigmoid functions. Previous studies [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>,<xref rid="B35-sensors-25-05411" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05411" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05411" ref-type="bibr">37</xref>] have demonstrated that sigmoid-based pointer acceleration achieves smoother transitions between precise and rapid movements, while avoiding excessive jitter or drift. It also improves the ergonomics and precision of the system. We adopted the following function:<disp-formula id="FD5-sensors-25-05411"><label>(5)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>*</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where: <italic toggle="yes">K</italic> controls the maximum gain, determining how fast the pointer can move at high speeds; <italic toggle="yes">slope</italic> defines the steepness of the transition between low and high gain; larger values make the transition sharper; <italic toggle="yes">offset</italic> sets the inflection point on the input axis, i.e., the point at which the gain starts to increase significantly.</p><list list-type="bullet"><list-item><p>In our system, we set <italic toggle="yes">K</italic> = 1.2, <italic toggle="yes">slope</italic> = 0.1, and <italic toggle="yes">offset</italic> = 12. The resulting acceleration curve is illustrated in <xref rid="sensors-25-05411-f005" ref-type="fig">Figure 5</xref>, showing a smooth transition from low to high gain as the input speed increases.</p></list-item></list></sec><sec id="sec3dot1dot4-sensors-25-05411"><title>3.1.4. Mapping Facial Expressions to Mouse and Keyboard Actions</title><p>Early studies employed sensors [<xref rid="B11-sensors-25-05411" ref-type="bibr">11</xref>,<xref rid="B14-sensors-25-05411" ref-type="bibr">14</xref>] or dwell-click mechanisms [<xref rid="B20-sensors-25-05411" ref-type="bibr">20</xref>], combined with a limited set of simple actions or expressions [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>,<xref rid="B19-sensors-25-05411" ref-type="bibr">19</xref>] to trigger mouse events. More recent approaches have shifted toward using more intuitive and user-friendly expressions such as smiling, raising eyebrows, or opening the mouth to enhance usability and reduce fatigue [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>,<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>,<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>]. However, these systems typically utilize only a small number of facial expressions [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>,<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>], as some expressions have been reported to be difficult to perform or maintain and may cause facial fatigue. In addition, certain expressions can be easily confused with one another, reducing the reliability of the input [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>,<xref rid="B25-sensors-25-05411" ref-type="bibr">25</xref>].</p><p>A straightforward way to address this issue is to use only easily recognizable and distinguishable facial expressions that are less likely to be confused with others. However, this approach inherently limits the number of distinct actions the system can support.</p><p>To overcome this limitation, we introduce a priority-based triggering mechanism, which favors less distinguishable expressions over more easily recognizable ones when multiple expressions are detected simultaneously. For instance, when a user smiles, the model may also detect mouth opening due to overlapping facial features. In such cases, our system prioritizes the smile over the mouth opening to ensure consistent and reliable input. By using this mechanism, our system supports a wider range of actions compared to existing systems (<xref rid="sensors-25-05411-t002" ref-type="table">Table 2</xref>), while also reducing false positives and confusion.</p><p>Moreover, in our system, we also utilized directional eye gaze (left, right, up, down) as a form of expressive input, similar to facial expressions.</p></sec></sec><sec id="sec3dot2-sensors-25-05411"><title>3.2. Voice Processing Module</title><p>The voice processing module utilizes command recognition to handle basic interactive instructions and accessibility controls. It serves as a complementary input method to the facial expression control system, providing users with multiple interaction modalities. The module should use a pretrained model, support offline functionality, and offer high processing speed. Recent advances in speech recognition, such as Whisper [<xref rid="B38-sensors-25-05411" ref-type="bibr">38</xref>] and Vosk [<xref rid="B39-sensors-25-05411" ref-type="bibr">39</xref>], provide powerful accuracy and flexibility across languages. Nevertheless, these systems involve complex implementation and their real-time voice command recognition performance is limited on lightweight devices.</p><p>Since voice command is not the primary focus of our system, we prioritize deployment efficiency and adopt Microsoft&#8217;s native Speech API (SAPI5) [<xref rid="B40-sensors-25-05411" ref-type="bibr">40</xref>] as a standard solution that satisfies the required criteria. Integrated through the Dragonfly library, SAPI5 delivers consistent performance with low latency and minimal hardware requirements, while executing all processing locally. This ensures that the feature operates without requiring an Internet connection, thereby enhancing privacy and system reliability.</p><p>The voice processor module creates a self-contained, thread-safe service that listens for user-defined voice commands (<xref rid="sensors-25-05411-f002" ref-type="fig">Figure 2</xref>). When a command is recognized by the SAPI5 engine, it executes the corresponding keyboard or mouse action via the PyAutoGUI. Furthermore, it interfaces with other modules within the application, allowing voice commands to modify their behavior. This multimodal approach also enhances the user experience. For instance, by implementing a check to determine whether the user&#8217;s mouth is open during command recognition, the system can avoid misinterpreting external ambient sounds as commands, leading to more reliable activation. Voice commands can also be used to dynamically adjust the mouse movement speed, enabling users to fine-tune their control in real time without relying on manual input. Furthermore, the voice command module can be flexibly configured to trigger interactive actions within the system, enhancing the overall user convenience.</p></sec></sec><sec id="sec4-sensors-25-05411"><title>4. Materials and Methods</title><p>This section outlines the development environment and the methodology used to evaluate the performance and usability of our multimodal interaction system. The evaluation comprises two types of tests. The first is an experimental test designed to examine system stability and responsiveness under various environmental and hardware conditions, including different CPU generations, operating systems, lighting environments, and background noise. The purpose is to determine the minimum requirements necessary for smooth operation. The second test involves task-based usability testing, in which users are asked to perform a series of predefined actions such as cursor movement and target selection. Objective performance metrics, including latency, accuracy, jitterness, and task completion time, are recorded and compared across systems. In addition, we conducted a short survey to collect subjective feedback from participants who had experienced all three systems, with the results presented in <xref rid="sec5-sensors-25-05411" ref-type="sec">Section 5</xref>. This approach allows for a comprehensive analysis of both the technical efficiency and user experience.</p><sec id="sec4dot1-sensors-25-05411"><title>4.1. Development Platform</title><p>We used two different laptops during the development process to ensure stable software performance under varying hardware conditions. The first laptop was equipped with a Ryzen 5 5500U CPU, 12 GB RAM, AMD Radeon Vega 7 integrated graphics, a built-in 720 p 30 fps camera, and an integrated microphone. The second laptop featured an Intel(R) Core(TM) i7-13650HX CPU, 16 GB RAM, a dedicated NVIDIA GeForce RTX 4060 GPU with 8 GB GDDR6 VRAM, a built-in 720 p 15 fps camera, and an integrated microphone.</p><p>The system was developed through iterative prototyping, combined with regular internal testing and feedback from university instructors and experts with experience in assistive technologies. This feedback loop allowed us to continuously improve the system while keeping it accessible and practical. To implement our application, we chose Python as the primary programming language due to its extensive ecosystem, cross-platform compatibility, and active developer community. Python also simplifies rapid prototyping and integration with computer vision and audio processing tools, which are central to our system. The key Python libraries utilized included: (i) OpenCV for video capture and preprocessing, (ii) Mediapipe for extracting facial landmarks and facial expression analyses, (iii) Customtkinter for building modern and customizable graphical user interfaces (GUIs), (iv) Dragonfly for a voice control framework that maps spoken commands to computer actions, (v) PyAutoGUI for accessing the mouse and keyboard functionalities, and (vi) NumPy for efficient numerical computations.</p><p>The main interface of the application is illustrated in <xref rid="sensors-25-05411-f006" ref-type="fig">Figure 6</xref>. Detailed information about the project and the source code of the project are available at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ndtuananh04/3-Modal-Human-Computer-Interaction">https://github.com/ndtuananh04/3-Modal-Human-Computer-Interaction</uri> (accessed on 16 July 2025).</p></sec><sec sec-type="methods" id="sec4dot2-sensors-25-05411"><title>4.2. Testing Methodology</title><p>Following the evaluation of the methodology proposed in [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>], we assessed the minimum operating requirements of our system under various environmental and technical conditions, such as lighting, background noise, and hardware configurations, as follows: (a) different environmental lighting conditions; (b) more than one face detected by the camera; (c) background noise; (d) different hardware and software features of the computer. In addition to these tests, we also conducted task-based usability testing with existing systems, in order to highlight the effectiveness of the improvements proposed in our study: (a) jitterness; (b) responsiveness (task completion time); (c) accuracy.</p><p>Before performing the tasks, participants were given time to freely explore and adjust the mouse control system to ensure maximum comfort. They were instructed to select two facial expressions of their choice and assign them to left and right click actions, based on what they found most intuitive and easy to perform. Once these settings were configured, participants received clear instructions on how to interact with the testing application.</p><p>The first task involved a sequence-based interaction test, where users were required to move the cursor to predefined targets on the screen, perform either a left or right click as instructed, and proceed to the next target (<xref rid="sensors-25-05411-f007" ref-type="fig">Figure 7</xref>). This process continued until all targets were completed. The task was used to assess accuracy and responsiveness, based on metrics such as the completion time and cursor deviation.</p><p>The second task required users to keep their heads still for a fixed duration while the system was running. This allowed us to measure unintended cursor movement or drift, providing insight into the system&#8217;s stability when idle.</p><p>Finally, a short post-test survey was conducted to gather subjective feedback from users who had experienced all three systems (<xref rid="sensors-25-05411-t003" ref-type="table">Table 3</xref>). All questions were evaluated on a numeric rating scale from 1 (very bad) to 10 (excellent). The results of this evaluation, as well as the code and configurations used in the testing application, are available in our public GitHub repository.</p></sec></sec><sec sec-type="results" id="sec5-sensors-25-05411"><title>5. Results and Discussion</title><sec id="sec5dot1-sensors-25-05411"><title>5.1. Robustness Testing Under Environmental and Hardware Variations</title><p>We evaluated the system&#8217;s functionality and performance under varied lighting, multiple faces, and different hardware and operating systems, as well as identify the minimum hardware and environmental requirements necessary for stable operation.</p><sec id="sec5dot1dot1-sensors-25-05411"><title>5.1.1. Lighting Condition Test</title><p>To assess the robustness of 3M-HCI under real-world usage, we conducted experiments in four different lighting environments, with corresponding results shown in <xref rid="sensors-25-05411-f008" ref-type="fig">Figure 8</xref>a&#8211;d. In each scenario, we visualized the 147 facial landmarks used by MediaPipe Face Mesh to detect expressions, enabling a detailed qualitative assessment of detection stability under varying illumination conditions. From our experiments, we can conclude the following:<list list-type="bullet"><list-item><p>Bright eEnvironment (<xref rid="sensors-25-05411-f008" ref-type="fig">Figure 8</xref>a,b): Whether in a brightly lit room or a dim room with high screen brightness, the system performed flawlessly. Facial landmarks were immediate and accurate. The mouse control operated smoothly without any jitter or delay. This represents the optimal environment for system usage.</p></list-item><list-item><p>Dim room with medium screen brightness (<xref rid="sensors-25-05411-f008" ref-type="fig">Figure 8</xref>c): Under significantly darker conditions, where only moderate screen brightness was present, the system remained functional. Facial landmarks still worked, but occasional instability in mouse movement was observed. The system was still usable with minor degradation.</p></list-item><list-item><p>Near-total darkness with low screen brightness (<xref rid="sensors-25-05411-f008" ref-type="fig">Figure 8</xref>d): In the most extreme case, with no external light and very low screen brightness, the system struggled. The Mediapipe framework could still detect the facial landmarks. However, the detection was inconsistent and unreliable. Landmarks often flickered or were lost entirely, making interaction with the system ineffective in this condition</p></list-item></list></p><p>From the experiments above, we conclude that the most critical factor for the system&#8217;s performance is the clarity of the captured facial image. While ambient lighting conditions have a limited impact on the overall outcome, ensuring a well-defined face is essential. Notably, MediaPipe demonstrated impressive robustness. It was able to detect facial landmarks even in extremely low-light scenarios where the human eye struggles to distinguish facial features.</p></sec><sec id="sec5dot1dot2-sensors-25-05411"><title>5.1.2. Multiple Faces in a Frame</title><p>This test evaluated Mediapipe&#8217;s behavior when multiple faces appear in the frame. When configured to detect only a single face, Mediapipe selects the one it detects with the highest confidence. In practice, this is often the face closest to the camera, which typically belongs to the user. However, the most confidently detected face is not always the intended user&#8217;s face, especially in dynamic or crowded environments. Our experiments show that when two faces are present in the frame, Mediapipe may occasionally select the one farther from the camera, which disrupts the system&#8217;s operation.</p><p>A simple strategy to address this issue is to enable Mediapipe&#8217;s multi-face detection mode. In this configuration, the system detects all visible faces and compares them to the face identified in the previous frame, selecting the one with the most consistent position or landmark pattern. While this improves the accuracy of user tracking in multi-face scenarios, it also introduces a higher computational load, which may reduce the real-time performance, particularly on lower-end devices. For this reason, we did not adopt this approach in our implementation, as it caused noticeable lag during runtime, making the interaction experience less smooth and responsive.</p></sec><sec id="sec5dot1dot3-sensors-25-05411"><title>5.1.3. Background Noise</title><p>In contrast to previous systems that relied solely on voice recognition, making them vulnerable to ambient noise and unintended speech, 3M-HCI integrates a mouth-open detection mechanism using facial landmarks. To evaluate its robustness, we conducted a test scenario where two people held a conversation near the system. While earlier studies [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>] reported performance degradation due to microphone sensitivity and background noise, our method was unaffected. Since voice commands in our system are only executed when the user&#8217;s mouth has been recently detected as open, environmental noise or nearby conversations had no impact on command triggering. This approach significantly reduces false positives and enhances reliability in shared or noisy environments.</p><p>However, this feature relies on the system&#8217;s ability to consistently detect the user&#8217;s full face. If the face is partially occluded, out of frame, or poorly lit, the mouth-open detection may fail to activate, thereby preventing valid voice commands from being registered. Ensuring a clear and stable view of the user&#8217;s face is therefore essential for maintaining the robustness of this mechanism.</p></sec><sec id="sec5dot1dot4-sensors-25-05411"><title>5.1.4. Different Hardware and Software Features of the Computer</title><p>We evaluated the software performance across different machines and conducted a comparative analysis of three applications: 3M-HCI, Project GameFace, and CameraMouseAI. The results are summarized in <xref rid="sensors-25-05411-t004" ref-type="table">Table 4</xref> below.</p><p>Compared to other applications, our software runs efficiently and stably. Despite integrating voice commands, our system still consumes fewer computational resources than Google&#8217;s solution. CameraMouseAI exhibits lower computational cost than ours, likely because it is a simpler tool, supporting only 2&#8211;3 basic mouse actions.</p><p>Based on <xref rid="sensors-25-05411-t004" ref-type="table">Table 4</xref>, we recommend the following minimum system requirements: Windows 10 or higher, a CPU equivalent to Intel Core i5-10th Gen or above, no dedicated GPU is required, a functional camera and microphone, and at least 8 GB of RAM. These are very lightweight requirements, making the system compatible with nearly all modern laptops.</p></sec></sec><sec id="sec5dot2-sensors-25-05411"><title>5.2. Empirical Task-Based Test</title><p>We conducted a pilot user study involving eight participants to empirically evaluate the usability, responsiveness, and precision of the proposed 3M-HCI system in real-world interaction scenarios. All participants provided informed consent prior to the study. The objective of this test was to assess how effectively users could perform common cursor-based tasks using hands-free input, in comparison with traditional mouse input and two baseline systems: Project GameFace and CameraMouseAI. Each participant was asked to complete a set of target selection and click tasks under identical conditions across all systems. Key performance metrics, including pointer accuracy, latency, jitter, and number of successful clicks, were recorded. Additionally, a post-test survey was conducted to capture user satisfaction and perceived usability. The findings from this pilot study provide preliminary insights into the practical viability and comparative advantages of our system in assistive computing contexts.</p><sec id="sec5dot2dot1-sensors-25-05411"><title>5.2.1. System Accuracy</title><p>We measured the system&#8217;s accuracy by calculating the deviation of four systems (CameraMouseAI, Project GameFace (Googl)e, our method (3M-HCI), and normal mouse) from the optimal path (straight path) to the target, along with the number of clicks required to complete the task. The results are presented in <xref rid="sensors-25-05411-f009" ref-type="fig">Figure 9</xref> and <xref rid="sensors-25-05411-f010" ref-type="fig">Figure 10</xref> below.</p><p>Overall, our system demonstrated higher accuracy compared to CameraMouseAI and Project GameFace. Overshooting was not observed in our system, in contrast to CameraMouseAI, where it occurred frequently during cursor movement. Regarding the clicking mechanism, both CameraMouseAI and Project Gameface experienced unintended cursor movement during facial expression activation, leading to incorrect clicks. Our system did not encounter this issue due to a more optimal selection of anchor points.</p></sec><sec id="sec5dot2dot2-sensors-25-05411"><title>5.2.2. System Responsiveness</title><p>We also measured the time taken to complete the first task as an indicator of system responsiveness. The result is shown in <xref rid="sensors-25-05411-f011" ref-type="fig">Figure 11</xref>.</p><p>As shown in the figure, our method consistently outperformed both CameraMouseAI and Project GameFace in terms of movement latency. Across all target transitions (e.g., 1 &#8594; 2, 2 &#8594; 3, etc.), our system maintained latency values of around 2 to 3 s, with minimal fluctuation. In contrast, CameraMouseAI exhibited the highest latency, with several spikes exceeding 6 s&#8212;most notably in the 5 &#8594; 6 transition. Project GameFace also showed relatively high latency, particularly during transitions 4 &#8594; 5 and 6 &#8594; 7.</p><p>Moreover, the total average latency of our system (green dashed line) is clearly lower than that of CameraMouseAI (blue dashed line) and Project GameFace (orange dashed line), indicating faster and more consistent performance. While traditional mouse input (red line) remained the fastest as expected, our method showed a strong balance between speed and usability, especially considering its hands-free nature.</p></sec><sec id="sec5dot2dot3-sensors-25-05411"><title>5.2.3. System Jitterness</title><p>As illustrated in <xref rid="sensors-25-05411-f012" ref-type="fig">Figure 12</xref>, our method achieved significantly lower jitter deviation compared to other hands-free systems. Specifically, the CameraMouseAI showed the highest instability with a jitter deviation of over 120 pixels, followed by Project GameFace with approximately 80 pixels. In contrast, our method maintained a low deviation of under 10 pixels, indicating stable and precise cursor control. As expected, traditional mouse usage yielded the lowest jitter rate, serving as a performance baseline.</p><p>This finding highlights the importance of incorporating adaptive filter algorithms and activation mechanisms in hands-free systems. By minimizing cursor jitter, our approach improves not only task precision but also user comfort and trust, which are critical for sustained use, especially among individuals with motor impairments.</p></sec></sec><sec id="sec5dot3-sensors-25-05411"><title>5.3. Survey Results</title><p>The survey results provide insights into the subjective usability, responsiveness, and comfort of our system compared to existing solutions, including CameraMouseAI, Project GameFace, and traditional mouse control. We present the results below in <xref rid="sensors-25-05411-t005" ref-type="table">Table 5</xref>.</p><p>The results in the table indicate that our proposed 3M-HCI system achieved high subjective ratings across all categories. Notably, it received an average score of 8.25 &#177; 1.09 for the responsiveness of left and right mouse clicks and 8.88 &#177; 0.6 for overall cursor responsiveness, comparable to the traditional mouse and outperforming both CameraMouseAI and Project GameFace.</p><p>In terms of ease of use, participants reported that our system required relatively little time to master (7.25 &#177; 1.71), and clicking actions were less difficult (7 &#177; 1.87) compared to other hands-free systems. One contributing factor is that our system allows users to choose from multiple facial expressions for click activation. Since different users may find certain expressions easier or more natural to perform, this flexibility improves comfort and accessibility. Additionally, our system maintains cursor stability during facial expression recognition, avoiding unintended cursor jumps&#8212;an issue observed in other systems such as CameraMouseAI and Project GameFace. Precision and directional control (both vertical and horizontal) were also rated higher than for alternatives, indicating more stable and accurate performance.</p><p>Fatigue levels while using the system were moderate (7.25 &#177; 1.79), significantly better than CameraMouseAI (2.62 &#177; 1.93) and slightly better than Project GameFace (7 &#177; 1.87), showing the ergonomic advantages of our approach. Importantly, our system received the highest rating (7.85 &#177; 2.71) in terms of perceived applicability for people with disabilities, suggesting strong user confidence in its real-world assistive potential.</p></sec><sec id="sec5dot4-sensors-25-05411"><title>5.4. Limitations</title><p>Despite the overall positive results, several limitations were identified based on user feedback during testing. One notable issue was the lack of intuitive parameter tuning, as users found it difficult to adjust the minCutoff and beta values of the 1-Euro filter. This difficulty stems from the technical definitions of minCutoff and beta, making it challenging to grasp their impact on the filtering process. Consequently, this limitation significantly hinders accessibility and the ability to optimize the filter&#8217;s performance. This complexity contrasts with earlier approaches, where the low-pass filter coefficient alpha was abstracted into a single, more intuitive parameter for smoothing or responsiveness. Another concern was microphone instability on low-end devices, where users reported delays in microphone initialization or instances where speech was not recognized, particularly during system startup or under constrained hardware conditions.</p><p>In this paper, the voice command component was not explored in depth. We selected a lightweight and general-purpose voice recognition module to ensure broad compatibility and minimal computational overhead. The primary design criteria were simplicity, low latency, and ease of integration. However, more advanced alternatives could be considered. For instance, integrating modern speech recognition frameworks such as OpenAI Whisper [<xref rid="B38-sensors-25-05411" ref-type="bibr">38</xref>] may offer improved robustness, especially in noisy environments. In addition, exploring non-speech voice command systems [<xref rid="B41-sensors-25-05411" ref-type="bibr">41</xref>] could further enhance the responsiveness, which is particularly beneficial for gamers. Additionally, our system currently underutilizes eye inputs. While eye direction is used as a gesture trigger, the system does not yet leverage richer gaze data for pointer control or attention estimation. Enhancing the eye-tracking integration could significantly improve the precision and interaction depth, especially for users with limited facial mobility.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05411"><title>6. Conclusions</title><p>This paper presents 3M-HCI, a novel, low-cost, and hands-free human-computer interaction system that integrates facial expressions, head movements, eye gaze, and voice commands through a unified processing pipeline. The central contribution of 3M-HCI lies in its unified processing architecture, which integrates three key components: (1) a cross-modal coordination mechanism that synchronizes facial, vocal, and eye-based inputs to enhance reliability and reduce false triggers; (2) an adaptive signal filtering method that suppresses input noise while maintaining low-latency responsiveness; (3) a refined input-to-cursor mapping strategy that improves control accuracy and minimizes jitter.</p><p>Our system enhances stability by reducing cursor jitter eightfold to under 10 pixels, while maintaining better responsiveness and accuracy than recent systems. Our system also performs well under challenging conditions, such as background noise, poor lighting, and low-end hardware configurations, compared to previous systems. This superiority was also reflected in direct user feedback, which awarded our system high scores for responsiveness (8.88/10), precision (8.37/10), and usability (7.85/10).</p><p>Although experimental results demonstrate that 3M-HCI outperforms recent baseline models in both accuracy and responsiveness, the system still requires further refinement. Future work will focus on three key directions. First, we aim to improve the usability by simplifying the tuning of the One Euro Filter through a real-time interface that abstracts low-level parameters such as the minCutoff and beta. Second, we plan to explore richer use of eye inputs, extending beyond simple gesture triggers toward gaze-based pointer control and attention estimation. Finally, we intend to investigate more versatile voice modules that provide greater robustness in noisy environments and broader adaptability across devices. Together, these directions will enhance the adaptability, precision, and inclusiveness of the 3M-HCI system.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, B.H.Q.; methodology, B.H.Q. and N.D.T.A.; software, B.H.Q. and N.D.T.A.; validation, B.H.Q., N.D.T.A., H.V.P. and B.T.T.; writing&#8212;original draft preparation, B.H.Q., H.V.P. and N.D.T.A.; writing&#8212;review and editing, B.T.T.; visualization, B.H.Q. and H.V.P. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data supporting the findings of this study are available from the corresponding author upon request. All source codes of the applications are available in this public repository: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ndtuananh04/3-Modal-Human-Computer-Interaction">https://github.com/ndtuananh04/3-Modal-Human-Computer-Interaction</uri> (accessed on 20 August 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">3M-HCI</td><td align="left" valign="middle" rowspan="1" colspan="1">3-Modal Human-Computer Interaction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ALS</td><td align="left" valign="middle" rowspan="1" colspan="1">Amyotrophic Lateral Sclerosis (ALS)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AI</td><td align="left" valign="middle" rowspan="1" colspan="1">Artificial Intelligence</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolution Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Central Processing Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Graphics Processing Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RAM</td><td align="left" valign="middle" rowspan="1" colspan="1">Random Access Memory</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RGB</td><td align="left" valign="middle" rowspan="1" colspan="1">Red Green Blue</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OS</td><td align="left" valign="middle" rowspan="1" colspan="1">Operating System</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05411"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You only look once: Unified, real-time object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>779</fpage><lpage>788</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id></element-citation></ref><ref id="B2-sensors-25-05411"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ramachandra</surname><given-names>C.K.</given-names></name><name name-style="western"><surname>Joseph</surname><given-names>A.</given-names></name></person-group><article-title>IEyeGASE: An Intelligent Eye Gaze-Based Assessment System for Deeper Insights into Learner Performance</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>6783</elocation-id><pub-id pub-id-type="doi">10.3390/s21206783</pub-id><pub-id pub-id-type="pmid">34695995</pub-id><pub-id pub-id-type="pmcid">PMC8540391</pub-id></element-citation></ref><ref id="B3-sensors-25-05411"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Walle</surname><given-names>H.</given-names></name><name name-style="western"><surname>De Runz</surname><given-names>C.</given-names></name><name name-style="western"><surname>Serres</surname><given-names>B.</given-names></name><name name-style="western"><surname>Venturini</surname><given-names>G.</given-names></name></person-group><article-title>A Survey on Recent Advances in AI and Vision-Based Methods for Helping and Guiding Visually Impaired People</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>2308</elocation-id><pub-id pub-id-type="doi">10.3390/app12052308</pub-id></element-citation></ref><ref id="B4-sensors-25-05411"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ramos</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zapata</surname><given-names>M.</given-names></name><name name-style="western"><surname>Valencia</surname><given-names>K.</given-names></name><name name-style="western"><surname>Vargas</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ramos-Galarza</surname><given-names>C.</given-names></name></person-group><article-title>Low-Cost Human&#8211;Machine Interface for Computer Control with Facial Landmark Detection and Voice Commands</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>9279</elocation-id><pub-id pub-id-type="doi">10.3390/s22239279</pub-id><pub-id pub-id-type="pmid">36501980</pub-id><pub-id pub-id-type="pmcid">PMC9735627</pub-id></element-citation></ref><ref id="B5-sensors-25-05411"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zapata</surname><given-names>M.</given-names></name><name name-style="western"><surname>Valencia-Arag&#243;n</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ramos-Galarza</surname><given-names>C.</given-names></name></person-group><article-title>Experimental Evaluation of EMKEY: An Assistive Technology for People with Upper Limb Disabilities</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>4049</elocation-id><pub-id pub-id-type="doi">10.3390/s23084049</pub-id><pub-id pub-id-type="pmid">37112394</pub-id><pub-id pub-id-type="pmcid">PMC10144790</pub-id></element-citation></ref><ref id="B6-sensors-25-05411"><label>6.</label><element-citation publication-type="webpage"><article-title>Introducing Project Gameface: A hands-free, AI-powered gaming mouse</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://blog.google/technology/ai/google-project-gameface/" ext-link-type="uri">https://blog.google/technology/ai/google-project-gameface/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-14">(accessed on 14 July 2025)</date-in-citation></element-citation></ref><ref id="B7-sensors-25-05411"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>MacLellan</surname><given-names>L.E.</given-names></name><name name-style="western"><surname>Stepp</surname><given-names>C.E.</given-names></name><name name-style="western"><surname>Fager</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Mentis</surname><given-names>M.</given-names></name><name name-style="western"><surname>Boucher</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Abur</surname><given-names>D.</given-names></name><name name-style="western"><surname>Cler</surname><given-names>G.J.</given-names></name></person-group><article-title>Evaluating Camera Mouse as a computer access system for augmentative and alternative communication in cerebral palsy: A case study</article-title><source>Assist. Technol.</source><year>2024</year><volume>36</volume><fpage>217</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1080/10400435.2023.2242893</pub-id><pub-id pub-id-type="pmid">37699111</pub-id><pub-id pub-id-type="pmcid">PMC10927611</pub-id></element-citation></ref><ref id="B8-sensors-25-05411"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Karimli</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jain</surname><given-names>S.</given-names></name><name name-style="western"><surname>Akosah</surname><given-names>E.S.</given-names></name><name name-style="western"><surname>Betke</surname><given-names>M.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>W.</given-names></name></person-group><article-title>Demonstration of CameraMouseAI: A Head-Based Mouse-Control System for People with Severe Motor Disabilities</article-title><source>Proceedings of the 26th ACM SIGACCESS Conference on Computers and Accessibility (ASSETS 2024)</source><conf-loc>Atlanta, GA, USA</conf-loc><conf-date>7&#8211;10 October 2024</conf-date><fpage>124:1</fpage><lpage>124:6</lpage><pub-id pub-id-type="doi">10.1145/3663548.3688499</pub-id></element-citation></ref><ref id="B9-sensors-25-05411"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Song</surname><given-names>F.</given-names></name></person-group><article-title>The global burden of traumatic amputation in 204 countries and territories</article-title><source>Front. Public Health</source><year>2023</year><volume>11</volume><elocation-id>1258853</elocation-id><pub-id pub-id-type="doi">10.3389/fpubh.2023.1258853</pub-id><pub-id pub-id-type="pmid">37927851</pub-id><pub-id pub-id-type="pmcid">PMC10622756</pub-id></element-citation></ref><ref id="B10-sensors-25-05411"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mehta</surname><given-names>P.</given-names></name><name name-style="western"><surname>Raymond</surname><given-names>J.</given-names></name><name name-style="western"><surname>Nair</surname><given-names>T.</given-names></name><name name-style="western"><surname>Han</surname><given-names>M.</given-names></name><name name-style="western"><surname>Berry</surname><given-names>J.</given-names></name><name name-style="western"><surname>Punjani</surname><given-names>R.</given-names></name><name name-style="western"><surname>Larson</surname><given-names>T.</given-names></name><name name-style="western"><surname>Mohidul</surname><given-names>S.</given-names></name><name name-style="western"><surname>Horton</surname><given-names>D.K.</given-names></name></person-group><article-title>Amyotrophic lateral sclerosis estimated prevalence cases from 2022 to 2030, data from the National ALS Registry</article-title><source>Amyotroph. Lateral Scler. Front. Degener.</source><year>2025</year><volume>26</volume><fpage>290</fpage><lpage>295</lpage><pub-id pub-id-type="doi">10.1080/21678421.2024.2447919</pub-id><pub-id pub-id-type="pmid">39749668</pub-id></element-citation></ref><ref id="B11-sensors-25-05411"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.-L.</given-names></name></person-group><article-title>Application of tilt sensors in human&#8211;computer mouse interface for people with disabilities</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2001</year><volume>9</volume><fpage>289</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1109/7333.948457</pub-id><pub-id pub-id-type="pmid">11561665</pub-id></element-citation></ref><ref id="B12-sensors-25-05411"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mishra</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bhalla</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kharad</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yadav</surname><given-names>D.</given-names></name></person-group><article-title>HMOS: Head Control Mouse Person with Disability</article-title><source>Int. J. Recent Innov. Trends Comput. Commun.</source><year>2017</year><volume>5</volume><fpage>576</fpage><lpage>583</lpage></element-citation></ref><ref id="B13-sensors-25-05411"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ribas-Xirgo</surname><given-names>L.</given-names></name><name name-style="western"><surname>L&#243;pez-Varquiel</surname><given-names>F.</given-names></name></person-group><article-title>Accelerometer-Based Computer Mouse for People with Special Needs</article-title><source>J. Access. Des. All.</source><year>2017</year><volume>7</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.17411/jacces.v7i1.113</pub-id></element-citation></ref><ref id="B14-sensors-25-05411"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>M.</given-names></name><name name-style="western"><surname>Anumas</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yoo</surname><given-names>J.</given-names></name></person-group><article-title>Head Mouse System Based on Gyro- and Opto-Sensors</article-title><source>Proceedings of the International Conference on Biomedical Engineering and Informatics</source><conf-loc>Yantai, China</conf-loc><conf-date>16&#8211;18 October 2010</conf-date><pub-id pub-id-type="doi">10.1109/BMEI.2010.5639399</pub-id></element-citation></ref><ref id="B15-sensors-25-05411"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pereira</surname><given-names>C.A.M.</given-names></name><name name-style="western"><surname>Bolliger Neto</surname><given-names>R.</given-names></name><name name-style="western"><surname>Reynaldo</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Luzo</surname><given-names>M.C.M.</given-names></name><name name-style="western"><surname>Oliveira</surname><given-names>R.P.</given-names></name></person-group><article-title>Development and evaluation of a head-controlled human-computer interface with mouse-like functions for physically disabled users</article-title><source>Clinics</source><year>2009</year><volume>64</volume><fpage>975</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1590/S1807-59322009001000007</pub-id><pub-id pub-id-type="pmid">19841704</pub-id><pub-id pub-id-type="pmcid">PMC2763072</pub-id></element-citation></ref><ref id="B16-sensors-25-05411"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>C.-S.</given-names></name><name name-style="western"><surname>Ho</surname><given-names>C.-W.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>C.-N.</given-names></name><name name-style="western"><surname>Chau</surname><given-names>C.-R.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.-C.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>M.-S.</given-names></name></person-group><article-title>An Eye-Tracking and Head-Control System Using Movement Increment-Coordinate Method</article-title><source>Opt. Laser Technol.</source><year>2007</year><volume>39</volume><fpage>1218</fpage><lpage>1225</lpage><pub-id pub-id-type="doi">10.1016/j.optlastec.2006.08.002</pub-id></element-citation></ref><ref id="B17-sensors-25-05411"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Betke</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gips</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fleming</surname><given-names>P.</given-names></name></person-group><article-title>The Camera Mouse: Visual Tracking of Body Features to Provide Computer Access for People with Severe Disabilities</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2002</year><volume>10</volume><fpage>1021581</fpage><pub-id pub-id-type="doi">10.1109/TNSRE.2002.1021581</pub-id><pub-id pub-id-type="pmid">12173734</pub-id></element-citation></ref><ref id="B18-sensors-25-05411"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Su</surname><given-names>S.Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.D.</given-names></name></person-group><article-title>A low-cost vision-based human-computer interface for people with severe disabilities</article-title><source>Biomed. Eng. Appl. Basis Commun.</source><year>2005</year><volume>17</volume><fpage>284</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.4015/S1016237205000433</pub-id></element-citation></ref><ref id="B19-sensors-25-05411"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Naizhong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jun</surname><given-names>W.</given-names></name></person-group><article-title>Hand-free head mouse control based on mouth tracking</article-title><source>Proceedings of the IEEE International Conference on Computational Science and Education (ICCSE 2015)</source><conf-loc>Cambridge, UK</conf-loc><conf-date>22&#8211;24 July 2015</conf-date><pub-id pub-id-type="doi">10.1109/ICCSE.2015.7250337</pub-id></element-citation></ref><ref id="B20-sensors-25-05411"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arai</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mardiyanto</surname><given-names>R.</given-names></name></person-group><article-title>Camera as Mouse and Keyboard for Handicap Person with Troubleshooting Ability, Recovery, and Complete Mouse Events</article-title><source>Int. J. Hum. Comput. Interact.</source><year>2010</year><volume>1</volume><fpage>46</fpage><lpage>56</lpage></element-citation></ref><ref id="B21-sensors-25-05411"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ismail</surname><given-names>A.</given-names></name><name name-style="western"><surname>Al Hajjar</surname><given-names>A.E.S.</given-names></name><name name-style="western"><surname>Hajjar</surname><given-names>M.</given-names></name></person-group><article-title>A prototype system for controlling a computer by head movements and voice commands</article-title><source>arXiv</source><year>2011</year><pub-id pub-id-type="arxiv">1109.1454</pub-id><pub-id pub-id-type="doi">10.5121/ijma.2011.3302</pub-id></element-citation></ref><ref id="B22-sensors-25-05411"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sawicki</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kowalczyk</surname><given-names>P.</given-names></name></person-group><article-title>Head Movement Based Interaction in Mobility</article-title><source>Int. J. Hum.-Comput. Interact.</source><year>2017</year><volume>34</volume><fpage>653</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1080/10447318.2017.1392078</pub-id></element-citation></ref><ref id="B23-sensors-25-05411"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abiyev</surname><given-names>R.H.</given-names></name><name name-style="western"><surname>Arslan</surname><given-names>M.</given-names></name></person-group><article-title>Head mouse control system for people with disabilities</article-title><source>Expert Syst.</source><year>2019</year><volume>37</volume><fpage>e12398</fpage><pub-id pub-id-type="doi">10.1111/exsy.12398</pub-id></element-citation></ref><ref id="B24-sensors-25-05411"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rahmaniar</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ma&#8217;Arif</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>T.-L.</given-names></name></person-group><article-title>Touchless Head-Control (THC): Head Gesture Recognition for Cursor and Orientation Control</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2022</year><volume>30</volume><fpage>1817</fpage><lpage>1828</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2022.3187472</pub-id><pub-id pub-id-type="pmid">35771790</pub-id></element-citation></ref><ref id="B25-sensors-25-05411"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zelinskyi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Boyko</surname><given-names>Y.</given-names></name></person-group><article-title>Using Facial Expressions for Custom Actions: Development and Evaluation of a Hands-Free Interaction Method</article-title><source>Computer Syst. Inf. Technol.</source><year>2024</year><volume>4</volume><fpage>116</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.31891/csit-2024-4-14</pub-id></element-citation></ref><ref id="B26-sensors-25-05411"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>A real-time camera-based gaze-tracking system involving dual interactive modes and its application in gaming</article-title><source>Multimed. Syst.</source><year>2024</year><volume>30</volume><fpage>15</fpage><pub-id pub-id-type="doi">10.1007/s00530-023-01204-9</pub-id></element-citation></ref><ref id="B27-sensors-25-05411"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bradski</surname><given-names>G.</given-names></name></person-group><article-title>The OpenCV Library</article-title><source>Dr. Dobb&#8217;s J. Softw. Tools</source><year>2000</year><volume>25</volume><fpage>120</fpage><lpage>126</lpage></element-citation></ref><ref id="B28-sensors-25-05411"><label>28.</label><element-citation publication-type="webpage"><article-title>Dlib</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dlib.net/python/" ext-link-type="uri">https://dlib.net/python/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-17">(accessed on 17 July 2025)</date-in-citation></element-citation></ref><ref id="B29-sensors-25-05411"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Viola</surname><given-names>P.A.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>M.</given-names></name></person-group><article-title>Rapid Object Detection using a Boosted Cascade of Simple Features</article-title><source>Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2001)</source><conf-loc>Kauai, HI, USA</conf-loc><conf-date>8&#8211;14 December 2001</conf-date><fpage>511</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2001.990517</pub-id></element-citation></ref><ref id="B30-sensors-25-05411"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>J.</given-names></name><name name-style="western"><surname>Modi</surname><given-names>N.</given-names></name></person-group><article-title>A robust, real-time camera-based eye gaze tracking system to analyze users&#8217; visual attention using deep learning</article-title><source>Interact. Learn. Environ.</source><year>2022</year><volume>30</volume><fpage>409</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1080/10494820.2022.2088561</pub-id></element-citation></ref><ref id="B31-sensors-25-05411"><label>31.</label><element-citation publication-type="webpage"><article-title>Mediapipe</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ai.google.dev/edge/mediapipe/solutions/guide" ext-link-type="uri">https://ai.google.dev/edge/mediapipe/solutions/guide</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-15">(accessed on 15 July 2025)</date-in-citation></element-citation></ref><ref id="B32-sensors-25-05411"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name></person-group><article-title>Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks</article-title><source>IEEE Signal Process. Lett.</source><year>2016</year><volume>23</volume><fpage>1499</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1109/LSP.2016.2603342</pub-id></element-citation></ref><ref id="B33-sensors-25-05411"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>S.</given-names></name></person-group><article-title>YuNet: A Tiny Millisecond-level Face Detector</article-title><source>Mach. Intell. Res.</source><year>2023</year><volume>20</volume><fpage>656</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1007/s11633-023-1423-y</pub-id></element-citation></ref><ref id="B34-sensors-25-05411"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Casiez</surname><given-names>G.</given-names></name><name name-style="western"><surname>Roussel</surname><given-names>N.</given-names></name><name name-style="western"><surname>Vogel</surname><given-names>D.</given-names></name></person-group><article-title>1 &#8364; Filter: A Simple Speed-based Low-pass Filter for Noisy Input in Interactive Systems</article-title><source>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &#8217;12)</source><conf-loc>Austin, TX, USA</conf-loc><conf-date>5&#8211;10 May 2012</conf-date><fpage>2527</fpage><lpage>2530</lpage><pub-id pub-id-type="doi">10.1145/2207676.2208639</pub-id></element-citation></ref><ref id="B35-sensors-25-05411"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sidenmark</surname><given-names>L.</given-names></name><name name-style="western"><surname>Weidner</surname><given-names>F.</given-names></name><name name-style="western"><surname>Newn</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gellersen</surname><given-names>H.</given-names></name></person-group><article-title>HeadShift: Head Pointing with Dynamic Control-Display Gain</article-title><source>ACM Trans. Comput.-Hum. Interact.</source><year>2025</year><volume>32</volume><fpage>2</fpage><pub-id pub-id-type="doi">10.1145/3689434</pub-id></element-citation></ref><ref id="B36-sensors-25-05411"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Voelker</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hueber</surname><given-names>S.</given-names></name><name name-style="western"><surname>Corsten</surname><given-names>C.</given-names></name><name name-style="western"><surname>Remy</surname><given-names>C.</given-names></name></person-group><article-title>HeadReach: Using Head Tracking to Increase Reachability on Mobile Touch Devices</article-title><source>Proceedings of the 2020 ACM SIGCHI Conference on Human Factors in Computing Systems (CHI &#8217;20)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>25&#8211;30 April 2020</conf-date><fpage>739:1</fpage><lpage>739:12</lpage><pub-id pub-id-type="doi">10.1145/3313831.3376868</pub-id></element-citation></ref><ref id="B37-sensors-25-05411"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nancel</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chapuis</surname><given-names>O.</given-names></name><name name-style="western"><surname>Pietriga</surname><given-names>E.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.-D.</given-names></name><name name-style="western"><surname>Irani</surname><given-names>P.P.</given-names></name><name name-style="western"><surname>Beaudouin-Lafon</surname><given-names>M.</given-names></name></person-group><article-title>High-Precision Pointing on Large Wall Displays Using Small Handheld Devices</article-title><source>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &#8216;13)</source><conf-loc>Paris, France</conf-loc><conf-date>27 April&#8211;2 May 2013</conf-date><fpage>831</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1145/2470654.2470773</pub-id></element-citation></ref><ref id="B38-sensors-25-05411"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Brockman</surname><given-names>G.</given-names></name><name name-style="western"><surname>McLeavey</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sutskever</surname><given-names>I.</given-names></name></person-group><article-title>Robust Speech Recognition via Large-Scale Weak Supervision</article-title><source>Proceedings of the 40th International Conference on Machine Learning (ICML 2023)</source><conf-loc>Baltimore, MD, USA</conf-loc><conf-date>23&#8211;29 July 2023</conf-date><fpage>28492</fpage><lpage>28518</lpage></element-citation></ref><ref id="B39-sensors-25-05411"><label>39.</label><element-citation publication-type="webpage"><article-title>Vosk</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://alphacephei.com/vosk/" ext-link-type="uri">https://alphacephei.com/vosk/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-20">(accessed on 20 August 2025)</date-in-citation></element-citation></ref><ref id="B40-sensors-25-05411"><label>40.</label><element-citation publication-type="webpage"><article-title>SAPI5 (Dragonfly)</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dragonfly2.readthedocs.io/en/latest/index.html" ext-link-type="uri">https://dragonfly2.readthedocs.io/en/latest/index.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-15">(accessed on 15 July 2025)</date-in-citation></element-citation></ref><ref id="B41-sensors-25-05411"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Harada</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wobbrock</surname><given-names>J.O.</given-names></name><name name-style="western"><surname>Landay</surname><given-names>J.A.</given-names></name></person-group><article-title>Voice Games: Investigation Into the Use of Non-speech Voice Input for Making Computer Games More Accessible</article-title><source>Proceedings of the 13th IFIP TC 13 International Conference on Human-Computer Interaction (INTERACT 2011)</source><conf-loc>Lisbon, Portugal</conf-loc><conf-date>5&#8211;9 September 2011</conf-date><comment>Lecture Notes in Computer Science</comment><volume>Volume 6946</volume><fpage>11</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-23774-4_4</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05411-f001" orientation="portrait"><label>Figure 1</label><caption><p>Overview of system architecture and processing modules.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g001.jpg"/></fig><fig position="float" id="sensors-25-05411-f002" orientation="portrait"><label>Figure 2</label><caption><p>Multimodal input processing pipeline.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g002.jpg"/></fig><fig position="float" id="sensors-25-05411-f003" orientation="portrait"><label>Figure 3</label><caption><p>Mediapipe 478 landmarks. Each landmark point corresponds to a specific part of the face.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g003.jpg"/></fig><fig position="float" id="sensors-25-05411-f004" orientation="portrait"><label>Figure 4</label><caption><p>Two inner eye corners (p133 and p362 in Mediapipe).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g004.jpg"/></fig><fig position="float" id="sensors-25-05411-f005" orientation="portrait"><label>Figure 5</label><caption><p>Acceleration curve.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g005.jpg"/></fig><fig position="float" id="sensors-25-05411-f006" orientation="portrait"><label>Figure 6</label><caption><p>The 3M-HCI graphical user interfaces built with Customtkinter.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g006.jpg"/></fig><fig position="float" id="sensors-25-05411-f007" orientation="portrait"><label>Figure 7</label><caption><p>Moving and clicking tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g007.jpg"/></fig><fig position="float" id="sensors-25-05411-f008" orientation="portrait"><label>Figure 8</label><caption><p>Mediapipe facial landmarks detection in different light conditions. (<bold>a</bold>,<bold>b</bold>) Bright environment. (<bold>c</bold>) Dim environment. (<bold>d</bold>) Dark environment.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g008.jpg"/></fig><fig position="float" id="sensors-25-05411-f009" orientation="portrait"><label>Figure 9</label><caption><p>Deviations from optimal path.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g009.jpg"/></fig><fig position="float" id="sensors-25-05411-f010" orientation="portrait"><label>Figure 10</label><caption><p>Number of clicks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g010.jpg"/></fig><fig position="float" id="sensors-25-05411-f011" orientation="portrait"><label>Figure 11</label><caption><p>Movement latency.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g011.jpg"/></fig><fig position="float" id="sensors-25-05411-f012" orientation="portrait"><label>Figure 12</label><caption><p>Jitter deviation of different systems.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05411-g012.jpg"/></fig><table-wrap position="float" id="sensors-25-05411-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05411-t001_Table 1</object-id><label>Table 1</label><caption><p>Face processing algorithm comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithm/Library</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of<break/> Landmarks</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Detection Time (second)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Detection Rate <sup>1</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Facial Expression</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Iris <break/> Tracking</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dlib [<xref rid="B28-sensors-25-05411" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">68</td><td align="center" valign="middle" rowspan="1" colspan="1">0.036</td><td align="center" valign="middle" rowspan="1" colspan="1">0.78</td><td align="center" valign="middle" rowspan="1" colspan="1">Some facial expressions can be computed manually <sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mediapipe [<xref rid="B31-sensors-25-05411" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">478</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0037</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">52 built-in blendshapes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Haar Cascade [<xref rid="B29-sensors-25-05411" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0056</td><td align="center" valign="middle" rowspan="1" colspan="1">0.65</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MTCNN [<xref rid="B32-sensors-25-05411" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.215</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YuNet [<xref rid="B33-sensors-25-05411" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.026</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> Percentage of faces correctly detected among all images containing a face. <sup>2</sup> Following [<xref rid="B4-sensors-25-05411" ref-type="bibr">4</xref>], we can calculate the mouth being open, for example.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05411-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05411-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of our system with facial expression control interfaces.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">System</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">#Facial <break/> Expression</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mouse Control</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Keyboard Control</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">System Control</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Triggering Mechanism</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">EMKEY [<xref rid="B5-sensors-25-05411" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">x</td><td align="center" valign="middle" rowspan="1" colspan="1">Predefined Threshold</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CameraMouseAI [<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">x</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">User-Defined Threshold</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Project GameFace [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">x</td><td align="center" valign="middle" rowspan="1" colspan="1">x</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">User-Defined Threshold</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Zelinskyi et al. [<xref rid="B34-sensors-25-05411" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">x</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">Predefined Threshold</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3M-HCI (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">x</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">x</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">x</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">User-Defined Threshold with Priority</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05411-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05411-t003_Table 3</object-id><label>Table 3</label><caption><p>List of survey questions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Question</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q1</td><td align="left" valign="middle" rowspan="1" colspan="1">Does it take a lot of time to master the application?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q2</td><td align="left" valign="middle" rowspan="1" colspan="1">Is the response of left/right mouse clicks fast?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q3</td><td align="left" valign="middle" rowspan="1" colspan="1">Is the cursor movement responsive?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q4</td><td align="left" valign="middle" rowspan="1" colspan="1">Is it difficult to click the left/right mouse button?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q5</td><td align="left" valign="middle" rowspan="1" colspan="1">Is it difficult to move the cursor precisely?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q6</td><td align="left" valign="middle" rowspan="1" colspan="1">Is it difficult to move the cursor vertically?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q7</td><td align="left" valign="middle" rowspan="1" colspan="1">Is it difficult to move the cursor horizontally?</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q8</td><td align="left" valign="middle" rowspan="1" colspan="1">Does moving the cursor cause fatigue?</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Do you think this mouse system can be applied to people with disabilities?</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05411-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05411-t004_Table 4</object-id><label>Table 4</label><caption><p>Software performance on different laptops.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Laptop</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CPU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RAM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">OS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall<break/> Performance</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computational<break/> Cost (3M-HCI)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computational<break/> Cost [<xref rid="B6-sensors-25-05411" ref-type="bibr">6</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computational<break/> Cost [<xref rid="B8-sensors-25-05411" ref-type="bibr">8</xref>]</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dell Inspiron 15 3530</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i7-1355U</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Excellent</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.1%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.7%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dell XPS 13 9360</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i7-7660U</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OK. The microphone takes time to boot</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Unable to run</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dell G15 5530</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i7-13650HX</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Excellent</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.5%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lenovo ThinkPad T480</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i5-8350U</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OK. The program is a bit laggy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.8%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.2%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dell Precision 7510</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i7-6820HQ</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Excellent</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.9%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.5%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.8%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSI GF63 Thin 11UD</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i7-11800H</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows<break/>11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OK. The microphone takes time to boot</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Unable to run</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dell Inspiron 16 5620</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i5-1240P</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Excellent</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.1%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.56%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.2%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HP Laptop 16-d0xxx</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i5-11400H</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Excellent</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.1%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.05%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASUS TUF Gaming F15 </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Intel Core i5-10300H</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Some commands cannot be recognized.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.88%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.37%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.4%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSI Modern 15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AMD Ryzen 5 5500U</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12 GB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Windows 11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Voice command runs badly.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.6%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.4%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.2%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05411-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05411-t005_Table 5</object-id><label>Table 5</label><caption><p>Survey results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Question</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CameraMouseAI</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Project GameFace</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">3M-HCI (Ours)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mouse</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q1</td><td align="left" valign="middle" rowspan="1" colspan="1">Does it take a lot of time to master the application?</td><td align="center" valign="middle" rowspan="1" colspan="1">3.5 &#177; 1.87</td><td align="center" valign="middle" rowspan="1" colspan="1">6.75 &#177; 1.98</td><td align="center" valign="middle" rowspan="1" colspan="1">7.25 &#177; 1.71</td><td align="center" valign="middle" rowspan="1" colspan="1">10.0 &#177; 0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q2</td><td align="left" valign="middle" rowspan="1" colspan="1">Is the response of left/right mouse click fast?</td><td align="center" valign="middle" rowspan="1" colspan="1">3.5 &#177; 2.35</td><td align="center" valign="middle" rowspan="1" colspan="1">7.5 &#177; 1.58</td><td align="center" valign="middle" rowspan="1" colspan="1">8.25 &#177; 1.09</td><td align="center" valign="middle" rowspan="1" colspan="1">10.0 &#177; 0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q3</td><td align="left" valign="middle" rowspan="1" colspan="1">Is the cursor movement responsive?</td><td align="center" valign="middle" rowspan="1" colspan="1">5.38 &#177; 2.06</td><td align="center" valign="middle" rowspan="1" colspan="1">6.62 &#177; 1.11</td><td align="center" valign="middle" rowspan="1" colspan="1">8.88 &#177; 0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">10.0 &#177; 0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q4</td><td align="left" valign="middle" rowspan="1" colspan="1">Is it difficult to click the left/right mouse button?</td><td align="center" valign="middle" rowspan="1" colspan="1">4 &#177; 2.55</td><td align="center" valign="middle" rowspan="1" colspan="1">6.25 &#177; 1.56</td><td align="center" valign="middle" rowspan="1" colspan="1">7 &#177; 1.87</td><td align="center" valign="middle" rowspan="1" colspan="1">10.0 &#177; 0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q5</td><td align="left" valign="middle" rowspan="1" colspan="1">Is it difficult to move the cursor precisely?</td><td align="center" valign="middle" rowspan="1" colspan="1">3.5 &#177; 2.45</td><td align="center" valign="middle" rowspan="1" colspan="1">6.87 &#177; 1.17</td><td align="center" valign="middle" rowspan="1" colspan="1">8.37 &#177; 0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">10.0 &#177; 0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q6</td><td align="left" valign="middle" rowspan="1" colspan="1">Is it difficult to move the cursor vertically?</td><td align="center" valign="middle" rowspan="1" colspan="1">4.5 &#177; 2.18</td><td align="center" valign="middle" rowspan="1" colspan="1">7.5 &#177; 1.41</td><td align="center" valign="middle" rowspan="1" colspan="1">8.37 &#177; 0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">10.0 &#177; 0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q7</td><td align="left" valign="middle" rowspan="1" colspan="1">Is it difficult to move the cursor horizontally?</td><td align="center" valign="middle" rowspan="1" colspan="1">4.5 &#177; 2.18</td><td align="center" valign="middle" rowspan="1" colspan="1">7.5 &#177; 1.41</td><td align="center" valign="middle" rowspan="1" colspan="1">8.37 &#177; 0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">10.0 &#177; 0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Q8</td><td align="left" valign="middle" rowspan="1" colspan="1">Does moving the cursor cause fatigue?</td><td align="center" valign="middle" rowspan="1" colspan="1">2.62 &#177; 1.93</td><td align="center" valign="middle" rowspan="1" colspan="1">7 &#177; 1.87</td><td align="center" valign="middle" rowspan="1" colspan="1">7.25 &#177; 1.79</td><td align="center" valign="middle" rowspan="1" colspan="1">9.87 &#177; 0.33</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Q9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Do you think this mouse system can be applied for people with disabilities?</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4 &#177; 3.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.12 &#177; 2.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.85 &#177; 2.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.75 &#177; 3.9</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>