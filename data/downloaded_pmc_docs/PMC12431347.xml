<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431347</article-id><article-id pub-id-type="pmcid-ver">PMC12431347.1</article-id><article-id pub-id-type="pmcaid">12431347</article-id><article-id pub-id-type="pmcaiid">12431347</article-id><article-id pub-id-type="doi">10.3390/s25175531</article-id><article-id pub-id-type="publisher-id">sensors-25-05531</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>RST-Net: A Semantic Segmentation Network for Remote Sensing Images Based on a Dual-Branch Encoder Structure</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Yang</surname><given-names initials="N">Na</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05531" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Tian</surname><given-names initials="C">Chuanzhao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05531" ref-type="aff">1</xref><xref rid="af2-sensors-25-05531" ref-type="aff">2</xref><xref rid="c1-sensors-25-05531" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Gu</surname><given-names initials="X">Xingfa</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05531" ref-type="aff">1</xref><xref rid="af3-sensors-25-05531" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="Y">Yanting</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-05531" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="X">Xuewen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05531" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="F">Feng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05531" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Haxha</surname><given-names initials="S">Shyqyri</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05531"><label>1</label>College of Remote Sensing and Information Engineering, North China Institute of Aerospace Engineering, Langfang 065000, China; <email>19935342291@163.com</email> (N.Y.); <email>zhangyanting0309@163.com</email> (Y.Z.); <email>15192839463@163.com</email> (X.L.); <email>zhangfeng202303@163.com</email> (F.Z.)</aff><aff id="af2-sensors-25-05531"><label>2</label>Collaborative Innovation Center of Aerospace Remote Sensing Information Processing and Application of Hebei Province, Langfang 065000, China</aff><aff id="af3-sensors-25-05531"><label>3</label>Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China</aff><author-notes><corresp id="c1-sensors-25-05531"><label>*</label>Correspondence: <email>tiancz@radi.ac.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5531</elocation-id><history><date date-type="received"><day>28</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>19</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>05</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 13:25:28.783"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05531.pdf"/><abstract><p>High-resolution remote sensing images often suffer from inadequate fusion between global and local features, leading to the loss of long-range dependencies and blurred spatial details, while also exhibiting limited adaptability to multi-scale object segmentation. To overcome these limitations, this study proposes RST-Net, a semantic segmentation network featuring a dual-branch encoder structure. The encoder integrates a ResNeXt-50-based CNN branch for extracting local spatial features and a Shunted Transformer (ST) branch for capturing global contextual information. To further enhance multi-scale representation, the multi-scale feature enhancement module (MSFEM) is embedded in the CNN branch, leveraging atrous and depthwise separable convolutions to dynamically aggregate features. Additionally, the residual dynamic feature fusion (RDFF) module is incorporated into skip connections to improve interactions between encoder and decoder features. Experiments on the Vaihingen and Potsdam datasets show that RST-Net achieves promising performance, with MIoU scores of 77.04% and 79.56%, respectively, validating its effectiveness in semantic segmentation tasks.</p></abstract><kwd-group><kwd>dual-branch encoder structure</kwd><kwd>remote sensing images</kwd><kwd>semantic segmentation</kwd><kwd>multi-scale feature fusion</kwd></kwd-group><funding-group><award-group><funding-source>Science Research Project of Hebei Education Department</funding-source><award-id>BJK2024115</award-id></award-group><funding-statement>This research was funded by Science Research Project of Hebei Education Department, grant number BJK2024115.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05531"><title>1. Introduction</title><p>Semantic segmentation of remote sensing images (RSI), an interdisciplinary topic between computer vision and geographic information science, targets the pixel-wise classification of land cover to facilitate fine-scale interpretation and analysis. Enabled by advancements in high-resolution satellite imagery and multi-modal sensing technologies, RSI semantic segmentation has been widely applied in land resource monitoring [<xref rid="B1-sensors-25-05531" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05531" ref-type="bibr">2</xref>], disaster response [<xref rid="B3-sensors-25-05531" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05531" ref-type="bibr">4</xref>], and urban infrastructure planning [<xref rid="B5-sensors-25-05531" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05531" ref-type="bibr">6</xref>]. Traditional methods like support vector machines (SVM) [<xref rid="B7-sensors-25-05531" ref-type="bibr">7</xref>] and decision trees [<xref rid="B8-sensors-25-05531" ref-type="bibr">8</xref>] depend heavily on hand-crafted spectral and texture features, limiting their effectiveness to low-resolution images. However, they struggle to capture the spectral&#8211;spatial correlations essential for accurate modeling in high-resolution images.</p><p>The rise of deep learning has established the convolutional neural network (CNN) as the predominant approach for semantic segmentation, facilitating automated hierarchical feature abstraction from low-level visual patterns to high-level semantic representations. Since the introduction of the fully convolutional networks (FCN) [<xref rid="B9-sensors-25-05531" ref-type="bibr">9</xref>], a wide range of derivative structures have been developed to enhance segmentation performance. For instance, the SDFCNv1 framework [<xref rid="B10-sensors-25-05531" ref-type="bibr">10</xref>] enhances feature fusion using dense skip connections, while SDFCNv2 [<xref rid="B11-sensors-25-05531" ref-type="bibr">11</xref>] further extends the receptive field and reduces parameter overhead, thus improving segmentation efficiency. Predominantly adopting an encoder&#8211;decoder structure, current methods utilize the encoder for deep semantic feature extraction, with the decoder restoring spatial details progressively via skip connections and upsampling operations. Owing to its symmetric encoder&#8211;decoder structure and effective cross-layer feature propagation, U-Net [<xref rid="B12-sensors-25-05531" ref-type="bibr">12</xref>] has become a prevalent choice for RSI segmentation. SegNet [<xref rid="B13-sensors-25-05531" ref-type="bibr">13</xref>] enhances feature reconstruction by retaining max-pooling indices from the encoder to guide the decoder&#8217;s upsampling process. PSPNet [<xref rid="B14-sensors-25-05531" ref-type="bibr">14</xref>] incorporates a spatial pyramid pooling (SPP) module to improve the fusion of multi-scale contextual features. The DeepLab family [<xref rid="B15-sensors-25-05531" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05531" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05531" ref-type="bibr">17</xref>] utilizes atrous spatial pyramid pooling (ASPP), a mechanism designed to expand receptive fields while aggregating multi-scale contextual features. Despite their efficacy in natural image segmentation, these methods exhibit restricted applicability to RSI owing to the inability of fixed-size convolutional kernels to accommodate varying object scales. To address this, various multi-branch convolutional structures have been proposed to enhance multi-scale feature representation. For example, HRNet [<xref rid="B18-sensors-25-05531" ref-type="bibr">18</xref>] maintains multi-resolution representations via parallel convolution streams, enabling the preservation of fine-grained spatial details and substantially improving the segmentation accuracy of linear features such as roads and rivers. RefineNet [<xref rid="B19-sensors-25-05531" ref-type="bibr">19</xref>] refines feature map fusion via multi-path refinement modules, thereby improving the detection of small-scale targets and object boundaries. ResNeXt [<xref rid="B20-sensors-25-05531" ref-type="bibr">20</xref>] extends ResNet [<xref rid="B21-sensors-25-05531" ref-type="bibr">21</xref>] by introducing the concept of &#8220;cardinality&#8221;, which enhances feature diversity via parallel convolution branches and achieves improved scale adaptability without increasing computational cost.</p><p>In addition, attention mechanisms have been incorporated into CNN to improve feature discriminability and contextual representation. SE-Net [<xref rid="B22-sensors-25-05531" ref-type="bibr">22</xref>] explicitly models inter-channel dependencies via squeeze-and-excitation modules, allowing adaptive recalibration of channel-wise responses. Attention U-Net [<xref rid="B23-sensors-25-05531" ref-type="bibr">23</xref>] introduces a spatial attention gating mechanism to dynamically emphasize salient regions. CBAM [<xref rid="B24-sensors-25-05531" ref-type="bibr">24</xref>] refines feature representation by jointly applying channel and spatial attention, while DANet [<xref rid="B25-sensors-25-05531" ref-type="bibr">25</xref>] similarly employs dual attention to capture long-range dependencies. Based on the self-attention mechanism, the Transformer [<xref rid="B26-sensors-25-05531" ref-type="bibr">26</xref>] significantly improves global context perception by modeling long-range pixel-wise dependencies. For instance, Lu et al. proposed a regularized Transformer [<xref rid="B27-sensors-25-05531" ref-type="bibr">27</xref>] with adaptive token fusion for Alzheimer&#8217;s disease diagnosis in brain magnetic resonance imaging, dynamically selecting and fusing informative image tokens while regularizing feature diversity, and introduced a large adaptive filter and report-guided multi-level alignment network [<xref rid="B28-sensors-25-05531" ref-type="bibr">28</xref>] in chest X-rays, leveraging textual reports for weakly supervised alignment. Moreover, CTBViT [<xref rid="B29-sensors-25-05531" ref-type="bibr">29</xref>], a ViT variant with an efficient block design and randomized classifier for tuberculosis classification, has been proposed to enhance feature efficiency and generalization. Vision Transformer [<xref rid="B30-sensors-25-05531" ref-type="bibr">30</xref>] pioneered the division of images into sequential patches, yet its global attention incurs quadratic complexity with image size, limiting its scalability for high-resolution RSI. Swin Transformer [<xref rid="B31-sensors-25-05531" ref-type="bibr">31</xref>] mitigates computation via local window-based attention, but the fixed window size hampers the continuity of elongated structures such as roads and rivers. Shunted Transformer [<xref rid="B32-sensors-25-05531" ref-type="bibr">32</xref>] introduces a dynamic receptive field mechanism, allowing each attention head to adaptively attend to spatial regions at multiple scales. While retaining linear computational complexity, it effectively enhances multi-scale feature representation, particularly under complex urban conditions involving vegetation and dense built-up areas. Transformer-based structures have demonstrated superior global consistency over CNN in tasks like farmland boundary delineation and urban structure extraction. However, pure Transformer models lack inherent local inductive bias, making them less sensitive to fine-grained textures and computationally inefficient for high-resolution images. Owing to the complementary capabilities of CNN in local feature extraction and Transformers in global contextual modeling, their integration has become a prevailing strategy for advancing segmentation performance. For instance, TransUNet [<xref rid="B33-sensors-25-05531" ref-type="bibr">33</xref>] enhances global semantic consistency by embedding Transformer layers into the encoder; however, its single-path encoder structure is insufficient for preserving high-frequency spatial details. Swin-UNet [<xref rid="B34-sensors-25-05531" ref-type="bibr">34</xref>] improves segmentation accuracy in remote sensing images using window-based self-attention, but its fixed window size limits the ability to dynamically model objects at varying scales. RSFormer [<xref rid="B35-sensors-25-05531" ref-type="bibr">35</xref>] introduces a dual-branch spectral&#8211;spatial attention mechanism to optimize multi-modal feature fusion, yet its edge reconstruction performance on high-resolution images remains suboptimal. Although these hybrid CNN&#8211;Transformer structures have achieved promising results in remote sensing semantic segmentation, they still fall short in capturing the complex nonlinear interactions between local detail features and global semantic representations, thereby limiting their adaptability to objects of diverse scales.</p><p>To overcome these limitations, we present RST-Net, a novel semantic segmentation network with an encoder&#8211;decoder structure. The encoder adopts a dual-branch design that combines the strengths of CNN and Transformer to capture multi-scale features. A multi-scale feature enhancement module (MSFEM) is embedded in the CNN branch to enrich the feature representation. Moreover, a residual dynamic feature fusion (RDFF) module is introduced into skip connections to adaptively align and fuse multi-scale encoder features with high-level decoder semantics, thereby optimizing the feature transmission pathway and enhancing segmentation performance.</p><p>The core contributions of this study are as follows:<list list-type="order"><list-item><p>We propose a novel dual-branch encoder that integrates a ResNeXt-50-based CNN and a Transformer in parallel, effectively combining local detail extraction and global context modeling to generate rich multilevel feature representations.</p></list-item><list-item><p>We design the MSFEM that integrates various convolution operations, such as atrous and depthwise separable convolutions, to dynamically extract and aggregate multi-scale features, thereby addressing the limitations of inadequate multi-scale representation.</p></list-item><list-item><p>We introduce the RDFF module to alleviate semantic&#8211;detail conflicts in conventional skip connections by integrating residual connections with channel&#8211;spatial dual attention, enabling the adaptive fusion of deep semantic and shallow detail features.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05531"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05531"><title>2.1. Remote Sensing Image Semantic Segmentation</title><p>Remote sensing image semantic segmentation is a pixel-wise classification task that leverages deep learning to enable accurate and automated identification of land cover types, thereby supporting a wide range of remote sensing applications. Over the past decade, a series of deep learning-based architectures have been developed to enhance segmentation accuracy and efficiency. FCN pioneered the application of end-to-end deep learning in semantic segmentation, serving as a foundational milestone. Since then, various architectures have been developed to improve segmentation performance across diverse remote sensing scenarios. Among them, U-Net effectively captures multi-scale contextual information via a symmetric encoder&#8211;decoder structure and skip connections, enhancing boundary localization accuracy. SegNet innovatively introduces pooling indices to optimize upsampling and reduce computational burden. Despite these advancements, the limited receptive field of conventional CNN impedes the modeling of long-range dependencies. This limitation reduces segmentation accuracy in complex scenes with multi-scale objects. To address this, numerous multi-scale feature fusion strategies have been explored. For example, DeepLabv3+ employs ASPP to extract multi-scale context. Similarly, PSPNet aggregates global features using pyramid pooling. Nevertheless, these methods still rely on the localized perception of convolutional operations.</p><p>To overcome these localization constraints, attention mechanisms have been introduced to refine feature interactions via reweighting strategies. Wang et al. proposed ECANet [<xref rid="B36-sensors-25-05531" ref-type="bibr">36</xref>], which integrates channel-wise attention with an efficient computation strategy to further improve model performance. Similarly, CCNet [<xref rid="B37-sensors-25-05531" ref-type="bibr">37</xref>], developed by Huang et al., improves CNN&#8217;s ability to model spatial&#8211;channel relationships, facilitating better capture of contextual information across channels, scales, and orientations. Although attention mechanisms enable interaction between local and global features through reweighting, they remain limited by the fixed receptive fields of convolutional kernels. Therefore, explicit global dependency modeling is necessary. In this regard, Transformer models significantly improve segmentation consistency for large-scale features by dynamically constructing global context via inter-pixel relationships. Nonetheless, the quadratic computational complexity of self-attention with increasing image size restricts its use in real-time, high-resolution remote sensing applications.</p><p>Consequently, hybrid architectures synergizing CNN-driven local feature extraction and Transformer-based global context modeling have emerged. Representatively, TransUNet employs feature fusion to integrate both information types; however, deep Transformer stages may attenuate high-frequency spatial details. BANet [<xref rid="B38-sensors-25-05531" ref-type="bibr">38</xref>] addresses this by introducing a bidirectional attention mechanism, leveraging convolutional branches to preserve fine-grained details while using Transformer branches to capture long-range dependencies. In contrast, UNetFormer [<xref rid="B39-sensors-25-05531" ref-type="bibr">39</xref>] employs a CNN&#8211;Transformer hybrid architecture characterized by enhanced modularity, where CNN serves as the encoder and a Transformer functions as the decoder. CMTFNet [<xref rid="B40-sensors-25-05531" ref-type="bibr">40</xref>] further enhances multi-scale global representation by incorporating multi-scale self-attention in an encoder&#8211;decoder architecture. These trends motivate our design of a parallel dual-branch encoder, which integrates CNN and Transformer branches to independently extract and subsequently fuse multilevel features, thereby synergizing localized detail representation and global contextual modeling capabilities.</p></sec><sec id="sec2dot2-sensors-25-05531"><title>2.2. Multi-Scale Feature Extraction</title><p>The core challenge in semantic segmentation of RSI lies in the considerable variation of object scales across spatial dimensions, necessitating the development of feature extraction networks capable of robust multi-scale representation. Early studies primarily concentrated on improving the structural capabilities of convolutional operations for multi-scale feature learning. For instance, the ASPP module in DeepLabv3+ constructs a cross-scale context aggregation framework via multi-rate atrous convolutions. Building upon this, Shang et al. [<xref rid="B41-sensors-25-05531" ref-type="bibr">41</xref>] proposed the multi-scale context extraction module (MSCEM), which innovatively integrates atrous convolutions with varying dilation rates and global average pooling to achieve parallel multi-scale context feature extraction. With the advancement of feature fusion theory, the feature pyramid network (FPN) [<xref rid="B42-sensors-25-05531" ref-type="bibr">42</xref>] was introduced to tackle the multi-scale feature representation problem by adopting a top-down pathway with lateral connections, enabling the construction of semantically rich multilevel feature maps. Subsequent improvements, such as Recursive-FPN [<xref rid="B43-sensors-25-05531" ref-type="bibr">43</xref>] and BiFPN [<xref rid="B44-sensors-25-05531" ref-type="bibr">44</xref>], have further enhanced the flow and depth of multi-scale information by leveraging recursive stacking and efficient bidirectional cross-scale pathways, thus improving scale-aware feature characterization. Despite their effectiveness, these approaches rely heavily on fixed-size convolutional kernels or static attention windows, which limit their adaptability to dynamic and heterogeneous object scales in complex scenes.</p><p>To overcome this limitation, attention has shifted toward dynamic and adaptive mechanisms. Wang et al. [<xref rid="B45-sensors-25-05531" ref-type="bibr">45</xref>] introduced a multi-scale attention pyramid framework to adaptively enhance semantic regions using attention-driven feature selection. Similarly, Xiao et al. [<xref rid="B46-sensors-25-05531" ref-type="bibr">46</xref>] achieved multilevel feature extraction via dynamically adjustable receptive fields using a window-based attention mechanism. Fan et al. [<xref rid="B47-sensors-25-05531" ref-type="bibr">47</xref>] proposed the Multi-scale Vision Transformer (MViT), which hierarchically increases the channel capacity while progressively reducing the spatial resolution across stages. This design effectively integrates Transformer structures with multi-scale feature hierarchies. A seminal advancement in this domain was achieved by Ren et al. through their Shunted Transformer architecture, which incorporates a novel mixed-scale attention mechanism that enables concurrent processing of features at varying spatial resolutions. This innovative design substantially enhances the model&#8217;s capacity for capturing cross-scale contextual relationships. Building upon this foundation, Yu et al. [<xref rid="B48-sensors-25-05531" ref-type="bibr">48</xref>] developed MS-TCNet, integrating a shunted transformer encoder with a pyramid-structured decoder to hierarchically extract and refine multi-scale features. Zhou et al. [<xref rid="B49-sensors-25-05531" ref-type="bibr">49</xref>] subsequently advanced this paradigm by architecting a hybrid framework that synergistically combines the Shunted Transformer with a multi-scale convolutional attention network (MSCAN), specifically optimized for remote sensing image segmentation tasks. This design not only ensures segmentation accuracy but also reduces computational overhead. Building upon these advances, this study employs the Shunted Transformer as one of the encoder branches to leverage its unique mixed-scale self-attention (SSA) mechanism for efficient multi-scale feature extraction. In parallel, the MSFEM is embedded within the CNN branch. This module utilizes a content-adaptive dynamic aggregation strategy, enabling the network to flexibly recalibrate multi-scale feature representations based on input characteristics, ultimately enhancing segmentation performance in complex remote sensing scenarios.</p></sec><sec id="sec2dot3-sensors-25-05531"><title>2.3. Skip Connections</title><p>Skip connections were originally designed to address the challenge of vanishing gradients. For instance, ResNet introduced skip connections via cross-layer identity mapping but lacks sufficient flexibility for adaptive multi-scale feature enhancement. In contrast, DenseNet [<xref rid="B50-sensors-25-05531" ref-type="bibr">50</xref>] extended this concept by densely connecting each layer to all preceding ones, thereby promoting feature reuse and gradient flow. However, its fixed concatenation pattern lacks robustness against background clutter in remote sensing images. UNet employs symmetric skip connections that help retain spatial detail. However, its fixed weighting strategy limits adaptability to ambiguous boundaries and hinders effective interaction between semantic-level features.</p><p>To accommodate the multi-scale characteristics of RSI, skip connections have progressively evolved into a core mechanism for feature fusion. FPN introduced a top-down pyramid structure that effectively addresses the challenge of fusing shallow spatial details with deep semantic features. HRNet employs a parallel multi-resolution stream structure with densely connected skip links to maintain high-resolution representations, significantly improving the segmentation accuracy of small objects. However, its complex multi-branch design increases computational overhead. MCAT-UNet [<xref rid="B51-sensors-25-05531" ref-type="bibr">51</xref>] integrates cross-shaped window attention (CSWT) into skip connections, enabling self-attention computation within local windows to retain the long-range dependency modeling capability of Transformers while reducing computational complexity. UCTransNet [<xref rid="B52-sensors-25-05531" ref-type="bibr">52</xref>] redefines skip connections in U-Net by introducing a channel Transformer (CTrans) module, which adaptively bridges semantic gaps between encoder and decoder features, achieving precise image segmentation through enhanced global context modeling. However, its high computational cost hinders seamless integration into other network structures. The dynamic feature fusion (DFF) [<xref rid="B53-sensors-25-05531" ref-type="bibr">53</xref>] module adaptively weights and fuses local feature maps based on global information, demonstrating advantages in feature selection. However, it suffers from two limitations: excessive reliance on global statistics may result in the loss of local contextual information, and the lack of residual learning mechanisms constrains deep feature representation. To address these issues, we propose the RDFF module, which integrates residual connections and attention mechanisms. RDFF preserves the global modeling capability of DFF while enhancing spatial detail retention, thereby achieving effective fusion of deep semantic features from the encoder and shallow spatial features from the decoder.</p></sec></sec><sec id="sec3-sensors-25-05531"><title>3. Proposed Method</title><p>This study proposes a novel model, RST-Net, based on an encoder&#8211;decoder structure. The overall framework is illustrated in <xref rid="sensors-25-05531-f001" ref-type="fig">Figure 1</xref>. The encoder adopts a parallel dual-branch design, constructed based on ResNeXt-50 convolutional neural networks and the Shunted Transformer (ST), aiming to jointly extract local spatial details and global semantic information. Both branches adopt a four-stage progressive downsampling structure. In the final stage, the CNN branch incorporates the MSFEM, which enhances multi-scale representation by parallel processing with multi-scale convolutional kernels and feature fusion. The ST branch performs downsampling through linear embedding layers and captures multi-scale global context using the Shunted Self-Attention (SSA) mechanism. At the end of the encoder, features from both branches undergo deep interaction and fusion through deformable convolutions, producing a hierarchical representation that integrates local details with global semantics. The decoder takes the fused features as input and progressively restores spatial resolution via cascaded transposed convolutions. After each upsampling stage, an RDFF module adaptively integrates the corresponding features from the ST branch encoder with the current decoder features, effectively compensating for information loss and enhancing boundary accuracy. Finally, the refined decoder features, obtained through multi-scale feature fusion, are processed by a classifier to generate the final pixel-wise semantic segmentation map.</p><sec id="sec3dot1-sensors-25-05531"><title>3.1. Dual-Branch Encoder Structure</title><p>To concurrently capture localized spatial details and global semantic contexts in RSI, this paper introduces a dual-branch encoder structure featuring parallel CNN and Transformer pathways. These complementary branches specialize in extracting fine-grained textural features and holistic contextual representations, respectively. Each branch includes four progressive stages that generate multi-scale feature maps with resolutions of 1/4, 1/8, 1/16, and 1/32 of the input image.</p><p>The CNN branch employs ResNeXt-50 as the backbone due to its superior performance on ImageNet and its ability to increase feature diversity via enhanced cardinality. The input image is processed through four convolutional stages with downsampling to generate multi-scale feature maps <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8712;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The Transformer branch employs a hierarchical Shunted Transformer structure with flexible receptive fields to enhance global context modeling. Each stage consists of a feature embedding layer and the SSA module. In Stage 1, the Patch Embedding block is employed to downsample the input image to 1/4 of its original resolution and expand channels. The remaining three stages adopt an OverlapPatch Embedding block based on convolution to maintain spatial continuity and progressively increase channel dimensions.</p><p>The SSA module is implemented via the Shunted Transformer Block, as shown in <xref rid="sensors-25-05531-f002" ref-type="fig">Figure 2</xref>, which integrates a Multi-scale Token Aggregation (MTA) mechanism to capture both local and global dependencies. The attention computation is as follows:<disp-formula id="FD1-sensors-25-05531"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05531"><label>(2)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05531"><label>(3)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the query, key, and value tensors of the ith head, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> are linear projection weights, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the downsampling ratio, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the local enhancement module based on depthwise convolutions. Attention heads use varying downsampling rates <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to spatially compress <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, followed by depthwise separable convolutions for local enhancement. The outputs from all heads are concatenated and passed through a residual feed-forward network to obtain the output feature maps <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8712;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>At the cross-branch fusion stage, the output features <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> from the fourth stage are concatenated along the channel dimension. To suppress redundancy, a <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution compresses concatenated features from 1024 to 512 channels, while a <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> deformable convolution with adaptive spatial alignment dynamically corrects spatial misalignment between CNN and Transformer branches. Prioritizing high-response regions, this design synergistically enhances local textural details and global long-range dependencies to provide high-quality representations for the decoder.</p></sec><sec id="sec3dot2-sensors-25-05531"><title>3.2. Multi-Scale Feature Enhancement Module (MSFEM)</title><p>Traditional convolutional neural networks, constrained by their fixed receptive fields and static convolutional kernels, face challenges in effectively modeling multi-scale targets and complex spatial configurations. To address this issue, this study introduces the MSFEM. The module innovatively integrates heterogeneous convolutional architectures with learnable dynamic adjustment mechanisms, explicitly expanding the model&#8217;s receptive field scope while enabling adaptive feature re-weighting, fundamentally overcoming the static constraints of conventional convolutions. The structure of the MSFEM is illustrated in <xref rid="sensors-25-05531-f003" ref-type="fig">Figure 3</xref>.</p><p>Given an input feature map <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, a four-branch parallel structure is employed for multi-scale feature extraction. The 1 <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> 1 convolution at the first layer of each branch is mainly used for channel compression and computational optimization. To capture local spatial details, a standard 3 <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> 3 convolution combined with a <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution is utilized at branch 1 for feature extraction in the base space, thereby enhancing the characterization of local details:<disp-formula id="FD4-sensors-25-05531"><label>(4)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution. To capture large receptive field features and contextual information in both horizontal and vertical directions, complementary atrous convolution pyramids are constructed by cascading asymmetric convolutions and a <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> atrous convolution with a dilation rate of 5 in branches 2 and 3. This approach extracts directional features effectively:<disp-formula id="FD5-sensors-25-05531"><label>(5)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05531"><label>(6)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes a 3 <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> 3 atrous convolution with a dilation rate of 5. Branch 4 employs a depthwise separable convolution to enhance local feature sensitivity through channel-wise computations, allowing the network to dynamically focus on salient regions and perform spatially adaptive feature modulation:<disp-formula id="FD7-sensors-25-05531"><label>(7)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes a 3 <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> 3 depthwise separable convolution. The outputs from the four branches, including micro-level detail <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, horizontal and vertical contextual features <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as well as dynamic calibration features <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, are concatenated along the channel dimension to construct a rich multi-dimensional feature representation:<disp-formula id="FD8-sensors-25-05531"><label>(8)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes channel merging.</p><p>Finally, a <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution is applied to facilitate cross-channel information interaction. Combined with a learnable scaling factor, the module performs adaptive weighted feature fusion:<disp-formula id="FD9-sensors-25-05531"><label>(9)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>U</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents a learnable scaling factor that dynamically allocates the importance of each feature component. Here we set <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, which effectively balances the convergence speed and final accuracy required for the task.</p></sec><sec id="sec3dot3-sensors-25-05531"><title>3.3. Residual Dynamic Feature Fusion (RDFF)</title><p>Traditional skip connections often exhibit low feature fusion efficiency due to the inherent conflict between deep semantic features and shallow spatial details. The DFF module addresses this issue by adaptively learning feature importance and performing weighted fusion to improve segmentation accuracy. Inspired by this concept, we propose the RDFF module, which integrates encoder-derived semantic features with decoder-generated spatial details through a synergistic design of channel-spatial dual attention and residual learning.</p><p>The RDFF module is embedded within the skip connections, where it integrates the upsampled shallow decoder features from the decoder with the deep encoder features from the corresponding stage of the ST branch. These features are fused via the RDFF module. The structure of the RDFF module is illustrated in <xref rid="sensors-25-05531-f004" ref-type="fig">Figure 4</xref>.</p><p>Given the shallow feature map <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> from the decoder and the deep feature map <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> from the encoder, both are first passed through separate <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions to align their channel dimensions. The transformed features are then concatenated along the channel dimension.<disp-formula id="FD10-sensors-25-05531"><label>(10)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the concatenation operation along the channel dimension, and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution operation. Subsequently, global average pooling is used to extract global information, and a <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution is used to compute the channel attention weights of the combined feature map, which are then applied to the combined feature map.<disp-formula id="FD11-sensors-25-05531"><label>(11)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05531"><label>(12)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mo>&#8855;</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Sigmoid activation function, <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes adaptive average pooling, and <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8855;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes element-wise multiplication. To enhance feature representation capabilities and capture broader contextual information, a <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution is further applied to the weighted feature map for channel dimension reduction, followed by a <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution layer for spatial feature extraction.<disp-formula id="FD13-sensors-25-05531"><label>(13)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution operation. To adaptively adjust the spatial weights of features from different input layers, a spatial attention mechanism is introduced. Spatial attention weights are computed via <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions. The two attention weights are then summed and normalized to obtain the spatial attention weight.<disp-formula id="FD14-sensors-25-05531"><label>(14)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, a residual connection is employed to combine the spatially attended features with the original shallow feature map, ensuring feature integrity.<disp-formula id="FD15-sensors-25-05531"><label>(15)</label><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mo>&#8855;</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec4-sensors-25-05531"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05531"><title>4.1. Datasets</title><p>To holistically evaluate model efficacy, we conduct experimental validation on the ISPRS-released Vaihingen and Potsdam benchmark datasets. These publicly available datasets encompass six semantic classes: impervious surfaces, buildings, low vegetation, trees, cars, and background.</p><p>The Vaihingen dataset encompasses urban regions of Vaihingen, Germany, comprising 33 orthorectified aerial images at 9 cm spatial resolution with heterogeneous spatial coverage. The images exhibit heterogeneous spatial coverage, averaging 2494 &#215; 2064 pixels in size. Each image features near-infrared, red, and green spectral bands (IRRG), complemented by both digital surface models (DSMs) and normalized DSMs (nDSMs). This investigation designated 16 orthophotos (IDs 1&#8211;16) for training purposes, reserving the remaining 17 images (IDs 17&#8211;33) for testing procedures.</p><p>The Potsdam dataset spans the historic city center of Potsdam, Germany, and comprises 38 orthorectified images with a spatial resolution of 5 cm and consistent dimensions. All images have consistent dimensions of 6000 &#215; 6000 pixels. The dataset provides images in both true-color (RGB) and IRRG formats, along with corresponding DSM and nDSM data. Compared to Vaihingen, the Potsdam dataset presents a denser urban fabric, characterized by large building complexes, narrow roadways, and intricate residential areas. In this study, 24 RGB-band images were selected for training, and the remaining 14 images (IDs: 2_13&#8211;7_13) were used for testing.</p></sec><sec id="sec4dot2-sensors-25-05531"><title>4.2. Experimental Settings</title><p>Experimental execution leveraged the PyTorch 1.13 framework with model construction and training conducted on an NVIDIA RTX A4000 GPU. The Ranger optimizer, featuring a 0.001 weight decay coefficient, was implemented to bolster convergence stability and counteract overfitting. Training spanned 100 epochs using a batch size of 16 alongside an initial 0.01 learning rate adaptively regulated through cosine annealing. Input images were standardized by random cropping into 256 &#215; 256 pixel patches. Stochastic combinations of augmentation techniques containing random flipping, scaling, brightness and contrast adjustments enhanced model generalization.</p><p>To mitigate class imbalance in the Vaihingen and Potsdam datasets, the cross-entropy loss function is utilized, mathematically expressed as follows:<disp-formula id="FD16-sensors-25-05531"><label>(16)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of classes, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the true value, and <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the softmax probability of the i-th class.</p></sec><sec id="sec4dot3-sensors-25-05531"><title>4.3. Evaluation Metrics</title><p>To ensure a comprehensive and objective assessment, the present study employs Overall Accuracy (OA), Mean Intersection over Union (MIoU), and the F1 score as primary evaluation metrics for RSI semantic segmentation performance. These metrics evaluate model performance from multiple perspectives, encompassing overall classification accuracy, object boundary localization, and class-wise prediction balance. The mathematical formulations of these metrics are defined as follows:<disp-formula id="FD17-sensors-25-05531"><label>(17)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-sensors-25-05531"><label>(18)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD19-sensors-25-05531"><label>(19)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD20-sensors-25-05531"><label>(20)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD21-sensors-25-05531"><label>(21)</label><mml:math id="mm79" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the total number of target categories, and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> correspond to the number of true positive, false positive, true negative and false negative samples for class <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the F1 score for class i, and <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the precision and recall values for class i, respectively.</p></sec><sec id="sec4dot4-sensors-25-05531"><title>4.4. Ablation Study</title><p>To comprehensively evaluate the contributions of the dual-branch encoder structure, MSFEM, and RDFF to the model&#8217;s performance, systematic ablation experiments were conducted on the Vaihingen dataset. Specifically, six different model configurations were constructed as detailed in <xref rid="sensors-25-05531-t001" ref-type="table">Table 1</xref>. All experiments were performed under consistent hyperparameter and training settings, with analyses focusing on the following three aspects:</p><sec id="sec4dot4dot1-sensors-25-05531"><title>4.4.1. Effectiveness of the Dual-Branch Encoder Structure</title><p>To validate the effectiveness of the dual-branch encoder structure, it was compared against single-branch CNN and single-branch ST models. As shown in the first three groups of <xref rid="sensors-25-05531-t001" ref-type="table">Table 1</xref>, the dual-branch CNN+ST model significantly outperforms the single-branch counterparts, achieving OA, m-F1, and MIoU scores of 88.23%, 86.09%, and 76.06%, respectively. This corresponds to a 4.53% OA improvement over the single-branch CNN model, and MIoU and m-F1 gains of 2.66% and 1.73% over the single-branch ST model. As illustrated in <xref rid="sensors-25-05531-f005" ref-type="fig">Figure 5</xref>a, the CNN model exhibits severe fragmentation in elongated vegetation regions, primarily due to the loss of spatial resolution in deep layers. Conversely, the single-branch ST model captures global contextual dependencies but lacks sensitivity to fine-grained textures, leading to segmentation omissions around building boundaries. By integrating the local detail awareness of CNN with the global semantic modeling strength of Transformers, the proposed dual-branch structure achieves complementarity: it substantially mitigates vegetation fragmentation while more accurately delineating building boundaries. These results demonstrate that the dual-branch model effectively leverages the strengths of both CNN and Transformer, learning more discriminative feature representations that enhance semantic consistency and boundary delineation.</p></sec><sec id="sec4dot4dot2-sensors-25-05531"><title>4.4.2. Effectiveness of the MSFEM</title><p>To further assess the MSFEM, it was integrated into the dual-branch CNN+ST model alongside the RDFF module. Comparing the results between the third and fourth groups in <xref rid="sensors-25-05531-t001" ref-type="table">Table 1</xref>, incorporating MSFEM improved the model&#8217;s MIoU to 76.40%, an increase of 0.34% over the dual-branch baseline. Notably, segmentation accuracy for low vegetation and trees increased by 1.30% and 2.76%, reaching 72.15% and 71.17%, respectively, indicating the module&#8217;s effectiveness in enhancing multi-scale morphological representation. <xref rid="sensors-25-05531-f005" ref-type="fig">Figure 5</xref>b shows that without MSFEM, the model struggles to distinguish tree from low vegetation in dense vegetation scenes due to fixed-scale convolution kernels, leading to blurred and missed segmentation. MSFEM&#8217;s multi-scale parallel convolutions capture hierarchical details from large-scale buildings to small-scale trees, significantly improving segmentation precision for heterogeneous land covers and enabling clear differentiation between small trees and low vegetation.</p></sec><sec id="sec4dot4dot3-sensors-25-05531"><title>4.4.3. Effectiveness of the RDFF Module</title><p>To verify the role of the RDFF module, it was also integrated into the dual-branch CNN+ST model. As shown in the third and fifth groups of <xref rid="sensors-25-05531-t001" ref-type="table">Table 1</xref>, the addition of RDFF increased MIoU from 76.06% to 76.62%, and car IoU from 66.65% to 67.13%. Visualization in <xref rid="sensors-25-05531-f005" ref-type="fig">Figure 5</xref>d indicates severe adhesion issues in densely packed car areas when RDFF is absent. Incorporating RDFF effectively enhances local boundary features such as car contours through cross-level residual connections and channel-spatial dual attention mechanisms, significantly suppressing object adhesion. <xref rid="sensors-25-05531-f005" ref-type="fig">Figure 5</xref>c illustrates how RDFF leverages global average pooling to capture semantic consistency, combined with attention mechanisms to optimize local details, thereby alleviating boundary blurring in deep networks and reducing false positives in low vegetation areas.</p><p>In summary, the dual-branch encoder structure establishes the feature extraction foundation, the MSFEM module enhances multi-scale perception robustness, and the RDFF module resolves feature fusion conflicts through dynamic weighting. Their synergy enables the complete RST-Net model to achieve an MIoU of 77.04%, with Impervious Surface IoU exceeding 88.44%, and car IoU improving to 69.24%. Visualized segmentation results in <xref rid="sensors-25-05531-f005" ref-type="fig">Figure 5</xref> clearly demonstrate that the full RST-Net yields superior edge clarity and category discrimination compared to partial-module configurations. The complete RST-Net model outperforms those containing only some modules by producing clearer edge details, effectively recognizing multi-scale land covers, and substantially reducing misclassification of individual objects.</p></sec></sec><sec id="sec4dot5-sensors-25-05531"><title>4.5. Comparative Experiments</title><p>To systematically evaluate the performance of RST-Net, we compare it with several representative semantic segmentation approaches on the Vaihingen and Potsdam datasets, including classical CNN-based models (UNet [<xref rid="B12-sensors-25-05531" ref-type="bibr">12</xref>], SegNet [<xref rid="B13-sensors-25-05531" ref-type="bibr">13</xref>], DeepLabV3+ [<xref rid="B16-sensors-25-05531" ref-type="bibr">16</xref>], PSPNet [<xref rid="B14-sensors-25-05531" ref-type="bibr">14</xref>]) and recent Transformer-based structures (HST-Net [<xref rid="B49-sensors-25-05531" ref-type="bibr">49</xref>], UNetFormer [<xref rid="B39-sensors-25-05531" ref-type="bibr">39</xref>], and CMTFNet [<xref rid="B40-sensors-25-05531" ref-type="bibr">40</xref>]). For fair comparison, all approaches are trained using the same experimental settings, data preprocessing pipeline, and augmentation strategies as RST-Net, and evaluated on identical training and test splits.</p><sec id="sec4dot5dot1-sensors-25-05531"><title>4.5.1. Results on the Vaihingen Dataset</title><p>The quantitative results on the Vaihingen dataset are summarized in <xref rid="sensors-25-05531-t002" ref-type="table">Table 2</xref>. As shown in <xref rid="sensors-25-05531-t002" ref-type="table">Table 2</xref>, RST-Net consistently outperforms all baseline methods in overall performance. Specifically, RST-Net achieves an OA of 88.48%, an m-F1 of 86.77%, and a MIoU of 77.04%. Further analysis reveals that DeepLabV3+ underperforms in segmenting trees and low vegetation due to limited multi-scale representation. PSPNet, despite incorporating pyramid pooling for global context, still struggles with identifying small objects like cars. Although recent transformer-based models such as HST-Net, UNetFormer, and CMTFNet improve long-range dependency modeling and show gains across metrics, their overall performance remains inferior to RST-Net.</p><p><xref rid="sensors-25-05531-f006" ref-type="fig">Figure 6</xref> presents visual segmentation results for four representative scenes, further demonstrating the advantages of RST-Net. Experimental results indicate that the UNet model suffers from significant blurring at the boundaries of various land-cover classes and especially underperforms in segmenting small objects. For example, missed car detections are observed in the lower-right corner of <xref rid="sensors-25-05531-f006" ref-type="fig">Figure 6</xref>a and the upper region of <xref rid="sensors-25-05531-f006" ref-type="fig">Figure 6</xref>c. The SegNet model shows confusion in distinguishing between impervious surfaces and vegetation areas, which is particularly evident in <xref rid="sensors-25-05531-f006" ref-type="fig">Figure 6</xref>b. The DeeplabV3+ model, by introducing dilated convolutions to enlarge the receptive field, achieves significantly improved classification accuracy compared to UNet and SegNet. However, it still suffers from under-segmentation and over-segmentation issues, failing to effectively address the adhesion between adjacent car targets. The PSPNet model alleviates the adhesion problem through a multi-scale feature fusion strategy. Compared to classical segmentation approaches, recent semantic segmentation models demonstrate substantially improved capability in distinguishing between different land-cover categories and significantly reduce misclassification rates. Nevertheless, segmentation deviations persist in complex scenes, making it difficult to accurately reconstruct the true spatial distribution. The proposed RST-Net model exhibits strong performance across various land-cover segmentation tasks, with notable advantages in both detail preservation and overall segmentation accuracy, thereby validating its effectiveness in semantic understanding under complex remote sensing scenarios.</p></sec><sec id="sec4dot5dot2-sensors-25-05531"><title>4.5.2. Results on the Potsdam Dataset</title><p>The quantitative comparison results on the ISPRS Potsdam dataset are presented in <xref rid="sensors-25-05531-t003" ref-type="table">Table 3</xref>. The experimental results indicate that the proposed RST-Net achieves superior overall performance. Although the OA of RST-Net is slightly lower than that of CMTFNet by 0.51%, reaching 87.24%, it achieves the highest scores in both m-F1 and MIoU. Specifically, RST-Net achieves a MIoU of 79.56%, surpassing the second-best method, CMTFNet, by 1.53 percentage points. Its m-F1 reaches 88.51%, exceeding UNetFormer by 1.20 percentage points. A detailed category-wise analysis shows that RST-Net attains an IoU of 89.73% for buildings, outperforming UNet by 0.74 percentage points. For the car class, it achieves an IoU of 79.53%, surpassing SegNet by 3.19 points. Notably, for the tree class, RST-Net reaches 76.23% IoU, which is 8.31 points higher than DeeplabV3+.</p><p>The visual results on the Potsdam dataset, shown in <xref rid="sensors-25-05531-f007" ref-type="fig">Figure 7</xref>, further confirm RST-Net&#8217;s enhanced segmentation performance across various complex urban scenes. Compared to other models, RST-Net yields more precise segmentation results. Specifically, for distinguishing low vegetation from trees in <xref rid="sensors-25-05531-f007" ref-type="fig">Figure 7</xref>a,c, classical models like UNet exhibit notable misclassification due to spectral similarity, whereas recent methods such as HST-Net and UNetFormer improve classification accuracy but still face challenges in accurately delineating boundaries. By leveraging multi-scale feature fusion, RST-Net effectively distinguishes between low vegetation and trees, achieving higher segmentation accuracy than Deeplab V3+ and PSPNet. For the car segmentation task shown in <xref rid="sensors-25-05531-f007" ref-type="fig">Figure 7</xref>d, several comparative models suffer from misclassification caused by spectral mixing under tree canopies. RST-Net effectively addresses this issue through refined boundary modeling, reducing object adhesion and enhancing contour completeness. For building segmentation, RST-Net significantly preserves the geometric integrity of building boundaries, accurately reconstructing the spatial morphology and extent of real-world buildings. Overall, these results validate the effectiveness and superiority of RST-Net in multi-category semantic segmentation tasks for high-resolution RSI.</p></sec><sec id="sec4dot5dot3-sensors-25-05531"><title>4.5.3. Efficiency Analysis</title><p>Practical remote sensing deployments critically depend on model efficiency. <xref rid="sensors-25-05531-t004" ref-type="table">Table 4</xref> summarizes the number of parameters, computational complexity (FLOPs), speed (FPS), and segmentation performance (MIoU) of each model on the Potsdam dataset. Compared with the conventional model DeepLabV3+, which contains 122.01 MB of parameters, requires 52.21 GFLOPs of computation, and runs at 41.87 FPS, RST-Net achieves superior resource utilization and speed. Specifically, RST-Net reduces the parameter count by 53.2% to 57.17 MB and decreases computational complexity by 34.7% to 34.11 GFLOPs, while simultaneously achieving a significantly higher frame rate of 75.20 FPS, which is 79.6% faster than DeepLabV3+. Despite this reduction, it improves the MIoU by 4.83 percentage points, reaching 79.56%. Although the UNetFormer model presents a lightweight design with 11.7 MB of parameters and 11.14 GFLOPs of computation, achieving the highest FPS of 115.42, its MIoU score reaches only 77.69 percent, which is still 1.87 percentage points lower than that of RST-Net. The results indicate that RST-Net achieves a well-balanced optimization between segmentation accuracy and computational efficiency within a manageable range of parameters and operations, providing a practical solution for real-time analysis of high-resolution RSI.</p></sec></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05531"><title>5. Discussion</title><p>The proposed RST-Net demonstrates significant performance advantages in RSI, attributable to three core technical innovations. The dual-branch architecture leverages complementary CNN and Transformer designs to preserve local details while capturing global semantics. Concurrently, the MSFEM addresses the spectral variations within the same category challenge in RSI through parallel atrous convolutions and depthwise separable convolutions, enabling adaptive recognition of targets across scales. Furthermore, the RDFF mechanism optimizes hierarchical feature alignment via residual connections and substantially mitigates ground object adhesion through spatial attention-based weight redistribution.</p><p>However, this study exhibits limitations: the dataset exclusively covers German urban areas without validation in extreme environments. Additionally, the dual-branch encoder and attention mechanisms incur higher computational costs than lightweight CNN models, potentially compromising real-time performance on edge devices or during large-scale satellite image processing. Future work will integrate multispectral and SAR data to enhance classification robustness while developing dynamic network pruning strategies to accelerate inference without sacrificing accuracy.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-05531"><title>6. Conclusions</title><p>This paper proposes a novel semantic segmentation structure, RST-Net, to overcome the limitations of global-local feature fusion and limited adaptability to multi-scale object segmentation in RSI. The network adopts a dual-branch encoder structure, combining a ResNeXt-50-based CNN branch and a Shunted Transformer branch to effectively extract local details and capture global context. To enhance multi-scale representation, the proposed MSFEM dynamically aggregates features at different scales, significantly improving the model&#8217;s responsiveness to spatial variability. Furthermore, the RDFF module facilitates seamless fusion between semantic and detail features across network stages through a coordinated attention mechanism.</p><p>Extensive experiments conducted on the ISPRS Vaihingen and Potsdam datasets validate the effectiveness of RST-Net, which achieves MIoU scores of 77.04% and 79.56%, respectively. Compared to DeepLabV3+, a representative encoder&#8211;decoder model, RST-Net achieves reductions of 53.2% in model size and 34.7% in computational complexity, while maintaining competitive segmentation accuracy.</p><p>Subsequent research efforts will aim to incorporate lightweight techniques, including but not limited to model pruning and knowledge distillation. This integration seeks to achieve further model compression and enhanced computational efficiency, thereby addressing the computational constraints inherent in real-time processing of high-resolution RSI.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, N.Y.; methodology, N.Y.; software, N.Y.; validation, X.G. and C.T.; formal analysis, N.Y. and Y.Z.; investigation, N.Y.; resources, N.Y.; data curation, N.Y.; writing&#8212;original draft preparation, N.Y.; writing&#8212;review and editing, N.Y.; visualization, N.Y.; supervision, X.L.; project administration, F.Z.; funding acquisition, C.T. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. The source code and trained models for RST-Net will be made publicly available upon acceptance at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/white-yang1/RST-Net.git">https://github.com/white-yang1/RST-Net.git</uri> (accessed on 2 September 2025). </p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05531"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>M.</given-names></name></person-group><article-title>DDPM-SegFormer: Highly refined feature land use and land cover segmentation with a fused denoising diffusion probabilistic model and transformer</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2024</year><volume>133</volume><fpage>104093</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2024.104093</pub-id></element-citation></ref><ref id="B2-sensors-25-05531"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Du</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>LULC-SegNet: Enhancing Land Use and Land Cover Semantic Segmentation with Denoising Diffusion Feature Fusion</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>4573</elocation-id><pub-id pub-id-type="doi">10.3390/rs16234573</pub-id></element-citation></ref><ref id="B3-sensors-25-05531"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>A Multiscale Attention Segment Network-Based Semantic Segmentation Model for Landslide Remote Sensing Images</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>1712</elocation-id><pub-id pub-id-type="doi">10.3390/rs16101712</pub-id></element-citation></ref><ref id="B4-sensors-25-05531"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaushal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Sehgal</surname><given-names>V.K.</given-names></name></person-group><article-title>A semantic segmentation framework with UNet-pyramid for landslide prediction using remote sensing data</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>30071</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-79266-6</pub-id><pub-id pub-id-type="pmid">39627305</pub-id><pub-id pub-id-type="pmcid">PMC11614895</pub-id></element-citation></ref><ref id="B5-sensors-25-05531"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jia</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Sang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Semantic segmentation of deep learning remote sensing images based on band combination principle: Application in urban planning and land use</article-title><source>Comput. Commun.</source><year>2024</year><volume>217</volume><fpage>97</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.comcom.2024.01.032</pub-id></element-citation></ref><ref id="B6-sensors-25-05531"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shengoku</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shibasaki</surname><given-names>R.J.I.</given-names></name></person-group><article-title>Semantic Segmentation for Urban Planning Maps based on U-Net</article-title><source>Proceedings of the IGARSS 2018&#8212;2018 IEEE International Geoscience and Remote Sensing Symposium</source><conf-loc>Valencia, Spain</conf-loc><conf-date>22&#8211;27 July 2018</conf-date></element-citation></ref><ref id="B7-sensors-25-05531"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>Z.</given-names></name></person-group><article-title>Multiple Kernel-Based SVM Classification of Hyperspectral Images by Combining Spectral, Spatial, and Semantic Information</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><elocation-id>120</elocation-id><pub-id pub-id-type="doi">10.3390/rs12010120</pub-id></element-citation></ref><ref id="B8-sensors-25-05531"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friedl</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Brodley</surname><given-names>C.E.</given-names></name></person-group><article-title>Decision tree classification of land cover from remotely sensed data</article-title><source>Remote Sens. Environ.</source><year>1997</year><volume>61</volume><fpage>399</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1016/S0034-4257(97)00049-7</pub-id></element-citation></ref><ref id="B9-sensors-25-05531"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shelhamer</surname><given-names>E.</given-names></name><name name-style="western"><surname>Long</surname><given-names>J.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully Convolutional Networks for Semantic Segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="B10-sensors-25-05531"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>F.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>K.</given-names></name></person-group><article-title>Symmetrical Dense-Shortcut Deep Fully Convolutional Networks for Semantic Segmentation of Very-High-Resolution Remote Sensing Images</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2018</year><volume>11</volume><fpage>1633</fpage><lpage>1644</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2018.2810320</pub-id></element-citation></ref><ref id="B11-sensors-25-05531"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>SDFCNv2: An Improved FCN Framework for Remote Sensing Images Semantic Segmentation</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>4902</elocation-id><pub-id pub-id-type="doi">10.3390/rs13234902</pub-id></element-citation></ref><ref id="B12-sensors-25-05531"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>Proceedings of the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention&#8212;MICCAI 2015</source><conf-loc>Munich, Germany</conf-loc><conf-date>5&#8211;9 October 2015</conf-date></element-citation></ref><ref id="B13-sensors-25-05531"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kendall</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cipolla</surname><given-names>R.</given-names></name></person-group><article-title>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>39</volume><fpage>2481</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type="pmid">28060704</pub-id></element-citation></ref><ref id="B14-sensors-25-05531"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Pyramid Scene Parsing Network</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>6230</fpage><lpage>6239</lpage></element-citation></ref><ref id="B15-sensors-25-05531"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Schroff</surname><given-names>F.</given-names></name><name name-style="western"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Munich, Germany</conf-loc><conf-date>14 September 2018</conf-date></element-citation></ref><ref id="B16-sensors-25-05531"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Schroff</surname><given-names>F.</given-names></name><name name-style="western"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>Rethinking Atrous Convolution for Semantic Image Segmentation</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1706.05587</pub-id><pub-id pub-id-type="arxiv">1706.05587v3</pub-id></element-citation></ref><ref id="B17-sensors-25-05531"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kokkinos</surname><given-names>I.</given-names></name><name name-style="western"><surname>Murphy</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yuille</surname><given-names>A.L.</given-names></name></person-group><article-title>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2018</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><pub-id pub-id-type="pmid">28463186</pub-id></element-citation></ref><ref id="B18-sensors-25-05531"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>K.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Deep High-Resolution Representation Learning for Human Pose Estimation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>16&#8211;20 June 2019</conf-date><fpage>5686</fpage><lpage>5696</lpage></element-citation></ref><ref id="B19-sensors-25-05531"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>G.</given-names></name><name name-style="western"><surname>Milan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Reid</surname><given-names>I.</given-names></name></person-group><article-title>RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date></element-citation></ref><ref id="B20-sensors-25-05531"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Tu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><article-title>Aggregated Residual Transformations for Deep Neural Networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date></element-citation></ref><ref id="B21-sensors-25-05531"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#8211;1 July 2016</conf-date></element-citation></ref><ref id="B22-sensors-25-05531"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-Excitation Networks</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date></element-citation></ref><ref id="B23-sensors-25-05531"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Oktay</surname><given-names>O.</given-names></name><name name-style="western"><surname>Schlemper</surname><given-names>J.</given-names></name><name name-style="western"><surname>Folgoc</surname><given-names>L.L.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>M.</given-names></name><name name-style="western"><surname>Heinrich</surname><given-names>M.</given-names></name><name name-style="western"><surname>Misawa</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mori</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mcdonagh</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hammerla</surname><given-names>N.Y.</given-names></name><name name-style="western"><surname>Kainz</surname><given-names>B.</given-names></name></person-group><article-title>Attention U-Net: Learning Where to Look for the Pancreas</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1804.03999</pub-id><pub-id pub-id-type="arxiv">1804.03999</pub-id></element-citation></ref><ref id="B24-sensors-25-05531"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Woo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>CBAM: Convolutional Block Attention Module</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1807.06521</pub-id><pub-id pub-id-type="arxiv">1807.06521</pub-id></element-citation></ref><ref id="B25-sensors-25-05531"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name></person-group><article-title>Dual Attention Network for Scene Segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date></element-citation></ref><ref id="B26-sensors-25-05531"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.M.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>L.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is All you Need</article-title><source>Proceedings of the Neural Information Processing Systems</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#8211;9 December 2017</conf-date></element-citation></ref><ref id="B27-sensors-25-05531"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>S.-Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.-D.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Y.-D.</given-names></name></person-group><article-title>A regularized transformer with adaptive token fusion for Alzheimer&#8217;s disease diagnosis in brain magnetic resonance images</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>155</volume><fpage>111058</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2025.111058</pub-id></element-citation></ref><ref id="B28-sensors-25-05531"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>S.-Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.-D.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>Y.-D.</given-names></name></person-group><article-title>Tuberculosis and pneumonia diagnosis in chest X-rays by large adaptive filter and aligning normalized network with report-guided multi-level alignment</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>158</volume><fpage>111575</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2025.111575</pub-id></element-citation></ref><ref id="B29-sensors-25-05531"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>S.-Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>CTBViT: A novel ViT for tuberculosis classification with efficient block and randomized classifier</article-title><source>Biomed. Signal Process. Control</source><year>2025</year><volume>100</volume><elocation-id>106981</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2024.106981</pub-id></element-citation></ref><ref id="B30-sensors-25-05531"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style="western"><surname>Houlsby</surname><given-names>N.</given-names></name></person-group><article-title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2010.11929</pub-id></element-citation></ref><ref id="B31-sensors-25-05531"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>B.</given-names></name></person-group><article-title>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date></element-citation></ref><ref id="B32-sensors-25-05531"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>He</surname><given-names>S.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Shunted Self-Attention via Multi-Scale Token Aggregation</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date></element-citation></ref><ref id="B33-sensors-25-05531"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group><article-title>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2102.04306</pub-id><pub-id pub-id-type="arxiv">2102.04306</pub-id></element-citation></ref><ref id="B34-sensors-25-05531"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name></person-group><article-title>Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation</article-title><source>Proceedings of the European Conference on Computer Vision 2022</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;24 October 2022</conf-date></element-citation></ref><ref id="B35-sensors-25-05531"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>C.</given-names></name></person-group><source>RSFormer: Medical Image Segmentation Based on Dual Model Channel Merging</source><publisher-name>SPIE</publisher-name><publisher-loc>Cergy, France</publisher-loc><year>2024</year><volume>Volume 13250</volume></element-citation></ref><ref id="B36-sensors-25-05531"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Q.</given-names></name></person-group><article-title>ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date></element-citation></ref><ref id="B37-sensors-25-05531"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name></person-group><article-title>CCNet: Criss-Cross Attention for Semantic Segmentation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>603</fpage><lpage>612</lpage></element-citation></ref><ref id="B38-sensors-25-05531"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Qiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Latecki</surname><given-names>L.J.</given-names></name></person-group><article-title>BANet: Boundary-Assistant Encoder-Decoder Network for Semantic Segmentation</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>25259</fpage><lpage>25270</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3194213</pub-id></element-citation></ref><ref id="B39-sensors-25-05531"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Atkinson</surname><given-names>P.M.</given-names></name></person-group><article-title>UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2022</year><volume>190</volume><fpage>196</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2022.06.008</pub-id></element-citation></ref><ref id="B40-sensors-25-05531"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>X.</given-names></name></person-group><article-title>CMTFNet: CNN and Multiscale Transformer Fusion Network for Remote-Sensing Image Semantic Segmentation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2023.3314641</pub-id></element-citation></ref><ref id="B41-sensors-25-05531"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Stolkin</surname><given-names>R.</given-names></name></person-group><article-title>Multi-scale Adaptive Feature Fusion Network for Semantic Segmentation in Remote Sensing Images</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><elocation-id>872</elocation-id><pub-id pub-id-type="doi">10.3390/rs12050872</pub-id></element-citation></ref><ref id="B42-sensors-25-05531"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Dollar</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name></person-group><article-title>Feature Pyramid Networks for Object Detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date></element-citation></ref><ref id="B43-sensors-25-05531"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.V.</given-names></name></person-group><article-title>EfficientDet: Scalable and Efficient Object Detection</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date></element-citation></ref><ref id="B44-sensors-25-05531"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qiao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Yuille</surname><given-names>A.</given-names></name></person-group><article-title>DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2020</conf-date></element-citation></ref><ref id="B45-sensors-25-05531"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>Y.</given-names></name></person-group><article-title>Adaptive Multi-scale Dual Attention Network for Semantic Segmentation</article-title><source>Neurocomputing</source><year>2021</year><volume>460</volume><fpage>39</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2021.06.068</pub-id></element-citation></ref><ref id="B46-sensors-25-05531"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name></person-group><article-title>Enhancing Multiscale Representations With Transformer for Remote Sensing Image Semantic Segmentation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2023.3256064</pub-id></element-citation></ref><ref id="B47-sensors-25-05531"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>B.</given-names></name><name name-style="western"><surname>Mangalam</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Malik</surname><given-names>J.</given-names></name><name name-style="western"><surname>Feichtenhofer</surname><given-names>C.</given-names></name></person-group><article-title>Multiscale Vision Transformers</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>6804</fpage><lpage>6815</lpage></element-citation></ref><ref id="B48-sensors-25-05531"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>B.</given-names></name><name name-style="western"><surname>Miao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name></person-group><article-title>MS-TCNet: An effective Transformer&#8211;CNN combined network using multi-scale feature learning for 3D medical image segmentation</article-title><source>Comput. Biol. Med.</source><year>2024</year><volume>170</volume><elocation-id>108057</elocation-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.108057</pub-id><pub-id pub-id-type="pmid">38301516</pub-id></element-citation></ref><ref id="B49-sensors-25-05531"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>P.</given-names></name></person-group><article-title>Hybrid Shunted Transformer embedding UNet for remote sensing image semantic segmentation</article-title><source>Neural Comput. Appl.</source><year>2024</year><volume>36</volume><fpage>15705</fpage><lpage>15720</lpage><pub-id pub-id-type="doi">10.1007/s00521-024-09888-4</pub-id></element-citation></ref><ref id="B50-sensors-25-05531"><label>50.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Maaten</surname><given-names>L.V.D.</given-names></name><name name-style="western"><surname>Weinberger</surname><given-names>K.Q.</given-names></name></person-group><article-title>Densely Connected Convolutional Networks</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>2261</fpage><lpage>2269</lpage></element-citation></ref><ref id="B51-sensors-25-05531"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>E.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>MCAT-UNet: Convolutional and Cross-Shaped Window Attention Enhanced UNet for Efficient High-Resolution Remote Sensing Image Segmentation</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2024</year><volume>17</volume><fpage>9745</fpage><lpage>9758</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2024.3397488</pub-id></element-citation></ref><ref id="B52-sensors-25-05531"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zaiane</surname><given-names>O.R.</given-names></name></person-group><article-title>UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer</article-title><source>Proc. Conf. AAAI Artif. Intell.</source><year>2021</year><volume>36</volume><fpage>2442</fpage><lpage>2449</lpage><pub-id pub-id-type="doi">10.1609/aaai.v36i3.20144</pub-id></element-citation></ref><ref id="B53-sensors-25-05531"><label>53.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name></person-group><article-title>Dynamic Feature Fusion for Semantic Edge Detection</article-title><source>Proceedings of the International Joint Conference on Artificial Intelligence</source><conf-loc>Macao, China</conf-loc><conf-date>10&#8211;16 August 2019</conf-date></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05531-f001" orientation="portrait"><label>Figure 1</label><caption><p>Overall structure of RST-Net, which consists of a dual-branch encoder structure composed of a CNN branch and an ST branch, a decoder, as well as the MSFEM and the RDFF module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05531-g001.jpg"/></fig><fig position="float" id="sensors-25-05531-f002" orientation="portrait"><label>Figure 2</label><caption><p>Structure of the Shunted Transformer Block, which is composed of MSA, MLP, and LN.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05531-g002.jpg"/></fig><fig position="float" id="sensors-25-05531-f003" orientation="portrait"><label>Figure 3</label><caption><p>Structure of the MSFEM.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05531-g003.jpg"/></fig><fig position="float" id="sensors-25-05531-f004" orientation="portrait"><label>Figure 4</label><caption><p>Structure of the RDFF module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05531-g004.jpg"/></fig><fig position="float" id="sensors-25-05531-f005" orientation="portrait"><label>Figure 5</label><caption><p>Visual results of ablation experiments on the Vaihingen dataset across four representative scenes. (<bold>a</bold>) Building and impervious surface integration. (<bold>b</bold>) Precise building boundary delineation with small car identification. (<bold>c</bold>) Hierarchical vegetation structure differentiation. (<bold>d</bold>) High-density car spatial partitioning. <bold>CNN</bold>: single-branch ResNeXt-50 model; <bold>ST</bold>: single-branch Shunted Transformer branch; <bold>CNN+ST</bold>: dual-branch model; <bold>CNN+ST+MSFEM</bold>: dual-branch with MSFEM; <bold>CNN+ST+RDFF</bold>: dual-branch with RDFF; <bold>CNN+ST+MSFEM+RDFF</bold>: full RST-Net with both modules. Red boxes highlight critical variation areas.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05531-g005.jpg"/></fig><fig position="float" id="sensors-25-05531-f006" orientation="portrait"><label>Figure 6</label><caption><p>Visual results of comparative experiments on the Vaihingen dataset across four representative scenes. (<bold>a</bold>) Vegetation-tree-impervious surface integration. (<bold>b</bold>) Fine vegetation differentiation under shadow interference. (<bold>c</bold>) Spatial distribution analysis of high-density cars. (<bold>d</bold>) Structural separation of trees and low vegetation. Red boxes highlight critical variation areas.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05531-g006.jpg"/></fig><fig position="float" id="sensors-25-05531-f007" orientation="portrait"><label>Figure 7</label><caption><p>Visual results of comparative experiments on the Potsdam dataset across four representative scenes. (<bold>a</bold>) Inter-class differentiation of cars and low vegetation. (<bold>b</bold>) Precise boundary delineation between buildings and low vegetation. (<bold>c</bold>) Fine-grained edge refinement within low vegetation. (<bold>d</bold>) Car recognition under tree occlusion. Red boxes highlight critical variation areas.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05531-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05531-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05531-t001_Table 1</object-id><label>Table 1</label><caption><p>Ablation study results on the Vaihingen dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="5" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">IoU (%)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Evaluation Index</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Impervious Surface</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Building</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low Vegetation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tree</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OA (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m-F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MioU (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">82.20</td><td align="center" valign="middle" rowspan="1" colspan="1">84.48</td><td align="center" valign="middle" rowspan="1" colspan="1">70.31</td><td align="center" valign="middle" rowspan="1" colspan="1">69.24</td><td align="center" valign="middle" rowspan="1" colspan="1">58.77</td><td align="center" valign="middle" rowspan="1" colspan="1">83.70</td><td align="center" valign="middle" rowspan="1" colspan="1">84.05</td><td align="center" valign="middle" rowspan="1" colspan="1">73.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ST</td><td align="center" valign="middle" rowspan="1" colspan="1">81.83</td><td align="center" valign="middle" rowspan="1" colspan="1">84.22</td><td align="center" valign="middle" rowspan="1" colspan="1">70.60</td><td align="center" valign="middle" rowspan="1" colspan="1">70.27</td><td align="center" valign="middle" rowspan="1" colspan="1">60.08</td><td align="center" valign="middle" rowspan="1" colspan="1">86.39</td><td align="center" valign="middle" rowspan="1" colspan="1">84.36</td><td align="center" valign="middle" rowspan="1" colspan="1">73.40</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN+ST</td><td align="center" valign="middle" rowspan="1" colspan="1">84.55</td><td align="center" valign="middle" rowspan="1" colspan="1">89.85</td><td align="center" valign="middle" rowspan="1" colspan="1">70.85</td><td align="center" valign="middle" rowspan="1" colspan="1">68.41</td><td align="center" valign="middle" rowspan="1" colspan="1">66.65</td><td align="center" valign="middle" rowspan="1" colspan="1">88.23</td><td align="center" valign="middle" rowspan="1" colspan="1">86.09</td><td align="center" valign="middle" rowspan="1" colspan="1">76.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN+ST+MSFEM</td><td align="center" valign="middle" rowspan="1" colspan="1">84.19</td><td align="center" valign="middle" rowspan="1" colspan="1">89.98</td><td align="center" valign="middle" rowspan="1" colspan="1">72.15</td><td align="center" valign="middle" rowspan="1" colspan="1">71.17</td><td align="center" valign="middle" rowspan="1" colspan="1">64.51</td><td align="center" valign="middle" rowspan="1" colspan="1">88.47</td><td align="center" valign="middle" rowspan="1" colspan="1">86.31</td><td align="center" valign="middle" rowspan="1" colspan="1">76.40</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN+ST+RDFF</td><td align="center" valign="middle" rowspan="1" colspan="1">84.37</td><td align="center" valign="middle" rowspan="1" colspan="1">89.85</td><td align="center" valign="middle" rowspan="1" colspan="1">71.65</td><td align="center" valign="middle" rowspan="1" colspan="1">70.13</td><td align="center" valign="middle" rowspan="1" colspan="1">67.13</td><td align="center" valign="middle" rowspan="1" colspan="1">88.40</td><td align="center" valign="middle" rowspan="1" colspan="1">86.49</td><td align="center" valign="middle" rowspan="1" colspan="1">76.62</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN+ST+MSFEM+RDFF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.04</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05531-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05531-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of semantic segmentation results on the Vaihingen dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="5" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">IoU (%)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Evaluation Index</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Impervious Surface</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Building</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low Vegetation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tree</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OA (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m-F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MioU (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">UNet [<xref rid="B12-sensors-25-05531" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">82.20</td><td align="center" valign="middle" rowspan="1" colspan="1">84.48</td><td align="center" valign="middle" rowspan="1" colspan="1">70.31</td><td align="center" valign="middle" rowspan="1" colspan="1">69.24</td><td align="center" valign="middle" rowspan="1" colspan="1">58.77</td><td align="center" valign="middle" rowspan="1" colspan="1">83.70</td><td align="center" valign="middle" rowspan="1" colspan="1">84.05</td><td align="center" valign="middle" rowspan="1" colspan="1">73.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegNet [<xref rid="B13-sensors-25-05531" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">81.83</td><td align="center" valign="middle" rowspan="1" colspan="1">87.33</td><td align="center" valign="middle" rowspan="1" colspan="1">69.20</td><td align="center" valign="middle" rowspan="1" colspan="1">67.35</td><td align="center" valign="middle" rowspan="1" colspan="1">59.21</td><td align="center" valign="middle" rowspan="1" colspan="1">86.80</td><td align="center" valign="middle" rowspan="1" colspan="1">83.98</td><td align="center" valign="middle" rowspan="1" colspan="1">72.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeeplabV3+ [<xref rid="B16-sensors-25-05531" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">81.61</td><td align="center" valign="middle" rowspan="1" colspan="1">87.48</td><td align="center" valign="middle" rowspan="1" colspan="1">68.54</td><td align="center" valign="middle" rowspan="1" colspan="1">67.81</td><td align="center" valign="middle" rowspan="1" colspan="1">58.35</td><td align="center" valign="middle" rowspan="1" colspan="1">86.65</td><td align="center" valign="middle" rowspan="1" colspan="1">83.81</td><td align="center" valign="middle" rowspan="1" colspan="1">72.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PSPNet [<xref rid="B14-sensors-25-05531" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">82.32</td><td align="center" valign="middle" rowspan="1" colspan="1">86.88</td><td align="center" valign="middle" rowspan="1" colspan="1">69.65</td><td align="center" valign="middle" rowspan="1" colspan="1">69.39</td><td align="center" valign="middle" rowspan="1" colspan="1">61.34</td><td align="center" valign="middle" rowspan="1" colspan="1">87.08</td><td align="center" valign="middle" rowspan="1" colspan="1">84.68</td><td align="center" valign="middle" rowspan="1" colspan="1">73.92</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HST-Net [<xref rid="B49-sensors-25-05531" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">82.33</td><td align="center" valign="middle" rowspan="1" colspan="1">87.94</td><td align="center" valign="middle" rowspan="1" colspan="1">70.01</td><td align="center" valign="middle" rowspan="1" colspan="1">68.26</td><td align="center" valign="middle" rowspan="1" colspan="1">62.38</td><td align="center" valign="middle" rowspan="1" colspan="1">87.27</td><td align="center" valign="middle" rowspan="1" colspan="1">84.84</td><td align="center" valign="middle" rowspan="1" colspan="1">74.18</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UNetFormer [<xref rid="B39-sensors-25-05531" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">83.04</td><td align="center" valign="middle" rowspan="1" colspan="1">88.24</td><td align="center" valign="middle" rowspan="1" colspan="1">69.98</td><td align="center" valign="middle" rowspan="1" colspan="1">69.11</td><td align="center" valign="middle" rowspan="1" colspan="1">64.59</td><td align="center" valign="middle" rowspan="1" colspan="1">84.53</td><td align="center" valign="middle" rowspan="1" colspan="1">85.41</td><td align="center" valign="middle" rowspan="1" colspan="1">74.99</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CMTFNet [<xref rid="B40-sensors-25-05531" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">84.17</td><td align="center" valign="middle" rowspan="1" colspan="1">89.80</td><td align="center" valign="middle" rowspan="1" colspan="1">70.50</td><td align="center" valign="middle" rowspan="1" colspan="1">68.83</td><td align="center" valign="middle" rowspan="1" colspan="1">62.34</td><td align="center" valign="middle" rowspan="1" colspan="1">88.04</td><td align="center" valign="middle" rowspan="1" colspan="1">85.41</td><td align="center" valign="middle" rowspan="1" colspan="1">75.13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RST-Net (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.04</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05531-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05531-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of semantic segmentation results on the Potsdam dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="5" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">IoU (%)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Evaluation Index</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Impervious Surface</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Building</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low Vegetation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tree</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OA (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m-F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MioU (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">UNet [<xref rid="B12-sensors-25-05531" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">76.22</td><td align="center" valign="middle" rowspan="1" colspan="1">88.99</td><td align="center" valign="middle" rowspan="1" colspan="1">70.11</td><td align="center" valign="middle" rowspan="1" colspan="1">71.93</td><td align="center" valign="middle" rowspan="1" colspan="1">68.50</td><td align="center" valign="middle" rowspan="1" colspan="1">85.21</td><td align="center" valign="middle" rowspan="1" colspan="1">85.62</td><td align="center" valign="middle" rowspan="1" colspan="1">75.15</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegNet [<xref rid="B13-sensors-25-05531" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">75.90</td><td align="center" valign="middle" rowspan="1" colspan="1">85.89</td><td align="center" valign="middle" rowspan="1" colspan="1">69.45</td><td align="center" valign="middle" rowspan="1" colspan="1">71.37</td><td align="center" valign="middle" rowspan="1" colspan="1">76.34</td><td align="center" valign="middle" rowspan="1" colspan="1">84.68</td><td align="center" valign="middle" rowspan="1" colspan="1">86.11</td><td align="center" valign="middle" rowspan="1" colspan="1">75.79</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeeplabV3+ [<xref rid="B16-sensors-25-05531" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">75.38</td><td align="center" valign="middle" rowspan="1" colspan="1">86.59</td><td align="center" valign="middle" rowspan="1" colspan="1">68.96</td><td align="center" valign="middle" rowspan="1" colspan="1">67.92</td><td align="center" valign="middle" rowspan="1" colspan="1">74.79</td><td align="center" valign="middle" rowspan="1" colspan="1">84.05</td><td align="center" valign="middle" rowspan="1" colspan="1">85.38</td><td align="center" valign="middle" rowspan="1" colspan="1">74.73</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PSPNet [<xref rid="B14-sensors-25-05531" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">76.41</td><td align="center" valign="middle" rowspan="1" colspan="1">86.46</td><td align="center" valign="middle" rowspan="1" colspan="1">70.37</td><td align="center" valign="middle" rowspan="1" colspan="1">71.30</td><td align="center" valign="middle" rowspan="1" colspan="1">74.83</td><td align="center" valign="middle" rowspan="1" colspan="1">84.79</td><td align="center" valign="middle" rowspan="1" colspan="1">86.16</td><td align="center" valign="middle" rowspan="1" colspan="1">75.86</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HST-Net [<xref rid="B49-sensors-25-05531" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">76.84</td><td align="center" valign="middle" rowspan="1" colspan="1">87.28</td><td align="center" valign="middle" rowspan="1" colspan="1">70.99</td><td align="center" valign="middle" rowspan="1" colspan="1">71.50</td><td align="center" valign="middle" rowspan="1" colspan="1">76.61</td><td align="center" valign="middle" rowspan="1" colspan="1">85.28</td><td align="center" valign="middle" rowspan="1" colspan="1">86.66</td><td align="center" valign="middle" rowspan="1" colspan="1">76.64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UNetFormer [<xref rid="B39-sensors-25-05531" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">77.94</td><td align="center" valign="middle" rowspan="1" colspan="1">88.65</td><td align="center" valign="middle" rowspan="1" colspan="1">71.09</td><td align="center" valign="middle" rowspan="1" colspan="1">72.28</td><td align="center" valign="middle" rowspan="1" colspan="1">78.00</td><td align="center" valign="middle" rowspan="1" colspan="1">85.89</td><td align="center" valign="middle" rowspan="1" colspan="1">87.31</td><td align="center" valign="middle" rowspan="1" colspan="1">77.69</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CMTFNet [<xref rid="B40-sensors-25-05531" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">77.05</td><td align="center" valign="middle" rowspan="1" colspan="1">87.54</td><td align="center" valign="middle" rowspan="1" colspan="1">72.18</td><td align="center" valign="middle" rowspan="1" colspan="1">74.79</td><td align="center" valign="middle" rowspan="1" colspan="1">78.57</td><td align="center" valign="middle" rowspan="1" colspan="1">87.75</td><td align="center" valign="middle" rowspan="1" colspan="1">87.56</td><td align="center" valign="middle" rowspan="1" colspan="1">78.03</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RST-Net (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.56</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05531-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05531-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of Params, FLOPs, FPS, and MIoU on the Potsdam dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters (MB)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MioU (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">UNet [<xref rid="B12-sensors-25-05531" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">23.89</td><td align="center" valign="middle" rowspan="1" colspan="1">18.85</td><td align="center" valign="middle" rowspan="1" colspan="1">75.15</td><td align="center" valign="middle" rowspan="1" colspan="1">87.35</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegNet [<xref rid="B13-sensors-25-05531" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">80.63</td><td align="center" valign="middle" rowspan="1" colspan="1">28.08</td><td align="center" valign="middle" rowspan="1" colspan="1">75.79</td><td align="center" valign="middle" rowspan="1" colspan="1">60.12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeeplabV3+ [<xref rid="B16-sensors-25-05531" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>122.01</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">52.21</td><td align="center" valign="middle" rowspan="1" colspan="1">74.73</td><td align="center" valign="middle" rowspan="1" colspan="1">41.87</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PSPNet [<xref rid="B14-sensors-25-05531" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">92.45</td><td align="center" valign="middle" rowspan="1" colspan="1">44.54</td><td align="center" valign="middle" rowspan="1" colspan="1">75.86</td><td align="center" valign="middle" rowspan="1" colspan="1">46.05</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HST-Net [<xref rid="B49-sensors-25-05531" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">28.03</td><td align="center" valign="middle" rowspan="1" colspan="1">22.83</td><td align="center" valign="middle" rowspan="1" colspan="1">76.64</td><td align="center" valign="middle" rowspan="1" colspan="1">90.28</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UNetFormer [<xref rid="B39-sensors-25-05531" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">11.7</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>11.14</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">77.69</td><td align="center" valign="middle" rowspan="1" colspan="1">115.42</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CMTFNet [<xref rid="B40-sensors-25-05531" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">28.67</td><td align="center" valign="middle" rowspan="1" colspan="1">17.14</td><td align="center" valign="middle" rowspan="1" colspan="1">78.03</td><td align="center" valign="middle" rowspan="1" colspan="1">100.15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RST-Net (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>79.56</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.20</bold>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>