<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430882</article-id><article-id pub-id-type="pmcid-ver">PMC12430882.1</article-id><article-id pub-id-type="pmcaid">12430882</article-id><article-id pub-id-type="pmcaiid">12430882</article-id><article-id pub-id-type="doi">10.3390/s25175443</article-id><article-id pub-id-type="publisher-id">sensors-25-05443</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Physics-Driven Computational Multispectral Imaging for Accurate Color Measurement</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Yi</surname><given-names initials="H">Haoyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05443" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhou</surname><given-names initials="M">Mingwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af2-sensors-25-05443" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Xie</surname><given-names initials="H">Hao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05443" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Chen</surname><given-names initials="B">Bingshan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-sensors-25-05443" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="Y">Yaqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05443" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="F">Fei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af3-sensors-25-05443" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Shen</surname><given-names initials="J">Jiefei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af3-sensors-25-05443" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8759-7377</contrib-id><name name-style="western"><surname>Shen</surname><given-names initials="J">Junfei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05443" ref-type="aff">1</xref><xref rid="c1-sensors-25-05443" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Zhang</surname><given-names initials="Y">Yun</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05443"><label>1</label>College of Electronics and Information Engineering, Sichuan University, Chengdu 610065, China; <email>yhyyyds315@gmail.com</email> (H.Y.); <email>hxie@stu.scu.edu.cn</email> (H.X.); <email>j65822304@gmail.com</email> (B.C.); <email>yaqiw.ece@utexas.edu</email> (Y.W.)</aff><aff id="af2-sensors-25-05443"><label>2</label>Mindray Bio-Medical Electronics Co., Ltd., Shenzhen 518132, China; <email>13096300597@163.com</email></aff><aff id="af3-sensors-25-05443"><label>3</label>Department of Prosthodontics, West China Hospital of Stomatology, Sichuan University, Chengdu 610041, China; <email>liufei.hxkq@scu.edu.cn</email> (F.L.); <email>shenjiefei@scu.edu.cn</email> (J.S.)</aff><author-notes><corresp id="c1-sensors-25-05443"><label>*</label>Correspondence: <email>shenjunfei@scu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5443</elocation-id><history><date date-type="received"><day>13</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>14</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>28</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05443.pdf"/><abstract><p>Accurate color measurement is crucial for ensuring reliable sensing performance in vision-based applications. However, existing color measurement methods suffer from illumination variability, operational complexity, and perceptual subjectivity. In this study, dental color measurement, with its strict perceptual and spectral fidelity demands, is adopted to validate the proposed method. Using self-made resin-permeated ceramic teeth, this study proposes a deep-learned end-to-end spectral reflectance prediction framework to achieve snapshot teeth spectral reflectance from RGB images under complex light sources in the fundamental spectral domain through the construction of a physically interpretable network that enables physically informed feature fusion. A dual-attention modular-information fusion neural network is developed to recover the spectral reflectance directly from the RGB image for natural teeth and ceramics across multiple scenarios. A dataset containing 4000 RGB&#8211;hyperspectral image pairs is built from a self-designed optical system with complex illumination conditions. Results confirm that the proposed framework demonstrates effective performance in predicting teeth spectral reflectance with an MSE of 0.0024 and an SSIM of 0.8724. This method achieves high-accuracy color measurement while avoiding the color mismatch caused by metamerism, which empowers various advanced applications including optical property characterization, 3D surface reconstruction, and computer-aided restorative design.</p></abstract><kwd-group><kwd>color measurement</kwd><kwd>spectral reflectance</kwd><kwd>deep learning</kwd><kwd>physically informed network</kwd><kwd>hyperspectral imaging</kwd></kwd-group><funding-group><award-group><funding-source>National Key Research and Development Program of China</funding-source><award-id>2022YFC2410102</award-id></award-group><award-group><funding-source>Sichuan Science and Technology Program</funding-source><award-id>2025YFHZ0333</award-id></award-group><award-group><funding-source>Research and Develop Program, West China Hospital, Sichuan University</funding-source><award-id>RD-03-202408</award-id></award-group><award-group><funding-source>Young Elite Scientists Sponsorship Program by China Association for Science and Technology (CAST)</funding-source><award-id>2022QNRC001</award-id></award-group><award-group><funding-source>Sichuan &#8220;Tianfu Emei Plan&#8221; talent project</funding-source><award-id>A0103602</award-id></award-group><funding-statement>This work was supported by the National Key Research and Development Program of China (2022YFC2410102); Sichuan Science and Technology Program (2025YFHZ0333); Research and Develop Program, West China Hospital, Sichuan University (RD-03-202408); Young Elite Scientists Sponsorship Program by China Association for Science and Technology (CAST) (2022QNRC001); and Sichuan &#8220;Tianfu Emei Plan&#8221; talent project (A0103602).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05443"><title>1. Introduction</title><p>As of 2022, approximately 3.5 billion people worldwide suffer from oral diseases, with untreated dental caries affecting around 2.5 billion individuals, highlighting the critical need for dental restoration [<xref rid="B1-sensors-25-05443" ref-type="bibr">1</xref>]. The color of teeth serves as a significant determinant of individuals&#8217; satisfaction with their dental and facial aesthetics [<xref rid="B2-sensors-25-05443" ref-type="bibr">2</xref>], which plays a critical role in shaping social perceptions of personal attributes [<xref rid="B3-sensors-25-05443" ref-type="bibr">3</xref>]. The color and appearance of teeth constitute a complex phenomenon influenced by multiple factors, including lighting conditions, translucency, opacity, light scattering, surface gloss, and human visual perception [<xref rid="B4-sensors-25-05443" ref-type="bibr">4</xref>]. Therefore, achieving accurate and efficient dental color measurement represents a critical step in the process of dental restoration. Considering differences in the dimensionality and completeness of color data acquisition, most can be divided into two types: RGB analysis and spectral analysis.</p><p><bold>RGB analysis</bold>. RGB analysis refers to the process of acquiring and interpreting color information based on the red, green, and blue components of an image. In dental color measurement, it involves assessing tooth color either visually by the human eye or through RGB color data captured by instruments. From an instrumentation perspective, RGB analysis methods can be broadly categorized into four types: visual assessment, colorimeters, digital cameras, and intraoral scanners (IOS).</p><list list-type="simple"><list-item><label>(1)</label><p>Visual analysis. This method is regarded as the gold standard in clinical application, relying on the subjective judgment of the dentist or technician, who compares the color of the patient&#8217;s natural teeth to a shade guide (a set of standardized color samples) to select the closest match. However, due to the subjectivity of human visual analysis and the variations in lighting conditions, visual determination of shade selection has been found to be subjective and inconsistent [<xref rid="B5-sensors-25-05443" ref-type="bibr">5</xref>]. Historically, visual shade assessment has been inherently constrained by several factors, including metamerism, the observer&#8217;s age, visual fatigue, and the influence of mood or medication. For instance, color vision deteriorates with age [<xref rid="B6-sensors-25-05443" ref-type="bibr">6</xref>], compromising the consistency of shade-based diagnosis. Metamerism is the matching of apparent color of objects with lights that have different spectral power distributions [<xref rid="B7-sensors-25-05443" ref-type="bibr">7</xref>], which indicates the same color under one lighting condition may appear different under other conditions. Therefore, the ability to communicate the degree and nature of these differences is lacking [<xref rid="B8-sensors-25-05443" ref-type="bibr">8</xref>], highlighting the pressing need for more objective, reproducible, and technology-assisted approaches in clinical shade selection.</p></list-item><list-item><label>(2)</label><p>Colorimeters. Colorimeters measure color tristimulus values from light reflectance of a specimen after the light source pass through a series of filters [<xref rid="B9-sensors-25-05443" ref-type="bibr">9</xref>]. When conducting color measurement, the intensity of electromagnetic radiation within the visible spectrum wavelengths of an object or solution after it has been transmitted or reflected will be measured. It can offer potential objective and quantitative assessment of tooth color, independent of the examiner&#8217;s experience and environmental conditions [<xref rid="B10-sensors-25-05443" ref-type="bibr">10</xref>]. Repeatability may be compromised due to filter aging, and metamerism of the object can pose a challenge to measurement accuracy [<xref rid="B11-sensors-25-05443" ref-type="bibr">11</xref>]. Moreover, if the angle or position of measurement is incorrect or deviates, it can lead to inconsistent results, especially for teeth that have irregular shapes or complex surface morphology. Such variations can introduce errors in color measurement.</p></list-item><list-item><label>(3)</label><p>Digital cameras. The usage of digital cameras has been widely adopted in the fields of dentistry [<xref rid="B12-sensors-25-05443" ref-type="bibr">12</xref>]. Shade matching using the digital images can minimize the gap of color communication between dentists and technicians [<xref rid="B13-sensors-25-05443" ref-type="bibr">13</xref>]. The process of digital photographic color-matching emphasizes the importance of environmental lighting control, color calibration, and software analysis. However, color consistency can be influenced by various factors during image acquisition, including ambient lighting, tooth hydration or dehydration, camera settings, shade-taking protocols, use of cross-polarization, image file formats, color balance adjustments via shade analysis software, and calibration of laboratory monitors [<xref rid="B14-sensors-25-05443" ref-type="bibr">14</xref>]. For example, unreliable illumination and unfixed distances confound the photographic outcome. In other words, variations in dental color measurement can result from differences in the light source&#8217;s brightness, hue, and proximity to the teeth.</p></list-item><list-item><label>(4)</label><p>Intraoral scanners (IOSs). An IOS (intraoral scanner) is a medical device consisting of a handheld camera (hardware), a computer, and software [<xref rid="B15-sensors-25-05443" ref-type="bibr">15</xref>]. Although intraoral scanners have been initially used for digital impression, a tool for dental shade measurement has been added to some scanners [<xref rid="B14-sensors-25-05443" ref-type="bibr">14</xref>]. For intraoral scanners to function effectively in shade selection, they must meet two critical criteria: precise color imaging and robust data processing. More precisely, the accuracy of tooth color measurement depends primarily on two critical variables: the spectral characteristics of the illumination source and the chromatic analysis algorithms employed by the software&#8212;both of which reflect the fundamental factors governing imaging accuracy in digital cameras. Although these devices can record tooth shade and 3D topographic data, inherent factors may introduce variability, thereby limiting their color measurement accuracy.</p></list-item></list><p>Traditional colorimetric methodologies, which depend on either human visual assessment or tristimulus value quantification, are susceptible to variations in lighting conditions and observational environments, and are inherently incapable of circumventing the challenges posed by metamerism. Traditional RGB cameras and colorimeters capture limited color information, whereas spectral reflectance measurements offer a precise and objective method for assessing the optical properties of dental structures across various wavelengths by quantifying the ratio of reflected light to incident illumination. This technique effectively minimizes the influence of external variables such as light-source intensity, spectral composition, and environmental conditions.</p><p><bold>Spectral analysis</bold>. Spectral analysis is a high-precision technique for color and material characterization, capable of capturing detailed reflectance information across multiple wavelengths. In dental color measurement, spectral analysis is grounded in the optical properties of teeth and involves measuring their spectral reflectance across the visible spectrum (typically 380&#8211;780 nm) to accurately characterize tooth color. The primary instrument employed for spectral analysis is the spectrophotometer.</p><p>A spectrophotometer is an analytical instrument that measures the spectral reflectance or transmittance curve of a specimen, or reflected by a material as a function of wavelength [<xref rid="B16-sensors-25-05443" ref-type="bibr">16</xref>]. Spectrophotometers measure the amount of light energy reflected from an object at 1&#8211;25 nm intervals along the visible spectrum [<xref rid="B17-sensors-25-05443" ref-type="bibr">17</xref>]. It contains a source of optical radiation, a means for dispersing light, an optical system for measuring and detecting, and a means for converting light obtained to a signal that can be analyzed [<xref rid="B18-sensors-25-05443" ref-type="bibr">18</xref>]. In modern shade assessment, spectrophotometers are widely used as objective instruments. They typically employ either a tungsten-filament bulb or an LED lamp as the white-light source, delivering continuous illumination across the visible spectrum (400&#8211;700 nm) to ensure accurate color measurement. This polychromatic light is diffracted by a prism into discrete wavelength bands (10&#8211;20 nm bandwidth), which then interact with the sample through reflection, transmission, or scattering phenomena. Compared with observations by the human eye, or conventional techniques, it was found that spectrophotometers offered a 33% increase in accuracy and a more objective match in 93.3% of cases [<xref rid="B8-sensors-25-05443" ref-type="bibr">8</xref>]. While spectrophotometers provide high precision and objectivity in dental color measurement, their high cost, operational complexity, sensitivity to measurement conditions, and poor portability limit their widespread use in clinical practice.</p><p>As previously demonstrated, existing methods encounter one or more of the following challenges: time-consuming procedures, metamerism, subjective measurements, high costs, and operational complexity. To address the limitations outlined above, this investigation proposes a deep-learned, end-to-end spectral reflectance prediction framework (SRNet) for reconstructing the spectral reflectance of teeth from RGB images under complex illumination conditions. A custom optical imaging system was independently developed, and a comprehensive training dataset comprising 4000 paired samples was generated, each containing synchronously captured RGB images and their corresponding hyperspectral image cubes. The spectral characteristics of both the illumination source and dental substrates were integrated through channel-wise fusion, and a physically interpretable neural network was proposed to predict the spectral reflectance of tooth structures. During training, different scene spectra features supplied by different components are jointly learned, and the mapping relationship between RGB image and hyperspectral image can be automatically built. Prior to analysis, two reference images must be acquired: (1) the target object under standardized illumination conditions, and (2) the light source reflected from a calibrated white reference board. These paired inputs enable the neural network to accurately compute the object&#8217;s spectral reflectance. Additionally, a customized loss function, referred to as the structure&#8211;pixel loss, together with attention mechanisms, is employed to enhance the performance of the network. Experiments validate the proposed approach on real captured data, and shows precise recovery of spectral reflectance.</p><p>Our paper provides the following key contributions:<list list-type="simple"><list-item><label>(1)</label><p>For accurate spectral reflectance prediction and color measurement, an end-to-end, physically interpretable multi-module deep learning framework (SRNet) is developed, which reconstructs spectral reflectance from RGB images under complex lighting conditions, enabling color measurement and diffuse spectrum matching between natural teeth and ceramics across multiple scenes, advancing spectral imaging network design and optimization.</p></list-item><list-item><label>(2)</label><p>A dataset containing 4000 RGB&#8211;hyperspectral image pairs was built and tailored to the experiment setup consisting of a programmable illumination box featuring LED arrays with adjustable intensity and chromaticity and a dual-mode optical system incorporating a beam splitter to simultaneously capture hyperspectral image cubes and RGB images.</p></list-item></list></p></sec><sec sec-type="methods" id="sec2-sensors-25-05443"><title>2. Methodology</title><p>The proposed approach aims to reconstruct spectral reflectance from its corresponding RGB representation, formulating hyperspectral image (HSI) recovery from spectrally sparse measurements as a spectral enhancement task. This problem is inherently ill-posed due to the substantial loss of spectral details during image acquisition, where the observed radiance results from the product of the object&#8217;s spectral reflectance and the illumination&#8217;s spectral power distribution. In the absence of explicit illumination data, the reconstruction must implicitly estimate both factors, which increases ambiguity. A light-source image, obtained by capturing a spectrally flat reference (e.g., a diffuse white target) under identical illumination, provides a direct measurement of the illumination spectrum. Incorporating this information constrains the inverse problem and mitigates spectral estimation errors, leveraging inter-band correlations in hyperspectral datasets to expand the three-channel input into a higher-dimensional spectral representation, as supported by prior findings on the critical role of illumination in reflectance reconstruction accuracy [<xref rid="B19-sensors-25-05443" ref-type="bibr">19</xref>].</p><p>To address this challenging problem, a physics-informed, multi-model deep-learning framework &#8220;SRNet&#8221; is proposed. A beam splitter was utilized to simultaneously capture co-registered RGB and hyperspectral images of both dental specimens and illumination sources, resulting in a curated dataset comprising 4000 paired samples. The sample RGB image and the light-source RGB image are separately input into SRNet-P and SRNet-L, where they are reconstructed into their corresponding hyperspectral images. After concatenation along the channel dimension, the two hyperspectral images are further processed by the SRNet-L network under physical constraints to reconstruct the final spectral reflectance. Through the incorporation of an attention mechanism, the framework adaptively integrates multi-modal information to form a specialized spectral learning architecture, thereby facilitating precise reconstruction of spectral reflectance. A subset of the training samples is presented in <xref rid="app1-sensors-25-05443" ref-type="app">Appendix A</xref>.</p><sec id="sec2dot1-sensors-25-05443"><title>2.1. Spectral Mapping and Reconstruction Principles</title><p>An RGB image is generated by projecting a hyperspectral image (HSI) along the spectral dimension using spe<sub>c</sub>ific spectral response functions corresponding to the RGB channels. <italic toggle="yes">S</italic>(<italic toggle="yes">x, y, &#955;</italic>) represents the spectral reflectance at spatial coordinates (<italic toggle="yes">x, y</italic>) and wavelength <italic toggle="yes">&#955;</italic>, and <italic toggle="yes">C<sub>k</sub></italic>(<italic toggle="yes">&#955;</italic>) denotes the camera response curve. When imaged in the human eye, <italic toggle="yes">C<sub>k</sub></italic>(<italic toggle="yes">&#955;</italic>) becomes the visual response curve of the ocular eye. The relationship between the RGB image and <italic toggle="yes">I</italic>, <italic toggle="yes">S</italic>, <italic toggle="yes">C<sub>k</sub></italic>(<italic toggle="yes">&#955;</italic>) can be formulated as:<disp-formula id="FD1-sensors-25-05443"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8747;</mml:mo></mml:mstyle><mml:mrow><mml:mn>380</mml:mn><mml:mi>nm</mml:mi></mml:mrow><mml:mrow><mml:mn>780</mml:mn><mml:mi>nm</mml:mi></mml:mrow></mml:msubsup><mml:mi>I</mml:mi><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced><mml:mi>S</mml:mi><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced><mml:mi>d</mml:mi><mml:mi>&#955;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">k</italic> &#8712; {<italic toggle="yes">R, G, B</italic>} is defined as the spectral channel index, and <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>) refers to the spectral power distribution of the light source.</p><p>When the light source illuminates a standard diffuse whiteboard, its spectral reflectance is equal to 1, since the whiteboard is considered an ideal diffuse reflector. The relationship can be expressed as:<disp-formula id="FD2-sensors-25-05443"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8747;</mml:mo></mml:mstyle><mml:mrow><mml:mn>380</mml:mn><mml:mi>nm</mml:mi></mml:mrow><mml:mrow><mml:mn>780</mml:mn><mml:mi>nm</mml:mi></mml:mrow></mml:msubsup><mml:mi>I</mml:mi><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced><mml:mi>d</mml:mi><mml:mi>&#955;</mml:mi><mml:mfenced><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>At the same time, Equations (1) and (2) are transformed into their discrete vector&#8211;matrix representation, given by:<disp-formula id="FD3-sensors-25-05443"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext>&#160;</mml:mtext><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mi>&#955;</mml:mi></mml:munder><mml:mi>I</mml:mi><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced><mml:mi>S</mml:mi><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05443"><label>(4)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#160;</mml:mo><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mi>&#955;</mml:mi></mml:munder><mml:mi>I</mml:mi><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The training dataset is generated based on Equations (3) and (4), with further details provided in <xref rid="sec3-sensors-25-05443" ref-type="sec">Section 3</xref>.</p><p>The acquisition of <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>)<italic toggle="yes">S</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">&#955;</italic>) and <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>) can be discretized as follows by employing SRNet:<disp-formula id="FD5-sensors-25-05443"><label>(5)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext>&#160;</mml:mtext><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mover><mml:mo>&#8594;</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mover><mml:mi>I</mml:mi><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced><mml:mi>S</mml:mi><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05443"><label>(6)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mtext>&#160;</mml:mtext><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mover><mml:mo>&#8594;</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mover><mml:mi>I</mml:mi><mml:mfenced><mml:mi>&#955;</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05443"><label>(7)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8855;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mover><mml:mo>&#8594;</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mover><mml:mi>S</mml:mi><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equation (7), the symbol &#8216;&#8855;&#8217; denotes concatenation of the two inputs along the channel dimension.</p><p>According to Equations (3) and (4), the projection from HSI to RGB contains two main steps: (1) spectral response encoding and (2) integration summation, in which the 3D hyperspectral cube is compressed into a 2D image. Therefore, the HSI can be completely rebuilt from each channel of the RGB image by solving a compressive-imaging inverse problem.</p><p>The reconstruction process is formulated to reconstruct the original data from its observed projections: <italic toggle="yes">G</italic><sub>1</sub>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) &#8594; <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>)<italic toggle="yes">S</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">&#955;</italic>), <italic toggle="yes">G</italic><sub>2</sub>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) &#8594; <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>). Through <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>)<italic toggle="yes">S</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">&#955;</italic>) and <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>), we can obtain the final spectral reflectance. To manage this problem, the proposed SRNet framework integrates two dedicated modules: the SRNet-P block and the SRNet-L block. The schematic representation of the method is presented in <xref rid="sensors-25-05443-f001" ref-type="fig">Figure 1</xref>. The network receives a paired RGB image and light-source image as input and outputs the reconstructed spectral reflectance corresponding to the scene.</p><p><bold>Step 1</bold>: <italic toggle="yes">G</italic><sub>1</sub>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) &#8594; <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>)<italic toggle="yes">S</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">&#955;</italic>)</p><p>According to Equation (5), SRNet-P is tasked with reconstructing the hyperspectral image from the RGB image of the tooth sample. SRNet-P adopts a U-Net-like network architecture. Rather than performing upsampling via simple interpolation, the U-Net structure progressively upsamples the RGB image to the target spectral resolution and maps these features into a specific representation, enabling the generation of <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>)<italic toggle="yes">S</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">&#955;</italic>) based on the unique spectral characteristics of the scene. To further mitigate the impact of noise and improve reconstruction accuracy, attention modules are incorporated into the U-Net-like structure within SRNet-P, allowing the network to extract salient spectral features and integrate them with highly correlated contextual information.</p><p><bold>Step 2</bold>: <italic toggle="yes">G</italic><sub>2</sub>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) &#8594; <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>)</p><p>As indicated by Equation (6), our goal remains to reconstruct hyperspectral images from RGB images. In SRNet-L, convolutional layers are used in place of fixed upsampling operations to better extract image information. Unlike fixed operations such as max pooling and interpolation, convolutional layers can adaptively learn spatial and semantic relationships through training, thereby preserving more contextual and structural information. This substitution not only enables more flexible and fine-grained feature extraction and reconstruction, but also facilitates the integration of attention mechanisms and physical priors, ultimately improving the overall performance of the network.</p><p><bold>Step 3</bold>: <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>)<italic toggle="yes">S</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">&#955;</italic>) &#8855; <italic toggle="yes">I</italic>(<italic toggle="yes">&#955;</italic>) &#8594; <italic toggle="yes">S</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">&#955;</italic>)</p><p>Prior to Equation (7), SRNet-P reconstructs the hyperspectral image from the tooth sample&#8217;s RGB image, while SRNet-L reconstructs the hyperspectral image from the light-source image. These two hyperspectral outputs are concatenated along the channel dimension and then fed back into SRNet-L to extract spectral information for decoding, ultimately yielding the final spectral reflectance.</p><p>The &#8221;&#8855;&#8221; in inversion step 3 indicates that the hyperspectral data of the light source and the hyperspectral images of the teeth are concatenated along the channel dimension. During backpropagation, the network parameters are optimized by minimizing the loss between the reconstructed hyperspectral image (HSI) and the ground-truth HSI. The detailed internal architectures of each submodule are presented in <xref rid="sec2dot2-sensors-25-05443" ref-type="sec">Section 2.2</xref>.</p></sec><sec id="sec2dot2-sensors-25-05443"><title>2.2. SRNet</title><sec id="sec2dot2dot1-sensors-25-05443"><title>2.2.1. SRNet-P</title><p>U-Net [<xref rid="B20-sensors-25-05443" ref-type="bibr">20</xref>] is a widely adopted architecture in image segmentation and recognition tasks, characterized by its encoder&#8211;decoder structure with symmetric downsampling and upsampling paths. A distinctive feature of U-Net is its skip connections, which facilitate the transfer of fine-grained, low-level features directly to the upsampling layers, thereby mitigating model degradation and preserving spatial details. The downsampling convolutions enable extraction of high-dimensional representations, which are subsequently decoded and classified through upsampling operations. The architecture of SRNet-P, as proposed in our SRNet framework and illustrated in <xref rid="sensors-25-05443-f002" ref-type="fig">Figure 2</xref>, adopts a similar design. It accepts an RGB image as input and reconstructs a hyperspectral image with spatial resolution <italic toggle="yes">H &#215; W</italic> and spectral dimension <italic toggle="yes">C</italic>.</p><p>SRNet-P takes an input feature map <italic toggle="yes">X<sub>in</sub></italic> &#8712; <italic toggle="yes">R<sup>B&#215;C&#215;H&#215;W</sup></italic> and first projects it into a latent feature space via a 3 &#215; 3 convolution:<disp-formula id="FD8-sensors-25-05443"><label>(8)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The encoded features then pass through multiple hierarchical stages, each comprising several local&#8211;global fusion (LGF) extraction modules followed by a downsampling convolution, progressively capturing spatial and spectral representations.<disp-formula id="FD9-sensors-25-05443"><label>(9)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>G</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>&#8230;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">K</italic> in <italic toggle="yes">LGF<sub>k</sub></italic>(&#183;) indicates k refers to the number of consecutively applied LGF layers and <italic toggle="yes">N</italic> denotes the number of downsampling and upsampling in the stage.</p><p>LGF modules in feature extraction further refines the deepest features before the decoder reconstructs the spatial resolution through transposed convolutions:<disp-formula id="FD10-sensors-25-05443"><label>(10)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>G</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>At each decoding stage, features are fused with corresponding encoder outputs enhanced by a dual-stat attention mechanism, allowing the network to emphasize relevant spectral information.<disp-formula id="FD11-sensors-25-05443"><label>(11)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05443"><label>(12)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05443"><label>(13)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>G</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">Concat</italic>(<italic toggle="yes">A</italic>, <italic toggle="yes">B</italic>) denotes concatenation along the channel dimension.</p><p>Finally, a convolutional layer maps the features back to the original dimension, and a residual connection adds the input to the output, yielding the reconstructed output:<disp-formula id="FD14-sensors-25-05443"><label>(14)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mn>0</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This architecture effectively integrates multi-scale feature extraction, attention, and residual learning to improve hyperspectral reconstruction accuracy.</p><p>The LGF (local&#8211;global fusion) module and DSA (dual-stat attention) module are described in <xref rid="app2-sensors-25-05443" ref-type="app">Appendix B</xref>.</p></sec><sec id="sec2dot2dot2-sensors-25-05443"><title>2.2.2. SRNet-L</title><p>SRNet-L is designed to reconstruct the light-source spectra from an RGB image and to decode the fused spectral information. The structure introduced in our SRNet is demonstrated in <xref rid="sensors-25-05443-f003" ref-type="fig">Figure 3</xref>. It takes the RGB image as input and outputs the raw hyperspectral image with <italic toggle="yes">H &#215; W</italic> spatial resolution and <italic toggle="yes">C</italic> spectral bands. It processes the input RGB image of the light source to recover the underlying light-source spectral information, enabling the decoding of the fusion spectra. Spatial attention (SA) and channel attention (CA) mechanisms are applied separately to model long-range semantic dependencies across spatial locations and spectral bands, respectively. The L-block employs a residual learning strategy through a convolutional shortcut connection, facilitating element-wise addition between the attention-weighted features and the linearly transformed input features. This architectural design not only preserves intrinsic spectral characteristics throughout the network depth but also mitigates gradient vanishing, thereby enhancing training stability while maintaining spectral information integrity during hyperspectral reconstruction. This architecture effectively extracts and refines shallow features through attention mechanisms and recursive L-blocks, facilitating the generation of light-source hyperspectral imagery with high spatial and spectral fidelity.</p><p>The input to the network is a feature tensor <italic toggle="yes">X<sub>in</sub></italic> &#8712; <italic toggle="yes">R<sup>B&#215;</sup><sup>C&#215;</sup><sup>H&#215;</sup><sup>W</sup>,</italic> where <italic toggle="yes">B</italic>, <italic toggle="yes">C</italic>, <italic toggle="yes">H</italic>, and <italic toggle="yes">W</italic> represent batch size, number of channels, height, and width, respectively. Initially, shallow feature extraction is performed through two parallel convolutional paths, each consisting of a 3 &#215; 3 convolution followed by ReLU activation, channel attention (CA), and spatial attention (SA) modules, producing feature maps <italic toggle="yes">F</italic><sub>1</sub> and <italic toggle="yes">F</italic><sub>2</sub>:<disp-formula id="FD15-sensors-25-05443"><label>(15)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ReLu</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-05443"><label>(16)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">ReLu</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These initial features are then further refined by applying 1 &#215; 1 convolutions, ReLU, and the same attention mechanisms, yielding <italic toggle="yes">F</italic><sub>3</sub> and <italic toggle="yes">F</italic><sub>4</sub>:<disp-formula id="FD17-sensors-25-05443"><label>(17)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">ReLu</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-sensors-25-05443"><label>(18)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">ReLu</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Subsequently, these four feature maps are concatenated along the channel dimension to form a fused feature representation:<disp-formula id="FD19-sensors-25-05443"><label>(19)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To enhance the fused features, a sequence of N L-blocks&#8212;each comprising multiple convolutional layers, attention modules, and residual connections&#8212;is recursively applied:<disp-formula id="FD20-sensors-25-05443"><label>(20)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The output after the final L-block, <italic toggle="yes">F<sub>out</sub> = F<sub>N</sub>,</italic> is then projected via a 1 &#215; 1 convolution to the desired output <italic toggle="yes">L<sub>out</sub></italic>:<disp-formula id="FD21-sensors-25-05443"><label>(21)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec2dot3-sensors-25-05443"><title>2.3. Loss Function</title><p>To ensure the stability of the training process, a tailored loss function is introduced that accounts for the spatial locations. The disparity between the ground-truth and the reconstructed hyperspectral image (HSI) is evaluated using different loss components. The structure&#8211;pixel loss function, <italic toggle="yes">L<sub>overall</sub></italic>, can be described as follows:<disp-formula id="FD22-sensors-25-05443"><label>(22)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#945;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">L<sub>MSE</sub></italic> denotes the mean squared error (MSE) used to quantify the pixel-wise difference, and is defined as follows:<disp-formula id="FD23-sensors-25-05443"><label>(23)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">M</italic> denotes the total number of pixels in the hyperspectral image (HSI), while <italic toggle="yes">H*</italic> and <italic toggle="yes">H</italic> correspond to the pixel values of the reference (ground-truth) and the reconstructed HSI, respectively.</p><p>During the training process, to ensure both the integrity of the overall structure and the accuracy of individual pixels, we set <italic toggle="yes">&#945;</italic> and <italic toggle="yes">&#946;</italic> to 1. The MSE (mean squared error) loss function enforces precise spectral reconstruction through direct pixel-wise penalization of deviations between reconstructed and ground-truth reflectance values.</p><p><italic toggle="yes">L<sub>SSIM</sub></italic> = 1 &#8722; <italic toggle="yes">SSIM</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>), where <italic toggle="yes">SSIM</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) denotes the local structural similarity index between <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>. This metric evaluates the consistency of luminance, contrast, and structural information between the reconstructed image and the ground-truth. The <italic toggle="yes">SSIM</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) is defined as:<disp-formula id="FD24-sensors-25-05443"><label>(24)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mfenced><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mfenced><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">C</italic><sub>1</sub> and <italic toggle="yes">C</italic><sub>2</sub> are predefined parameters assigned values of 0.0001 and 0.0009 based on empirical evaluation. <italic toggle="yes">&#956;<sub>x</sub></italic> represents the average intensity of <italic toggle="yes">x</italic>.</p><p>The variables <italic toggle="yes">&#963;<sub>x</sub></italic><sup>2</sup> and <italic toggle="yes">&#963;<sub>xy</sub></italic> are the variance of <italic toggle="yes">x</italic> and the co-variance of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>. <italic toggle="yes">&#956;<sub>y</sub></italic> and <italic toggle="yes">&#963;<sub>y</sub><sup>2</sup></italic> represent the average intensity and variance of <italic toggle="yes">y</italic>, respectively. Furthermore, <italic toggle="yes">L<sub>SSIM</sub></italic> is bounded within the interval [0, 1], owing to the fact that <italic toggle="yes">SSIM</italic> (<italic toggle="yes">x, y</italic>) itself ranges between 0 and 1.</p><p>The SSIM loss function evaluates local image patches through three complementary metrics: (1) luminance consistency, (2) contrast preservation, and (3) structural correlation. This multi-dimensional assessment effectively maintains the spatial&#8211;spectral coherence of reconstructed spectra, preserving both inter-band correlations and intra-band spatial transitions.</p><p>When reconstructing high-dimensional spectral reflectance from RGB or low-resolution spectral inputs, the joint loss function effectively prevents spectral distortion by simultaneously optimizing for both local accuracy and global spectral fidelity. This composite loss combines complementary error metrics to maintain the physical plausibility of recovered spectra while preserving fine spectral features.</p></sec></sec><sec id="sec3-sensors-25-05443"><title>3. Experiment</title><sec id="sec3dot1-sensors-25-05443"><title>3.1. Experiment System</title><p>As shown in <xref rid="sensors-25-05443-f001" ref-type="fig">Figure 1</xref>, each training pair comprises five components: an RGB image of the tooth and corresponding ground-truth hyperspectral image, an RGB image of the light source reflected by a standard whiteboard captured by the same camera, the corresponding ground-truth hyperspectral image, and the ground-truth hyperspectral image of spectral reflectance. The ground-truth hyperspectral image of spectral reflectance is computed from the sample and light-source hyperspectral images, retaining the same spatial dimensions as the originals and representing the intrinsic spectral properties of the sample. The detailed computation procedure is provided in <xref rid="sec3dot2dot3-sensors-25-05443" ref-type="sec">Section 3.2.3</xref>.</p><p>To maintain uniformity in the experimental input data pairs and enhance the robustness of the spectral reconstruction framework, the optical system was designed and assembled in accordance with the following specifications. Instead of utilizing the pre-existing simulated dataset, a binocular imaging system was designed and implemented to acquire a real-world dataset. A beam splitter was employed to divide the incident light into two separate beams, enabling the simultaneous acquisition of ground-truth and corresponding images sharing an identical field of view. As depicted in <xref rid="sensors-25-05443-f004" ref-type="fig">Figure 4</xref>, the light reflected from the object is incident on the semi-transparent, semi-reflective beam splitter at a 45-degree angle. This arrangement guarantees that the reflected image is aligned perpendicular to the optical axis of the DSLR camera (Nikon Corporation, Tokyo, Japan), while the transmitted image is oriented perpendicular to the optical axis of the spectral camera lens. Both devices were calibrated and securely mounted to ensure a stable and consistent field of view throughout data acquisition. To aid readers in comprehending the essential details of the equipment and materials used in the experimental setup, the key information has been consolidated into a table. For further details, please refer to <xref rid="app3-sensors-25-05443" ref-type="app">Appendix C</xref>.</p><sec id="sec3dot1dot1-sensors-25-05443"><title>3.1.1. Collection and Preparation of Tooth Samples</title><p>The collection and preparation of tooth specimens for this investigation were conducted at the West China Hospital of Stomatology, Sichuan University. Restorative specimens were fabricated by bonding custom-designed ceramic laminates onto extracted human teeth.</p><p>For the ceramic component, presintered white zirconia discs (comprising ZrO<sub>2</sub>, HfO<sub>2</sub>, and Y<sub>2</sub>O<sub>3</sub> &gt; 99%; Al<sub>2</sub>O<sub>3</sub> &lt; 0.5%; with a Y<sub>2</sub>O<sub>3</sub> content of 3 mol%) were utilized. The ceramic laminates were designed employing computer-aided design and manufacturing (CAD/CAM) technology, incorporating the anticipated shrinkage ratio during the sintering process. Milling of the presintered discs was performed using a dental ceramic milling apparatus (AMW-500, Aidite, Qinhuangdao, China), followed by sintering in a rapid sintering furnace (CSF 200, Aidite, Qinhuangdao, China). To offset material loss during polishing, the initial thickness of the specimens was fabricated to be 0.05 mm greater than the target final dimension.</p><p>Surface finishing was executed via an automated polishing system (Tegramin-30, Struers, Struers A/S, Ballerup, Denmark.). Sequential polishing employed silicon carbide abrasive papers with grit sizes of 320, 600, and 1500, utilizing water as the polishing medium. Each polishing step lasted 16 s at a rotational speed of 1500 rpm under unidirectional rotation. Subsequent polishing was conducted using diamond suspensions with particle sizes of 5 &#956;m and 2 &#956;m (Fuyun Technology, Shanghai, China), applied with canvas and silk polishing cloths, respectively, for 20 s at 1500 rpm under identical rotational conditions. Post-polishing, residual abrasive particles were removed by rinsing with distilled water, and samples were dried using compressed air prior to storage in screw-cap tubes. The final ceramic specimen thickness ranged between 0.4 mm and 2.0 mm.</p><p>Natural tooth specimens were collected from patients at the West China Hospital of Stomatology during the period from 1 December to 31 December 2023. Teeth (including premolars and molars) were extracted due to severe periodontitis or orthodontic indications. The collection protocol was approved by the institutional ethics committee (Approval No. WCH-SIRB-2022-257). Inclusion criteria mandated the absence of visible attrition, wear, carious lesions, restorations, developmental anomalies, or pronounced discoloration. Following extraction, teeth were cleaned to remove calculus and residual periodontal soft tissue, rinsed with physiological saline, and stored in a 0.1% thymol solution.</p><p>To fabricate the final restorative specimens, zirconia ceramic laminates were bonded to the prepared natural teeth. Enamel and superficial dentin from the buccal or labial surfaces were removed using a high-speed dental handpiece (T4, Dentsply Sirona) equipped with a diamond bur operating at 1200 rpm under continuous water cooling. The resulting tooth fragments were cleansed with 75% ethanol, air-dried using compressed air, and stored in screw-cap containers. The prepared tooth surfaces were then desiccated, and dual-cure resin cement (U200, 3M ESPE, Seefeld, Germany) was applied. Each zirconia laminate was carefully positioned onto the conditioned surface to ensure intimate contact without air entrapment. Excess cement was gently removed with a cotton swab, followed by light curing with a 565 nm LED curing unit (Philips, Shenzhen, China) for 20 s to complete the bonding process and finalize the restorative specimens.</p></sec><sec id="sec3dot1dot2-sensors-25-05443"><title>3.1.2. Light Box</title><p>To ensure the robustness of the model and reflect practical usage scenarios, an independent experimental light source in the form of an LED lighting box was designed, as illustrated in <xref rid="sensors-25-05443-f004" ref-type="fig">Figure 4</xref>. This lighting box primarily comprises three components: the light-emitting unit, the diffusion mechanism, and the control system. The light-emitting unit contains sixteen LEDs labeled 1 through 16. Among these, LEDs numbered 6, 11, and 16 emit warm white light with varying color rendering indices; LEDs 7, 10, and 13 produce neutral white light with different color rendering capabilities; and LEDs 1 and 4 generate cool white light with distinct color rendering characteristics. The remaining LEDs emit monochromatic light across various hues, resulting in a total of eleven distinct LED types. Beneath the LED array, a diffusion panel was placed to achieve homogeneous lighting through the scattering of photons spanning multiple wavelengths. The control system consists of a programmable device connected to the circuit board via a data interface, allowing reprogramming of the LED control software to modulate the output spectral characteristics of the LEDs. Details of the light box can be found in <xref rid="app4-sensors-25-05443" ref-type="app">Appendix D</xref>.</p></sec><sec id="sec3dot1dot3-sensors-25-05443"><title>3.1.3. Camera Specifications</title><p>Hyperspectral images were acquired using a Specim-IQ handheld imager (Spectral Imaging Ltd, Oulu, Finland), which captures spectral data across 204 bands within the wavelength range of 397.32&#8211;1003.58 nm, together with corresponding reference white calibration data. The resulting hyperspectral dataset includes source files with a spatial&#8211;spectral resolution of 512 &#215; 512 &#215; 20 and reference white files of size 512 &#215; 1 &#215; 204, which were subsequently employed for further data processing. For RGB image acquisition, a Nikon D3X digital single-lens reflex (DSLR) camera(Nikon Corporation, Tokyo, Japan) was employed. To minimize potential interference from subsequent algorithmic processing, the camera was configured to capture unmosaicked RAW images, producing single RGB images at a resolution of 6048 &#215; 4032 pixels.</p></sec><sec id="sec3dot1dot4-sensors-25-05443"><title>3.1.4. Geometrical Light Path</title><p>In the experimental setup, the optical layout was designed to enable the simultaneous acquisition of spectral and RGB data from the same object under identical illumination. As shown in <xref rid="sensors-25-05443-f005" ref-type="fig">Figure 5</xref>, the light reflected from the sample surface first encounters a beam splitter inclined at 45&#176;, which exhibits both reflective and transmissive properties. This component divides the light into two separate optical paths: one path is reflected at a right angle toward the digital single reflex lens camera, while the other passes straight through to the hyperspectral imaging unit. With this arrangement, both cameras observe exactly the same area of the target within an identical field of view, which effectively reduces spatial mismatch and ensures consistent data across both imaging modalities. Such a configuration is essential for reliable multi-source data integration, spectral characterization, and precise visual reconstruction.</p></sec></sec><sec id="sec3dot2-sensors-25-05443"><title>3.2. Dataset Acquisition</title><p>The experimental system generated a theoretically maximum of 1600 unique image datasets (16 light sources &#215; 20 tooth surfaces &#215; 5 replicates), where each tooth&#8217;s buccal and lingual surfaces were restored with ceramic veneers of three distinct thickness gradients.</p><sec id="sec3dot2dot1-sensors-25-05443"><title>3.2.1. Light Condition</title><p>For dataset construction, and considering the wide spectral variations in typical lighting conditions, sixteen distinct LED combinations were employed to simulate real-world light sources. These combinations are classified into four primary categories: (1) monochromatic lighting, which includes three variants of warm white, three of neutral white, and two of cool white LEDs; (2) binary white&#8211;white mixed lighting, encompassing mixtures such as neutral white LED 7 combined with warm white LED 6, warm white LED 6 mixed with neutral white LED 1, neutral white LED 7 combined with cool white LED 1, neutral white LED 7 mixed with cool white LED 4, and warm white LED 6 combined with cool white LED 4; (3) tricolor white mixtures, represented by two combinations&#8212;neutral, warm, and cool white mixes 7-6-1 and 7-6-4; and (4) multicolor mixed lighting, which involves an eleven-LED combination (LEDs 1, 2, 3, 5, 6, 7, 8, 9, 12, 14, and 15) generating a complex mixed white light. Details of the light box can be found in <xref rid="app4-sensors-25-05443" ref-type="app">Appendix D</xref>.</p></sec><sec id="sec3dot2dot2-sensors-25-05443"><title>3.2.2. Image Preprocessing and Augmentation</title><p>A series of ISP procedures, including black-level correction and bilinear interpolation demosaicking, were applied to produce linear RGB images. Because the DSLR camera and hyperspectral camera captured data at different resolutions, the RGB images were resampled using nearest-neighbor interpolation to align with the hyperspectral data resolution. RGB and hyperspectral images were co-registered using a checkerboard calibration method to ensure precise spatial correspondence between the two modalities. To augment the dataset, techniques such as random rotation, horizontal and vertical shifts, and cropping were performed, with all padding filled using a constant value of zero. After completing registration and augmentation, the dataset comprised 4000 pairs of images&#8212;RGB images sized 64 &#215; 64 &#215; 3 and hyperspectral images sized 64 &#215; 64 &#215; 32. <xref rid="sensors-25-05443-f006" ref-type="fig">Figure 6</xref> presents examples of data processed via image enhancement.</p></sec><sec id="sec3dot2dot3-sensors-25-05443"><title>3.2.3. Spectral Reflectance Generation</title><p>In our study, the ground-truth hyperspectral image of spectral reflectance was obtained by calculating the ratio between the hyperspectral image of the sample and that of the illumination source on a wavelength-by-wavelength basis. Specifically, for each wavelength <italic toggle="yes">&#955;</italic>, the spectral reflectance <italic toggle="yes">S</italic>(<italic toggle="yes">&#955;</italic>) was computed as:<disp-formula id="FD25-sensors-25-05443"><label>(25)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#955;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#955;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#955;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">I<sub>sample</sub></italic>(<italic toggle="yes">&#955;</italic>) the measured hyperspectral intensity of the sample and <italic toggle="yes">I<sub>illumination</sub></italic>(<italic toggle="yes">&#955;</italic>) is the measured hyperspectral intensity of the illumination (obtained from a standard white board).</p><p>Please note that during the calculation of spectral reflectance, preprocessing steps including black-level correction and spatial registration were already performed to ensure the accuracy of the obtained spectral reflectance.</p></sec></sec><sec id="sec3dot3-sensors-25-05443"><title>3.3. Training Details</title><p>The SRNet model was trained using 3300 image sets&#8212;comprising 3000 for training and 300 for validation&#8212;and evaluated on an independent test set of 100 image pairs, each containing RGB images alongside their corresponding hyperspectral cubes. The model optimization was performed within the PyTorch framework (version 2.7.1, CUDA 12.8) using the Adam optimizer, with hyperparameters set to <italic toggle="yes">&#946;</italic><sub>1</sub> = 0.9, <italic toggle="yes">&#946;</italic><sub>2</sub> = 0.999, and a weight decay of zero. The initial learning rate was established at 4 &#215; 10<sup>&#8722;4</sup>, with a batch size of 8. Training proceeded for a total of 54 epochs. Convergence was achieved at the end of this process. </p></sec></sec><sec id="sec4-sensors-25-05443"><title>4. Experimental Results</title><p>In the experiment, the mean squared error (MSE) and structural similarity index measure (SSIM) calculated between the reconstructed outputs and the ground-truth were chosen as objective metrics to assess network performance. Reconstruction accuracy was analyzed both spatially and spectrally. In addition to the aforementioned method, four alternative strategies employing different loss functions and network architectures were implemented for comparative evaluation.</p><p><bold>Evaluation 1.</bold> The structure&#8211;pixel loss &#8220;<italic toggle="yes">L</italic><sub>overall</sub>&#8221; in Equation (8) was substituted with <italic toggle="yes">L<sub>MSE</sub></italic> to examine the impact of alternative loss functions. The &#8220;<italic toggle="yes">L<sub>MSE</sub></italic>&#8221; loss is expressed as:<disp-formula id="FD26-sensors-25-05443"><label>(26)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#8721;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">M</italic> denotes the total number of pixels in the hyperspectral image (HSI), while <italic toggle="yes">H<sup>*</sup></italic> and <italic toggle="yes">H</italic> correspond to the pixel values of the reference (ground-truth) and the reconstructed HSI, respectively.</p><p><bold>Evaluation 2.</bold> To assess the contributions of the attention mechanism and the physical information fusion architecture, the attention module was completely removed from the network for ablation analysis.</p><p><bold>Evaluation 3.</bold> For comparisons to state-of-the-art models, HSCNN+, AWAN, HDNet, and MST++ were employed to benchmark the performance of the proposed method.</p><p><bold>Evaluation 4.</bold> Ablation experiments were conducted to evaluate the impact of removing key components and inputs from the proposed network. We removed key components&#8212;DSA, LGF, SRNet-P, SRNet-L&#8212;and illumination images to assess their impact. The pretrained model was also fine-tuned on a non-dental dataset to evaluate generalization. All experiments used consistent training and preprocessing. These studies quantify each component&#8217;s effect on spectral reconstruction.</p><p>To ensure a fair comparison, the pixel values of all images were normalized to the range [0, 1] during data preprocessing. Let <italic toggle="yes">&#952;</italic> represent the input image, the normalized image is calculated as:<disp-formula id="FD27-sensors-25-05443"><label>(27)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#952;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">max</italic>(<italic toggle="yes">&#952;</italic>) denotes the maximum pixel value within <italic toggle="yes">&#952;</italic>.</p><sec id="sec4dot1-sensors-25-05443"><title>4.1. Comparison of SRNet and CNN Baseline</title><p>The experimental results for Evaluation 1 and 2 are presented in <xref rid="sensors-25-05443-t001" ref-type="table">Table 1</xref>, where the final test metrics (MSE and SSIM) for each approach were computed. The Baseline 1 convolutional neural network employed in this work is based on the original U-Net architecture, which has been extensively validated in various image reconstruction and segmentation tasks [<xref rid="B20-sensors-25-05443" ref-type="bibr">20</xref>]. Baseline 2 corresponds to our original network with all attention mechanisms removed. This configuration retains the same overall architecture and number of layers as SRNet, but excludes the DSA module in SRNet-P and the attention modules in SRNet-L, allowing us to isolate and evaluate the contributions of attention mechanisms to reconstruction quality and structural fidelity.</p><p><xref rid="sensors-25-05443-t001" ref-type="table">Table 1</xref> presents the number of parameters, inference time, and reconstruction performance for the proposed SRNet variants and baseline networks. All SRNet variants incorporate attention mechanisms, including DSA in SRNet-P and attention modules in SRNet-L. To assess their impact, we also evaluated Baseline 2, which retains the same architecture but with all attention modules removed. Under the same loss function, SRNet achieves a substantial improvement over Baseline 2 in SSIM (0.8129 vs. 0.643) and a lower MSE, while the increase in parameter numbers and inference time remains modest. This indicates that the computational overhead of attention mechanisms is limited, whereas their contribution to structural and perceptual fidelity is substantial. Furthermore, the trade-off between model complexity, inference speed, and reconstruction quality is evident: SRNet variants achieve superior SSIM and lower MSE at the cost of slightly increased inference time, demonstrating that the network design effectively balances reconstruction accuracy and computational efficiency for practical deployment. Detailed information regarding SRNet&#8217;s number of parameters, memory consumption, and inference time is provided in <xref rid="app5-sensors-25-05443" ref-type="app">Appendix E</xref>.</p><p>As shown in <xref rid="sensors-25-05443-f007" ref-type="fig">Figure 7</xref>, schemes operating on SRNet outperform the conventional U-Net [<xref rid="B20-sensors-25-05443" ref-type="bibr">20</xref>] structure. Due to the absence of the attention module, the results of Baseline 1 have a consistently smaller SSIM than our SRNet, which demonstrates the superiority of network architecture. However, in Baseline 2, where all attention mechanisms are removed from SRNet, the reconstructed results exhibit noticeable artifacts. The main body of the tooth is often occluded, while some fine details, such as the tooth apex, are still partially preserved. This behavior can be attributed to the substantial reduction in SRNet-L&#8217;s ability to extract and utilize light-source information without the attention modules. As a result, the information fusion process is impaired, leading not only to a failure in providing effective guidance but in some cases even hindering accurate reconstruction. This observation highlights the critical role of attention mechanisms in SRNet for capturing spatial and spectral dependencies and ensuring high-fidelity reconstruction of dental structures.</p><p>Due to the substantial performance gap in MSE and SSIM between Baseline 2 and the other methods, Baseline 2 was excluded from the comparative analysis of MSE and SSIM metrics.</p><p>The MSE results are illustrated in <xref rid="sensors-25-05443-f008" ref-type="fig">Figure 8</xref>. The reconstruction results of &#8220;<italic toggle="yes">L<sub>overall</sub></italic>&#8221; exhibit superior accuracy and stability compared to those of &#8220;<italic toggle="yes">L<sub>MSE</sub></italic>,&#8221; while the baseline model achieves the lowest errors in both total and mean MSE. The elevated MSE observed in SRNet relative to the Baseline 1 can be attributed to its more intricate network architecture, which entails multiple nonlinear transformations and feature fusion. Although these design choices enhance the model&#8217;s capacity to capture high-frequency details and global contextual information, they may inadvertently introduce subtle pixel-level reconstruction discrepancies that aggregate, thereby increasing the overall MSE. Conversely, the baseline model&#8217;s comparatively simpler and more direct architecture prioritizes the minimization of pixel-level errors, resulting in superior performance in terms of MSE. Under the application of an identical loss function, SRNet-MSE demonstrates a significant improvement in SSIM relative to the baseline model, indicating enhanced preservation of structural and perceptual quality. Nevertheless, its MSE remains higher than that of the baseline, which suggests that the proposed physical fusion mechanism in SRNet prioritizes the integration of global contextual information over precise pixel-level reconstruction. This trade-off highlights the model&#8217;s design focus on capturing broader spectral and spatial dependencies, which benefits overall visual fidelity at the expense of localized numerical accuracy measured by MSE. However, SRNet (MSE) fails to preserve subtle spectral structures, leading to higher errors in channels with rapid transitions. In comparison, while the standard convolutional architecture of the CNN demonstrates a certain capability in retaining such fine structures to some extent, its overall perceptual quality remains compromised due to the lack of specialized spectral modeling and limited capacity for long-range dependency modeling.</p><p>The SSIM results are illustrated in <xref rid="sensors-25-05443-f009" ref-type="fig">Figure 9</xref>. SRNet outperforms both Baseline 1 and SRNet (MSE) by 14.3% and 7.3% in mean SSIM, respectively. Furthermore, SRNet (MSE) surpasses Baseline 1, indicating that even when optimized solely for MSE, the SRNet architecture maintains superior structural fidelity compared with a standard CNN architecture. Our SRNet combines SSIM and MSE, preserving both local and global structures. However, SRNet (MSE) over-penalizes large errors but ignores perceptual quality, while Baseline 1 lacks mechanisms to prioritize physically meaningful features. It demonstrates the superiority of our loss function and physical information fusion mechanism.</p></sec><sec id="sec4dot2-sensors-25-05443"><title>4.2. Comparison to State-of-the-Art Methods</title><p>Our method is compared with state-of-the-art models like HSCNN+ [<xref rid="B21-sensors-25-05443" ref-type="bibr">21</xref>], AWAN [<xref rid="B22-sensors-25-05443" ref-type="bibr">22</xref>], HDNet [<xref rid="B23-sensors-25-05443" ref-type="bibr">23</xref>], and MST++ [<xref rid="B24-sensors-25-05443" ref-type="bibr">24</xref>]. Their final test results of MSE loss and SSIM are shown in <xref rid="sensors-25-05443-t002" ref-type="table">Table 2</xref>. All models&#8212;including our proposed method and the comparison networks&#8212;were trained on the same dataset using an identical training protocol. The comparison models were fine-tuned on our specialized dataset to ensure adequate adaptation prior to evaluation. To guarantee fairness in the testing phase, we adjusted the input dimensions of all compared models to conform to the format and size requirements of our dataset. This adjustment was critical to ensure that all models were assessed under consistent baseline conditions, thereby enabling a fair and rigorous comparison of their performance.</p><p>The SSIM results demonstrate that our SRNet achieves significantly higher precision in structural similarity measurement compared to HSCNN+. Specifically, SRNet&#8217;s average SSIM score of 0.8724 substantially outperforms HSCNN+&#8217;s 0.8085, representing a 7.9% improvement in structural preservation accuracy. This precision advantage is consistent across all spectral channels, with particularly notable gains in the mid-range spectrum. In addition, the reconstructed images from different spectral channels and the corresponding difference-maps are also given in <xref rid="sensors-25-05443-f010" ref-type="fig">Figure 10</xref>.</p><p>In terms of reconstruction accuracy (MSE), MST++ achieves the lowest error, indicating superior pixel-level precision, likely due to its transformer-based architecture that effectively captures long-range spectral dependencies. HSCNN+ follows closely, suggesting that its CNN designs balance spectral and spatial feature extraction well. AWAN exhibits slightly higher error, possibly due to its attention mechanism focusing more on salient features rather than fine-grained reconstruction. Notably, SRNet has the highest MSE, which stems from its physics-informed structure prioritizing structural integrity over pixel-wise precision.</p><p>Conversely, in perceptual quality (SSIM), SRNet outperforms all competitors, demonstrating its strength in preserving structural details and natural image characteristics. This suggests that while SRNet may sacrifice some reconstruction fidelity (higher MSE), its physics-informed learning framework excels in maintaining high-level visual coherence. HDNet and AWAN follow, benefiting from their feature fusion and attention mechanisms, respectively. MST++ maintains competitive SSIM with its strong MSE performance, indicating a balanced approach. HSCNN+ lags behind, likely due to its simpler CNN-based architecture struggling with complex structural preservation.</p><p>As observed from the comparative maps in <xref rid="sensors-25-05443-f010" ref-type="fig">Figure 10</xref>, the reconstructed images using HSCNN+ exhibit noticeable background nonuniformity. In terms of edge preservation, the SRNet reconstruction demonstrates the sharpest anatomical boundaries, particularly in maintaining morphological features at the enamel&#8211;dentin junction and root apex regions, with notably superior edge definition compared to other algorithms. As evidenced by the difference maps, the MST++ reconstruction demonstrates superior pixel-wise fidelity to the ground-truth values compared to other methods. However, its performance in preserving fine edge details of dental structures and ceramic components remains slightly inferior to SRNet, as indicated by more pronounced discrepancies in these critical regions. This suggests that while MST++ achieves excellent global reconstruction accuracy, SRNet&#8217;s architecture may be better optimized for maintaining local structural details in significant areas. Regarding color reproduction fidelity, SRNet demonstrates superior performance in maintaining natural color transitions throughout the reconstructed images. In contrast, AWAN exhibits noticeable color distortion, particularly in background regions. The visual analysis further reveals MST++ suffers from over-smoothing that compromises cusp morphological details.</p><p>For overall reconstruction quality, SRNet generates images with optimal structural coherence, free from noticeable artifacts or distortions. In contrast, other algorithms display varying degrees of blurring or deformation in critical regions such as occlusal surface textures and root morphology. These qualitative observations align with and substantiate quantitative findings, collectively validating the superiority of the SRNet algorithm.</p><p>This comprehensive evaluation confirms SRNet&#8217;s advancements in three key aspects: (1) precise preservation of fine dental structures, (2) physiologically accurate color reproduction, and (3) robust avoidance of common reconstruction artifacts, establishing it as a state-of-the-art solution for dental spectral image reconstruction.</p><p><xref rid="sensors-25-05443-f011" ref-type="fig">Figure 11</xref> illustrates the structural similarity index measure (SSIM) results obtained using various state-of-the-art methods. The SSIM evaluation demonstrates that SRNet, as a physics-informed fusion network, achieves superior structural preservation by effectively integrating physical spectral constraints with data-driven learning. Analysis demonstrates SRNet&#8217;s superior performance across all spectral channels, particularly excelling in mid-range bands where its physics-informed architecture effectively combines spectral and spatial attention mechanisms. Unlike conventional attention-based networks, SRNet maintains more consistent structural preservation throughout the spectrum. This stable performance profile suggests SRNet&#8217;s physical constraints help optimize attention weighting for more reliable feature extraction compared to purely data-driven attention approaches.</p><p>Considering SSIM metrics alone, SRNet demonstrates advantages compared to both spectral transformer-based MST++ and attention-enhanced networks such as HDNet. However, this does not imply overall superiority across all evaluation criteria. The results suggest that incorporating physical priors can help preserve the structural integrity of hyperspectral data, and that combining physical information fusion with attention mechanisms may further improve performance by guiding the network to focus on more meaningful spectral&#8211;spatial relationships.</p><p><xref rid="sensors-25-05443-f012" ref-type="fig">Figure 12</xref> compares the mean squared error (MSE) performance across various state-of-the-art methods. The mean squared error (MSE) analysis of state-of-the-art methods across 32 channels reveals distinct performance trends. MST++ consistently demonstrates the lowest MSE values in most channels, particularly in channels 3&#8211;20, where its MSE remains below 0.001, indicating superior accuracy. HSCNN+ and AWAN perform moderately, with HSCNN+ achieving lower MSE than AWAN in early channels but being surpassed by AWAN in certain channels (e.g., channels 8&#8211;10). HDNet and SRNet exhibit higher MSE values, with SRNet showing significant fluctuations, especially in channels 13&#8211;16, where its MSE peaks above 0.003. SRNet&#8217;s MSE spikes in channels 13&#8211;16 may stem from physical constraints of information fusion, yet its SSIM superiority implies these constraints effectively maintain inter-band relationships critical for human or downstream task perception.</p><p>In contrast, MST++&#8217;s lower MSE but weaker SSIM performance highlights a limitation of purely data-driven methods: while optimized for pixel accuracy, they may fail to capture physically plausible structures. SRNet&#8217;s hybrid approach thus offers a task-adaptive advantage&#8212;particularly in applications where perceptual quality outweighs marginal MSE gains.</p></sec><sec id="sec4dot3-sensors-25-05443"><title>4.3. Ablation Experiment</title><p>To rigorously examine the contributions and significance of individual components within the proposed network architecture, we conducted a comprehensive series of ablation experiments. In these experiments, specific modules or input data types were systematically removed to isolate their effects, and the resulting changes in reconstruction accuracy were evaluated. This approach provides deeper insights into the extent to which each element influences the overall performance of spectral reflectance prediction. <xref rid="sensors-25-05443-f013" ref-type="fig">Figure 13</xref> presents the pseudo-color visualization results obtained from the ablation study.</p><p>The reconstruction results without the dual stat attention (DSA) module exhibit reduced background uniformity and diminished preservation of fine dental textures, particularly at critical regions such as the tooth roots and interfaces between teeth and ceramic components, relative to SRNet with DSA. The absence of DSA results in blurred and less distinct edges, impairing the capture of subtle morphological features.</p><p>Removal of the LGF module leads to a pronounced decline in reconstruction quality. Analysis of the pseudo-color maps and corresponding difference images reveals substantial errors throughout the scene, encompassing both dental regions and background areas, with marked deviations from the ground-truth. This deterioration arises from the pivotal role of the LGF module in SRNet-P, which is responsible for extracting essential spatial and spectral features; its absence results in the loss of critical information, including both anatomical details and background color cues. These findings highlight the indispensable function of the LGF module in maintaining accurate and detailed spectral reconstruction.</p><p>In the model variant without the SRNet-L module, dental sample images were directly used as input to generate the spectral reflectance of the teeth. The results show that using only SRNet-P achieves satisfactory overall tooth morphology, but fine details, such as tooth root shape, are less accurately reconstructed. Additionally, the reconstructed images exhibit noticeable color deviations, likely due to the absence of illumination spectral information, which leads to missing critical details in the color generation process.</p><p>Correspondingly, when employing only the SRNet-L network with illumination (light-source) images as input to reconstruct the spectral reflectance of teeth, the resulting reconstructions entirely lack dental structural details. This observation indicates that illumination information functions solely as an auxiliary input in the spectral reconstruction process. Despite the network&#8217;s capacity, it cannot infer or generate meaningful dental features from illumination data alone without prior learning from sample images.</p><p>To further examine the significance of illumination images within the proposed framework, the original illumination input was replaced with a constant value to effectively eliminate illumination information. The resulting reconstructions exhibit substantial deviations from the ground-truth. Although partial dental details are retained, the overall color distribution is markedly distorted. These findings suggest that while the SRNet-P network maintains the ability to extract dental structural features, the illumination information is indispensable as auxiliary input for accurate spectral reflectance reconstruction, and its absence critically undermines reconstruction performance.</p><p>Since the reconstruction results without the LGF module and those using only illumination images exhibit significant errors, these two methods have been excluded from the quantitative performance evaluation presented here. Results of the ablation study are presented in <xref rid="sensors-25-05443-t003" ref-type="table">Table 3</xref>.</p><p>From the SSIM perspective, the complete SRNet model demonstrates the highest reconstruction fidelity, reflecting superior preservation of structural details. The model without the DSA module exhibits degraded performance compared to the full network but still outperforms the variant lacking illumination input. These results highlight the effectiveness of the proposed sample-illumination fusion strategy in enhancing structural consistency.</p><p>Regarding MSE, the SRNet-P-only model achieves values close to the full SRNet, while the variant without illumination input shows significantly higher errors. This suggests that integrating illumination information introduces optical priors that constrain the reconstruction process, potentially limiting absolute pixel-wise accuracy. Therefore, although spectral fusion enhances structural similarity, it may impose restrictions on minimizing pixel-level reconstruction error.</p></sec><sec id="sec4dot4-sensors-25-05443"><title>4.4. Evaluation on a ColorChecker Dataset</title><p>To investigate the generalization capability of our model on non-dental datasets, it is essential to evaluate its performance on alternative data sources. However, due to the distinctive network architecture that simultaneously requires sample RGB images and corresponding light-source RGB images as inputs, suitable publicly available datasets are limited. Given that the ColorChecker encompasses a broad spectrum of colors, evaluation using this dataset provides a meaningful indication of the model&#8217;s capability to generalize across various colors and materials. Consequently, we acquired a dedicated ColorChecker dataset to facilitate comprehensive performance assessment under these conditions. The data acquisition protocol for this dataset closely follows that of the dental dataset used in this study, with the principal distinction being that it was collected outdoors to better simulate real-world conditions. Two random test data pairs were selected, and their sample RGB-board RGB input pairs and reconstructed spectral reflectance are shown in <xref rid="sensors-25-05443-f014" ref-type="fig">Figure 14</xref>.</p><p>The RGB images of spectral reflectance are presented as pseudo-color renderings, while the SSIM values are calculated based on the complete reconstructed spectra. The red rectangles in <xref rid="sensors-25-05443-f014" ref-type="fig">Figure 14</xref> highlight regions of nonuniform illumination, which are effectively suppressed in the output. The yellow rectangles denote occlusions present in the scene; notably, the model still reconstructs these occlusions, as they constitute genuine components of the actual spectral data. Although uncontrolled illumination and elevated noise levels contribute to a performance decline relative to the light box condition, the results remain within an acceptable range.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05443"><title>5. Discussion</title><p>Accurate color measurement is crucial for ensuring reliable sensing performance in vision-based applications. However, existing color measurement methods suffer from illumination variability, operational complexity, and perceptual subjectivity. In this study, dental color measurement, with its strict perceptual and spectral fidelity demands, is adopted to validate the proposed method. Traditional dental color measurement methods are limited by subjective perception, operational complexity, and metamerism&#8212;a phenomenon where the same color appears different under varying lighting conditions. Using self-made resin-permeated ceramic teeth, this investigation proposes a deep-learned, end-to-end spectral reflectance prediction framework to achieve snapshot teeth spectral reflectance from RGB images under complex light sources in the fundamental spectral domain through the construction of a physically interpretable network that enables physically informed feature fusion. The proposed framework consists of three key components: (1) a physically interpretable neural network for channel-wise fusion of teeth and light-source data; (2) a custom optical system within a self-designed light box; and (3) resin-infiltrated ceramics and natural teeth as samples.</p><p>Beam-splitting mirrors were utilized to simultaneously capture co-registered RGB and hyperspectral images of both dental specimens and illumination sources, resulting in a curated dataset comprising 4000 paired samples. The object&#8217;s reflected light strikes a 45&#176; semi-transparent beam splitter, directing the reflected image orthogonally to the center of the single-lens reflex camera lens and the transmitted image orthogonally to the spectral camera lens center. Dental and light-source images are jointly fed into SRNet. The model is trained under supervision using spectral ground-truth, with channel-wise fusion enabling feature integration. The network outputs tooth spectral reflectance, achieving precise color measurement. To enhance precision, in addition to a customized loss function, the attention submodules were designed to capture correlations across different spatial locations and spectral channels. The results demonstrate that the proposed method surpasses traditional CNN architectures, with the &#8220;SRNet + <italic toggle="yes">L<sub>overall</sub></italic>&#8221; approach achieving the best performance. A major advantage of the proposed method over previous techniques is that it eliminates the need for bulky equipment and stringent experimental conditions, thereby avoiding cumbersome procedures and heterochromatic artifacts, while enhancing both the efficiency and accuracy of color measurement. Comprehensive ablation and comparative experiments indicate that the proposed method attains the highest SSIM value of 0.8724, demonstrating its superior performance in maintaining structural fidelity. Nevertheless, the MSE of 0.0024 remains higher than that of MST++, indicating that the method is not yet optimal in terms of absolute pixel-level accuracy. This shortcoming may limit its applicability in certain clinical scenarios where stringent pixel-level precision is required. Addressing this limitation will constitute an important focus of subsequent research.</p><p>From a technical perspective, this work offers a straightforward, end-to-end, and scan-free solution for accurate color measurement. Our method effectively balances reconstruction quality and speed, making it versatile enough for diverse imaging scenarios. For high-precision color measurement, integrating camera response characteristics into the network architecture may improve spectral reconstruction accuracy. However, this approach may increase model parameters, prolong training duration, and reduce inference speed. We believe that this approach will inspire future research in multispectral imaging and support a broad range of applications.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-05443"><title>6. Conclusions</title><p>In this study, we developed an a deep-learned, end-to-end spectral reflectance prediction framework to achieve snapshot teeth spectral reflectance from RGB images under complex light sources in the fundamental spectral domain through the construction of a physically interpretable network that enables physically informed feature fusion. A dual-attention modular-information fusion neural network is developed to recover the spectral reflectance directly from the RGB image for natural teeth and ceramics across multiple scenarios. A dataset containing 4000 RGB&#8211;hyperspectral image pairs was built using a self-designed optical system with complex illumination conditions.</p><p>Our network achieved an SSIM of 0.8724, a substantial improvement over the 0.8085 achieved by the HSCNN+ model. For overall reconstruction quality, SRNet generates images with optimal structural coherence, free from noticeable artifacts or distortions. In contrast, other algorithms display varying degrees of blurring or deformation in critical regions such as occlusal surface textures and root morphology. Our method effectively balances reconstruction quality and speed, making it versatile enough for diverse imaging scenarios. For high-precision color measurement, integrating camera response characteristics into the network architecture may improve spectral reconstruction accuracy. However, this approach may increase model parameters, prolong training duration, and reduce inference speed.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to thank West China Hospital of Stomatology, Sichuan University, for providing the dental samples used in this study.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, H.Y. and M.Z.; methodology, H.Y. and M.Z.; software, H.Y.; validation, H.Y. and H.X.; formal analysis, F.L. and J.S. (Jiefei Shen); investigation, B.C. and Y.W.; resources, F.L. and J.S. (Jiefei Shen); data curation, H.Y. and Y.W.; writing&#8212;original draft preparation, H.Y.; writing&#8212;review and editing, H.Y. and J.S. (Junfei Shen); visualization, H.Y.; supervision, J.S. (Junfei Shen); project administration, J.S. (Junfei Shen); funding acquisition, J.S. (Junfei Shen). All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki, and approved by the Ethics Committee of West China Hospital of Stomatology (WCHSIRB-CT-2022-257).</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data are not publicly available due to privacy and ethical restrictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Mingwei Zhou was employed by the Mindray Bio-Medical Electronics, 518000, Shenzhen, China. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">IOS</td><td align="left" valign="middle" rowspan="1" colspan="1">Intraoral Scanner</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HSI</td><td align="left" valign="middle" rowspan="1" colspan="1">Hyperspectral Image</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LGF</td><td align="left" valign="middle" rowspan="1" colspan="1">Local&#8211;Global Fusion</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSA</td><td align="left" valign="middle" rowspan="1" colspan="1">Dual-Stat Attention</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MSA</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-Head Self-Attention</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SA</td><td align="left" valign="middle" rowspan="1" colspan="1">Spatial Attention</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CA</td><td align="left" valign="middle" rowspan="1" colspan="1">Channel Attention</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SSIM</td><td align="left" valign="middle" rowspan="1" colspan="1">Structural Similarity Index Measure</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MSE</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Square Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CCT</td><td align="left" valign="middle" rowspan="1" colspan="1">Correlated Color Temperature</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSLR</td><td align="left" valign="middle" rowspan="1" colspan="1">Digital Single-Lens Reflex</td></tr></tbody></array></p></glossary><app-group><app id="app1-sensors-25-05443"><title>Appendix A</title><p>This appendix presents a subset of the training samples utilized in this study. The samples in <xref rid="sensors-25-05443-f0A1" ref-type="fig">Figure A1</xref> are selected to illustrate the diversity and characteristics of the dataset, including variations in spectral signatures and spatial features. These examples provide insight into the data used to train the proposed hyperspectral reconstruction model.</p><fig position="anchor" id="sensors-25-05443-f0A1" orientation="portrait"><label>Figure A1</label><caption><p>(<bold>a</bold>) tooth sample images acquired with a DSLR camera and subsequently processed, (<bold>b</bold>) white board images acquired with a DSLR camera and subsequently processed (illumination), (<bold>c</bold>) pseudo-color visualizations of tooth samples derived from hyperspectral images, (<bold>d</bold>) pseudo-color visualizations of white board derived from hyperspectral images (illumination), and (<bold>e</bold>) pseudo-color visualizations of tooth sample spectral reflectance images.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g0A1.jpg"/></fig></app><app id="app2-sensors-25-05443"><title>Appendix B</title><p>Convolutional operations typically focus on local receptive fields, whereas attention mechanisms are capable of capturing long-range dependencies. By leveraging spatial and channel attention modules, the model can effectively capture global contextual information and enhance spatial&#8211;spectral feature representations. This capability is crucial for spectral imaging tasks, which demand precise characterization of features across diverse spatial positions and spectral bands.</p><p>Therefore, we proposed spatial&#8211;spectral enhancement modules, comprising local&#8211;global fusion (LGF) and dual-stat attention (DSA), which were integrated into SRNet to enhance performance in Step 1.</p><p>The local&#8211;global fusion (LGF) module addresses the challenge of simultaneously capturing fine-grained local spatial details and broad global contextual information in hyperspectral image reconstruction. Conventional convolutional networks often struggle to represent features across multiple scales, leading to incomplete spatial&#8211;spectral modeling and reduced reconstruction accuracy. By integrating features extracted at different receptive fields, the LGF module enhances the network&#8217;s ability to comprehensively capture complex spatial and spectral correlations inherent in hyperspectral data.</p><p>The dual-stat attention (DSA) module focuses on overcoming the redundancy and noise present in the numerous spectral channels of hyperspectral images, as well as the uneven importance of different spatial regions. Traditional convolutional operations lack the capability to dynamically recalibrate attention across channels and spatial locations, which may cause the network to distribute focus inefficiently. The DSA module adaptively computes attention weights based on statistical properties such as mean and standard deviation, allowing the network to emphasize informative spectral bands and spatial areas while suppressing irrelevant or noisy features. This selective feature enhancement improves both the robustness and accuracy of the spectral reflectance reconstruction.</p><sec id="secBdot1-sensors-25-05443"><title>Appendix B.1. Local&#8211;Global Fusion (LGF)</title><p>As illustrated in <xref rid="sensors-25-05443-f0A2" ref-type="fig">Figure A2</xref>, the LGF module is composed of multiple submodules, including layer normalization, multi-head self-attention (MSA), and a multi-convolutional block. By incorporating residual connections, LGF effectively captures long-range dependencies while preserving the flow of information.</p><p>The network structure includes two primary components: the self-attention layer (MSA) for extracting global features and the feedforward network (multi-convolutional block) for enhancing the model&#8217;s nonlinear expressive capacity. The architecture of the multi-convolutional block is depicted in <xref rid="sensors-25-05443-f0A2" ref-type="fig">Figure A2</xref>.</p><p>Given an input feature map <italic toggle="yes">F</italic> &#8712; <italic toggle="yes">R<sup>B&#215;H&#215;W&#215;C</sup></italic>, LGF consists of multiple sequential blocks, each performing multi-head self-attention (MSA) and multi-convolution (MC) operations with residual connections:<disp-formula id="FD28-sensors-25-05443"><label>(A1)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#8592;</mml:mo><mml:mi>F</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD29-sensors-25-05443"><label>(A2)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#8592;</mml:mo><mml:mi>F</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><fig position="anchor" id="sensors-25-05443-f0A2" orientation="portrait"><label>Figure A2</label><caption><p>Detailed structure of LGF and DSA. The &#8220;C&#8221; in <xref rid="sensors-25-05443-f0A2" ref-type="fig">Figure A2</xref> indicates that the two inputs are concatenated in the channel dimension. &#8853; indicates adding while &#8855; means multiplication.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g0A2.jpg"/></fig><sec id="secBdot1dot1-sensors-25-05443"><title>Appendix B.1.1. Multi-Head Self-Attention</title><p>Given the input <italic toggle="yes">X<sub>in</sub></italic> &#8712; <italic toggle="yes">R<sup>H&#215;W&#215;C</sup></italic> for MSA, it is first reshaped into a sequence of tokens. These tokens are subsequently transformed via linear projections into the query <italic toggle="yes">Q</italic> &#8712; <italic toggle="yes">R<sup>HW&#215;C</sup></italic>, key <italic toggle="yes">K</italic> &#8712; <italic toggle="yes">R <sup>HW&#215;C</sup></italic>, and value <italic toggle="yes">V</italic> &#8712; <italic toggle="yes">R<sup>HW&#215;C</sup></italic>, using the learnable weight matrices <italic toggle="yes">W<sub>Q</sub></italic>, <italic toggle="yes">W<sub>K</sub></italic>, <italic toggle="yes">W<sub>V</sub></italic> &#8712; <italic toggle="yes">R<sup>C&#215;C</sup>:</italic><disp-formula id="FD30-sensors-25-05443"><label>(A3)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Subsequently, <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, and <italic toggle="yes">V</italic> are partitioned into N heads along the spectral channel dimension, resulting in <italic toggle="yes">Q</italic> = [<italic toggle="yes">Q</italic><sub>1</sub>, &#8230;, <italic toggle="yes">Q<sub>N</sub></italic>], <italic toggle="yes">K</italic> = [<italic toggle="yes">K</italic><sub>1</sub>, &#8230;, <italic toggle="yes">K<sub>N</sub></italic>], <italic toggle="yes">V</italic> = [<italic toggle="yes">V</italic><sub>1</sub>, &#8230;, <italic toggle="yes">V<sub>N</sub></italic>], where the dimension of each head is:<disp-formula id="FD31-sensors-25-05443"><label>(A4)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In MSA, each spectral representation is regarded as an individual token, and self-attention for the <italic toggle="yes">head<sub>j</sub></italic> is computed as follows:<disp-formula id="FD32-sensors-25-05443"><label>(A5)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>K</mml:mi><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>Q</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">K<sub>j</sub><sup>T</sup></italic> indicates the transpose of matrix <italic toggle="yes">K<sub>j</sub></italic>, and <italic toggle="yes">&#963;<sub>j</sub></italic> &#8712; <italic toggle="yes">R</italic> is a learnable parameter.</p><p>To mitigate spectral variations, a learnable parameter adjusts the matrix product <italic toggle="yes">A<sub>j</sub></italic> within each head during the computation of <italic toggle="yes">K<sub>j</sub><sup>T</sup>Q<sub>j</sub></italic>.</p><p>Subsequently, the outputs from all <italic toggle="yes">N</italic> heads are concatenated and passed through a linear projection:<disp-formula id="FD33-sensors-25-05443"><label>(A6)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">W</italic> &#8712; <italic toggle="yes">R<sup>C&#215;C</sup></italic> is a learnable weight matrix and <italic toggle="yes">eps</italic>(&#183;) denotes the position embedding function.</p><p>Position embedding is critical for representing positional information across spectral channels, given that hyperspectral images are organized in ascending wavelength order along the spectral dimension. The detailed architecture of the Multi-Head Self-Attention (MSA) module is illustrated in <xref rid="sensors-25-05443-f0A3" ref-type="fig">Figure A3</xref>.</p><fig position="anchor" id="sensors-25-05443-f0A3" orientation="portrait"><label>Figure A3</label><caption><p>Details of the multi-head self-attention (MSA) block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g0A3.jpg"/></fig></sec><sec id="secBdot1dot2-sensors-25-05443"><title>Appendix B.1.2. Multi-Convolution</title><p>The multi-convolution module is designed to enhance local feature extraction by applying a sequence of convolutions interleaved with nonlinear activations and separable convolution, efficiently capturing spatial details while reducing computational cost.</p><p>Given an input feature map <italic toggle="yes">F</italic> &#8712; <italic toggle="yes">R<sup>B&#215;</sup><sup>C&#215;</sup><sup>H&#215;</sup><sup>W</sup></italic>, the module first expands the channel dimension by a point-wise 1 &#215; 1 convolution, then applies a nonlinear activation <italic toggle="yes">&#981;</italic>(&#183;), followed by a 3 &#215; 3 convolution, another nonlinear activation, and finally reduces the channel dimension back to C with another point-wise convolution. Formally, this can be expressed as:<disp-formula id="FD34-sensors-25-05443"><label>(A7)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#981;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#981;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">&#981;</italic>(&#183;) denotes GELU activation.</p><p>By reducing parameters and computational cost compared to standard convolutions, it efficiently captures local spatial correlations, while nonlinear activations enable the modeling of more complex feature interactions.</p></sec></sec><sec id="secBdot2-sensors-25-05443"><title>Appendix B.2. Dual-Stat Attention</title><p>The dual-stat attention (DSA) module is designed to enhance spatial feature representation by leveraging statistical pooling operations. Specifically, it utilizes both average pooling and max pooling along the channel dimension to extract complementary spatial descriptors from the input feature map. Average pooling captures the global contextual distribution of features, while max pooling emphasizes localized salient responses.</p><p>Also, given an input feature map <italic toggle="yes">F</italic> &#8712; <italic toggle="yes">R<sup>B&#215;</sup><sup>C&#215;</sup><sup>H&#215;</sup><sup>W</sup></italic>, DSA computes the average-pooled map and max-pooled map across the channel dimension as:<disp-formula id="FD35-sensors-25-05443"><label>(A8)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>C</mml:mi></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>c</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These two maps are concatenated along the channel axis and passed through a convolutional layer followed by a sigmoid activation to generate a spatial attention map <italic toggle="yes">A<sub>S</sub></italic> &#8712; <italic toggle="yes">R<sup>B&#215;</sup></italic><sup>1<italic toggle="yes">&#215;</italic></sup><italic toggle="yes"><sup>H&#215;</sup><sup>W</sup></italic>:<disp-formula id="FD36-sensors-25-05443"><label>(A9)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">Concat</italic>(<italic toggle="yes">A, B</italic>) denotes concatenation and <italic toggle="yes">&#963;</italic>(<italic toggle="yes">&#183;</italic>) indicates the sigmoid function.</p><p>The output feature map is then obtained by element-wise multiplication of the input and the spatial attention map broadcasted over channels:<disp-formula id="FD37-sensors-25-05443"><label>(A10)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo>&#8855;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>By combining these two statistical views, DSA generates a more informative and balanced spatial attention map. This attention is then used to selectively reweight spatial locations, enabling the network to focus on regions that are both contextually important and structurally distinctive. The DSA module is lightweight, fully convolutional, and integrates seamlessly within the encoder&#8211;decoder framework, particularly along skip connections to enhance multi-scale feature fusion.</p></sec></app><app id="app3-sensors-25-05443"><title>Appendix C</title><p><xref rid="sensors-25-05443-t0A1" ref-type="table">Table A1</xref> summarizes the key statistical characteristics of the dataset used in this study, including the distribution of light-source types, the number of samples acquired under each illumination condition, and other essential acquisition parameters. This statistical overview provides a concise yet comprehensive characterization of the dataset, supporting the interpretation of subsequent experimental results.</p><table-wrap position="anchor" id="sensors-25-05443-t0A1" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05443-t0A1_Table A1</object-id><label>Table A1</label><caption><p>Dataset acquisition specifications.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Category</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" colspan="1">Light Source</td><td align="center" valign="middle" rowspan="1" colspan="1">Number of Type</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Type</td><td align="center" valign="middle" rowspan="1" colspan="1">Please refer to <xref rid="sec3dot2-sensors-25-05443" ref-type="sec">Section 3.2</xref></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Intensity</td><td align="center" valign="middle" rowspan="1" colspan="1">Please refer to <xref rid="app4-sensors-25-05443" ref-type="app">Appendix D</xref> for details.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Light Source</td><td align="center" valign="middle" rowspan="1" colspan="1">Spectral Range</td><td align="center" valign="middle" rowspan="1" colspan="1">380&#8211;780 nm</td></tr><tr><td rowspan="4" align="center" valign="middle" colspan="1">Tooth samples</td><td align="center" valign="middle" rowspan="1" colspan="1">Ceramic Thickness</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4&#8211;2 mm</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ceramic Material</td><td align="center" valign="middle" rowspan="1" colspan="1">White zirconia presintered discs composed of ZrO<sub>2</sub> + HfO<sub>2</sub> + Y<sub>2</sub>O<sub>3</sub> (total content &gt; 99%) with Al<sub>2</sub>O<sub>3</sub> content less than 0.5% and yttria (Y<sub>2</sub>O<sub>3</sub>) doping at 3 mol%.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Teeth</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Source of teeth</td><td align="center" valign="middle" rowspan="1" colspan="1">Extracted teeth from West China Hospital, Sichuan University, Dec 2023, due to periodontitis and orthodontics.</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Camera (RGB)</td><td align="center" valign="middle" rowspan="1" colspan="1">Model</td><td align="center" valign="middle" rowspan="1" colspan="1">Nikon D3X</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Resolution</td><td align="center" valign="middle" rowspan="1" colspan="1">6048 &#215; 4032 pixels</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Camera (Spectral)</td><td align="center" valign="middle" rowspan="1" colspan="1">Model</td><td align="center" valign="middle" rowspan="1" colspan="1">Specim-IQ</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spectral range</td><td align="center" valign="middle" rowspan="1" colspan="1">397.32&#8211;1003.58 nm</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Viewpoint variation</td><td align="center" valign="middle" rowspan="1" colspan="1">Measured angle</td><td align="center" valign="middle" rowspan="1" colspan="1">Simulated by augmentation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset split</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Train/Val/Test</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3000/300/700.</td></tr></tbody></table></table-wrap></app><app id="app4-sensors-25-05443"><title>Appendix D</title><p>The lighting module of the experimental light box comprised eleven different types of light-emitting diodes (LEDs), including three variants of white light and eight monochromatic sources. The detailed characteristics of these LEDs are summarized in <xref rid="sensors-25-05443-t0A2" ref-type="table">Table A2</xref>. A total of 1700 LEDs were systematically arranged on a 700 mm &#215; 400 mm circuit board array. Through precise modulation of the individual LED output intensities, a visible illumination spectrum spanning the wavelength range of 380&#8211;780 nm was successfully generated.</p><table-wrap position="anchor" id="sensors-25-05443-t0A2" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05443-t0A2_Table A2</object-id><label>Table A2</label><caption><p>Models, parameters, and quantities of the LED beads in the light box. Abbreviations: Std. &#955;: standard peak wavelength; Meas. &#955;: measured peak wavelength; Min. &#934;: minimum luminous flux; FWHM: full width at half maximum.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Color</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Std. &#955;<break/>(nm/K)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Meas. &#955;<break/>(nm/K)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Min. &#934;<break/>(lm)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FWHM<break/>(nm)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Quantity</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Royal Blue</td><td align="center" valign="middle" rowspan="1" colspan="1">LXM2-PR02-0100</td><td align="center" valign="middle" rowspan="1" colspan="1">445&#8211;450</td><td align="center" valign="middle" rowspan="1" colspan="1">447</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td><td align="center" valign="middle" rowspan="1" colspan="1">24</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Blue</td><td align="center" valign="middle" rowspan="1" colspan="1">LXM2-PB01-0030</td><td align="center" valign="middle" rowspan="1" colspan="1">475&#8211;480</td><td align="center" valign="middle" rowspan="1" colspan="1">479</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">33</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cyan</td><td align="center" valign="middle" rowspan="1" colspan="1">LXML-PE01-0070</td><td align="center" valign="middle" rowspan="1" colspan="1">505&#8211;510</td><td align="center" valign="middle" rowspan="1" colspan="1">506</td><td align="center" valign="middle" rowspan="1" colspan="1">70</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Green</td><td align="center" valign="middle" rowspan="1" colspan="1">LXM2-PM01-0090</td><td align="center" valign="middle" rowspan="1" colspan="1">520&#8211;525</td><td align="center" valign="middle" rowspan="1" colspan="1">522</td><td align="center" valign="middle" rowspan="1" colspan="1">90</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Amber</td><td align="center" valign="middle" rowspan="1" colspan="1">LXML-PL01-0030</td><td align="center" valign="middle" rowspan="1" colspan="1">592&#8211;594</td><td align="center" valign="middle" rowspan="1" colspan="1">593</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">28</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Orange-Red</td><td align="center" valign="middle" rowspan="1" colspan="1">LXM3-PH01-0070</td><td align="center" valign="middle" rowspan="1" colspan="1">610&#8211;620</td><td align="center" valign="middle" rowspan="1" colspan="1">615</td><td align="center" valign="middle" rowspan="1" colspan="1">70</td><td align="center" valign="middle" rowspan="1" colspan="1">28</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Red</td><td align="center" valign="middle" rowspan="1" colspan="1">LXM2-PD01-0040</td><td align="center" valign="middle" rowspan="1" colspan="1">620&#8211;630</td><td align="center" valign="middle" rowspan="1" colspan="1">626</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">29</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep Red</td><td align="center" valign="middle" rowspan="1" colspan="1">LXM2-PD01-0030</td><td align="center" valign="middle" rowspan="1" colspan="1">650&#8211;660</td><td align="center" valign="middle" rowspan="1" colspan="1">657</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">29</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Warm White</td><td align="center" valign="middle" rowspan="1" colspan="1">LXML-PW30</td><td align="center" valign="middle" rowspan="1" colspan="1">3000 K</td><td align="center" valign="middle" rowspan="1" colspan="1">3000 K</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" rowspan="1" colspan="1">140</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Neutral White</td><td align="center" valign="middle" rowspan="1" colspan="1">LXML-PWN2</td><td align="center" valign="middle" rowspan="1" colspan="1">4100 K</td><td align="center" valign="middle" rowspan="1" colspan="1">4000 K</td><td align="center" valign="middle" rowspan="1" colspan="1">130</td><td align="center" valign="middle" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" rowspan="1" colspan="1">140</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cool White</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LXML-PWC2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5650 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5582 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">140</td></tr></tbody></table></table-wrap><p>The normalized emission spectra of the various LED types are illustrated in <xref rid="sensors-25-05443-f0A4" ref-type="fig">Figure A4</xref>. To ensure homogenous and uniform illumination, an optical diffuser was positioned within the optical path. This diffuser functions to spatially scatter the light, thereby mitigating sharp luminance gradients and preventing uneven brightness distribution caused by discrete LED sources.</p><fig position="anchor" id="sensors-25-05443-f0A4" orientation="portrait"><label>Figure A4</label><caption><p>Normalized spectral intensity of diverse LED beads in the light box.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g0A4.jpg"/></fig></app><app id="app5-sensors-25-05443"><title>Appendix E</title><p><xref rid="sensors-25-05443-t0A3" ref-type="table">Table A3</xref> summarizes the computational requirements of the SRNet components, including the number of parameters (Params), inference time per image, and peak memory consumption. The peak memory consumption was measured on an NVIDIA RTX 5070 GPU with a batch size of 4, using input images of size 64 &#215; 64 &#215; 3. As shown, SRNet-P, which has the largest number of parameters, requires the longest inference time, while SRNet-L achieves faster inference due to its reduced complexity. The SRNet-L for fusion demonstrates a slight increase in computational cost compared to SRNet-L alone, reflecting the additional processing for feature fusion. This information provides readers with a clear understanding of the trade-offs between model complexity and runtime performance, which is crucial for practical deployment in clinical or embedded systems.</p><table-wrap position="anchor" id="sensors-25-05443-t0A3" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05443-t0A3_Table A3</object-id><label>Table A3</label><caption><p>Model complexity and computational requirements of SRNet components, including number of parameters, inference time, and peak memory consumption.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Peak Memory Consumption</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hardware Environment</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SRNet-P</td><td align="center" valign="middle" rowspan="1" colspan="1">4.594</td><td align="center" valign="middle" rowspan="1" colspan="1">23.34</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">4.5 GB</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SRNet-L</td><td align="center" valign="middle" rowspan="1" colspan="1">4.101</td><td align="center" valign="middle" rowspan="1" colspan="1">15.23</td><td align="center" valign="middle" rowspan="1" colspan="1">Nvidia RTX 5070</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRNet-L (fusion)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.136</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr></tbody></table></table-wrap></app></app-group><ref-list><title>References</title><ref id="B1-sensors-25-05443"><label>1.</label><element-citation publication-type="webpage"><article-title>Almost Half of Us Worldwide Are Neglecting Oral Healthcare: WHO Report</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://news.un.org/en/story/2022/11/1130782" ext-link-type="uri">https://news.un.org/en/story/2022/11/1130782</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-12">(accessed on 12 July 2025)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-05443"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tin-Oo</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Saddki</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hassan</surname><given-names>N.</given-names></name></person-group><article-title>Factors influencing patient satisfaction with dental appearance and treatments they desire to improve aesthetics</article-title><source>BMC Oral Health</source><year>2011</year><volume>11</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1186/1472-6831-11-6</pub-id><pub-id pub-id-type="pmid">21342536</pub-id><pub-id pub-id-type="pmcid">PMC3059271</pub-id></element-citation></ref><ref id="B3-sensors-25-05443"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Newton</surname><given-names>J.T.</given-names></name><name name-style="western"><surname>Prabhu</surname><given-names>N.</given-names></name><name name-style="western"><surname>Robinson</surname><given-names>P.G.</given-names></name></person-group><article-title>The impact of dental appearance on the appraisal of personal characteristics</article-title><source>Int. J. Prosthodont.</source><year>2003</year><volume>16</volume><fpage>429</fpage><pub-id pub-id-type="pmid">12956500</pub-id></element-citation></ref><ref id="B4-sensors-25-05443"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Joiner</surname><given-names>A.</given-names></name></person-group><article-title>Tooth colour: A review of the literature</article-title><source>J. Dent.</source><year>2004</year><volume>32</volume><fpage>3</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.jdent.2003.10.013</pub-id><pub-id pub-id-type="pmid">14738829</pub-id></element-citation></ref><ref id="B5-sensors-25-05443"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McPhee</surname><given-names>E.R.</given-names></name></person-group><article-title>Light and color in dentistry. Part I&#8212;Nature and perception</article-title><source>J. Mich. Dent. Assoc.</source><year>1978</year><volume>60</volume><fpage>565</fpage><lpage>572</lpage><pub-id pub-id-type="pmid">290819</pub-id></element-citation></ref><ref id="B6-sensors-25-05443"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roy</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Podgor</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Collier</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gunkel</surname><given-names>R.D.</given-names></name></person-group><article-title>Color vision and age in a normal North American population</article-title><source>Graefes Arch. Clin. Exp. Ophthalmol.</source><year>1991</year><volume>229</volume><fpage>139</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1007/BF00170545</pub-id><pub-id pub-id-type="pmid">2044973</pub-id></element-citation></ref><ref id="B7-sensors-25-05443"><label>7.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>van der Burgt</surname><given-names>P.</given-names></name></person-group><article-title>Metamerism</article-title><source>Encyclopedia of Color Science and Technology</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2023</year><fpage>1196</fpage><lpage>1199</lpage></element-citation></ref><ref id="B8-sensors-25-05443"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chu</surname><given-names>S.J.</given-names></name><name name-style="western"><surname>Trushkowsky</surname><given-names>R.D.</given-names></name><name name-style="western"><surname>Paravina</surname><given-names>R.D.</given-names></name></person-group><article-title>Dental color matching instruments and systems. Review of clinical and research aspects</article-title><source>J. Dent.</source><year>2010</year><volume>38</volume><fpage>e2</fpage><lpage>e16</lpage><pub-id pub-id-type="doi">10.1016/j.jdent.2010.07.001</pub-id><pub-id pub-id-type="pmid">20621154</pub-id></element-citation></ref><ref id="B9-sensors-25-05443"><label>9.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Berns</surname><given-names>R.S.</given-names></name></person-group><source>Billmeyer and Saltzman&#8217;s Principles of Color Technology</source><edition>4th ed.</edition><publisher-name>John Wiley &amp; Sons</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2019</year></element-citation></ref><ref id="B10-sensors-25-05443"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.N.</given-names></name></person-group><article-title>Comparison of shade matching by visual observation and an intraoral dental colorimeter</article-title><source>J. Oral Rehabil.</source><year>2007</year><volume>34</volume><fpage>848</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2842.2006.01678.x</pub-id><pub-id pub-id-type="pmid">17919252</pub-id></element-citation></ref><ref id="B11-sensors-25-05443"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gardner</surname><given-names>J.L.</given-names></name></person-group><article-title>Comparison of calibration methods for tristimulus colorimeters</article-title><source>J. Res. Natl. Inst. Stand. Technol.</source><year>2007</year><volume>112</volume><fpage>129</fpage><pub-id pub-id-type="doi">10.6028/jres.112.010</pub-id><pub-id pub-id-type="pmid">27110460</pub-id><pub-id pub-id-type="pmcid">PMC4656001</pub-id></element-citation></ref><ref id="B12-sensors-25-05443"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wee</surname><given-names>A.G.</given-names></name><name name-style="western"><surname>Lindsey</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>Kuo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Johnston</surname><given-names>W.M.</given-names></name></person-group><article-title>Color accuracy of commercial digital cameras for use in dentistry</article-title><source>Dent. Mater.</source><year>2006</year><volume>22</volume><fpage>553</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1016/j.dental.2005.05.011</pub-id><pub-id pub-id-type="pmid">16198403</pub-id><pub-id pub-id-type="pmcid">PMC1808262</pub-id></element-citation></ref><ref id="B13-sensors-25-05443"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tam</surname><given-names>W.K.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>H.J.</given-names></name></person-group><article-title>Dental shade matching using a digital camera</article-title><source>J. Dent.</source><year>2012</year><volume>40</volume><fpage>e3</fpage><lpage>e10</lpage><pub-id pub-id-type="doi">10.1016/j.jdent.2012.06.004</pub-id><pub-id pub-id-type="pmid">22713739</pub-id></element-citation></ref><ref id="B14-sensors-25-05443"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tabatabaian</surname><given-names>F.</given-names></name><name name-style="western"><surname>Beyabanaki</surname><given-names>E.</given-names></name><name name-style="western"><surname>Alirezaei</surname><given-names>P.</given-names></name><name name-style="western"><surname>Epakchi</surname><given-names>S.</given-names></name></person-group><article-title>Visual and digital tooth shade selection methods, related effective factors and conditions, and their accuracy and precision: A literature review</article-title><source>J. Esthet. Restor. Dent.</source><year>2021</year><volume>33</volume><fpage>1084</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1111/jerd.12816</pub-id><pub-id pub-id-type="pmid">34498789</pub-id></element-citation></ref><ref id="B15-sensors-25-05443"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Richert</surname><given-names>R.</given-names></name><name name-style="western"><surname>Goujat</surname><given-names>A.</given-names></name><name name-style="western"><surname>Venet</surname><given-names>L.</given-names></name><name name-style="western"><surname>Viguie</surname><given-names>G.</given-names></name><name name-style="western"><surname>Viennot</surname><given-names>S.</given-names></name><name name-style="western"><surname>Robinson</surname><given-names>P.</given-names></name><name name-style="western"><surname>Farges</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Fages</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ducret</surname><given-names>M.</given-names></name></person-group><article-title>Intraoral scanner technologies: A review to make a successful impression</article-title><source>J. Healthc. Eng.</source><year>2017</year><volume>2017</volume><fpage>8427595</fpage><pub-id pub-id-type="doi">10.1155/2017/8427595</pub-id><pub-id pub-id-type="pmid">29065652</pub-id><pub-id pub-id-type="pmcid">PMC5605789</pub-id></element-citation></ref><ref id="B16-sensors-25-05443"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim-Pusateri</surname><given-names>S.</given-names></name><name name-style="western"><surname>Brewer</surname><given-names>J.D.</given-names></name><name name-style="western"><surname>Davis</surname><given-names>E.L.</given-names></name><name name-style="western"><surname>Wee</surname><given-names>A.G.</given-names></name></person-group><article-title>Reliability and accuracy of four dental shade-matching devices</article-title><source>J. Prosthet. Dent.</source><year>2009</year><volume>101</volume><fpage>193</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/S0022-3913(09)60028-7</pub-id><pub-id pub-id-type="pmid">19231572</pub-id></element-citation></ref><ref id="B17-sensors-25-05443"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khurana</surname><given-names>R.</given-names></name><name name-style="western"><surname>Tredwin</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Weisbloom</surname><given-names>M.</given-names></name><name name-style="western"><surname>Moles</surname><given-names>D.R.</given-names></name></person-group><article-title>A clinical evaluation of the individual repeatability of three commercially available colour measuring devices</article-title><source>Br. Dent. J.</source><year>2007</year><volume>203</volume><fpage>675</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1038/bdj.2007.1108</pub-id><pub-id pub-id-type="pmid">18084212</pub-id></element-citation></ref><ref id="B18-sensors-25-05443"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paul</surname><given-names>S.J.</given-names></name><name name-style="western"><surname>Peter</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rodoni</surname><given-names>L.</given-names></name><name name-style="western"><surname>Pietrobon</surname><given-names>N.</given-names></name></person-group><article-title>Conventional visual vs. spectrophotometric shade taking for porcelain-fused-to-metal crowns: A clinical comparison</article-title><source>J. Prosthet. Dent.</source><year>2004</year><volume>92</volume><fpage>577</fpage><pub-id pub-id-type="doi">10.1016/j.prosdent.2004.07.004</pub-id><pub-id pub-id-type="pmid">15227770</pub-id></element-citation></ref><ref id="B19-sensors-25-05443"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Spectral Reflectance Reconstruction Based on Spectral Optimization of Light Source</article-title><source>Proceedings of the 2019 Chinese Control Conference (CCC)</source><conf-loc>Guangzhou, China</conf-loc><conf-date>27&#8211;30 July 2019</conf-date><fpage>7149</fpage><lpage>7153</lpage><pub-id pub-id-type="doi">10.23919/ChiCC.2019.8865562</pub-id></element-citation></ref><ref id="B20-sensors-25-05443"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arXiv.1505.04597</pub-id><pub-id pub-id-type="arxiv">1505.04597</pub-id></element-citation></ref><ref id="B21-sensors-25-05443"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>F.</given-names></name></person-group><article-title>HSCNN+: Advanced CNN-based hyperspectral recovery from RGB image</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date></element-citation></ref><ref id="B22-sensors-25-05443"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Song</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name></person-group><article-title>Adaptive weighted attention network with camera spectral sensitivity prior for spectral reconstruction from RGB images</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date></element-citation></ref><ref id="B23-sensors-25-05443"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Timofte</surname><given-names>R.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>HDNet: High-resolution dual-domain learning for spectral compressive imaging</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;24 June 2022</conf-date></element-citation></ref><ref id="B24-sensors-25-05443"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Timofte</surname><given-names>R.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;24 June 2022</conf-date></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05443-f001" orientation="portrait"><label>Figure 1</label><caption><p>Overview of the proposed SRNet framework, which takes RGB images and light-source images as inputs and outputs spectral reflectance. The framework consists of two subnetworks, SRNet-P and SRNet-L. SRNet-P reconstructs hyperspectral data from the tooth images, while SRNet-L converts light-source images into hyperspectral data. Physical information fusion is achieved by aligning and concatenating the two hyperspectral outputs along the channel dimension. The final spectral reflectance is then obtained by further processing the fused spectral data through SRNet-L. The orange and blue arrows represent incident light from the light box and reflected light, respectively. The black and gray arrows indicate the transition to subsequent steps. The network architecture is introduced in <xref rid="sec2dot2-sensors-25-05443" ref-type="sec">Section 2.2</xref>, and the optical system and dataset construction are detailed in <xref rid="sec3-sensors-25-05443" ref-type="sec">Section 3</xref>. The letter &#8220;C&#8221; denotes concatenation of the two inputs along the channel dimension. <italic toggle="yes">N</italic> indicates the number of repeated modules. The black dashed arrows between modules indicate skip connections.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g001.jpg"/></fig><fig position="float" id="sensors-25-05443-f002" orientation="portrait"><label>Figure 2</label><caption><p>Detailed architecture of SRNet-P, mapping an RGB image input to a hyperspectral image output. The numbers below each layer denote its output dimensions. <italic toggle="yes">N</italic> indicates the number of repeated modules. The modules shown beneath the diagram are color coded: for example, blue represents upsampling layers and red represents downsampling layers. The black dashed arrows between modules indicate skip connections.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g002.jpg"/></fig><fig position="float" id="sensors-25-05443-f003" orientation="portrait"><label>Figure 3</label><caption><p>Detailed structure of SRNet-L. It takes an RGB image as input and outputs spectra with 32 channels. The network consists of three main modules: shallow feature extraction, deep feature extraction, and attention mechanism. In <xref rid="sensors-25-05443-f003" ref-type="fig">Figure 3</xref>, &#8853; denotes element-wise addition, &#8855; denotes multiplication, and &#8220;C&#8221; indicates concatenation along the channel dimension of the two inputs.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g003.jpg"/></fig><fig position="float" id="sensors-25-05443-f004" orientation="portrait"><label>Figure 4</label><caption><p>Experiment setup to capture the hyperspectral images and RGB images.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g004.jpg"/></fig><fig position="float" id="sensors-25-05443-f005" orientation="portrait"><label>Figure 5</label><caption><p>Schematic representation of the experimental imaging setup. The scene light is divided into two paths after a beam splitter. The yellow lines indicate the light-source pathways, while the purple lines represent the optical paths associated with the teeth. After passing through the beam splitter, the light is directed into both the hyperspectral and single-lens reflex cameras for imaging, thereby simultaneously capturing the RGB and hyperspectral images required for the dataset. The red and black dotted lines in the figure indicate the images acquired by the respective cameras.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g005.jpg"/></fig><fig position="float" id="sensors-25-05443-f006" orientation="portrait"><label>Figure 6</label><caption><p>Representative dental samples under different illumination spectra, with varying ceramic thicknesses and augmented viewing angles.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g006.jpg"/></fig><fig position="float" id="sensors-25-05443-f007" orientation="portrait"><label>Figure 7</label><caption><p>Visual comparison of Baseline 1, Baseline 2, and SRNet. Reconstruction results for four selected spectral bands (480 nm, 530 nm, 580 nm, and 630 nm) are shown.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g007.jpg"/></fig><fig position="float" id="sensors-25-05443-f008" orientation="portrait"><label>Figure 8</label><caption><p>Mean squared error (MSE) performance of the three methods evaluated across the full set of 32 spectral bands. SRNet (MSE) and SRNet represent the schemes of &#8220;SRNet + <italic toggle="yes">L<sub>MSE</sub></italic>&#8221; and &#8220;SRNet + <italic toggle="yes">L<sub>overall</sub></italic>&#8221;, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g008.jpg"/></fig><fig position="float" id="sensors-25-05443-f009" orientation="portrait"><label>Figure 9</label><caption><p>SSIM results of the three methods evaluated over all 32 spectral bands. SRNet (MSE) and SRNet represent the schemes of &#8220;SRNet + <italic toggle="yes">L<sub>MSE</sub></italic>&#8221; and &#8220;SRNet + <italic toggle="yes">L<sub>overall</sub></italic>&#8221;, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g009.jpg"/></fig><fig position="float" id="sensors-25-05443-f010" orientation="portrait"><label>Figure 10</label><caption><p>Comparative maps and difference maps of the contrast experiment. The color difference map was obtained by averaging the absolute differences between the reconstructed values and the ground-truth values across 32 wavelengths for each pixel. The comparative maps are shown above, and the difference maps are shown below.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g010.jpg"/></fig><fig position="float" id="sensors-25-05443-f011" orientation="portrait"><label>Figure 11</label><caption><p>SSIM results for HSCNN+, AWAN, MST++, HDNet, and SRNet computed over all 32 spectral bands, illustrating the comparative reconstruction quality across the full spectral range.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g011.jpg"/></fig><fig position="float" id="sensors-25-05443-f012" orientation="portrait"><label>Figure 12</label><caption><p>Mean squared error (MSE) results of HSCNN+, AWAN, MST++, HDNet, and SRNet evaluated across all 32 spectral bands, reflecting the overall reconstruction accuracy of each method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g012.jpg"/></fig><fig position="float" id="sensors-25-05443-f013" orientation="portrait"><label>Figure 13</label><caption><p>Comparative reconstruction results of various model configurations alongside the ground-truth and corresponding difference maps. The methods include (<bold>a</bold>) SRNet, (<bold>b</bold>) SRNet without the dual stat attention (DSA) module, (<bold>c</bold>) SRNet without the local&#8211;global fusion (LGF) module, (<bold>d</bold>) SRNet without the SRNet-P branch (sample input), (<bold>e</bold>) SRNet without the SRNet-L branch (illumination input), (<bold>f</bold>) SRNet without illumination input (illumination replaced by constant 1), and (<bold>g</bold>) ground-truth spectral reflectance. Each row presents the reconstructed spectral reflectance images and their corresponding difference maps relative to the ground-truth.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g013.jpg"/></fig><fig position="float" id="sensors-25-05443-f014" orientation="portrait"><label>Figure 14</label><caption><p>Two random test data pairs including sample input and board input. The red rectangles denote regions of nonuniform illumination, whereas the yellow rectangles indicate areas affected by random occlusions within the scene.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05443-g014.jpg"/></fig><table-wrap position="float" id="sensors-25-05443-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05443-t001_Table 1</object-id><label>Table 1</label><caption><p>Quantitative comparison, parameter count, and inference time between the Baseline 1 and SRNet variants. Lower MSE denotes superior accuracy, whereas higher SSIM reflects improved structural fidelity.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Loss Function</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params (M)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MSE &#8595;</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SSIM &#8593;</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Baseline 1</td><td align="center" valign="middle" rowspan="1" colspan="1">MSE</td><td align="center" valign="middle" rowspan="1" colspan="1">31.04</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>3.008</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0018</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7631</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Baseline 2</td><td align="center" valign="middle" rowspan="1" colspan="1">MSE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>12.816</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">36.8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.643</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SRNet (<italic toggle="yes">L<sub>MSE</sub></italic>)</td><td align="center" valign="middle" rowspan="1" colspan="1">MSE</td><td align="center" valign="middle" rowspan="1" colspan="1">12.832</td><td align="center" valign="middle" rowspan="1" colspan="1">54.22</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0031</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8129</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SRNet (<italic toggle="yes">L<sub>overall</sub></italic>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Structure&#8211;Pixel Loss</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.832</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8724</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>Note: &#8593; indicates higher values are better; &#8595; indicates lower values are better. Bold values indicate the best performance.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05443-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05443-t002_Table 2</object-id><label>Table 2</label><caption><p>MSE and SSIM of state-of-the-art methods and SRNet.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="right" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th align="right" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">MST++</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">HSCNN+</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">AWAN</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">HDNet</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">SRNet (Ours)</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metrics</th><th align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th></tr></thead><tbody><tr><td colspan="2" align="center" valign="middle" rowspan="1">MSE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0009</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0011</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0014</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0024</td></tr><tr><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">SSIM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8409</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8085</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8152</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8245</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8724</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Bold values indicate the best performance.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05443-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05443-t003_Table 3</object-id><label>Table 3</label><caption><p>MSE and SSIM performance metrics across reconstruction methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="right" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th align="right" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">SRNet w/o DSA</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">SRNet-P Only</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">SRNet w/o Illumination</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">SRNet</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metrics</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th></tr></thead><tbody><tr><td colspan="2" align="center" valign="middle" rowspan="1">MSE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0032</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0024</td><td align="center" valign="middle" rowspan="1" colspan="1">0.083</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.0024</bold>
</td></tr><tr><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">SSIM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8413 </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8393 </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3442</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8724</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Bold values indicate the best performance.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>