<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431292</article-id><article-id pub-id-type="pmcid-ver">PMC12431292.1</article-id><article-id pub-id-type="pmcaid">12431292</article-id><article-id pub-id-type="pmcaiid">12431292</article-id><article-id pub-id-type="doi">10.3390/s25175544</article-id><article-id pub-id-type="publisher-id">sensors-25-05544</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Co-Operative Perception System for Collision Avoidance Using C-V2X and Client&#8211;Server-Based Object Detection</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Park</surname><given-names initials="J">Jungme</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-05544" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Kavathekar</surname><given-names initials="V">Vaibhavi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Bhuduri</surname><given-names initials="S">Shubhang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Amin</surname><given-names initials="MH">Mohammad Hasan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Devaraj</surname><given-names initials="SS">Sriram Sanjeev</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Mart&#237;n-Sacrist&#225;n</surname><given-names initials="D">David</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Garcia-Roger</surname><given-names initials="D">David</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05544">College of Engineering, Kettering University, Flint, MI 48504, USA; <email>kava6160@kettering.edu</email> (V.K.); <email>bhud6825@kettering.edu</email> (S.B.); <email>amin3672@kettering.edu</email> (M.H.A.); <email>deva8103@kettering.edu</email> (S.S.D.)</aff><author-notes><corresp id="c1-sensors-25-05544"><label>*</label>Correspondence: <email>jpark@kettering.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5544</elocation-id><history><date date-type="received"><day>19</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>31</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>05</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 12:25:22.357"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05544.pdf"/><abstract><p>With the recent 5G communication technology deployment, Cellular Vehicle-to-Everything (C-V2X) significantly enhances road safety by enabling real-time exchange of critical traffic information among vehicles, pedestrians, infrastructure, and networks. However, further research is required to address real-time application latency and communication reliability challenges. This paper explores integrating cutting-edge C-V2X technology with environmental perception systems to enhance safety at intersections and crosswalks. We propose a multi-module architecture combining C-V2X with state-of-the-art perception technologies, GPS mapping methods, and the client&#8211;server module to develop a co-operative perception system for collision avoidance. The proposed system includes the following: (1) a hardware setup for C-V2X communication; (2) an advanced object detection module leveraging Deep Neural Networks (DNNs); (3) a client&#8211;server-based co-operative object detection framework to overcome computational limitations of edge computing devices; and (4) a module for mapping GPS coordinates of detected objects, enabling accurate and actionable GPS data for collision avoidance&#8212;even for detected objects not equipped with C-V2X devices. The proposed system was evaluated through real-time experiments at the GMMRC testing track at Kettering University. Results demonstrate that the proposed system enhances safety by broadcasting critical obstacle information with an average latency of 9.24 milliseconds, allowing for rapid situational awareness. Furthermore, the proposed system accurately provides GPS coordinates for detected obstacles, which is essential for effective collision avoidance. The technology integration in the proposed system offers high data rates, low latency, and reliable communication, which are key features that make it highly suitable for C-V2X-based applications.</p></abstract><kwd-group><kwd>AD</kwd><kwd>ADAS</kwd><kwd>C-V2X</kwd><kwd>client&#8211;server protocol</kwd><kwd>GPS mapping</kwd><kwd>object detection</kwd><kwd>DNN</kwd></kwd-group><funding-group><award-group><funding-source>US National Science Foundation (NSF)</funding-source><award-id>2128346</award-id></award-group><funding-statement>This research was funded by the US National Science Foundation (NSF), grant number 2128346.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05544"><title>1. Introduction</title><p>Perceiving the environment is essential to Intelligent Transportation Systems (ITS) for Autonomous Driving (AD) and Advanced Driver Assistance Systems (ADAS). Recent advancements in computer vision and Deep Neural Networks (DNNs) have achieved state-of-the-art performance in detecting objects on roads [<xref rid="B1-sensors-25-05544" ref-type="bibr">1</xref>]. Due to recent research, applying Artificial Intelligence (AI) to computer vision, specifically Convolutional Neural Networks (CNNs), has achieved significant advancements. CNNs can learn hierarchical image features and have demonstrated exceptional effectiveness in object detection [<xref rid="B2-sensors-25-05544" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05544" ref-type="bibr">3</xref>]. Nowadays, many vehicles are equipped with sensors that monitor their surroundings using AI-based environment perception algorithms. However, these environmental perception systems can generate erroneous outputs for various reasons, such as limited visibility. Although perfect environmental perception systems do not exist, many researchers are working to mitigate this issue by using redundant sensors, developing sensor-fused systems, and sharing traffic information using Vehicle-To-Everything (V2X) communication technology.</p><p>Based on the report by the National Transportation Safety Board (NTSB) [<xref rid="B4-sensors-25-05544" ref-type="bibr">4</xref>], many studies have confirmed that V2X technology significantly improves road safety. Thus, the NTSB has consistently recommended V2X technology for its potential to prevent crashes, as demonstrated in various investigations over the years. However, there are still challenges. Many experiments for V2X capabilities are often conducted in simulation environments rather than real-world settings [<xref rid="B5-sensors-25-05544" ref-type="bibr">5</xref>]. Connected and Autonomous Vehicles (CAVs), pivotal in advancing ITS, are the subject of significant research in both academic and industrial sectors. CAVs employ Cellular Vehicle-to-Everything (C-V2X) communication, including vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), vehicle-to-network (V2N), and vehicle-to-pedestrian (V2P) [<xref rid="B6-sensors-25-05544" ref-type="bibr">6</xref>]. C-V2X communication enhances vehicle situational awareness and supports non-line-of-sight safety features for collision avoidance and accident warnings.</p><p>This paper proposes an advanced collaborative perception system for collision avoidance using C-V2X and client&#8211;server-based object detection to enhance driving safety, particularly at intersections and crosswalks. The proposed system aims to implement a low-latency, advanced co-operative environmental perception framework for collision avoidance and warning by enabling the sharing of critical traffic information essential to ADAS and AD. Unlike simulations, all test results and conclusions are based on integrating a real C-V2X communication system, ensuring practical relevance. The proposed system combines cutting-edge technologies from multiple domains, including C-V2X communication, computer vision with AI for object detection, GPS coordinates mapping from 2D image coordinates, and a client&#8211;server protocol, to enable co-operative AI.</p><p>The optimal setup in C-V2X communications involves radio devices, typically called Road Side Units (RSUs) and On-Board Units (OBUs). In <xref rid="sensors-25-05544-f001" ref-type="fig">Figure 1</xref>, the overall architecture of the proposed system is presented. Initially, infrastructure for 5G C-V2X was deployed at the intersection area of the testing track in the GM Mobility Research Center (GMMRC) [<xref rid="B7-sensors-25-05544" ref-type="bibr">7</xref>] at Kettering University. This infrastructure included one RSU, one camera, and one edge computer. OBUs are installed within vehicles, transmitting vehicle-specific information to the RSUs while concurrently receiving data about traffic and surrounding conditions from RSUs. The edge computing device in the infrastructure is connected to the server computer via a client&#8211;server protocol to enable rapid data processing for the DNN-based object detection algorithm. A module that maps the detected objects&#8217; GPS locations is developed to provide helpful information about detected objects. This GPS mapping algorithm finds the corresponding GPS coordinates from the 2D image coordinates. Basic Safety Messages (BSMs) are constructed by aggregating critical traffic information, including the GPS coordinates of detected objects, their object types, and associated confidence scores. The BSMs are transmitted from the RSU to OBUs using 5G sidelink technology. This real-time traffic information sharing supports proactive collision avoidance by enabling vehicles to anticipate and respond to potential road hazards.</p><p>The remainder of this paper is organized as follows. <xref rid="sec2-sensors-25-05544" ref-type="sec">Section 2</xref> presents an extensive literature review on 5G C-V2X systems and their applications. <xref rid="sec3-sensors-25-05544" ref-type="sec">Section 3</xref> describes the methodology for developing a co-operative perception system, integrating C-V2X hardware, edge computing devices, a camera sensor paired with a DNN for object detection via a client&#8211;server protocol, and a GPS coordinate mapping module. <xref rid="sec4-sensors-25-05544" ref-type="sec">Section 4</xref> presents the real-time experimental results, including processing times across different computing platforms, the accuracy of GPS coordinate mapping, and C-V2X communication latency. Finally, <xref rid="sec5-sensors-25-05544" ref-type="sec">Section 5</xref> summarizes the key findings and insights derived from the study.</p></sec><sec id="sec2-sensors-25-05544"><title>2. Related Work</title><p>The IEEE 802.11p [<xref rid="B8-sensors-25-05544" ref-type="bibr">8</xref>] is an amendment of the IEEE 802.11 standard, standardizing wireless local area networks (WLANs). The 802.11p standard was specifically designed to support wireless connectivity in vehicular environments, enabling vehicles to share traffic information through direct communication with each other and with roadside infrastructure. It introduces Wireless Access in Vehicular Environments (WAVE) at both the physical and Medium Access Control (MAC) layers. The IEEE 802.11p standard [<xref rid="B8-sensors-25-05544" ref-type="bibr">8</xref>] provides the base for the well-known early V2X technology, Dedicated Short-Range Communication (DSRC) [<xref rid="B9-sensors-25-05544" ref-type="bibr">9</xref>]. Since its inception, DSRC has been in use for almost two decades. These technologies established the groundwork for modern vehicular communication, which is essential for developing ITSs. However, these technologies have a limited transmission range and cannot be integrated with existing cellular mobile networks. As a result, they have not achieved widespread commercial success [<xref rid="B10-sensors-25-05544" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05544" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05544" ref-type="bibr">12</xref>].</p><p>C-V2X [<xref rid="B13-sensors-25-05544" ref-type="bibr">13</xref>] emerged to enhance scalability, which can operate across the 5.9 GHz and cellular spectrum, thus facilitating long-range communication between vehicles and their environment. Compared to DSRC, C-V2X can cover a more extended range with higher reliability [<xref rid="B14-sensors-25-05544" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05544" ref-type="bibr">15</xref>]. C-V2X, developed based on cellular systems, has evolved from Long-Term Evolution (LTE)-V2X to New Radio (NR)-V2X. It provides low-latency, high-reliability, and high-throughput communications for various C-V2X applications [<xref rid="B11-sensors-25-05544" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05544" ref-type="bibr">12</xref>]. The development of the C-V2X standard within the 3rd Generation Partnership Project (3GPP) and its deployment milestones in the 5G Automotive Association (5GAA), along with the associated chipsets, were summarized in [<xref rid="B10-sensors-25-05544" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05544" ref-type="bibr">11</xref>]. C-V2X can be implemented using two different transmission modes: (1) Direct Communication: Direct communication enables direct communication between vehicles, infrastructure, and road pedestrians, operating independently of cellular networks via the PC5 interface. (2) Cellular Network Communication: In this mode, C-V2X utilizes the standard mobile network to deliver information to vehicles about road and traffic conditions in the vicinity. This is achieved using the Uu interface, a radio interface that links User Equipment (UE) to the Radio Access Network (RAN) [<xref rid="B11-sensors-25-05544" ref-type="bibr">11</xref>].</p><p>Latency remains one of the biggest challenges in deploying V2X applications. To tackle this, many researchers have explored or reviewed approaches like task offloading and resource allocation aimed at reducing latency and improving reliability in V2X systems [<xref rid="B16-sensors-25-05544" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05544" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05544" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05544" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05544" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05544" ref-type="bibr">21</xref>]. In [<xref rid="B16-sensors-25-05544" ref-type="bibr">16</xref>], the authors proposed specific resource allocation strategies and optimization methods to improve network latency performance. In the proposed methods, combining adaptive spectrum selection with hierarchical scheduling reduces latency and improves reliability. They contributed an adaptive, latency-aware approach to resource allocation. However, its real-world applicability is limited, as it depends heavily on cellular coverage and has not yet been thoroughly validated through practical testing. The authors in [<xref rid="B17-sensors-25-05544" ref-type="bibr">17</xref>] point out that latency is one of the key challenges in rolling out LTE-V2X for AD. Their study shows that latency can be affected by several factors, such as antenna height, network coverage, and environmental conditions. On average, they observed latency around 50 ms in major European cities&#8212;but under certain conditions, it could spike to anywhere between 150 and 350 ms. In [<xref rid="B18-sensors-25-05544" ref-type="bibr">18</xref>], a new approach, the vehicle-to-vehicle-to-infrastructure (V2V2I) paradigm for delay-aware offloading, is introduced. A source vehicle not within range of an RSU sends its data through one or more nearby vehicles using V2V communication. These intermediate hops help the data reach another car connected to an RSU, which then forwards the information via V2I. A centralized MEC server handles this by predicting which V2V2I paths can meet a set delay limit, making sure the data is delivered on time. The authors say their simulations show that this innovative, delay-aware offloading method works better than the traditional approach, where offloading only happens if the source vehicle is directly connected to an RSU. However, because their evaluation is based solely on simulations, its immediate real-world applicability may be limited. The authors in [<xref rid="B20-sensors-25-05544" ref-type="bibr">20</xref>] introduce how V2X is being developed within 3GPP, especially as mobile networks move toward 5G with ultra-reliable, low-latency communications support. They highlight ongoing efforts by 3GPP and oneM2M to improve end-to-end data delivery, which is really important for connected vehicles. However, the paper tends to oversimplify the challenges in integrating legacy systems and edge computing, leaving important questions about operational readiness and full end-to-end reliability mostly unanswered.</p><p>In recent years, the field of AD technologies has seen the conduct of numerous scholarly surveys [<xref rid="B22-sensors-25-05544" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05544" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05544" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05544" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05544" ref-type="bibr">26</xref>], focusing on state-of-the-art developments, conventional methodologies, advanced deep learning approaches, and strategies emphasizing communication efficiency. Tasks such as object detection and classification using DNNs are typically resource-intensive, requiring significant computational power, memory, and energy. A key challenge lies in the vast volume of sensor data processed by DNN algorithms for AD services, which can severely strain the computational efficiency of embedded systems with limited memory. This underscores the need to optimize DNN algorithms for embedded devices to improve computational performance. In [<xref rid="B27-sensors-25-05544" ref-type="bibr">27</xref>], the authors proposed a modular edge inference framework that combines split-point selection, model compression, and task-oriented encoding to better balance computation and communication. Their approach showed latency improvements in controlled tests. However, its real-world impact remains uncertain due to the absence of validation in practical deployment scenarios. In [<xref rid="B28-sensors-25-05544" ref-type="bibr">28</xref>], the authors suggest splitting and combining the heavy layers in DNNs to work around memory limits on edge devices. This way, the workload can be spread out across edge clusters, which reduces communication overhead and speeds up inference. Nevertheless, concerns remain regarding its scalability and robustness in real-world deployments since the evaluation was conducted in stable simulated environments.</p><p>In [<xref rid="B29-sensors-25-05544" ref-type="bibr">29</xref>], the authors present an application of C-V2X technology for daytime visibility detection and prewarning on expressways. Their prewarning system uses C-V2X communication between the OBU and RSU, achieving a coverage radius of over 500 m and an impressively low end-to-end transmission delay of 30&#8211;35 ms. A strength of this work is its real-world implementation on an actual expressway. However, due to the absence of nearby meteorological stations and the legal and safety issues around placing distance markers on expressways, the authors were unable to calibrate the actual visibility-distance values precisely. Additionally, the detection method, which relies on contrast-based image processing, struggles with distance estimation near image boundaries, which can affect accuracy. In [<xref rid="B30-sensors-25-05544" ref-type="bibr">30</xref>], the authors present an experimental study on a V2X application to improve road safety through a vulnerable road user collision warning system. The system works by processing camera input on the AI server to identify whether the detected object is a pedestrian or a vehicle. This information is transmitted to the RSUs via fiber optic cables and a 4G telecom network, relaying it wirelessly to vehicles in range. Each vehicle&#8217;s OBU receives the processed data and combines it with GNSS information from the car&#8217;s control unit. The study shows that the proposed system works effectively and has promising potential to improve safety in AD. However, the study also points out some limitations, namely, that manually entering geographic data such as latitude, longitude, and elevation can cause inaccuracies, potentially affecting the promptness of the warning messages.</p><p>While prior studies have made progress in vehicular communication and edge-assisted perception, they often lack real-world validation and detailed analysis of communication latency for real-time deployment. More research is needed to address this gap by implementing time-sensitive C-V2X applications and validating transmission events under realistic conditions to provide accurate latency benchmarks.</p></sec><sec id="sec3-sensors-25-05544"><title>3. Development of a Co-Operative Perception System Using C-V2X and the Client&#8211;Server Model</title><p>To enable a co-operative perception system for collision avoidance at intersections and crosswalks, the proposed system needs the integration of several advanced technologies. These include C-V2X communication, computer vision with AI for object detection, a GPS mapping algorithm to generate objects&#8217; GPS coordinates, and a client&#8211;server communication protocol. First, to facilitate communication between infrastructure and vehicles, deploying telematics units&#8212;specifically OBUs and RSUs&#8212;is necessary to ensure seamless data exchange. A DNN-based AI module is deployed on an edge computing device at the intersection to monitor the intersection area for real-time object detection. However, due to the high computational demands of the DNN module, a client&#8211;server architecture is used to compensate for the limited processing power of the edge computing device. The detected objects&#8217; accurate locations must be broadcast in real time to make the camera-detected objects at the infrastructure helpful for nearby vehicles. To achieve this, this paper proposes two innovative GPS coordinate mapping algorithms to enhance the precision of object localization.</p><sec id="sec3dot1-sensors-25-05544"><title>3.1. C-V2X Hardware Setup for Communication</title><p>The traffic information shared through C-V2X communication improves road safety by making vehicles aware of their surroundings, such as other cars, pedestrians, and cyclists, especially near intersections. Several devices are needed to implement the C-V2X communication system for the co-operative perception system at intersection areas, including 5G communication devices, camera sensors, and edge computing devices, as shown in <xref rid="sensors-25-05544-f002" ref-type="fig">Figure 2</xref> and <xref rid="sensors-25-05544-f003" ref-type="fig">Figure 3</xref>. As presented in <xref rid="sensors-25-05544-f002" ref-type="fig">Figure 2</xref>, the infrastructure consists of one RSU, one stereo camera, and one NVIDIA Jetson device. The Zed 2 camera by Stereolabs [<xref rid="B31-sensors-25-05544" ref-type="bibr">31</xref>] was chosen to monitor the intersection area.</p><p>For the RSU device at infrastructure, the Cohda Mk6C RSU [<xref rid="B32-sensors-25-05544" ref-type="bibr">32</xref>] was selected due to its 20 MHz bandwidth and compatibility with the 5.9 GHz ITS spectrum. Traffic information is sent from the edge computing unit to the RSU using the User Datagram Protocol (UDP), which is known for its low-latency communication. The Cohda Mk6C RSU&#8217;s communication framework, as shown in <xref rid="sensors-25-05544-f002" ref-type="fig">Figure 2</xref>, is specially designed for reliable communication in dynamic environments. This setup allows the RSU to broadcast WAVE short messages, which are crucial for V2X communication [<xref rid="B33-sensors-25-05544" ref-type="bibr">33</xref>].</p><p>The OBUs can be used for smooth communication with the RSU or nearby OBUs. The OBUs are easily integrated into vehicles, as shown in <xref rid="sensors-25-05544-f003" ref-type="fig">Figure 3</xref>. The GPS receiver is placed on the roof of the testing vehicle. The GPS module used in this system should be placed under open-sky conditions, with the GPS signal assumed to be consistently available and unaffected by significant obstructions such as tall buildings or dense foliage. Using the Cohda Wireless MK6C EVK as the OBU [<xref rid="B34-sensors-25-05544" ref-type="bibr">34</xref>], it provides a 20 MHz bandwidth and works within the 5.9 GHz ITS spectrum. When the OBU receives WAVE short messages, it decodes them and sends the data to the vehicle&#8217;s computing unit through an Ethernet connection with the UDP. The edge computing device processes the received information to support the vehicle&#8217;s autonomous or driver-assistance systems [<xref rid="B33-sensors-25-05544" ref-type="bibr">33</xref>].</p></sec><sec id="sec3dot2-sensors-25-05544"><title>3.2. DNN Based Object Detection System</title><p>Over the last few decades, DNN-based object detection has achieved state-of-the-art performance in various computer vision tasks, even in complex environments. Among these models, YOLO (You Only Look Once) [<xref rid="B35-sensors-25-05544" ref-type="bibr">35</xref>] has gained widespread attention and adoption due to its high inference speed and relatively strong accuracy.</p><p>The YOLO algorithm has undergone continuous updates to improve both speed and accuracy [<xref rid="B36-sensors-25-05544" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05544" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05544" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05544" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05544" ref-type="bibr">40</xref>]. Among the many YOLO versions, YOLOv5 [<xref rid="B38-sensors-25-05544" ref-type="bibr">38</xref>] was selected for this study because it offers a well-balanced trade-off between speed, accuracy, and ease of use&#8212;factors that align closely with our research requirements. Although newer versions such as YOLOv6 to YOLOv8 [<xref rid="B39-sensors-25-05544" ref-type="bibr">39</xref>] provide incremental improvements, YOLOv5 remains a robust and widely adopted solution for real-time object detection tasks. Given that the proposed co-operative perception system requires low-latency processing and efficient computation on mobile platforms, YOLOv5 provides an optimal balance between detection accuracy and computational efficiency.</p><p>YOLOv5 consists of the following three parts: backbone, neck, and head, as shown in <xref rid="sensors-25-05544-f004" ref-type="fig">Figure 4</xref>. YOLOv5 [<xref rid="B38-sensors-25-05544" ref-type="bibr">38</xref>] uses CSPDarknet53 as a feature extraction backbone, which is inspired by CSPNet (Cross Stage Partial Network) that enhances feature learning while reducing computational cost. In the neck of YOLOv5, a combination of Path Aggregation Network (PANet) and Feature Pyramid Network (FPN) fuses information for better feature extraction [<xref rid="B40-sensors-25-05544" ref-type="bibr">40</xref>]. PANet creates a bottom-up path to pass low-level features back into deeper layers, improving localization and small object detection. FPN is represented by the upsample and concat operations in the neck. It merges high-level semantic features from deeper layers with low-level spatial features from shallower layers, enhancing multi-scale feature detection. In YOLOv5&#8217;s head, the detection results are generated in detected objects&#8217; bounding boxes, object detection scores, and class probabilities, similar to previous YOLO versions [<xref rid="B40-sensors-25-05544" ref-type="bibr">40</xref>] but with improved anchor box predictions and post-processing.</p><p><xref rid="sensors-25-05544-f005" ref-type="fig">Figure 5</xref> presents the detection results by the YOLOv5 model. The system is designed to detect two types of target objects: pedestrians and cars. The demo image displays the detected object classes and their confidence scores, where the target objects are correctly classified with high confidence.</p></sec><sec id="sec3dot3-sensors-25-05544"><title>3.3. Client&#8211;Server Protocol for Co-Operative AI</title><p>Receiving traffic updates regarding hazards beyond the vehicle&#8217;s line of sight is critical for road safety. C-V2X applications often necessitate performing tasks that require both low latency and high computational power. A typical example is the real-time processing of image-based object recognition, such as identifying pedestrians or other vehicles, which demands extensive computation through deep CNNs [<xref rid="B2-sensors-25-05544" ref-type="bibr">2</xref>]. In many cases, such processing demands are often beyond the capabilities of standard edge computing devices [<xref rid="B3-sensors-25-05544" ref-type="bibr">3</xref>].</p><p>This research aims to develop a C-V2X application where the edge computing device connected to the RSU captures a livestream at an intersection, conducts object detection on the livestream, and then the RSU broadcasts to nearby OBUs by sending BSMs. However, capturing the livestream data from the camera sensor and applying the DNN-based object detection to the image data on the standalone edge computing device often increases the processing time significantly. This delay is due to hardware limitations and the high computational load of simultaneously processing the livestream and object detection on a single device. Such delays could be critical to deploying the system for real-time applications where quick decision-making is vital for road safety. In many cases, edge computing devices in the C-V2X systems have limited processing power, making it challenging to handle real-time tasks. To mitigate this issue, the client&#8211;server model is used in this research to help offload computational tasks from the edge computing device.</p><p>In a client&#8211;server model, a client is typically an application or device that requests services from a server. It generally initiates communication and is responsible for processing the information or service provided by the server. A server receives and processes client requests and then sends the necessary information or service back to the client. Servers typically have more computational power, storage, and resources than clients [<xref rid="B41-sensors-25-05544" ref-type="bibr">41</xref>].</p><p>Two key frameworks in client&#8211;server models are the Open Systems Interconnection (OSI) Model and the Transmission Control Protocol/Internet Protocol (TCP/IP) Model, which define how computer systems communicate over a network [<xref rid="B41-sensors-25-05544" ref-type="bibr">41</xref>]. One of the primary differences between the two models is that the OSI model divides multiple functions into separate layers, while the TCP/IP model consolidates them into fewer layers [<xref rid="B41-sensors-25-05544" ref-type="bibr">41</xref>]. The TCP/IP model is more practical and widely used in most modern networks.</p><p><xref rid="sensors-25-05544-f006" ref-type="fig">Figure 6</xref> presents the steps involved in the TCP/IP model. The application layer in the client generates raw data, which is passed to the transport layer and encapsulated into a TCP or UDP segment, depending on the protocol. This layer adds a header with details such as source port, destination port, and sequence number. The segment then moves to the Internet layer, where the IP protocol adds its header containing the source and destination IP addresses. Then, the IP packet is passed to the network access layer for physical transmission using Wi-Fi or Ethernet protocols [<xref rid="B42-sensors-25-05544" ref-type="bibr">42</xref>]. On the receiving host, the process occurs in reverse order.</p><p>Socket programming establishes client&#8211;server communication, where a socket serves as an endpoint for sending or receiving data over a network. Socket programming enables data exchange between clients and servers through network connections. The client&#8211;server communication process involves capturing real-time video streams using an edge computing device attached to an RSU sending this livestream data to a server computer. The server computer conducts object detection using the DNN model, such as YOLOv5 [<xref rid="B38-sensors-25-05544" ref-type="bibr">38</xref>]. The server identifies and locates objects in the livestream, generating bounding boxes around detected objects and classification labels (e.g., vehicle and pedestrian). After object detection and processing, the server returns the detection information to the client, which then displays the results, as shown in <xref rid="sensors-25-05544-f005" ref-type="fig">Figure 5</xref>.</p></sec><sec id="sec3dot4-sensors-25-05544"><title>3.4. Calculating GPS Locations for the Detected Objects</title><p>To develop a reliable co-operative perception system for collision avoidance, accurate locations of detected objects are critical for vehicles that utilize received traffic information. If the GPS coordinates of detected objects are broadcast, each nearby vehicle can assess potential hazards by calculating the distance between the broadcasted object&#8217;s GPS location and its own.</p><p>When a detected object is a vehicle equipped with an OBU, it can transmit its GPS information to nearby C-V2X devices. The infrastructure at the intersection&#8212;comprising a camera and an RSU&#8212;can then correlate the detected objects in the image with the received GPS data. However, this approach does not apply to all detected objects, such as pedestrians, non-equipped vehicles, or other objects without OBUs. In such cases, a mapping algorithm that estimates GPS coordinates from image pixel coordinates becomes necessary.</p><p>This paper reviews the existing mapping algorithm, homogeneous transformation, and proposes two new mapping methods to map image pixel coordinates to GPS coordinates: one based on mapping map generation and another using a machine learning-based approach.</p><sec id="sec3dot4dot1-sensors-25-05544"><title>3.4.1. Mapping Through Homogeneous Transformation</title><p>The homogeneous transformation method is a mapping algorithm [<xref rid="B29-sensors-25-05544" ref-type="bibr">29</xref>]. Equation (1) presents the homogeneous transformation method, where <italic toggle="yes">M</italic> is a projection matrix, <italic toggle="yes">M</italic> &#8712; <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD1-sensors-25-05544"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mtext>&#160;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The coordinates (<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) in Equation (1) represent the image pixel coordinates obtained by the camera, while <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the corresponding GPS coordinates, longitude and latitude, respectively. In Equation (1), the projection matrix <italic toggle="yes">M</italic> maps the image coordinates to GPS coordinates through a homogeneous transform [<xref rid="B29-sensors-25-05544" ref-type="bibr">29</xref>].</p><p>If there are more than eight pairs of image pixel coordinates (<inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and GPS points (<inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), the projection matrix <italic toggle="yes">M</italic> can be found using a Direct Linear Transformation (DLT) method [<xref rid="B29-sensors-25-05544" ref-type="bibr">29</xref>]. With the projection matrix <italic toggle="yes">M</italic>, the corresponding GPS coordinates of the given image coordinates can be found using Equation (1).</p><p>A homogeneous transformation method to map image pixels to GPS coordinates offers a simple and computationally efficient way to relate 2D image points to real-world locations, especially when the scene is approximately planar, such as flat road surfaces at intersections. It is easy to implement, works well with sufficient point correspondences, and can be made robust using techniques like RANSAC [<xref rid="B43-sensors-25-05544" ref-type="bibr">43</xref>]. However, this method does not account for the Earth&#8217;s curvature inherent in GPS coordinates. As a result, it becomes inaccurate in non-planar scenes or larger-scale areas where 3D effects and geographic distortions are non-negligible.</p></sec><sec id="sec3dot4dot2-sensors-25-05544"><title>3.4.2. Mapping Through Map Generation</title><p>Mapping image coordinates to GPS coordinates can be implemented by generating a mapping map utilizing C-V2X communication. A single test vehicle equipped with an OBU drove the intersection area at the GMMRC proving ground for this task. As the car navigated the intersection area shown in <xref rid="sensors-25-05544-f007" ref-type="fig">Figure 7</xref>a, the OBU continuously broadcasted real-time GPS information to the nearby RSU. This information was then relayed to an edge computing device connected to the RSU. Simultaneously, the edge computer connected to the RSU used the DNN model to detect the test vehicle within camera images. The testing vehicle&#8217;s position in each frame was represented by the Bounding Box (BB) coordinates (<italic toggle="yes">x</italic><sub>min</sub>, <italic toggle="yes">y</italic><sub>min</sub>, <italic toggle="yes">x</italic><sub>max</sub>, <italic toggle="yes">y</italic><sub>max</sub>) in a 2D image, where the coordinates (<italic toggle="yes">x</italic><sub>min</sub>, <italic toggle="yes">y</italic><sub>min</sub>) represent the upper left corner of the BB and the coordinates (<italic toggle="yes">x</italic><sub>max</sub>, <italic toggle="yes">y</italic><sub>max</sub>) represent the bottom right corner of the BB. The detected vehicle coordinate in the 2D image space and the corresponding GPS coordinates from the OBU in the testing vehicle were recorded and stored together in a file, as illustrated in <xref rid="sensors-25-05544-f007" ref-type="fig">Figure 7</xref>b. Each data sample contains a pair of image coordinates and the corresponding GPS coordinates.</p><p>To generate the mapping map of the image coordinates to GPS coordinates, a dataset of 9031 samples was collected at the GMMRC testing track, specifically by driving through the intersection area several times. A 2D mapping map was created using collected data samples in the same size as the 2D image. The creation of the mapping map can be performed by processing the following steps:</p><list list-type="bullet"><list-item><p>Set the offset pixel <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p>For the coordinates of the ith BB: (<inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</p></list-item></list><p>(i) Calculate the horizontal middle coordinate of the ith BB:<disp-formula id="FD2-sensors-25-05544"><label>(2)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mtext>&#160;</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a function for the unsigned integer.</p><p>(ii) Set the bottom middle coordinates of the <italic toggle="yes">i</italic>th BB as follows:<disp-formula id="FD3-sensors-25-05544"><label>(3)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#160;&#160;</mml:mtext><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(iii) Using <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the ith BB, assign the corresponding GPS coordinate of the ith BB in the mapping map:<disp-formula id="FD5-sensors-25-05544"><label>(4)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#8195;&#8195;</mml:mtext><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05544"><label>(5)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#8195;&#8195;</mml:mtext><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The procedure uses an offset of <italic toggle="yes">&#945;</italic> = 5 pixels to assign identical GPS coordinates to small neighboring pixels belonging to the same object. Once the mapping maps are generated, they are used to determine the GPS coordinates for the real-time detected object in the camera image.</p><p>By utilizing C-V2X communication, mapping maps can be generated in advance, and the process becomes relatively straightforward. However, this approach can only predict the corresponding GPS coordinates for areas that were previously recorded. GPS coordinates cannot be predicted for regions that were not included in the map generation process.</p></sec><sec id="sec3dot4dot3-sensors-25-05544"><title>3.4.3. Mapping Using a Machine Learning Model</title><p>To solve the challenge of mapping image coordinates to GPS coordinates, the dataset of 9031 samples is used to develop a Machine Learning (ML) model. Each data sample contains a pair consisting of image BB coordinates and the corresponding GPS coordinates, as shown in <xref rid="sensors-25-05544-f007" ref-type="fig">Figure 7</xref>b. Since the goal of the mapping is to find out the corresponding GPS coordinates given the image BB coordinates, the input to the ML model are the BB coordinates (<inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mtext>&#160;</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> and the horizontal middle point of the BB, <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The output of the ML model is the corresponding predicted longitude and latitude (<inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as presented in <xref rid="sensors-25-05544-f008" ref-type="fig">Figure 8</xref>. The backpropagation Neural Network (NN) [<xref rid="B44-sensors-25-05544" ref-type="bibr">44</xref>] was used for an ML&#8211;based mapping method that maps corresponding GPS coordinates using the bounding box information of the detected objects in 2D images.</p><p>The NNs were trained by varying the number of hidden nodes in the hidden layers, and the architecture with the best performance was selected. The Levenberg&#8211;Marquardt algorithm [<xref rid="B45-sensors-25-05544" ref-type="bibr">45</xref>] was used to train the NNs. The Mean Squared Error (MSE) cost function shown in Equation (6) was used for the Levenberg&#8211;Marquardt optimization algorithm.<disp-formula id="FD7-sensors-25-05544"><label>(6)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mtext>&#160;</mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> is the total number of samples used during training and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> are the NN-predicted GPS coordinates for the <italic toggle="yes">i</italic>th BB. The best-performing NN architecture with the minimal MSE was selected as shown in <xref rid="sensors-25-05544-f008" ref-type="fig">Figure 8</xref>.</p><p>The selected NN architecture contains four layers: one input layer with five input features, two hidden layers, and one output layer with two output nodes. The hidden layer 1 contains 37 nodes, while the hidden layer 2 contains 19. The notation <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mtext>&#160;</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the <italic toggle="yes">i</italic>th node in the <italic toggle="yes">j</italic>th layer. The bias terms in layers were represented using the subscript 0. Among 9031 data samples, 85% of the data is used for training a backpropagation NN and the remaining 15% for validation. The NN model&#8217;s performance is measured using the Haversine formula [<xref rid="B46-sensors-25-05544" ref-type="bibr">46</xref>]. The Haversine formula in Equations (7) and (8) calculates the distance of two GPS points on the Earth, (<inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and (<inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">R</italic> is the Earth&#8217;s radius (mean radius = 6,371,000 m).<disp-formula id="FD8-sensors-25-05544"><label>(7)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">cos</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#183;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">cos</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#183;</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05544"><label>(8)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>R</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn><mml:mo>(</mml:mo><mml:msqrt><mml:mi>a</mml:mi></mml:msqrt><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msqrt><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>a</mml:mi></mml:msqrt><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec3dot5-sensors-25-05544"><title>3.5. A Proposed Co-Operative Perception System Using C-V2X and the Client&#8211;Server Model</title><p>The proposed system integrates recent cutting-edge technologies, including computer vision, DNNs for object detection, a client&#8211;server model, GPS mapping algorithms, and C-V2X communication to enable co-operative C-V2X for advanced perception systems at intersections and crosswalks. The system requires C-V2X hardware components, as illustrated in <xref rid="sensors-25-05544-f002" ref-type="fig">Figure 2</xref> and <xref rid="sensors-25-05544-f003" ref-type="fig">Figure 3</xref>. <xref rid="sensors-25-05544-f009" ref-type="fig">Figure 9</xref> presents the software architecture of the proposed perception system for collision avoidance using C-V2X and co-operative AI.</p><p>The steps for a co-operative C-V2X perception system for collision avoidance can be as follows: In Module 1, the system captures images of the intersection area using a camera sensor. These images are converted into byte-format BSMs by the client computer and transmitted to the server. In Module 2, the server decodes the received data and runs a DNN-based object detection algorithm, YOLOv5. The detection output&#8212;BB coordinates, object types, and confidence scores&#8212;is then encoded into byte-format BSMs and returned to the client. In Module 3, the client runs a GPS mapping algorithm to determine the GPS coordinates of each detected object, utilizing the bounding box information. In Module 4, the client transmits the processed data&#8212;including object GPS coordinates, types, and confidence scores&#8212;to the RSU using the UDP. The RSU then broadcasts the BSMs via 5G sidelink communication to nearby OBUs. In Module 5, OBUs installed in nearby vehicles forward the encoded data to their edge computing devices. These devices process the received data and can use it for their ADAS applications.</p></sec></sec><sec id="sec4-sensors-25-05544"><title>4. Experimental Results</title><p>The proposed co-operative perception system for collision avoidance with C-V2X and client&#8211;server protocol was fully integrated and tested at the GMMRC proving ground. The infrastructure setup&#8212;illustrated in <xref rid="sensors-25-05544-f002" ref-type="fig">Figure 2</xref>&#8212;was successfully completed, and four test vehicles were equipped with OBUs and edge computing devices, as depicted in <xref rid="sensors-25-05544-f003" ref-type="fig">Figure 3</xref>. Various testing scenarios were conducted to evaluate the following: (1) client&#8211;server model processing time, (2) accuracy of GPS mapping algorithms, (3) C-V2X communication latency, and (4) overall system deployment performance.</p><sec id="sec4dot1-sensors-25-05544"><title>4.1. Evaluation of the Client&#8211;Server Processing Time</title><p>In this project, NVIDIA computing devices such as Jetson Nano [<xref rid="B47-sensors-25-05544" ref-type="bibr">47</xref>] and Jetson Orin [<xref rid="B48-sensors-25-05544" ref-type="bibr">48</xref>] are selected as edge computing devices to implement an advanced perception system using C-V2X and client&#8211;server protocol technologies. The NVIDIA Jetson devices [<xref rid="B49-sensors-25-05544" ref-type="bibr">49</xref>] are deployed on the road, serving infrastructure and testing vehicles. These energy-efficient processing units are designed to deploy AI and robotics applications for real-time inferencing. On the other hand, the NVIDIA Drive Orin [<xref rid="B50-sensors-25-05544" ref-type="bibr">50</xref>] device is chosen as a computing server, providing a powerful computing platform specifically designed for processing high computational tasks such as environmental perception with DNNs.</p><p>To assess the effectiveness of the client&#8211;server model in reducing processing time, five computing configurations were implemented as shown in <xref rid="sensors-25-05544-t001" ref-type="table">Table 1</xref> to run an object detection program, both with and without the client&#8211;server model. <xref rid="sensors-25-05544-t001" ref-type="table">Table 1</xref> presents the processing times, including communication overhead for configurations utilizing the client&#8211;server model if the platform uses the client&#8211;server model.</p><p>The performance comparison between the different computing platforms highlights significant differences in processing times due to the computing power of each platform. Object detection performed on the standalone Drive AGX Orin was highly effective, having an average processing time of 24.56 milliseconds (ms) due to its powerful GPU capabilities. However, the Drive AGX Orin developer kit is relatively expensive, making it unsuitable for deployment as an edge computing device on the testing track. Its high cost makes it an impractical solution for large-scale deployment, especially in testing environments where cost-effectiveness and scalability are key considerations.</p><p>The processing time difference between NVIDIA Jetson Nano and NVIDIA Jetson Orin is due to the difference in their computing hardware capabilities. The Jetson Orin has a powerful GPU and advanced architecture, including more CUDA cores, greater memory bandwidth, and a faster overall processing pipeline. On the other hand, the Jetson Nano is an affordable device with slower memory and fewer cores, resulting in higher processing time. As a result, the average processing time for object detection on the Jetson Nano is 161 ms. On the other hand, the average processing time on the Jetson Orin is 37.65 ms.</p><p>The client&#8211;server model is used to mitigate the computational power limitations of edge computing devices. Two different client&#8211;server platforms were tested. In the first setup, the Jetson Orin computer is used as the client and the Drive Orin as the server. In this configuration, the processing time is 32.83 ms. This client&#8211;server model improves the processing time by 13.26% compared to the standalone Jetson Orin model. In the second platform, the Jetson Nano is used as the client, and the Drive Orin serves as the server. This client&#8211;server model significantly reduces the processing time from 161 ms to 38.61 ms&#8212;an improvement of approximately 76%.</p><p>The client&#8211;server models can offload intensive tasks to a more powerful server (Drive Orin), and both tested configurations demonstrated significant reductions in processing time. The Jetson Orin&#8211;Drive Orin setup achieved a modest improvement of 13.26%, while the Jetson Nano&#8211;Drive Orin configuration showed a substantial 76% reduction. These results indicate that a client&#8211;server architecture can significantly enhance performance, particularly when using lower-end edge devices like the Jetson Nano, which has the lowest computing power among the tested computing devices.</p></sec><sec id="sec4dot2-sensors-25-05544"><title>4.2. Evaluation of C-V2X Communication Latency</title><p>Low communication latency is critical for time-sensitive applications like collision avoidance, where even delays can impact vehicle response times and overall system safety. Therefore, evaluating the communication delay within the C-V2X pipeline is essential to ensure the system meets real-time performance requirements.</p><p>To evaluate the communication delay within the C-V2X pipeline, data was transmitted from an edge computing device connected to the RSU, passed through the RSU, and then received by another edge computing device linked to the OBU. For these C-V2X latency experiments, precise time synchronization between the RSU and the OBU plays a vital role. The C-V2X communication hardware&#8212;specifically the Cohda Wireless OBU and RSU&#8212;utilizes a Network Time Protocol (NTP) daemon [<xref rid="B51-sensors-25-05544" ref-type="bibr">51</xref>] to align system clocks with either GPS signals or NTP servers, thereby maintaining a consistent time framework. For edge computing devices, synchronization over Ethernet or Wi-Fi adjusts internal clocks to match global time standards, providing consistent timestamping.</p><p>A timestamp T<sub>send</sub> was recorded just before transmission from the infrastructure-side computing device, and another timestamp T<sub>received</sub> was captured upon successful reception and decoding at the OBU-side computing device. The transmission latency T<sub>latency</sub> was determined using Equation (9):<disp-formula>T<sub>latency</sub> = T<sub>received</sub> &#8722; T<sub>send</sub><label>(9)</label></disp-formula></p><p>Equation (9) captures the end-to-end delay over the network, explicitly excluding any processing or model inference time, to isolate the pure communication latency of the C-V2X link.</p><p>A total of 29,967 transmission events were evaluated, as shown in <xref rid="sensors-25-05544-f010" ref-type="fig">Figure 10</xref>. In <xref rid="sensors-25-05544-f010" ref-type="fig">Figure 10</xref>, a histogram with nineteen bins is presented, with each bin labeled on the x-axis by its center value. The histogram illustrates the frequency distribution of the measured latencies. The average latency was 9.24 ms, with a minimum latency of 2 ms and a maximum of 20 ms.</p></sec><sec id="sec4dot3-sensors-25-05544"><title>4.3. Evaluation of GPS Prediction Accuracy</title><p>Determining GPS coordinates from 2D image coordinates is critical for utilizing object detection results related to vehicles on the road. If the GPS coordinates of detected obstacles are broadcast by the RSU, each nearby vehicle equipped with an OBU can determine the obstacles&#8217; locations relative to their own position.</p><p>To evaluate the accuracy of the GPS mapping algorithms, including the homogeneous transformation method [<xref rid="B29-sensors-25-05544" ref-type="bibr">29</xref>], the proposed map-based method, and the proposed ML method discussed in <xref rid="sec3dot4-sensors-25-05544" ref-type="sec">Section 3.4</xref>, a test vehicle equipped with an OBU drove through the intersection and transmitted its true GPS coordinates to the RSU located at the infrastructure. The edge computer at the infrastructure performed object detection (with only one object present in the test area for this evaluation) and calculated the detected object&#8217;s GPS location. The true GPS coordinates from the OBU were then compared to the GPS coordinates produced by each mapping algorithm. Mapping accuracy was measured as the distance between the mapped and actual GPS coordinates using the Haversine formula in Equations (7) and (8). <xref rid="sensors-25-05544-t002" ref-type="table">Table 2</xref> presents the experimental results for the three mapping algorithms, obtained at the proving ground of the GMMRC. A total of 1828 testing data points were collected at the GMMRC testing track on a different day from when the training data were gathered. These points were evaluated using three different GPS mapping methods.</p><p>First, a 3 &#215; 3 transformation matrix M was computed from the training data discussed in <xref rid="sec3dot4dot2-sensors-25-05544" ref-type="sec">Section 3.4.2</xref>, using the Direct Linear Transformation (DLT) method [<xref rid="B29-sensors-25-05544" ref-type="bibr">29</xref>] combined with RANSAC. Then, the matrix M was used to map 2D image coordinates to GPS coordinates using Equation (1). The average distance error for the homogeneous transformation method, calculated using Equations (7) and (8), was 5.1627 m with a standard deviation of 4.9129 m. The proposed map-based mapping method, which used the pre-generated map from <xref rid="sec3dot4dot2-sensors-25-05544" ref-type="sec">Section 3.4.2</xref>, achieved a lower average distance error of 2.4472 m than the homogeneous transformation method. In contrast, the proposed NN-based mapping method demonstrated the best performance, with an average distance error of only 1.1914 m and a standard deviation of 2.2552 m, indicating the most consistent measurements, with values closely clustered around the average distance error.</p></sec><sec id="sec4dot4-sensors-25-05544"><title>4.4. Real-Time Deployment of the Co-Operative Perception System for Collision Avoidance</title><p>The entire system was tested using four test vehicles to evaluate the proposed co-operative perception system for collision avoidance at the GMMRC proving ground at Kettering University, as shown in <xref rid="sensors-25-05544-f011" ref-type="fig">Figure 11</xref>a. The evaluation system utilizes a client&#8211;server platform, with the Jetson Orin serving as the client and the Drive Orin as the server for object detection. It also employs the NN-based GPS mapping algorithm for localization.</p><p><xref rid="sensors-25-05544-f011" ref-type="fig">Figure 11</xref>b&#8211;d show examples of object detection results displayed on the computer monitor at the infrastructure. In <xref rid="sensors-25-05544-f012" ref-type="fig">Figure 12</xref>b,c, the test vehicles are shown driving through the intersection and are successfully detected. <xref rid="sensors-25-05544-f011" ref-type="fig">Figure 11</xref>d illustrates pedestrians crossing the road, along with the corresponding detection results. The processing time for object detection using the client&#8211;server model&#8212;where the Jetson Orin serves as the client and the Drive Orin as the server&#8212;is 32.83 ms, corresponding to a processing rate of 30.45 frames per second (FPS). Those results demonstrate the successful implementation of the client&#8211;server model for the proposed co-operative AI.</p><p>Then, the client computer at the infrastructure maps the detected objects&#8217; image coordinates to their corresponding GPS coordinates using the proposed ML-based mapping algorithm. The proposed GPS mapping system is implemented on an edge computer located at the infrastructure, although the system was trained using data collected from a vehicle equipped with an OBU, which was used to generate ground-truth mapping. The RSU at the infrastructure broadcasts the GPS coordinates of detected objects to nearby OBUs in vehicles. Through C-V2X communication, critical information&#8212;including the objects&#8217; GPS coordinates, types, and confidence scores&#8212;is broadcast via PC5 sidelink from the RSU to nearby OBUs. As a result, vehicles equipped with OBUs can receive the GPS coordinates of detected obstacles from the RSU. However, the broadcasted information includes the locations of all detected obstacles, even those not equipped with C-V2X devices, such as pedestrians or vehicles without OBUs.</p><p><xref rid="sensors-25-05544-f012" ref-type="fig">Figure 12</xref> shows that test vehicles successfully received and visualized the real-time critical traffic information transmitted to their edge computing devices. This information includes object types, GPS coordinates, and confidence scores. The data is broadcast live from the RSU at the intersection to nearby OBUs in vehicles. In <xref rid="sensors-25-05544-f012" ref-type="fig">Figure 12</xref>a, the display monitor of one test vehicle shows the locations of three detected pedestrians. Since the GPS coordinates are provided by the RSU, the vehicle can determine the exact positions of the obstacles relative to its own location. It is important to note that these detected pedestrians do not carry any devices; rather, their positions are inferred using the mapping algorithm described in <xref rid="sec3dot4dot3-sensors-25-05544" ref-type="sec">Section 3.4.3</xref>. Similarly, <xref rid="sensors-25-05544-f012" ref-type="fig">Figure 12</xref>b displays the GPS locations of three detected cars, which are broadcast from the RSU to the OBUs. Using the received GPS data, vehicles can accurately determine the locations of these obstacles relative to their own positions. These results show that the system combines fast object detection, accurate GPS mapping, and wide-reaching C-V2X broadcasting. The testing results prove its potential to improve co-operative safety in connected and autonomous vehicle environments.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05544"><title>5. Discussion and Conclusions</title><sec id="sec5dot1-sensors-25-05544"><title>5.1. Discussion</title><p>The implementation and evaluation of the proposed low-latency co-operative perception system for collision avoidance reveal several key insights and implications for ITS. By integrating C-V2X communication, edge computing, and DNN-based object detection, the system presents a practical approach to enhancing road safety at intersections. Utilizing a client&#8211;server architecture effectively mitigates the limited computing capabilities of edge devices, as demonstrated by performance gains. In the first setup, a Jetson Orin is used as the client and a Drive Orin as the server. This configuration reduces processing time by 13.26% compared to the standalone Jetson Orin setup. In a more constrained scenario, the client&#8211;server model, where a Jetson Nano serves as the client and Drive Orin as the server, achieves a 76% reduction in processing time. Such improvements are critical for real-time applications, where timely detection and communication of potential hazards are essential to preventing collisions.</p><p>Additionally, the system&#8217;s average C-V2X latency of 9.24 ms highlights the technology&#8217;s suitability for time-sensitive safety applications. This low latency is vital for enabling real-time interaction between vehicles and infrastructure. The development and evaluation of two innovative GPS mapping algorithms further enhance system reliability and accuracy. The NN-based GPS mapping algorithm achieves an average localization error of 1.1914 m, outperforming the traditional homogeneous transformation-based method. Accurate obstacle localization is essential for making effective collision avoidance decisions. Field tests involving four vehicles equipped with OBUs demonstrated the feasibility of real-time object detection and information dissemination in an intersection environment. These tests validate that the integration of RSUs, OBUs, camera sensors, and edge computing can deliver scalable and practical solutions for improving intersection safety.</p><p>In addition, the proposed framework also enhances situational awareness by pinpointing detected objects on GPS and sharing that information through the RSU via the PC5 sidelink to nearby OBUs. It is worth noting that the broadcast information includes connected vehicles and unconnected road users&#8212;such as pedestrians&#8212;whose positions are inferred through the proposed GPS mapping algorithm. This feature represents a significant advancement over traditional V2X systems, which usually limit data sharing to vehicles that are already equipped. By broadcasting the GPS coordinates of detected obstacles&#8212;including vulnerable road users and other unconnected traffic participants&#8212;the system substantially broadens the scope of co-operative safety measures.</p><p>Nonetheless, certain limitations and opportunities for improvement remain. The current test scenario&#8212;conducted at the GMMRC proving ground at Kettering University&#8212;represents an idealized environment with minimal external interference. The average latency of 9.24 ms, calculated from 19,690 transmission events, provides a useful baseline under controlled conditions. However, real-world environments are more complex and subject to various sources of interference, including network congestion, environmental factors, and competing wireless devices, all of which can affect latency. Future research should focus on evaluating system performance in more dynamic and realistic scenarios to better reflect real-world deployment conditions.</p><p>Moreover, the current study does not address functional safety certification, which is crucial for real-world deployment in compliance with standards such as ISO 26262. Further research is needed to assess and ensure functional safety.</p><p>Finally, system performance may vary depending on the camera&#8217;s field of view (FOV). Incorporating multiple camera setups and leveraging sensor fusion&#8212;by combining camera data with LiDAR and radar&#8212;could significantly improve detection reliability. The proposed system was tested under clear weather conditions (e.g., no heavy rain, fog, or snow) to minimize the impact of environmental factors on camera visibility and overall system performance. Evaluation under adverse weather conditions is beyond the scope of this study and will be addressed in future work. Assessing system performance under such challenging conditions would provide valuable insights into its robustness and practical viability.</p></sec><sec id="sec5dot2-sensors-25-05544"><title>5.2. Conclusions</title><p>This paper presents a low-latency co-operative perception system that leverages C-V2X communication, DNN-based object detection, a client&#8211;server architecture, and GPS-based localization. The proposed system was implemented and tested in a proving ground environment, demonstrating significant latency reduction (with an average of 9.24 ms), reliable GPS-based object localization, and successful real-time C-V2X communication using OBUs and RSUs.</p><p>Several key contributions are highlighted in this work. First, scalable client&#8211;server models are implemented to offload computationally intensive tasks from resource-constrained edge devices. The performance of various client&#8211;server models is thoroughly evaluated, showcasing their impact on system efficiency. Second, two novel GPS mapping algorithms are proposed, both of which outperform traditional techniques in localization accuracy. Using the received GPS data, the vehicles can accurately determine the positions of obstacles relative to their own location. Additionally, the system&#8217;s real-time detection and communication capabilities were validated through field tests, demonstrating its potential for proactive collision avoidance.</p><p>Future work will focus on expanding the system&#8217;s field of view by integrating additional cameras, enhancing robustness through sensor fusion (e.g., by incorporating LiDAR and radar), and evaluating performance in more dynamic, real-world environments. Furthermore, addressing functional safety standards will be crucial for ensuring the system&#8217;s readiness for real-world deployment.</p></sec></sec></body><back><ack><title>Acknowledgments</title><p>We gratefully acknowledge the scholarship support provided by the Robert Bosch Centennial Professorship at Kettering University and the GMMRC at Kettering University, which enabled us to conduct testing for this research at the GMMRC testing track.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.P., V.K. and M.H.A.; methodology, J.P., V.K., S.B. and M.H.A.; software, J.P., V.K., S.B. and M.H.A.; validation, S.B., S.S.D. and V.K.; formal analysis, J.P., V.K., S.B. and S.S.D.; investigation, J.P. and V.K.; resources, S.B. and S.S.D.; data curation, S.B., S.S.D. and V.K.; writing&#8212;original draft preparation, J.P., V.K. and M.H.A.; writing&#8212;review and editing, J.P., V.K. and M.H.A.; visualization, S.B., S.S.D. and V.K.; supervision, J.P.; project administration, J.P.; funding acquisition, J.P. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">AD</td><td align="left" valign="middle" rowspan="1" colspan="1">Autonomous Driving</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ADAS</td><td align="left" valign="middle" rowspan="1" colspan="1">Advanced Driver Assistance Systems</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AI</td><td align="left" valign="middle" rowspan="1" colspan="1">Artificial Intelligence</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BB</td><td align="left" valign="middle" rowspan="1" colspan="1">Bounding Box</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BSM</td><td align="left" valign="middle" rowspan="1" colspan="1">Basic Safety Message</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CAV</td><td align="left" valign="middle" rowspan="1" colspan="1">Connected and Autonomous Vehicle</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">C-V2X</td><td align="left" valign="middle" rowspan="1" colspan="1">Cellular Vehicle-to-Everything</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DLT</td><td align="left" valign="middle" rowspan="1" colspan="1">Direct Linear Transformation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Deep Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSRC</td><td align="left" valign="middle" rowspan="1" colspan="1">Dedicated Short-Range Communication</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FOV</td><td align="left" valign="middle" rowspan="1" colspan="1">Field-of-View</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FPS</td><td align="left" valign="middle" rowspan="1" colspan="1">Frames Per Second</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FPN</td><td align="left" valign="middle" rowspan="1" colspan="1">Feature Pyramid Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GMMRC</td><td align="left" valign="middle" rowspan="1" colspan="1">GM Mobility Research Center</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="left" valign="middle" rowspan="1" colspan="1">Global Navigation Satellite System</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPS</td><td align="left" valign="middle" rowspan="1" colspan="1">Global Positioning System</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Graphics Processing Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ISO</td><td align="left" valign="middle" rowspan="1" colspan="1">International Organization for Standardization</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ITS</td><td align="left" valign="middle" rowspan="1" colspan="1">Intelligent Transportation Systems</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LTE-V2X</td><td align="left" valign="middle" rowspan="1" colspan="1">Long-Term Evolution Vehicle-to-Everything</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MAC</td><td align="left" valign="middle" rowspan="1" colspan="1">Medium Access Control</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MEC</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-Edge Computing</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ML</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine Learning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MSE</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Squared Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NN</td><td align="left" valign="middle" rowspan="1" colspan="1">Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NTP</td><td align="left" valign="middle" rowspan="1" colspan="1">Network Time Protocol</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NR-V2X</td><td align="left" valign="middle" rowspan="1" colspan="1">New Radio Vehicle-to-Everything</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OBU</td><td align="left" valign="middle" rowspan="1" colspan="1">On-Board Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OSI</td><td align="left" valign="middle" rowspan="1" colspan="1">Open Systems Interconnection</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PANet</td><td align="left" valign="middle" rowspan="1" colspan="1">Path Aggregation Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RSU</td><td align="left" valign="middle" rowspan="1" colspan="1">Road Side Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TCP</td><td align="left" valign="middle" rowspan="1" colspan="1">Transmission Control Protocol</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UDP</td><td align="left" valign="middle" rowspan="1" colspan="1">User Datagram Protocol</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UE</td><td align="left" valign="middle" rowspan="1" colspan="1">User Equipment</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UTRAN</td><td align="left" valign="middle" rowspan="1" colspan="1">UMTS Terrestrial Radio Access Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">V2I</td><td align="left" valign="middle" rowspan="1" colspan="1">Vehicle-to-Infrastructure</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">V2N</td><td align="left" valign="middle" rowspan="1" colspan="1">Vehicle-to-Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">V2P</td><td align="left" valign="middle" rowspan="1" colspan="1">Vehicle-to-Pedestrian</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">V2V</td><td align="left" valign="middle" rowspan="1" colspan="1">Vehicle-to-Vehicle</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WAVE</td><td align="left" valign="middle" rowspan="1" colspan="1">Wireless Access in Vehicular Environments</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO</td><td align="left" valign="middle" rowspan="1" colspan="1">You Only Look Once</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05544"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pal</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yadav</surname><given-names>P.</given-names></name><name name-style="western"><surname>Katal</surname><given-names>N.</given-names></name></person-group><article-title>RoadSegNet: A Deep Learning Framework for Autonomous Urban Road Detection</article-title><source>J. Eng. Appl. Sci.</source><year>2022</year><volume>69</volume><fpage>110</fpage><pub-id pub-id-type="doi">10.1186/s44147-022-00162-9</pub-id></element-citation></ref><ref id="B2-sensors-25-05544"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>S.</given-names></name></person-group><article-title>Indoor Localization Using Smartphone Magnetic and Light Sensors: A Deep LSTM Approach</article-title><source>Mob. Netw. Appl.</source><year>2019</year><volume>25</volume><fpage>819</fpage><lpage>832</lpage><pub-id pub-id-type="doi">10.1007/s11036-019-01302-x</pub-id></element-citation></ref><ref id="B3-sensors-25-05544"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vijay</surname><given-names>J.</given-names></name><name name-style="western"><surname>Boyali</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tehrani</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ishimaru</surname><given-names>K.</given-names></name><name name-style="western"><surname>Konishi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Mita</surname><given-names>S.</given-names></name></person-group><article-title>Estimation of Steering Angle and Collision Avoidance for Automated Driving Using Deep Mixture of Experts</article-title><source>IEEE Trans. Intell. Veh.</source><year>2018</year><volume>3</volume><fpage>571</fpage><lpage>584</lpage><pub-id pub-id-type="doi">10.1109/tiv.2018.2874555</pub-id></element-citation></ref><ref id="B4-sensors-25-05544"><label>4.</label><element-citation publication-type="gov"><person-group person-group-type="author"><collab>National Transportation Safety Board</collab></person-group><article-title>Vehicle Automation and V2X-Related Technologies</article-title><source>Safety Advocacy</source><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.ntsb.gov/Advocacy/SafetyIssues/Pages/Vehicle-Automation-Related-and-Supported-Technologies.aspx" ext-link-type="uri">https://www.ntsb.gov/Advocacy/SafetyIssues/Pages/Vehicle-Automation-Related-and-Supported-Technologies.aspx</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-09">(accessed on 9 July 2025)</date-in-citation></element-citation></ref><ref id="B5-sensors-25-05544"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zoghlami</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kacimi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Dhaou</surname><given-names>R.</given-names></name></person-group><article-title>5G-Enabled V2X Communications for Vulnerable Road Users Safety Applications: A Review</article-title><source>Wirel. Netw.</source><year>2023</year><volume>29</volume><fpage>1237</fpage><lpage>1267</lpage><pub-id pub-id-type="doi">10.1007/s11276-022-03191-7</pub-id></element-citation></ref><ref id="B6-sensors-25-05544"><label>6.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>NGMN Alliance</collab></person-group><source>V2X White Paper</source><year>2018</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://5gaa.org/content/uploads/2018/08/V2X_white_paper_v1_0.pdf" ext-link-type="uri">https://5gaa.org/content/uploads/2018/08/V2X_white_paper_v1_0.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-09">(accessed on 9 July 2025)</date-in-citation></element-citation></ref><ref id="B7-sensors-25-05544"><label>7.</label><element-citation publication-type="webpage"><article-title>Kettering University&#8212;Mobility Research Center</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kettering.edu/academics-research/research/mobility-research-center" ext-link-type="uri">https://www.kettering.edu/academics-research/research/mobility-research-center</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-09">(accessed on 9 July 2025)</date-in-citation></element-citation></ref><ref id="B8-sensors-25-05544"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Amadeo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Campolo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Molinaro</surname><given-names>A.</given-names></name></person-group><article-title>Enhancing IEEE 802.11p/WAVE to Provide Infotainment Applications in VANETs</article-title><source>Ad Hoc Netw.</source><year>2012</year><volume>10</volume><fpage>253</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1016/j.adhoc.2010.09.013</pub-id></element-citation></ref><ref id="B9-sensors-25-05544"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Miucic</surname><given-names>R.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Al-Stouhi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Misener</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>W.</given-names></name></person-group><article-title>Cars Talk to Phones: A DSRC Based Vehicle-Pedestrian Safety System</article-title><source>Proceedings of the IEEE 80th Vehicular Technology Conference (VTC-Fall)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>14&#8211;17 September 2014</conf-date><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B10-sensors-25-05544"><label>10.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>5GAA</collab></person-group><article-title>Connectivity Standards in the Automotive Industry</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.itu.int/en/ITU-T/extcoop/cits/Documents/Meeting-20191028-e-meeting/18_5GAA_ITS-status-update.pdf" ext-link-type="uri">https://www.itu.int/en/ITU-T/extcoop/cits/Documents/Meeting-20191028-e-meeting/18_5GAA_ITS-status-update.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-09">(accessed on 9 July 2025)</date-in-citation></element-citation></ref><ref id="B11-sensors-25-05544"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Miao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Virtusio</surname><given-names>J.J.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>K.-L.</given-names></name></person-group><article-title>PC5-Based Cellular-V2X Evolution and Deployment</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>843</elocation-id><pub-id pub-id-type="doi">10.3390/s21030843</pub-id><pub-id pub-id-type="pmid">33513998</pub-id><pub-id pub-id-type="pmcid">PMC7865993</pub-id></element-citation></ref><ref id="B12-sensors-25-05544"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name></person-group><article-title>A Vision of C-V2X: Technologies, Field Testing, and Challenges with Chinese Development</article-title><source>IEEE Internet Things J.</source><year>2020</year><volume>7</volume><fpage>3872</fpage><lpage>3881</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2020.2974823</pub-id></element-citation></ref><ref id="B13-sensors-25-05544"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Landfeldt</surname><given-names>B.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jamalipour</surname><given-names>A.</given-names></name></person-group><article-title>Traffic Differentiated Clustering Routing in DSRC and C-V2X Hybrid Vehicular Networks</article-title><source>IEEE Trans. Veh. Technol.</source><year>2020</year><volume>69</volume><fpage>7723</fpage><lpage>7734</lpage><pub-id pub-id-type="doi">10.1109/TVT.2020.2990174</pub-id></element-citation></ref><ref id="B14-sensors-25-05544"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boban</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kousaridas</surname><given-names>A.</given-names></name><name name-style="western"><surname>Manolakis</surname><given-names>K.</given-names></name><name name-style="western"><surname>Eichinger</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name></person-group><article-title>Connected Roads of the Future: Use Cases, Requirements, and Design Considerations for Vehicle-to-Everything Communications</article-title><source>IEEE Veh. Technol. Mag.</source><year>2018</year><volume>13</volume><fpage>110</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1109/MVT.2017.2777259</pub-id></element-citation></ref><ref id="B15-sensors-25-05544"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>T.V.</given-names></name><name name-style="western"><surname>Shailesh</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sudhir</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kapil</surname><given-names>G.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Malladi</surname><given-names>D.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>A Comparison of Cellular Vehicle-to-Everything and Dedicated Short Range Communication</article-title><source>Proceedings of the IEEE Vehicular Networking Conference (VNC)</source><conf-loc>Turin, Italy</conf-loc><conf-date>27&#8211;29 November 2017</conf-date><fpage>101</fpage><lpage>108</lpage></element-citation></ref><ref id="B16-sensors-25-05544"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abbas</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>Z.</given-names></name></person-group><article-title>A Novel Low-Latency V2V Resource Allocation Scheme Based on Cellular V2X Communications</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2019</year><volume>20</volume><fpage>2185</fpage><lpage>2197</lpage><pub-id pub-id-type="doi">10.1109/TITS.2018.2865173</pub-id></element-citation></ref><ref id="B17-sensors-25-05544"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kutila</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kauvo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Martinez</surname><given-names>V.G.</given-names></name><name name-style="western"><surname>Karppinen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Nyk&#228;nen</surname><given-names>L.</given-names></name></person-group><article-title>Influence of Infrastructure Antenna Location and Positioning System Availability to Open-Road C-V2X Supported Automated Driving</article-title><source>Proceedings of the IEEE 5G for CAM: Connected and Automated Mobility</source><conf-loc>Online</conf-loc><conf-date>11&#8211;12 May 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2021</year></element-citation></ref><ref id="B18-sensors-25-05544"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>C.-M.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>C.-F.</given-names></name></person-group><article-title>The Mobile Edge Computing (MEC)-Based Vehicle-to-Infrastructure (V2I) Data Offloading from Cellular Network to VANET Using the Delay-Constrained Computing Scheme</article-title><source>Proceedings of the 2020 International Computer Symposium (ICS)</source><conf-loc>Tainan, Taiwan</conf-loc><conf-date>17&#8211;19 December 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2020</year></element-citation></ref><ref id="B19-sensors-25-05544"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Baccour</surname><given-names>E.</given-names></name><name name-style="western"><surname>Chkirbene</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Erbad</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hamila</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hamdi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gabbouj</surname><given-names>M.</given-names></name></person-group><article-title>A Survey on Mobile Edge Computing for Video Streaming: Opportunities and Challenges</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2209.05761</pub-id><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3220694</pub-id></element-citation></ref><ref id="B20-sensors-25-05544"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Husain</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>K&#252;nz</surname><given-names>A.</given-names></name><name name-style="western"><surname>Prasad</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pateromichelakis</surname><given-names>E.</given-names></name><name name-style="western"><surname>Samdanis</surname><given-names>K.</given-names></name><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name></person-group><article-title>The Road to 5G V2X: Ultra-High Reliable Communications</article-title><source>Proceedings of the 2018 IEEE Conference on Standards for Communications and Networking (CSCN)</source><conf-loc>Paris, France</conf-loc><conf-date>29&#8211;31 October 2018</conf-date><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/CSCN.2018.8581819</pub-id></element-citation></ref><ref id="B21-sensors-25-05544"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Allouis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Dayoub</surname><given-names>I.</given-names></name><name name-style="western"><surname>Cherkaoui</surname><given-names>S.</given-names></name></person-group><article-title>On 5G-V2X Use Cases and Enabling Technologies: A Comprehensive Survey</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>107710</fpage><lpage>107737</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3100472</pub-id></element-citation></ref><ref id="B22-sensors-25-05544"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>W.</given-names></name></person-group><article-title>Edge Computing for Autonomous Driving: Opportunities and Challenges</article-title><source>Proc. IEEE</source><year>2019</year><volume>107</volume><fpage>1697</fpage><lpage>1716</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2019.2915983</pub-id></element-citation></ref><ref id="B23-sensors-25-05544"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yurtsever</surname><given-names>E.</given-names></name><name name-style="western"><surname>Lambert</surname><given-names>J.</given-names></name><name name-style="western"><surname>Carballo</surname><given-names>A.</given-names></name><name name-style="western"><surname>Takeda</surname><given-names>K.</given-names></name></person-group><article-title>A Survey of Autonomous Driving: Common Practices and Emerging Technologies</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>58443</fpage><lpage>58469</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2983149</pub-id></element-citation></ref><ref id="B24-sensors-25-05544"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grigorescu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Trasnea</surname><given-names>B.</given-names></name><name name-style="western"><surname>Cocias</surname><given-names>T.</given-names></name><name name-style="western"><surname>Macesanu</surname><given-names>G.</given-names></name></person-group><article-title>A Survey of Deep Learning Techniques for Autonomous Driving</article-title><source>J. Field Robot.</source><year>2020</year><volume>37</volume><fpage>362</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1002/rob.21918</pub-id></element-citation></ref><ref id="B25-sensors-25-05544"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Letaief</surname><given-names>K.B.</given-names></name></person-group><article-title>Communication-Efficient Edge AI: Algorithms and Systems</article-title><source>IEEE Commun. Surv. Tutor.</source><year>2020</year><volume>22</volume><fpage>2167</fpage><lpage>2191</lpage><pub-id pub-id-type="doi">10.1109/COMST.2020.3007787</pub-id></element-citation></ref><ref id="B26-sensors-25-05544"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yaqoob</surname><given-names>I.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>L.U.</given-names></name><name name-style="western"><surname>Kazmi</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>Imran</surname><given-names>M.</given-names></name><name name-style="western"><surname>Guizani</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>C.S.</given-names></name></person-group><article-title>Autonomous Driving Cars in Smart Cities: Recent Advances, Requirements, and Challenges</article-title><source>IEEE Netw.</source><year>2019</year><volume>34</volume><fpage>174</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1109/MNET.2019.1900120</pub-id></element-citation></ref><ref id="B27-sensors-25-05544"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Communication-Computation Trade-Off in Resource-Constrained Edge Inference</article-title><source>IEEE Commun. Mag.</source><year>2020</year><volume>58</volume><fpage>20</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1109/MCOM.001.2000373</pub-id></element-citation></ref><ref id="B28-sensors-25-05544"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Stahl</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Mueller-Gritschneder</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gerstlauer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Schlichtmann</surname><given-names>U.</given-names></name></person-group><article-title>Fully Distributed Deep Learning Inference on Resource-Constrained Edge Devices</article-title><source>Proceedings of the International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)</source><conf-loc>Pythagorion, Greece</conf-loc><conf-date>15&#8211;18 July 2019</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>77</fpage><lpage>90</lpage></element-citation></ref><ref id="B29-sensors-25-05544"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>W.</given-names></name></person-group><article-title>A New Cellular Vehicle-to-Everything Application: Daytime Visibility Detection and Prewarning on Expressways</article-title><source>IEEE Intell. Transp. Syst. Mag.</source><year>2023</year><volume>15</volume><fpage>85</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1109/MITS.2022.3181988</pub-id></element-citation></ref><ref id="B30-sensors-25-05544"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Miao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.-F.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>Y.-L.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>K.-L.</given-names></name></person-group><article-title>How Does C-V2X Help Autonomous Driving to Avoid Accidents?</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>686</elocation-id><pub-id pub-id-type="doi">10.3390/s22020686</pub-id><pub-id pub-id-type="pmid">35062647</pub-id><pub-id pub-id-type="pmcid">PMC8779724</pub-id></element-citation></ref><ref id="B31-sensors-25-05544"><label>31.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Stereolabs</collab></person-group><article-title>ZED 2-AI Stereo Camera</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.stereolabs.com/products/zed-2" ext-link-type="uri">https://www.stereolabs.com/products/zed-2</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B32-sensors-25-05544"><label>32.</label><element-citation publication-type="webpage"><article-title>Cohda Wireless MK6C RSU Product Brief</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.cohdawireless.com/wp-content/uploads/2022/10/CW_Product-Brief-sheet-MK6C-EVK-RSU-v2.pdf" ext-link-type="uri">https://www.cohdawireless.com/wp-content/uploads/2022/10/CW_Product-Brief-sheet-MK6C-EVK-RSU-v2.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B33-sensors-25-05544"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Amin</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Konjeti</surname><given-names>H.B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Integration of C-V2X with an Obstacle Detection System for ADAS Applications</article-title><source>Proceedings of the 2024 IEEE International Conference on Communication, Networks and Satellite (COMNETSAT)</source><conf-loc>Mataram, Indonesia</conf-loc><conf-date>28&#8211;30 November 2024</conf-date><fpage>752</fpage><lpage>758</lpage><pub-id pub-id-type="doi">10.1109/COMNETSAT63286.2024.10862830</pub-id></element-citation></ref><ref id="B34-sensors-25-05544"><label>34.</label><element-citation publication-type="webpage"><article-title>Cohda Wireless MK5 OBU Product Brief</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.cohdawireless.com/wp-content/uploads/2025/04/CW_DL_Product-Brief-sheet-MK6-OBU.pdf" ext-link-type="uri">https://www.cohdawireless.com/wp-content/uploads/2025/04/CW_DL_Product-Brief-sheet-MK6-OBU.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B35-sensors-25-05544"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You Only Look Once: Unified, Real-Time Object Detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>779</fpage><lpage>788</lpage><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1506.02640" ext-link-type="uri">https://arxiv.org/abs/1506.02640</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-04">(accessed on 4 September 2025)</date-in-citation></element-citation></ref><ref id="B36-sensors-25-05544"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>YOLOv3: An Incremental Improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1804.02767</pub-id><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1804.02767" ext-link-type="uri">https://arxiv.org/abs/1804.02767</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-04">(accessed on 4 September 2025)</date-in-citation><pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id></element-citation></ref><ref id="B37-sensors-25-05544"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.-Y.M.</given-names></name></person-group><article-title>YOLOv4: Optimal Speed and Accuracy of Object Detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2004.10934</pub-id><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/2004.10934" ext-link-type="uri">https://arxiv.org/abs/2004.10934</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-09-04">(accessed on 4 September 2025)</date-in-citation><pub-id pub-id-type="doi">10.48550/arXiv.2004.10934</pub-id></element-citation></ref><ref id="B38-sensors-25-05544"><label>38.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Jocher</surname><given-names>G.</given-names></name></person-group><article-title>YOLOv5 by Ultralytics. Zenodo 2020</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://docs.ultralytics.com/models/yolov5/" ext-link-type="uri">https://docs.ultralytics.com/models/yolov5/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B39-sensors-25-05544"><label>39.</label><element-citation publication-type="webpage"><article-title>YOLO Models</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://docs.ultralytics.com/models/" ext-link-type="uri">https://docs.ultralytics.com/models/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-07">(accessed on 7 August 2025)</date-in-citation></element-citation></ref><ref id="B40-sensors-25-05544"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>S.</given-names></name></person-group><article-title>Application of Improved YOLOv5 Algorithm in Lightweight Transmission Line Small Target Defect Detection</article-title><source>Electronics</source><year>2024</year><volume>13</volume><elocation-id>305</elocation-id><pub-id pub-id-type="doi">10.3390/electronics13020305</pub-id></element-citation></ref><ref id="B41-sensors-25-05544"><label>41.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tanenbaum</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Wetherall</surname><given-names>D.J.</given-names></name></person-group><source>Computer Networks</source><edition>5th ed.</edition><publisher-name>Pearson</publisher-name><publisher-loc>Upper Saddle River, NJ, USA</publisher-loc><year>2011</year></element-citation></ref><ref id="B42-sensors-25-05544"><label>42.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Forouzan</surname><given-names>B.A.</given-names></name></person-group><source>Data Communications and Networking</source><edition>5th ed.</edition><publisher-name>McGraw-Hill</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2013</year></element-citation></ref><ref id="B43-sensors-25-05544"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fischler</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Bolles</surname><given-names>R.C.</given-names></name></person-group><article-title>Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography</article-title><source>Commun. ACM</source><year>1981</year><volume>24</volume><fpage>381</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1145/358669.358692</pub-id></element-citation></ref><ref id="B44-sensors-25-05544"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rumelhart</surname><given-names>D.E.</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G.E.</given-names></name><name name-style="western"><surname>Williams</surname><given-names>R.J.</given-names></name></person-group><article-title>Learning Representations by Back-Propagating Errors</article-title><source>Nature</source><year>1986</year><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="B45-sensors-25-05544"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marquardt</surname><given-names>D.W.</given-names></name></person-group><article-title>An Algorithm for Least-Squares Estimation of Nonlinear Parameters</article-title><source>SIAM J. Appl. Math.</source><year>1963</year><volume>11</volume><fpage>431</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1137/0111030</pub-id></element-citation></ref><ref id="B46-sensors-25-05544"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sinnott</surname><given-names>R.W.</given-names></name></person-group><article-title>Virtues of the Haversine</article-title><source>Sky Telesc.</source><year>1984</year><volume>68</volume><fpage>159</fpage><lpage>160</lpage></element-citation></ref><ref id="B47-sensors-25-05544"><label>47.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>NVIDIA</collab></person-group><article-title>Jetson Nano</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/product-development/" ext-link-type="uri">https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/product-development/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B48-sensors-25-05544"><label>48.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>NVIDIA</collab></person-group><article-title>Jetson Orin Technical Brief</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.nvidia.com/content/dam/en-zz/Solutions/gtcf21/jetson-orin/nvidia-jetson-agx-orin-technical-brief.pdf" ext-link-type="uri">https://www.nvidia.com/content/dam/en-zz/Solutions/gtcf21/jetson-orin/nvidia-jetson-agx-orin-technical-brief.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B49-sensors-25-05544"><label>49.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>NVIDIA</collab></person-group><article-title>Jetson Modules</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://developer.nvidia.com/embedded/jetson-modules" ext-link-type="uri">https://developer.nvidia.com/embedded/jetson-modules</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B50-sensors-25-05544"><label>50.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>NVIDIA</collab></person-group><article-title>Drive Orin</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://developer.nvidia.com/drive/ecosystem-orin" ext-link-type="uri">https://developer.nvidia.com/drive/ecosystem-orin</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B51-sensors-25-05544"><label>51.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>NTP</collab></person-group><article-title>Synchronise the System Clock with NTP Servers. Chrony Project</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://chrony-project.org/index.html" ext-link-type="uri">https://chrony-project.org/index.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-19">(accessed on 19 July 2025)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05544-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall architecture of the proposed co-operative perception system using C-V2X and a client&#8211;server model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g001.jpg"/></fig><fig position="float" id="sensors-25-05544-f002" orientation="portrait"><label>Figure 2</label><caption><p>Infrastructure setup at the GMMRC testing track: one stereo camera, one computing unit, one RSU, and one server computer.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g002.jpg"/></fig><fig position="float" id="sensors-25-05544-f003" orientation="portrait"><label>Figure 3</label><caption><p>In vehicle setup for C-V2X with one OBU and one in-vehicle computing unit.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g003.jpg"/></fig><fig position="float" id="sensors-25-05544-f004" orientation="portrait"><label>Figure 4</label><caption><p>The architecture of YOLOv5.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g004.jpg"/></fig><fig position="float" id="sensors-25-05544-f005" orientation="portrait"><label>Figure 5</label><caption><p>Example of object detection with YOLOv5.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g005.jpg"/></fig><fig position="float" id="sensors-25-05544-f006" orientation="portrait"><label>Figure 6</label><caption><p>Data packet transmission between client and server in TCP/IP model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g006.jpg"/></fig><fig position="float" id="sensors-25-05544-f007" orientation="portrait"><label>Figure 7</label><caption><p>Mapping map generation: (<bold>a</bold>) the bounding box of the test vehicle equipped with the OBU. (<bold>b</bold>) The bounding box information of the detected test vehicle and the corresponding GPS coordinates from the OBU are recorded.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g007.jpg"/></fig><fig position="float" id="sensors-25-05544-f008" orientation="portrait"><label>Figure 8</label><caption><p>The architecture of the GPS estimation NN model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g008.jpg"/></fig><fig position="float" id="sensors-25-05544-f009" orientation="portrait"><label>Figure 9</label><caption><p>Software architecture of the proposed co-operative perception system using C-V2X and the client&#8211;server model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g009.jpg"/></fig><fig position="float" id="sensors-25-05544-f010" orientation="portrait"><label>Figure 10</label><caption><p>The experiment results on latency evaluation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g010.jpg"/></fig><fig position="float" id="sensors-25-05544-f011" orientation="portrait"><label>Figure 11</label><caption><p>Experiments on the proving ground at GMMRC. (<bold>a</bold>) The GMMRC testing track. (<bold>b</bold>&#8211;<bold>d</bold>) The detected objects are represented with bounding boxes, object types, and confidence scores.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g011.jpg"/></fig><fig position="float" id="sensors-25-05544-f012" orientation="portrait"><label>Figure 12</label><caption><p>Examples of broadcast traffic information from the RSU to the vehicles&#8217; OBUs. (<bold>a</bold>,<bold>b</bold>) Received traffic information, including object type, confidence score, and GPS coordinates.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05544-g012.jpg"/></fig><table-wrap position="float" id="sensors-25-05544-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05544-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of processing times across different computing platforms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Different Computing Platforms</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Processing Time (ms)</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Stand-Alone Model</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA Jetson Nano</td><td align="center" valign="middle" rowspan="1" colspan="1">161.00 ms</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA Jetson Orin</td><td align="center" valign="middle" rowspan="1" colspan="1">37.65 ms</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NVIDIA Drive Orin</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.56 ms</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Client&#8211;server Model</td><td align="center" valign="middle" rowspan="1" colspan="1">Client: NVIDIA Jetson Nano</td><td rowspan="2" align="center" valign="middle" colspan="1">38.61 ms</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Server: NVIDIA Drive Orin</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Client: NVIDIA Jetson Orin</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">32.83 ms</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Server: NVIDIA Drive Orin</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05544-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05544-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance comparison of three GPS mapping algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GPS Mapping Algorithm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Distance Error (m)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Standard<break/>Deviation (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">The homogeneous transformation [<xref rid="B29-sensors-25-05544" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">5.1627 m</td><td align="center" valign="middle" rowspan="1" colspan="1">4.9129 m</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">The mapping map-based method</td><td align="center" valign="middle" rowspan="1" colspan="1">2.4472 m</td><td align="center" valign="middle" rowspan="1" colspan="1">3.1522 m</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The NN-based method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.1914 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.2552 m</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>