<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430898</article-id><article-id pub-id-type="pmcid-ver">PMC12430898.1</article-id><article-id pub-id-type="pmcaid">12430898</article-id><article-id pub-id-type="pmcaiid">12430898</article-id><article-id pub-id-type="doi">10.3390/s25175548</article-id><article-id pub-id-type="publisher-id">sensors-25-05548</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Real-Time Driver Attention Detection in Complex Driving Environments via Binocular Depth Compensation and Multi-Source Temporal Bidirectional Long Short-Term Memory Network</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Zhou</surname><given-names initials="S">Shuhui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05548" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="W">Wei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af2-sensors-25-05548" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="Y">Yulong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05548" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Chen</surname><given-names initials="X">Xiaonian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-05548" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="H">Huajie</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-05548" ref-type="aff">2</xref><xref rid="c1-sensors-25-05548" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Doulamis</surname><given-names initials="A">Anastasios</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05548"><label>1</label>CGNPC Uranium Resources Co., Ltd., Beijing 100084, China</aff><aff id="af2-sensors-25-05548"><label>2</label>Suzhou Automotive Research Institute (Wujiang), Tsinghua University, Suzhou 215200, China</aff><author-notes><corresp id="c1-sensors-25-05548"><label>*</label>Correspondence: <email>liuhuajie@tsari.tsinghua.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5548</elocation-id><history><date date-type="received"><day>29</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>04</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>05</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05548.pdf"/><abstract><p>Driver distraction is a key factor contributing to traffic accidents. However, in existing computer vision-based methods for driver attention state recognition, monocular camera-based approaches often suffer from low accuracy, while multi-sensor data fusion techniques are compromised by poor real-time performance. To address these limitations, this paper proposes a Real-time Driver Attention State Recognition method (RT-DASR). RT-DASR comprises two core components: Binocular Vision Depth-Compensated Head Pose Estimation (BV-DHPE) and Multi-source Temporal Bidirectional Long Short-Term Memory (MSTBi-LSTM). BV-DHPE employs binocular cameras and YOLO11n (You Only Look Once) Pose to locate facial landmarks, calculating spatial distances via binocular disparity to compensate for monocular depth deficiency for accurate pose estimation. MSTBi-LSTM utilizes a lightweight Bidirectional Long Short-Term Memory (Bi-LSTM) network to fuse head pose angles, real-time vehicle speed, and gaze region semantics, bidirectionally extracting temporal features for continuous attention state discrimination. Evaluated under challenging conditions (e.g., illumination changes, occlusion), BV-DHPE achieved 44.7% reduction in head pose Mean Absolute Error (MAE) compared to monocular vision methods. RT-DASR achieved 90.4% attention recognition accuracy with 21.5 ms average latency when deployed on NVIDIA Jetson Orin. Real-world driving scenario tests confirm that the proposed method provides a high-precision, low-latency attention state recognition solution for enhancing the safety of mining vehicle drivers. RT-DASR can be integrated into advanced driver assistance systems to enable proactive accident prevention.</p></abstract><kwd-group><kwd>head pose estimation</kwd><kwd>convolutional neural network</kwd><kwd>long short-term memory</kwd><kwd>binocular vision</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05548"><title>1. Introduction</title><p>Road safety constitutes a core concern in the development of intelligent transportation systems. Driver distraction represents one of the critical human factors contributing to traffic accidents [<xref rid="B1-sensors-25-05548" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05548" ref-type="bibr">2</xref>]. The total monetized societal impact of motor vehicle crashes in the United States amounted to USD 83.6 billion in 2010, with approximately 15% attributable to distracted driving [<xref rid="B3-sensors-25-05548" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05548" ref-type="bibr">4</xref>]. Consequently, developing efficient and reliable methods for detecting driver attention states holds significant theoretical value and practical importance for enhancing driving safety and reducing accidents. Current driver attention state recognition methods are primarily classified into three categories: physiological signal-based [<xref rid="B5-sensors-25-05548" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05548" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05548" ref-type="bibr">7</xref>], driving behavior-based [<xref rid="B8-sensors-25-05548" ref-type="bibr">8</xref>], and computer vision-based analysis [<xref rid="B9-sensors-25-05548" ref-type="bibr">9</xref>]. Physiological signal-based methods (e.g., monitoring EEG, ECG, GSR) can directly reflect the driver&#8217;s physiological state [<xref rid="B10-sensors-25-05548" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05548" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05548" ref-type="bibr">12</xref>]. But they typically rely on contact devices, suffering from limitations such as high cost, poor comfort, and potential interference with driving operations [<xref rid="B10-sensors-25-05548" ref-type="bibr">10</xref>]. Driving behavior-based methods infer attention states indirectly by analyzing steering wheel operations, braking patterns, etc., offering non-intrusive advantages [<xref rid="B13-sensors-25-05548" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05548" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05548" ref-type="bibr">15</xref>]. Nevertheless, their detection efficacy is susceptible to complex traffic environments, road conditions, and individual driving habit variations, often exhibiting insufficient generalization capability and real-time performance. In contrast, computer vision-based methods, capturing and analyzing visual features like facial expressions and head pose, combine non-intrusiveness, low deployment cost, and strong scalability, emerging as the dominant research and application direction [<xref rid="B16-sensors-25-05548" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05548" ref-type="bibr">17</xref>].</p><p>Among visual approaches, facial expression analysis is vulnerable to illumination variations and occlusion [<xref rid="B18-sensors-25-05548" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05548" ref-type="bibr">19</xref>]. Head pose estimation, inferring the driver&#8217;s focus of attention by analyzing spatial position and orientation changes of the head, is considered a preferred solution for large-scale in-vehicle deployment due to its relative simplicity, moderate computational demands, good real-time performance, strong non-intrusiveness, and controllable cost. Within computer vision-based head pose estimation, traditional methods include 2D image analysis techniques based on feature point detection or template matching [<xref rid="B20-sensors-25-05548" ref-type="bibr">20</xref>]. But their accuracy is limited by the lack of depth information in monocular images, resulting in suboptimal performance. Three-dimensional methods based on geometric models (e.g., 3D facial modeling, Direct Linear Transformation) or point cloud alignment improve accuracy but incur high computational complexity, hindering real-time pose estimation [<xref rid="B21-sensors-25-05548" ref-type="bibr">21</xref>]. The rise of deep learning has significantly advanced this field. Convolutional Neural Networks (CNNs) learn effective features end-to-end from raw images [<xref rid="B22-sensors-25-05548" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05548" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05548" ref-type="bibr">24</xref>]. Transformer networks capture long-range dependencies via self-attention [<xref rid="B25-sensors-25-05548" ref-type="bibr">25</xref>]. Hybrid architectures (e.g., CNN-Transformer) attempt to fuse their strengths for enhanced feature representation [<xref rid="B26-sensors-25-05548" ref-type="bibr">26</xref>]. Nevertheless, existing methods still face significant challenges in robustness under complex driving scenarios (e.g., severe illumination changes, occlusion, rapid motion) and computational efficiency for millisecond-level real-time responsiveness.</p><p>Furthermore, unimodal approaches exhibit inherent limitations [<xref rid="B27-sensors-25-05548" ref-type="bibr">27</xref>]. Multimodal fusion strategies, integrating visual, physiological, and behavioral information, theoretically enhance detection accuracy and robustness [<xref rid="B28-sensors-25-05548" ref-type="bibr">28</xref>]. Current fusion methods explore advanced techniques such as attention mechanisms for dynamic weighting and federated learning for privacy protection [<xref rid="B29-sensors-25-05548" ref-type="bibr">29</xref>]. Despite advances, multimodal fusion encounters practical deployment barriers, including data synchronization difficulties, drastically increased system complexity, and substantial computational resource demands [<xref rid="B18-sensors-25-05548" ref-type="bibr">18</xref>]. In summary, vision-based head-pose estimation holds considerable promise for driver-attention monitoring. Nevertheless, monocular approaches suffer from limited accuracy owing to the absence of explicit depth cues, whereas point-cloud-based and multi-sensor fusion methods are still constrained in real-time performance. Moreover, the overwhelming majority of existing investigations have not been tailored to the specificities of mining-truck drivers. Consequently, a rapid yet accurate technique for assessing the attentional state of mining-truck drivers remains an open challenge.</p><p>To address the aforementioned challenges, this paper proposes RT-DASR. Within this framework, the BV-DHPE module is dedicated to estimating head orientation. BV-DHPE leverages a calibrated stereo rig to simultaneously capture facial imagery and dense depth, thereby eliminating the depth-ambiguity inherent in monocular setups. An off-the-shelf YOLO11n Pose [<xref rid="B30-sensors-25-05548" ref-type="bibr">30</xref>] model is adopted to directly regress a sparse set of facial landmarks. This single-shot paradigm circumvents the conventional two-stage pipeline of face detection followed by landmark localization, yielding both higher precision and lower latency. Facial landmarks and facial depth information are used to estimate the angle of the head posture. The extracted head-pose sequence, together with the driver&#8217;s dynamic gaze region and the vehicle velocity, is then forwarded to the MSTBi-LSTM module. By bidirectionally fusing these three low-dimensional yet temporally aligned cues, MSTBi-LSTM continuously discriminates subtle transitions in driver attention without incurring the computational burden typical of high-dimensional multi-modal fusion. Owing to the real-time capability of YOLO11n-Pose and the lightweight architecture of the Bi-LSTM network, RT-DASR achieves accurate, real-time recognition of mining-truck driver attention states under stringent latency constraints.</p><p>The main contributions of this work are as follows:<list list-type="simple"><list-item><label>(1)</label><p>Binocular Vision Depth-Compensated Head Pose Estimation component</p></list-item></list></p><p>Addressing the inaccuracy in pose estimation caused by depth deficiency in monocular images, we adopt a binocular camera framework. Key facial landmarks (inner/outer eye corners, eyebrow outer corners, nostrils) are extracted using the YOLO11n Pose model. Spatial distances are computed via binocular disparity, enabling precise head pose angle estimation. Experimental validation demonstrates a 44.7% reduction in MAE compared to monocular approaches.</p><list list-type="simple"><list-item><label>(2)</label><p>Multi-source Temporal Bidirectional Long Short-Term Memory Feature Fusion component</p></list-item></list><p>This method bidirectionally fuses temporal sequences of head pose dynamics, real-time gaze region shifts, and vehicle speed information. It utilizes a Bi-LSTM network to model the spatiotemporal dependencies of driving attention states. Achieving an attention state recognition accuracy of 93.2% in continuous driving scenarios.</p><list list-type="simple"><list-item><label>(3)</label><p>High-Accuracy, Low-Latency Driver Attention State Recognition</p></list-item></list><p>Through lightweight designs of the YOLO11n Pose and Bi-LSTM models, the RT-DASR response time is constrained within 21.5 ms, meeting the millisecond-level early warning requirements for driving safety. The proposed solution combines non-intrusiveness and low-cost characteristics, providing a viable technical pathway for the large-scale in-vehicle deployment of mining-truck driver attention state recognition systems.</p></sec><sec id="sec2-sensors-25-05548"><title>2. Method</title><p>This section details the YOLO11n Pose detector for facial landmarks localization, binocular vision-based feature point distance measurement for spatial coordinate calculation, binocular vision-driven driver head pose angle estimation, driver gaze region partitioning for semantic area identification, and the architecture of the multi-source information fusion Bi-LSTM model for temporal feature extraction and driver attention state classification.</p><sec id="sec2dot1-sensors-25-05548"><title>2.1. Introduction to the Real-Time Driver Attention State Recognition Component</title><p>The proposed driver attention state detection method is illustrated in <xref rid="sensors-25-05548-f001" ref-type="fig">Figure 1</xref>. This approach employs a binocular vision camera to capture facial images of the driver. The YOLO11n deep learning algorithm detects the facial region and locates 17 facial landmarks. Corner points less affected by facial expressions (e.g., eye corners, nostrils, outer eyebrow points) are selected as candidate points. Using the stereo image pair, a disparity map is computed to derive the 3D coordinates <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>~</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of each candidate point in the world coordinate system, where <italic toggle="yes">n</italic> represents the number of candidate points. For each video frame, a rotation matrix is calculated based on the world coordinates of these candidate points. This matrix is subsequently decomposed to obtain the three Euler angles (yaw, pitch, roll) representing head orientation. These 3D head pose angles (yaw, pitch, roll), along with real-time vehicle speed and the driver&#8217;s gaze region, serve as input to the MSTBi-LSTM module. The Bi-LSTM network extracts temporal features from this multi-source data sequence. Finally, the output of the Bi-LSTM is fed into Fully Connected layers (FC) followed by a Softmax layer for classification, determining whether the driver&#8217;s attention state is divided or focused.</p></sec><sec id="sec2dot2-sensors-25-05548"><title>2.2. Introduction to the YOLO11n Pose Detector</title><p>In order to be able to perform fast facial landmark detection in embedded systems, we chose the Pose model in YOLO11n for face detection and facial landmark detection, and the structure of the YOLO11n Pose model is shown in <xref rid="sensors-25-05548-f002" ref-type="fig">Figure 2</xref>.</p><p>The YOLO11n Pose model primarily consists of three components: the Backbone, Neck, and Head, which are constructed using fundamental modules including CBS (Convolution BatchNorm SiLU), C3, C2F, C3K2, SPPF (Spatial Pyramid Pooling Fast), Bottleneck layers, and the Decoupled Head. The CBR module of Backbone performs feature map downsampling through a 3 &#215; 3 convolution layer with stride 2, followed by Batch Normalization (BN) and a SiLU activation function. In the C3 module, the input feature map is first split into two parts: one part directly passes to a subsequent concatenation layer, while the other undergoes deep feature extraction via a Bottleneck module. The outputs from both paths are then concatenated along the channel dimension and compressed to the target channel number by a second CBS layer. This architecture enables efficient feature extraction and processing for pose estimation tasks.</p><p>The C2F module, designed based on the Cross Stage Partial (CSP) architecture, integrates two 1 &#215; 1 CBS layers with stride 1 and multiple bottleneck layers. These bottleneck layers enhance gradient flow through residual connections, with each module containing two 3 &#215; 3 convolution layers (stride 1) for high-level feature extraction. In processing, the input feature map first passes through the initial CBS layer, expanding the channel dimension to twice the input size. This output is then split into two branches: one directly routed to a subsequent concatenation layer, while the other undergoes deep feature extraction through multiple bottleneck modules. Finally, all bottleneck outputs are concatenated with the direct-pass features along the channel dimension, and the aggregated result is compressed to the target channel number by the second CBS layer.</p><p>The C3K2 module processes input feature maps by first propagating them through an initial CBS layer; the output is then bifurcated into two parallel paths: one branch directly feeds into a subsequent concatenation layer, while the other undergoes deep feature extraction via C3 modules. Ultimately, all processed outputs from the C3 modules are concatenated with the direct-pass features along the channel dimension, and the merged result is compressed to the target channel count through a subsequent CBS layer.</p><p>The C2PSA module implements a convolutional block incorporating attention mechanisms through the integration of a 1 &#215; 1 convolutional layer and a series of PSABlock modules, thereby enhancing feature extraction and processing capabilities. Within this architecture, the input feature map initially traverses a primary CBS layer. The resultant output is subsequently partitioned into two branches: one directly propagates to a succeeding concatenation layer, while the other undergoes deep feature extraction via PSABlock modules. Ultimately, all processed outputs from the PSABlock modules are concatenated with the direct-pass features along the channel dimension, and the aggregated representation is compressed to the target channel count through a secondary CBS layer.</p><p>The PSABlock module enhances the model&#8217;s capacity to capture critical features within input data by integrating multi-head attention mechanisms with feed-forward neural network layers. The input feature map is first propagated to the attention module for executing multi-head attention mechanisms, enabling the model to focus on salient features across different spatial positions during data processing. The output from the attention mechanism is then added to the original input through residual summation, preserving source information while facilitating gradient flow. The resultant tensor subsequently traverses a CBS layer for further feature extraction and transformation. The final output tensor synthesizes representations processed by both the attention mechanism and the feed-forward network layer.</p><p>The Attention module implements a multi-head self-attention mechanism, enabling the model to focus on salient features at distinct spatial positions during input data processing. Initially, the input features are refined through a CBS layer for feature extraction, then projected into queries (Q), keys (K), and values (V) via View and Split operations. Subsequently, matrix multiplication is performed between the transposed K matrix and Q to generate the attention score matrix. Following this, the attention scores are scaled by a normalization factor to mitigate vanishing gradients. Softmax activation is then applied to normalize these scaled scores across the feature dimension. The normalized attention weights are used to compute a weighted sum over the values (V), yielding the attention output. Finally, this output is projected back to the original dimensionality through a CBS layer.</p><p>The SPPF module enhances the model&#8217;s receptive field through multi-scale feature fusion while reducing computational redundancy. Its architecture incorporates two CBS layers with stride = 1 and 1 &#215; 1 kernels. The input features initially traverse the first CBS layer, then are partitioned into dual pathways: one directly propagates to a subsequent concatenation layer, while the other undergoes feature extraction through three successive 5 &#215; 5 max-pooling layers arranged in cascade. Following this, the pooled features and CBS processed features undergo channel concatenation. The merged output is then compressed to the target channel dimensionality via the second CBS layer. The decoupled head comprises two distinct branches: a classification branch for category prediction and a regression branch for boundary box estimation. Each branch contains two CBR blocks with 3 &#215; 3 kernels (stride = 1), followed by a 1 &#215; 1 convolutional layer (stride = 1). The unified detection head employs a 2D convolution layer to simultaneously generate category probabilities, confidence scores, and bounding box coordinates.</p><p>Within the YOLO11n Pose architecture, input images first traverse a CBS layer for initial feature extraction in the Backbone. The features subsequently undergo iterative processing: initially passing sequentially through CBS and C2F modules (executed twice), then through CBS and C3K2 modules (also repeated twice). Finally, the feature maps proceed sequentially through SPPF and C2PSA modules for hierarchical feature extraction. Output feature maps from the second C2F, second C3K2, and C2PSA modules in the Backbone are designated as [C3, C4, C5] respectively. These Backbone outputs then enter the Neck module, where C5 is first upsampled via nearest-neighbor interpolation. The upsampled features undergo channel concatenation with C4, followed by feature refinement through a C2F module. This refined output is again upsampled and concatenated with C3, then processed through another C2F module to generate the P3 feature map. The P3 map traverses a CBS layer before concatenation with C4, with the fused features fed into C2F to yield P4. Similarly, P4 passes through CBS and concatenates with C5, then advances through a C3K2 module to produce P5. The resultant [P3, P4, P5] feature pyramids are finally distributed to three detection heads for simultaneous prediction generation.</p></sec><sec id="sec2dot3-sensors-25-05548"><title>2.3. Binocular Vision-Based Feature Point Distance Measurement</title><p>In binocular vision systems, the pinhole camera model is conventionally employed to describe the imaging process. Consider a camera with its optical center at point <italic toggle="yes">O</italic>. The relationship between the image plane and camera coordinate system is defined as follows: the origin of the camera coordinate system coincides with the optical center <italic toggle="yes">O</italic>, while the optical axis remains perpendicular to the image plane. The pixel coordinates (<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) in the image plane are related to the 3D points (<italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic>, <italic toggle="yes">Z</italic>) in the camera coordinate system as follows:<disp-formula id="FD1-sensors-25-05548"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mo>&#8193;</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">f</italic> denotes the camera&#8217;s focal length, while <italic toggle="yes">Z</italic> represents the distance from the 3D point to the optical center <italic toggle="yes">O</italic> along the optical axis corresponding to depth information.</p><p>A binocular vision system typically comprises left and right cameras separated by a horizontal spatial interval known as the baseline distance <italic toggle="yes">B</italic>. The optical centers of the left and right cameras are denoted as <italic toggle="yes">O<sub>l</sub></italic> and <italic toggle="yes">O&#7523;</italic>, respectively. For a 3D point <italic toggle="yes">P</italic> (<italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic>, <italic toggle="yes">Z</italic>) in the scene, its projections on the image planes of the left and right cameras are <italic toggle="yes">p&#8343;</italic> (<italic toggle="yes">x&#8343;</italic>, <italic toggle="yes">y&#8343;</italic>) and <italic toggle="yes">p&#7523;</italic> (<italic toggle="yes">x&#7523;</italic>, <italic toggle="yes">y&#7523;</italic>). Under ideal conditions&#8212;where both camera optical axes are strictly parallel and focal lengths are identical <italic toggle="yes">f</italic>&#8212;the similar triangle principle yields the fundamental stereo correspondence:<disp-formula id="FD2-sensors-25-05548"><label>(2)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mo>&#8193;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05548"><label>(3)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mo>&#8193;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We derive the horizontal disparity <italic toggle="yes">d</italic> between corresponding image points.<disp-formula id="FD4-sensors-25-05548"><label>(4)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We establish the critical depth-disparity relationship as follows:<disp-formula id="FD5-sensors-25-05548"><label>(5)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In operational practice, binocular cameras capture left and right viewpoint images of the same scene. Stereo matching algorithms are then applied to generate a disparity map, which is converted to a depth map through this geometric principle. Candidate facial landmarks are identified on the depth map to locate registered points, enabling depth measurement from these features to the camera plane.</p></sec><sec id="sec2dot4-sensors-25-05548"><title>2.4. Depth-Compensated Head Pose Estimation</title><p>To enhance the accuracy of driver head-pose estimation, depth information was explicitly incorporated into the estimation framework. Driver head rotation exhibits three degrees of freedom in the coordinate system: pitch (rotation about the <italic toggle="yes">x</italic>-axis), in-plane roll (rotation about the <italic toggle="yes">y</italic>-axis), and yaw (rotation about the <italic toggle="yes">z</italic>-axis). For gaze direction analysis, visual transitions primarily arise from coupled yaw-pitch motions. Given depth measurements of facial landmarks obtained via binocular vision, driver pose estimation proceeds in three computational stages: First, world coordinates of the landmarks are computed using the camera&#8217;s intrinsic matrix <bold>K</bold> and extrinsic parameters [<bold>R</bold>|<bold>t</bold>]. Second, the rotation matrix <bold>R</bold> is derived from these feature coordinates with depth information through singular value decomposition. Finally, the three Euler angles (pitch, roll, yaw) are extracted from <bold>R</bold> via matrix decomposition. The camera intrinsic matrix <bold>K</bold>, rotation matrix <bold>R</bold>, and translation vector <bold>t</bold> are given by:<disp-formula id="FD6-sensors-25-05548"><label>(6)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05548"><label>(7)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>33</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Given the image coordinates (<italic toggle="yes">u</italic>, <italic toggle="yes">v</italic>), their transformation into distortion-free normalized camera coordinates <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> yields the following rigid transformation between world and camera coordinate systems:<disp-formula id="FD8-sensors-25-05548"><label>(8)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05548"><label>(9)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05548"><label>(10)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05548"><label>(11)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The coordinates of the feature points in the world coordinate system can be obtained by solving the following system of equations.<disp-formula id="FD12-sensors-25-05548"><label>(12)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Given a feature point with world coordinates <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> in the previous frame and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> in the current frame, the rigidity constraint of 3D Euclidean transformation implies:<disp-formula id="FD13-sensors-25-05548"><label>(13)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Consequently, the rotation matrix <bold>R</bold> can be determined, enabling subsequent derivation of the three attitude Euler angles. For the eight pre-selected feature points processed through this method, the computed Euler angles are averaged to obtain the driver&#8217;s final 3D rotational pose angles.</p></sec><sec id="sec2dot5-sensors-25-05548"><title>2.5. Driver Gaze Region Partition</title><p>To transform driver gaze regions into semantically meaningful information for driving safety and thereby distinguish safe attention from distraction behavior, this study partitions gaze areas according to three principles: functional relevance (regions must correspond to driving task requirements, e.g., defining the forward roadway as a mandatory observation zone), spatial rationality (physical boundaries are delineated based on actual cabin layouts such as instrument clusters and common mobile device placement areas), and quantifiability (regions require explicit boundaries to support objective computation of distraction levels through duration and frequency metrics). This framework enables precise determination of whether attention is directed appropriately. Based on these principles, the driver&#8217;s viewpoint distribution within the mining truck cabin is divided into 12 discrete regions, allowing viewpoint locations to be represented numerically (1&#8211;12), the resulting cabin segmentation is illustrated in <xref rid="sensors-25-05548-f003" ref-type="fig">Figure 3</xref>.</p></sec><sec id="sec2dot6-sensors-25-05548"><title>2.6. Multi-Source Information Fusion Bidirectional Long Short-Term Memory</title><p>To integrate multi-sensor data and effectively extract temporal features for driver attention assessment, this study establishes MSTBi-LSTM capable of learning long-term dependencies. The proposed MSTBi-LSTM fuses sequential data pertaining to driver head pose, gaze regions, and vehicle speed, modeling attention states from these temporal patterns to determine driver attentiveness. Bi-LSTM, derived from LSTM, structurally differs through its bidirectional architecture comprising forward and reverse propagating recurrent networks, whereas standard LSTM employs unidirectional processing. Fundamentally, LSTM units consist of forget gates, input gates, memory cell states, and output gates. Specifically, the forget gate <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> employs a sigmoid function to determine information discarded from the hidden layer, computed as:<disp-formula id="FD14-sensors-25-05548"><label>(14)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the input at time <italic toggle="yes">t</italic>, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the hidden state at time <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the output state of the forget gate at time <italic toggle="yes">t</italic>, <italic toggle="yes">&#963;</italic> signifies the sigmoid activation function, and <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> correspond to the weight matrix and bias vector of the forget gate, respectively. For inputs <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the forget gate yields a value within [0, 1]; an output of 0 signifies complete discarding of the memory cell <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> content, while an output of 1 indicates full retention. The input gate <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> regulates information retention from the current input, computed as:<disp-formula id="FD15-sensors-25-05548"><label>(15)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the input at time <italic toggle="yes">t</italic>, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the hidden state at time <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the output state of the input gate at time <italic toggle="yes">t</italic>, with <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> being the corresponding weight matrix and bias vector for the input gate, respectively; the LSTM unit produces a candidate memory cell state vector <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> via a tanh function, computed as:<disp-formula id="FD16-sensors-25-05548"><label>(16)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The LSTM updates the memory cell state from <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> through its forget gate and input gate, computed as:<disp-formula id="FD17-sensors-25-05548"><label>(17)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>&#8270;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mo>&#8270;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The output gate governs the proportion of the memory cell state <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> that is propagated to the current hidden state <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the LSTM, with its computation and the hidden state update formulated as:<disp-formula id="FD18-sensors-25-05548"><label>(18)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD19-sensors-25-05548"><label>(19)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the output state of the output gate at time <italic toggle="yes">t</italic>, with <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> being its corresponding weight matrix and bias vector, respectively.</p><p>The architecture of the MSTBi-LSTM model is illustrated in <xref rid="sensors-25-05548-f004" ref-type="fig">Figure 4</xref>. The model accepts multiple sets of input sequences, each comprising: (1) the driver&#8217;s head pose, (2) the focal region of the driver&#8217;s gaze, and (3) the vehicle&#8217;s velocity. During Bi-LSTM processing, input sequences are fed into two separate LSTM networks in chronological and reverse-chronological order for feature extraction. The network then concatenates the output vectors from both LSTMs to form the final output at each time step. The forward propagation of the hidden layer output sequence in MSTBi-LSTM is expressed as:<disp-formula id="FD20-sensors-25-05548"><label>(20)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the hidden state of the forward LSTM unit at time <italic toggle="yes">t</italic>, <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> the input value at time <italic toggle="yes">t</italic>, and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> the hidden state of the forward LSTM unit at time <italic toggle="yes">t</italic>&#8722;1.</p><p>The backward propagation of the hidden-layer output sequence in MSTBi-LSTM is formulated as:<disp-formula id="FD21-sensors-25-05548"><label>(21)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8592;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8592;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8592;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the hidden state of the backward LSTM unit at time <italic toggle="yes">t</italic>, <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> the input value at time <italic toggle="yes">t</italic>, and <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mo>&#8592;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> the hidden state of the backward LSTM unit at time <italic toggle="yes">t</italic>&#8722;1.</p><p>As illustrated in <xref rid="sensors-25-05548-f004" ref-type="fig">Figure 4</xref>, the output of Bi-LSTM is fed into a FC layer, ultimately generating the driver&#8217;s attention state through a softmax classifier. The feature dimensions per time step in the proposed MSTBi-LSTM model comprise: (1) three degrees of freedom (head pose), (2) twelve gaze regions, and (3) one vehicle speed metric, resulting in an input dimension of 16. The MSTBi-LSTM configuration employs a sequence length of 300, 128 hidden units, and 1 layer.</p></sec></sec><sec id="sec3-sensors-25-05548"><title>3. Experiment, Results and Discussion</title><sec id="sec3dot1-sensors-25-05548"><title>3.1. Experimental Environment Settings and Dataset</title><p>Model Training Environment: the experimental hardware comprised an Intel Xeon Silver 4210 processor and an NVIDIA RTX 3090 GPU. Software configurations included Ubuntu 22.04 LTS, PyTorch 2.3, CUDA 11.2, ONNX 1.8, and Python 3.12. Deployment Environment: the embedded terminal processor was deployed on an NVIDIA Jetson Orin platform running Ubuntu 20.04 OS, with JetPack 5.1.4 and TensorRT 8.5. To guarantee stable operation under low-illumination conditions, the stereo rig incorporates a near-infrared (NIR) LED illuminator whose dominant emission wavelength is fixed at 940 nm. The baseline of the system is precisely set to 43 mm, and the optics are equipped with lenses of 3.5 mm focal length.</p><p>The YOLO11n Pose model was trained on a dataset comprising 9798 images, partitioned into training, validation, and test sets at a 6:2:2 ratio. Training employed a learning rate of 0.01 with Stochastic Gradient Descent (SGD) optimization, mixed-precision acceleration, and mosaic data augmentation. The model underwent 300 epochs with early stopping triggered after 100 consecutive epochs of no improvement. Evaluation metrics included precision (ratio of true positives to predicted positives), recall (ratio of true positives to actual positives), AP<sub>50&#8211;95</sub> (mean average precision across intersection over union thresholds 0.5&#8211;0.95 in 0.05 increments), parameter count, computational complexity (GFLOPs), inference time, and post-processing time where precision and recall serve as core detection metrics, AP<sub>50-95</sub> reflects comprehensive localization performance.</p><p>For MSTBi-LSTM, training and testing data originated from driver videos segmented into 60 s clips (1500 frames at 25 fps). Training sampled every 5 frames, with features comprising normalized 3D Euler angles, gaze region <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (1~12), and vehicle speed <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> per frame k. The dataset contained 5000 samples (3000 training, 1000 validation, 1000 test). Training configuration used batch size 128, 300 epochs, 0.001 learning rate, Binary Cross Entropy (BCE) loss, and Adam optimization, halted after 100 epochs without accuracy improvement. Primary evaluation metrics were accuracy and inference time.</p><p>The performance of BV-DHPE was evaluated using mean absolute error, root mean square error (RMSE), and standard deviation (SD).</p></sec><sec id="sec3dot2-sensors-25-05548"><title>3.2. Model Training and Testing for Face Detection and Facial Landmark Detection</title><p>To verify operational compliance of YOLO11n Pose, comparative experiments were conducted using YOLOv8n Pose, YOLO11n Pose, and YOLO12n Pose models for face detection and facial landmark detection. Quantitative results for face detection performance are presented in <xref rid="sensors-25-05548-t001" ref-type="table">Table 1</xref>.</p><p>As shown in <xref rid="sensors-25-05548-t001" ref-type="table">Table 1</xref>, YOLO11n Pose achieved a precision of 99.8%, surpassing YOLOv8n Pose (99.7%) by 0.1 percentage points, indicating the lowest false detection rate. YOLO12n Pose (99.6%) showed marginally lower precision than both predecessors. YOLO11n Pose attained a recall of 99.9%, outperforming YOLOv8n Pose (99.6%) and YOLO12n Pose (99.8%), demonstrating optimal control over missed detections.</p><p>Both YOLOv8n Pose and YOLO12n Pose achieved an AP<sub>50-95</sub> of 89.2%, slightly below YOLO11n Pose&#8217;s 90.7%, confirming the latter&#8217;s superior average precision in object detection tasks. YOLO11n Pose and YOLO12n Pose exhibited parameter counts of 2.9 million and 2.8 million, respectively, significantly lower than YOLOv8n Pose&#8217;s 3.3 million. Additionally, YOLO11n Pose and YOLO12n Pose has 7.4 GFLOPs computational cost, indicating lower computational complexity compared to YOLOv8n Pose&#8217;s 9.2 GFLOPs.</p><p>In summary, YOLO11n Pose demonstrates exceptional performance in precision, recall, and average precision while outperforming YOLOv8n Pose in parameter efficiency and computational complexity despite having 0.1 million more parameters than YOLO12n Pose. These characteristics confirm YOLO11n Pose&#8217;s suitability for deployment in computationally constrained environments without compromising high accuracy.</p><p>To demonstrate YOLO11n Pose&#8217;s superior capability in facial landmark detection, quantitative experimental results for YOLOv8n Pose, YOLO11n Pose, and YOLO12n Pose are presented in <xref rid="sensors-25-05548-t002" ref-type="table">Table 2</xref>.</p><p>As shown in <xref rid="sensors-25-05548-t002" ref-type="table">Table 2</xref>, YOLO11n Pose demonstrates the highest recall (99.9%), while YOLOv8n Pose and YOLO12n Pose achieve 99.6% and 99.7% recall respectively. Regarding precision, both YOLO11n Pose and YOLOv8n Pose attain 99.7%, with YOLO12n Pose slightly lower at 99.5%. In this study, YOLO11n Pose achieves the highest AP<sub>50-95</sub> of 94.5%, indicating superior average precision across intersection over union thresholds, whereas YOLOv8n Pose and YOLO12n Pose score 93.0% and 93.1% respectively. Inference time and post-processing time serve as critical metrics for computational efficiency evaluation: YOLO11n Pose delivers optimal inference latency at 1.1 ms, marginally outperforming YOLOv8n Pose (1.1 ms) and YOLO12n Pose (1.7 ms). For post-processing time, YOLO11n Pose similarly excels at 1.0 ms compared to YOLOv8n Pose&#8217;s 1.2 ms and YOLO12n Pose&#8217;s 1.3 ms. Cumulatively, YOLO11n Pose surpasses counterparts in recall, inference efficiency, and post-processing speed, validating its efficacy and accuracy for facial landmark detection tasks; consequently, this research adopts YOLO11n Pose for both face detection and facial landmark localization.</p></sec><sec id="sec3dot3-sensors-25-05548"><title>3.3. Comparative Experiments on Driver Head Posture Estimation</title><p>To demonstrate the superior accuracy of the proposed head pose estimation method over monocular approaches, a comparative experiment was conducted. The IM600 sensor (Chenyi Electronic Technology Factory, Zhongshan City, China) captured ground-truth driver head poses, while pose estimations were simultaneously generated using both conventional monocular vision methods and the proposed BV-DHPE. <xref rid="sensors-25-05548-f005" ref-type="fig">Figure 5</xref> presents the Euler angles of head pose estimated through distinct methodologies, along with their quantitative error from ground truth.</p><p>As illustrated in <xref rid="sensors-25-05548-f005" ref-type="fig">Figure 5</xref>, experimental validation across pitch, roll, and yaw axes reveals that both monocular vision and the proposed BV-DHPE framework deliver accurate Euler-angle estimates under quasi-static conditions when referenced against IMU-derived ground truth. When the head undergoes rapid angular excursions, however, both modalities exhibit error amplification; yet the magnitude and sensitivity differ markedly between approaches. In the pitch axis, monocular vision registers errors up to 12.1&#176;, whereas BV-DHPE confines the peak error to 3.6&#176;. A similar trend is observed for roll and yaw: monocular estimates reach 9.3&#176; at extreme angles, while BV-DHPE maintains errors below 6.5&#176;. This consistent disparity is attributed to the degradation of facial-landmark reliability at large angles, which disproportionately degrades monocular precision. By integrating depth-constrained refinement, BV-DHPE attenuates these effects, sustaining sub-degree accuracy even under high-amplitude motion and corroborating the critical contribution of geometric constraints to robust pose estimation.</p><p><xref rid="sensors-25-05548-f006" ref-type="fig">Figure 6</xref> presents box-and-whisker plots comparing the angular errors (in degrees) for pitch, yaw, and roll Euler angles estimated using monocular vision and the proposed BV-DHPE method. BV-DHPE consistently demonstrates superior performance across all axes, yielding lower median errors, reduced extreme deviations, and tighter error distributions compared to the monocular baseline. Specifically, for pitch estimation, the monocular approach yields MAE = 2.0&#176;, RMSE = 2.7&#176;, median error = 1.3&#176;, maximum error = 12.1&#176;, and SD = 1.8&#176;. BV-DHPE significantly reduces these metrics to 0.8&#176;, 0.9&#176;, 0.7&#176;, 3.6&#176;, and 0.6&#176;, respectively, achieving a 60.0% improvement in MAE. In the yaw direction, monocular vision registers MAE = 1.4&#176;, RMSE = 2.0&#176;, median = 1.0&#176;, maximum = 9.3&#176;, and SD = 1.4&#176;; BV-DHPE lowers these to 1.0&#176;, 1.4&#176;, 0.7&#176;, 6.5&#176;, and 1.0&#176;, corresponding to a 28.6% MAE reduction. Similarly, for roll, monocular results (MAE = 1.1&#176;, RMSE = 1.5&#176;, median = 1.0&#176;, maximum = 7.7&#176;, SD = 0.9&#176;) are improved by BV-DHPE to 0.6&#176;, 0.8&#176;, 0.5&#176;, 3.5&#176;, and 0.5&#176;, representing a 45.5% MAE reduction. BV-DHPE achieved 44.7% reduction in head pose MAE compared to monocular vision methods. These results substantiate that integrating binocular depth cues effectively mitigates scale ambiguity inherent in monocular systems, substantially enhancing both the accuracy and robustness of head-pose estimation.</p></sec><sec id="sec3dot4-sensors-25-05548"><title>3.4. Model Training and Testing for Multi-Source Temporal Bidirectional Long Short-Term Memory</title><p>To demonstrate that MSTBi-LSTM&#8217;s fusion of multi-sensor data enhances predictive performance, ablation experiments were conducted, with results presented in <xref rid="sensors-25-05548-t003" ref-type="table">Table 3</xref>. This table delineates the impact of different data sources combinations on MSTBi-LSTM&#8217;s performance, specifically evaluating three features: head pose, fixation region, and truck speed. The study aims to assess how these combinations affect the model&#8217;s accuracy and inference time.</p><p>When solely utilizing head pose as input, MSTBi-LSTM achieved 85.2% accuracy. This indicates that head pose constitutes an effective feature capable of providing sufficient information for classification tasks. However, reliance on a single feature constrains the model&#8217;s capacity to capture complex patterns, thereby limiting overall performance.</p><p>Upon introducing the fixation region feature, accuracy increased to 89.7%. This improvement confirms the complementary relationship between fixation region and head pose features, enriching informational input and enhancing classification performance. Notably, inference time remained unchanged despite the added feature, demonstrating the model&#8217;s computational efficiency.</p><p>With the further inclusion of truck speed, accuracy rose to 93.2%. This result underscores the criticality of feature complementarity, as the dynamic temporal characteristics of vehicle speed provide behavioral cues that strengthen predictive capability. Crucially, inference time persisted at 0.1 ms even with three features, attesting to MSTBi-LSTM&#8217;s efficiency and stability in multi-feature processing.</p><p>The MSTBi-LSTM model was trained using five-fold cross-validation on the combined training and validation datasets. As shown in <xref rid="sensors-25-05548-t003" ref-type="table">Table 3</xref>, the accuracy achieved by MSTBi-LSTM through five-fold cross-validation is 0.3% higher than that obtained using only the validation set. The similarity in the results between the two approaches suggests that the training process yields a stable outcome.</p><p><xref rid="sensors-25-05548-t003" ref-type="table">Table 3</xref> reveals a progressive accuracy enhancement with feature augmentation while inference time remains stable. This demonstrates that strategic feature selection and fusion can substantially boost model performance without incurring significant computational overhead. The most pronounced accuracy gain occurred with truck speed integration, likely attributable to its delivery of critical dynamic driving behavior signals that empower MSTBi-LSTM to better interpret and predict driving states.</p></sec><sec id="sec3dot5-sensors-25-05548"><title>3.5. Deployment Experiment of BV-DHPE and MSTBi LSTM</title><p>To demonstrate sustained high performance of YOLO11n Pose and MSTBi-LSTM when deployed on Jetson Orin, we conducted deployment experiments. The trained YOLO11n Pose model was exported to TensorRT format and executed with FP16 precision on Jetson Orin Nano. Experimental results are presented in <xref rid="sensors-25-05548-t004" ref-type="table">Table 4</xref>.</p><p>As shown in <xref rid="sensors-25-05548-t004" ref-type="table">Table 4</xref>, metrics including precision, recall, and AP<sub>50-95</sub> decreased when using float16 precision compared to float32 on NVIDIA Jetson Orin. This degradation primarily stems from reduced numerical representation accuracy in float16, causing partial loss of fine-grained details that impairs detection accuracy. Due to the substantially lower computational power of NVIDIA Jetson Orin versus NVIDIA RTX 3090, inference latency increased significantly after deployment on NVIDIA Jetson Orin. Nevertheless, the model maintained real-time inference capability. Therefore, our approach employs float16 precision for model deployment.</p><p>The trained MSTBi-LSTM model was exported to TensorRT format and subsequently deployed on Jetson Orin for inference testing. Experimental results are presented in <xref rid="sensors-25-05548-t005" ref-type="table">Table 5</xref>.</p><p>As shown in <xref rid="sensors-25-05548-t005" ref-type="table">Table 5</xref>, when using float16 precision, the MSTBi-LSTM model trained with five-fold cross-validation experiences a 0.3% drop in accuracy and a 2-millisecond increase in inference latency. This performance trade-off is deemed acceptable; hence, our deployment framework employs float16 precision for MSTBi-LSTM inference.</p></sec><sec id="sec3dot6-sensors-25-05548"><title>3.6. In-Vehicle Testing of Driver Attention State Recognition Method</title><p>To comprehensively evaluate the performance of the RT-DASR method in real vehicular environments, this study employs four key metrics: Accuracy, TPR (True Positive Rate, recall), FPR (False Positive Rate, false detection rate), and detection latency. These metrics collectively assess RT-DASR&#8217;s discriminative capability and real-time performance from multiple perspectives.</p><p>The True Positive Rate (TPR) represents the proportion of actual distraction instances correctly identified by RT-DASR. This metric directly quantifies RT-DASR&#8217;s missed detection risk, indicating its ability to recognize genuine distraction events. Higher TPR values signify greater detection accuracy and lower missed detection risks. In practical applications, high TPR is critical for driving safety, effectively reducing potential accident risks caused by oversight.</p><p>The False Positive Rate (FPR) denotes the proportion of normal driving instances erroneously classified as distraction states by RT-DASR. This metric reflects the system&#8217;s false alarm risk, characterizing its tendency to misidentify normal driving behavior as distraction. Lower FPR values indicate higher recognition accuracy during normal driving and reduced false alarms. Maintaining low FPR is essential for optimizing user experience and minimizing unnecessary interventions.</p><p>Detection latency refers to the processing time required for RT-DASR to output attention state determinations, measured in milliseconds per frame. This metric directly determines system real-time performance, with timing benchmarks obtained from Jetson Orin platform measurements.</p><p>Experimental data comprises 1440 h of real-world operation records from 40 mining truck drivers, with video samples covering daylight (53%) and nighttime (47%) conditions. The collection of these data has been approved by 40 mining-truck drivers. From this dataset, 600 distracted-driving clips and 3000 non-distracted clips were annotated, each spanning 60 s. The distribution of the drivers&#8217; gaze regions in the dataset is illustrated in <xref rid="sensors-25-05548-f007" ref-type="fig">Figure 7</xref>, which intuitively reflects the attention distribution of drivers when operating mining trucks. The gaze region with the highest frequency of drivers&#8217; gaze is region 1, followed by region 4. This indicates that mining-truck drivers spend the majority of their time focusing on the road ahead. Gaze regions 2, 3, 5, and 6 represent the drivers&#8217; scanning motions of the road conditions in front of the vehicle. Gaze regions 7 and 8 typically correspond to the drivers&#8217; actions of observing vehicles behind. Gaze regions 9 and 10 usually represent the drivers&#8217; observation of the dashboard. The relatively low frequency of gaze in regions 11 and 12 suggests that the information provided by these regions is less frequently required by the drivers.</p><p>A comparative experiment on driver attention detection was conducted using a test dataset comprising data from 40 mining-truck drivers, pitting the conventional monocular approach against the RT-DASR method. The detailed results of the experiment are presented in <xref rid="sensors-25-05548-t006" ref-type="table">Table 6</xref>.</p><p>The monocular method demonstrates relatively lower performance in accuracy and TPR metrics at 80.1% and 80.4% respectively. This indicates limited accuracy and recall in identifying distraction levels. Furthermore, its higher FPR (19.8%) implies excessive false alarms during recognition, compromising practical reliability. A detection latency of 18.2 ms suggests satisfactory real-time performance.</p><p>RT-DASR exhibits significant advantages across all metrics. Achieving 90.4% accuracy and 90.7% TPR, the method demonstrates superior recognition accuracy and recall, indicating enhanced state discrimination capability through binocular depth information. The lower FPR (8.8%) further validates its effectiveness in reducing false alarms. The F1 score of RT-DASR is 12% higher than that of the monocular approach, indicating that RT-DASR maintains good performance even when the test data categories are imbalanced. Although the detection latency measures 21.5 ms, marginally higher than the monocular approach, this delay remains acceptable in practical applications, with performance benefits potentially offsetting the limitation.</p><p>Through visual analytics, we identified failure cases in RT-DASR recognition, with representative examples illustrated in <xref rid="sensors-25-05548-f008" ref-type="fig">Figure 8</xref>. As illustrated in <xref rid="sensors-25-05548-f008" ref-type="fig">Figure 8</xref>, false negatives in the proposed method primarily resulted from feature-point matching failures during extreme head rotations.</p><p>In summary, the proposed method achieves higher accuracy and reliability in distraction-level recognition tasks. Despite marginally increased detection latency, substantial improvements in accuracy, TPR, and FPR collectively demonstrate enhanced operational value for real-world deployment.</p></sec></sec><sec sec-type="conclusions" id="sec4-sensors-25-05548"><title>4. Conclusions</title><p>This study systematically evaluates RT-DASR for driver state recognition, integrating YOLO11n Pose facial landmark detection with MSTBi-LSTM temporal modeling. Key findings demonstrate that YOLO11n Pose achieves optimal balance between precision and computational efficiency (1.1 ms inference latency), outperforming comparable architectures in facial feature extraction. The progressive feature augmentation strategy reveals critical insights: fixation regions improve accuracy by 4.5%, while truck speed integration delivers a further 3.5% gain, collectively achieving 93.2% classification accuracy without compromising the 0.1 ms inference speed of MSTBi-LSTM.</p><p>The proposed BV-DHPE method demonstrably enhances head-pose estimation accuracy and robustness compared to monocular vision. Quantitative evaluation revealed significant reductions in angular error metrics across all Euler angles (pitch, yaw, roll) when using BV-DHPE. Specifically, MAE reduction of 44.7% was achieved, alongside markedly tighter error distributions and lower extreme deviations. These results demonstrate that binocular depth compensation effectively reduces errors in monocular head pose estimation caused by depth deficiency, enabling more accurate and robust pose estimation under dynamic conditions.</p><p>The proposed RT-DASR system exhibits significant advantages over monocular approaches, with 10.3% higher accuracy (90.4%) and 11.0% lower FPR (8.8%), validating the effectiveness of binocular depth information for distraction recognition. Despite a 3.3 ms latency increase compared to monocular systems, the 21.5 ms processing time remains practical for real-world deployment. When deployed on edge devices, Float16 inference maintains functional feasibility with accuracy degradation within 0.3%, overcoming computational constraints while preserving real-time performance.</p><p>This study has its limitations. Compared with conventional vehicles, mining trucks are substantially larger, demand a broader visual field from the operator, and require simultaneous monitoring of multiple control panels and displays. These characteristics increase the prevalence of behaviors that are classified as non-safety-critical distractions. Consequently, distraction-detection methodologies developed for mining trucks are not readily transferable to standard passenger vehicles. Furthermore, the dataset employed herein diverges markedly from those utilized in prior work, precluding the establishment of a unified evaluation benchmark and rendering direct comparison with state-of-the-art distraction-detection approaches impractical.</p><p>Error analysis indicates extreme head rotations as the primary failure mode, suggesting need for robust feature-point matching. This study advances driver monitoring systems by establishing a robust benchmark for real-time attention recognition. Subsequent research will focus on wide-angle head rotation scenarios, employing multi-camera configurations to construct multi-view stereo vision. This approach will achieve comprehensive head rotation coverage, thereby preventing pose estimation failures caused by missing facial landmarks.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, X.C.; Methodology, S.Z.; Validation, S.Z.; Formal analysis, W.Z.; Investigation, W.Z.; Resources, X.C.; Data curation, W.Z. and X.C.; Writing&#8212;original draft, S.Z.; Visualization, Y.L.; Supervision, Y.L., X.C. and H.L.; Project administration, Y.L. and H.L.; Funding acquisition, H.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Authors Shuhui Zhou and Yulong Liu was employed by the company CGNPC Uranium Resources Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relation-ships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05548"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Akiduki</surname><given-names>T.</given-names></name><name name-style="western"><surname>Nagasawa</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Omae</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Arakawa</surname><given-names>T.</given-names></name><name name-style="western"><surname>Takahashi</surname><given-names>H.</given-names></name></person-group><article-title>Inattentive Driving Detection Using Body-Worn Sensors: Feasibility Study</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>352</elocation-id><pub-id pub-id-type="doi">10.3390/s22010352</pub-id><pub-id pub-id-type="pmid">35009898</pub-id><pub-id pub-id-type="pmcid">PMC8749514</pub-id></element-citation></ref><ref id="B2-sensors-25-05548"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Halin</surname><given-names>A.</given-names></name><name name-style="western"><surname>Verly</surname><given-names>J.G.</given-names></name><name name-style="western"><surname>Van Droogenbroeck</surname><given-names>M.</given-names></name></person-group><article-title>Survey and Synthesis of State of the Art in Driver Monitoring</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>5558</elocation-id><pub-id pub-id-type="doi">10.3390/s21165558</pub-id><pub-id pub-id-type="pmid">34450999</pub-id><pub-id pub-id-type="pmcid">PMC8402294</pub-id></element-citation></ref><ref id="B3-sensors-25-05548"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jegham</surname><given-names>I.</given-names></name><name name-style="western"><surname>Ben Khalifa</surname><given-names>A.</given-names></name><name name-style="western"><surname>Alouani</surname><given-names>I.</given-names></name><name name-style="western"><surname>Mahjoub</surname><given-names>M.A.</given-names></name></person-group><article-title>A Novel Public Dataset for Multimodal Multiview and Multispectral Driver Distraction Analysis: 3MDAD</article-title><source>Signal Process. Image Commun.</source><year>2020</year><volume>88</volume><fpage>115960</fpage><pub-id pub-id-type="doi">10.1016/j.image.2020.115960</pub-id></element-citation></ref><ref id="B4-sensors-25-05548"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>G.</given-names></name><name name-style="western"><surname>Karray</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name></person-group><article-title>A Survey on Vision-Based Driver Distraction Analysis</article-title><source>J. Syst. Archit.</source><year>2021</year><volume>121</volume><fpage>102319</fpage><pub-id pub-id-type="doi">10.1016/j.sysarc.2021.102319</pub-id></element-citation></ref><ref id="B5-sensors-25-05548"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>L.</given-names></name></person-group><article-title>Driving Attention State Detection Based on GRU-EEGNet</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5086</elocation-id><pub-id pub-id-type="doi">10.3390/s24165086</pub-id><pub-id pub-id-type="pmid">39204804</pub-id><pub-id pub-id-type="pmcid">PMC11358947</pub-id></element-citation></ref><ref id="B6-sensors-25-05548"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Si</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>T.</given-names></name></person-group><article-title>Eye-SCAN: Eye-Movement-Attention-Based Spatial Channel Adaptive Network for Traffic Accident Prediction</article-title><source>Pattern Recognit.</source><year>2025</year><volume>165</volume><fpage>111590</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2025.111590</pub-id></element-citation></ref><ref id="B7-sensors-25-05548"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Si</surname><given-names>T.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Q.</given-names></name></person-group><article-title>Driver Cognitive Distraction Detection Based on Eye Movement Behavior and Integration of Multi-View Space-Channel Feature</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>266</volume><fpage>125975</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.125975</pub-id></element-citation></ref><ref id="B8-sensors-25-05548"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>S.</given-names></name><name name-style="western"><surname>Du</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name></person-group><article-title>SLAFusion: Attention Fusion Based on SAX and LSTM for Dangerous Driving Behavior Detection</article-title><source>Inf. Sci.</source><year>2023</year><volume>640</volume><fpage>119063</fpage><pub-id pub-id-type="doi">10.1016/j.ins.2023.119063</pub-id></element-citation></ref><ref id="B9-sensors-25-05548"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>R.</given-names></name><name name-style="western"><surname>Guan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name></person-group><article-title>An Efficient Multi-Task Learning CNN for Driver Attention Monitoring</article-title><source>J. Syst. Archit.</source><year>2024</year><volume>148</volume><fpage>103085</fpage><pub-id pub-id-type="doi">10.1016/j.sysarc.2024.103085</pub-id></element-citation></ref><ref id="B10-sensors-25-05548"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saleem</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Siddiqui</surname><given-names>H.U.R.</given-names></name><name name-style="western"><surname>Raza</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Rustam</surname><given-names>F.</given-names></name><name name-style="western"><surname>Dudley</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ashraf</surname><given-names>I.</given-names></name></person-group><article-title>A Systematic Review of Physiological Signals Based Driver Drowsiness Detection Systems</article-title><source>Cognit. Neurodyn.</source><year>2022</year><volume>17</volume><fpage>1229</fpage><lpage>1259</lpage><pub-id pub-id-type="doi">10.1007/s11571-022-09898-9</pub-id><pub-id pub-id-type="pmid">37786662</pub-id><pub-id pub-id-type="pmcid">PMC10542071</pub-id></element-citation></ref><ref id="B11-sensors-25-05548"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chung</surname><given-names>W.-Y.</given-names></name></person-group><article-title>Electroencephalogram-Based Approaches for Driver Drowsiness Detection and Management: A Review</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>1100</elocation-id><pub-id pub-id-type="doi">10.3390/s22031100</pub-id><pub-id pub-id-type="pmid">35161844</pub-id><pub-id pub-id-type="pmcid">PMC8840041</pub-id></element-citation></ref><ref id="B12-sensors-25-05548"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Satti</surname><given-names>A.T.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>E.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>S.</given-names></name></person-group><article-title>Microneedle Array Electrode-Based Wearable EMG System for Detection of Driver Drowsiness through Steering Wheel Grip</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>5091</elocation-id><pub-id pub-id-type="doi">10.3390/s21155091</pub-id><pub-id pub-id-type="pmid">34372329</pub-id><pub-id pub-id-type="pmcid">PMC8347525</pub-id></element-citation></ref><ref id="B13-sensors-25-05548"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name></person-group><article-title>Automatic Detection of Driver Fatigue Using Driving Operation Information for Transportation Safety</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>1212</elocation-id><pub-id pub-id-type="doi">10.3390/s17061212</pub-id><pub-id pub-id-type="pmid">28587072</pub-id><pub-id pub-id-type="pmcid">PMC5492517</pub-id></element-citation></ref><ref id="B14-sensors-25-05548"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arakawa</surname><given-names>T.</given-names></name></person-group><article-title>Trends and Future Prospects of the Drowsiness Detection and Estimation Technology</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>7921</elocation-id><pub-id pub-id-type="doi">10.3390/s21237921</pub-id><pub-id pub-id-type="pmid">34883924</pub-id><pub-id pub-id-type="pmcid">PMC8659813</pub-id></element-citation></ref><ref id="B15-sensors-25-05548"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>X.</given-names></name></person-group><article-title>Recognition of Driver&#8217;s Mental Workload Based on Physiological Signals, a Comparative Study</article-title><source>Biomed. Signal Process. Control</source><year>2022</year><volume>71</volume><elocation-id>103094</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2021.103094</pub-id></element-citation></ref><ref id="B16-sensors-25-05548"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>R.U.</given-names></name><name name-style="western"><surname>Leonardi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Migliorati</surname><given-names>P.</given-names></name><name name-style="western"><surname>Benini</surname><given-names>S.</given-names></name></person-group><article-title>Head Pose Estimation: A Survey of the Last Ten Years</article-title><source>Signal Process. Image Commun.</source><year>2021</year><volume>99</volume><fpage>116479</fpage><pub-id pub-id-type="doi">10.1016/j.image.2021.116479</pub-id></element-citation></ref><ref id="B17-sensors-25-05548"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>R.-Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.-L.</given-names></name></person-group><article-title>Driving Stress Estimation in Physiological Signals Based on Hierarchical Clustering and Multi-View Intact Space Learning</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>13141</fpage><lpage>13154</lpage><pub-id pub-id-type="doi">10.1109/TITS.2021.3120435</pub-id></element-citation></ref><ref id="B18-sensors-25-05548"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Debie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Fernandez Rojas</surname><given-names>R.</given-names></name><name name-style="western"><surname>Fidock</surname><given-names>J.</given-names></name><name name-style="western"><surname>Barlow</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kasmarik</surname><given-names>K.</given-names></name><name name-style="western"><surname>Anavatti</surname><given-names>S.</given-names></name><name name-style="western"><surname>Garratt</surname><given-names>M.</given-names></name><name name-style="western"><surname>Abbass</surname><given-names>H.A.</given-names></name></person-group><article-title>Multimodal Fusion for Objective Assessment of Cognitive Workload: A Review</article-title><source>IEEE Trans. Cybern.</source><year>2021</year><volume>51</volume><fpage>1542</fpage><lpage>1555</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2019.2939399</pub-id><pub-id pub-id-type="pmid">31545761</pub-id></element-citation></ref><ref id="B19-sensors-25-05548"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barra</surname><given-names>P.</given-names></name><name name-style="western"><surname>Barra</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bisogni</surname><given-names>C.</given-names></name><name name-style="western"><surname>De Marsico</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nappi</surname><given-names>M.</given-names></name></person-group><article-title>Web-Shaped Model for Head Pose Estimation: An Approach for Best Exemplar Selection</article-title><source>IEEE Trans. Image Process.</source><year>2020</year><volume>29</volume><fpage>5457</fpage><lpage>5468</lpage><pub-id pub-id-type="doi">10.1109/TIP.2020.2984373</pub-id></element-citation></ref><ref id="B20-sensors-25-05548"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Celestino</surname><given-names>J.</given-names></name><name name-style="western"><surname>Marques</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nascimento</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Costeira</surname><given-names>J.P.</given-names></name></person-group><article-title>2D Image Head Pose Estimation via Latent Space Regression under Occlusion Settings</article-title><source>Pattern Recognit.</source><year>2023</year><volume>137</volume><fpage>109288</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2022.109288</pub-id></element-citation></ref><ref id="B21-sensors-25-05548"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Jha</surname><given-names>S.</given-names></name><name name-style="western"><surname>Busso</surname><given-names>C.</given-names></name></person-group><article-title>Temporal Head Pose Estimation from Point Cloud in Naturalistic Driving Conditions</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>8063</fpage><lpage>8076</lpage><pub-id pub-id-type="doi">10.1109/TITS.2021.3075350</pub-id></element-citation></ref><ref id="B22-sensors-25-05548"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name></person-group><article-title>Lightweight Convolutional Neural Network for Counting Densely Piled Steel Bars</article-title><source>Autom. Constr.</source><year>2023</year><volume>146</volume><fpage>104692</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2022.104692</pub-id></element-citation></ref><ref id="B23-sensors-25-05548"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>K.</given-names></name></person-group><article-title>Recognition of Gangues from Color Images Using Convolutional Neural Networks with Attention Mechanism</article-title><source>Measurement</source><year>2023</year><volume>206</volume><fpage>112273</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2022.112273</pub-id></element-citation></ref><ref id="B24-sensors-25-05548"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Essahraui</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lamaakal</surname><given-names>I.</given-names></name><name name-style="western"><surname>El Hamly</surname><given-names>I.</given-names></name><name name-style="western"><surname>Maleh</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ouahbi</surname><given-names>I.</given-names></name><name name-style="western"><surname>El Makkaoui</surname><given-names>K.</given-names></name><name name-style="western"><surname>Filali Bouami</surname><given-names>M.</given-names></name><name name-style="western"><surname>P&#322;awiak</surname><given-names>P.</given-names></name><name name-style="western"><surname>Alfarraj</surname><given-names>O.</given-names></name><name name-style="western"><surname>Abd El-Latif</surname><given-names>A.A.</given-names></name></person-group><article-title>Real-Time Driver Drowsiness Detection Using Facial Analysis and Machine Learning Techniques</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>812</elocation-id><pub-id pub-id-type="doi">10.3390/s25030812</pub-id><pub-id pub-id-type="pmid">39943451</pub-id><pub-id pub-id-type="pmcid">PMC11819803</pub-id></element-citation></ref><ref id="B25-sensors-25-05548"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>Semantically-Enhanced Feature Extraction with CLIP and Transformer Networks for Driver Fatigue Detection</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>7948</elocation-id><pub-id pub-id-type="doi">10.3390/s24247948</pub-id><pub-id pub-id-type="pmid">39771685</pub-id><pub-id pub-id-type="pmcid">PMC11679248</pub-id></element-citation></ref><ref id="B26-sensors-25-05548"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Valle</surname><given-names>R.</given-names></name><name name-style="western"><surname>Buenaposada</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Baumela</surname><given-names>L.</given-names></name></person-group><article-title>Multi-Task Head Pose Estimation in-the-Wild</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>43</volume><fpage>2874</fpage><lpage>2881</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.3046323</pub-id><pub-id pub-id-type="pmid">33351746</pub-id></element-citation></ref><ref id="B27-sensors-25-05548"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>D.</given-names></name></person-group><article-title>A Survey of Identity Recognition via Data Fusion and Feature Learning</article-title><source>Inf. Fusion</source><year>2023</year><volume>91</volume><fpage>694</fpage><lpage>712</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2022.10.032</pub-id></element-citation></ref><ref id="B28-sensors-25-05548"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name></person-group><article-title>Content-Based Multiple Evidence Fusion on EEG and Eye Movements for Mild Depression Recognition</article-title><source>Comput. Methods Programs Biomed.</source><year>2022</year><volume>226</volume><elocation-id>107100</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2022.107100</pub-id><pub-id pub-id-type="pmid">36162244</pub-id></element-citation></ref><ref id="B29-sensors-25-05548"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jaafar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Lachiri</surname><given-names>Z.</given-names></name></person-group><article-title>Multimodal Fusion Methods with Deep Neural Networks and Meta-Information for Aggression Detection in Surveillance</article-title><source>Expert Syst. Appl.</source><year>2023</year><volume>211</volume><fpage>118523</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2022.118523</pub-id></element-citation></ref><ref id="B30-sensors-25-05548"><label>30.</label><element-citation publication-type="webpage"><article-title>GitHub-Ultralytics/Ultralytics: Ultralytics YOLO11</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/ultralytics" ext-link-type="uri">https://github.com/ultralytics/ultralytics</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-23">(accessed on 23 July 2025)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05548-f001" orientation="portrait"><label>Figure 1</label><caption><p>The framework of RT-DASR method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05548-g001.jpg"/></fig><fig position="float" id="sensors-25-05548-f002" orientation="portrait"><label>Figure 2</label><caption><p>Structure of the YOLO11n Pose model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05548-g002.jpg"/></fig><fig position="float" id="sensors-25-05548-f003" orientation="portrait"><label>Figure 3</label><caption><p>Schematic representation of driver gaze region distribution.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05548-g003.jpg"/></fig><fig position="float" id="sensors-25-05548-f004" orientation="portrait"><label>Figure 4</label><caption><p>Structure of the MSTBi-LSTM model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05548-g004.jpg"/></fig><fig position="float" id="sensors-25-05548-f005" orientation="portrait"><label>Figure 5</label><caption><p>Head Pose estimates derived from distinct methodologies and their quantitative error from ground truth. (<bold>a</bold>) experimental results of pitch angle; (<bold>b</bold>) experimental results of roll angle; (<bold>c</bold>) experimental results of yaw angle.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05548-g005.jpg"/></fig><fig position="float" id="sensors-25-05548-f006" orientation="portrait"><label>Figure 6</label><caption><p>Box-and-whisker plots comparing head pose estimation methods.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05548-g006.jpg"/></fig><fig position="float" id="sensors-25-05548-f007" orientation="portrait"><label>Figure 7</label><caption><p>Distribution of driver gaze regions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05548-g007.jpg"/></fig><fig position="float" id="sensors-25-05548-f008" orientation="portrait"><label>Figure 8</label><caption><p>Failure case visualization of attention state recognition errors in RT-DASR.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05548-g008.jpg"/></fig><table-wrap position="float" id="sensors-25-05548-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05548-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparative face detection results across models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AP<sub>50-95</sub> (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters (M)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GFLOPs</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8n Pose</td><td align="center" valign="middle" rowspan="1" colspan="1">99.7</td><td align="center" valign="middle" rowspan="1" colspan="1">99.6</td><td align="center" valign="middle" rowspan="1" colspan="1">89.2</td><td align="center" valign="middle" rowspan="1" colspan="1">3.3</td><td align="center" valign="middle" rowspan="1" colspan="1">9.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLO11n Pose</td><td align="center" valign="middle" rowspan="1" colspan="1">99.8</td><td align="center" valign="middle" rowspan="1" colspan="1">99.9</td><td align="center" valign="middle" rowspan="1" colspan="1">90.7</td><td align="center" valign="middle" rowspan="1" colspan="1">2.9</td><td align="center" valign="middle" rowspan="1" colspan="1">7.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO12n Pose</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.4</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05548-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05548-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparative facial landmark detection results across models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AP<sub>50-95</sub> (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference (ms)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Postprocess (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8n Pose</td><td align="center" valign="middle" rowspan="1" colspan="1">99.7</td><td align="center" valign="middle" rowspan="1" colspan="1">99.6</td><td align="center" valign="middle" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.1</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLO11n Pose</td><td align="center" valign="middle" rowspan="1" colspan="1">99.7</td><td align="center" valign="middle" rowspan="1" colspan="1">99.9</td><td align="center" valign="middle" rowspan="1" colspan="1">94.5</td><td align="center" valign="middle" rowspan="1" colspan="1">1.1</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO12n Pose</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05548-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05548-t003_Table 3</object-id><label>Table 3</label><caption><p>Impact of distinct input data sources on MSTBi-LSTM performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Head Pose</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Fixation Region</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Truck Speed</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Five-Fold Cross-Validation</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">85.2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">93.2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05548-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05548-t004_Table 4</object-id><label>Table 4</label><caption><p>Deployment experiment results of BV-DHPE.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AP<sub>50-95</sub> (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO11n Pose</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05548-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05548-t005_Table 5</object-id><label>Table 5</label><caption><p>Deployment experiment results of MSTBi-LSTM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Five-Fold Cross-Validation</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">MSTBi-LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" rowspan="1" colspan="1">2.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSTBi-LSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05548-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05548-t006_Table 6</object-id><label>Table 6</label><caption><p>Real vehicle experimental results of monocular method and RT-DASR.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1 Score</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TPR (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPR (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">monocular </td><td align="center" valign="middle" rowspan="1" colspan="1">80.1</td><td align="center" valign="middle" rowspan="1" colspan="1">80.3</td><td align="center" valign="middle" rowspan="1" colspan="1">80.4</td><td align="center" valign="middle" rowspan="1" colspan="1">19.8</td><td align="center" valign="middle" rowspan="1" colspan="1">18.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RT-DASR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.5</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>