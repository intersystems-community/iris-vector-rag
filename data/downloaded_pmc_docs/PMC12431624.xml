<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431624</article-id><article-id pub-id-type="pmcid-ver">PMC12431624.1</article-id><article-id pub-id-type="pmcaid">12431624</article-id><article-id pub-id-type="pmcaiid">12431624</article-id><article-id pub-id-type="doi">10.3390/s25175435</article-id><article-id pub-id-type="publisher-id">sensors-25-05435</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>High-Accuracy Deep Learning-Based Detection and Classification Model in Color-Shift Keying Optical Camera Communication Systems</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-0506-7850</contrib-id><name name-style="western"><surname>Vera</surname><given-names initials="FVV">Francisca V. Vera</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05435" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-3037-292X</contrib-id><name name-style="western"><surname>Mu&#241;oz</surname><given-names initials="L">Leonardo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05435" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2390-2133</contrib-id><name name-style="western"><surname>P&#233;rez</surname><given-names initials="F">Francisco</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05435" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3588-6115</contrib-id><name name-style="western"><surname>Alvarez</surname><given-names initials="LB">Lisandra Bravo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05435" ref-type="aff">1</xref><xref rid="c1-sensors-25-05435" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1622-3180</contrib-id><name name-style="western"><surname>Montejo-S&#225;nchez</surname><given-names initials="S">Samuel</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-05435" ref-type="aff">2</xref><xref rid="c1-sensors-25-05435" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4262-3882</contrib-id><name name-style="western"><surname>Icaza</surname><given-names initials="VM">Vicente Matus</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af3-sensors-25-05435" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0550-0253</contrib-id><name name-style="western"><surname>Rodr&#237;guez-L&#243;pez</surname><given-names initials="L">Lien</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af4-sensors-25-05435" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5450-3661</contrib-id><name name-style="western"><surname>Saavedra</surname><given-names initials="G">Gabriel</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05435" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Guzman</surname><given-names initials="BG">Borja Genoves</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Morales-C&#233;spedes</surname><given-names initials="M">Maximo</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05435"><label>1</label>Department of Electrical Engineering, Universidad de Concepci&#243;n, Edmundo Larenas 219, Concepci&#243;n 4030000, Chile; <email>franciscavera92@gmail.com</email> (F.V.V.V.); <email>lemunoz2017@udec.cl</email> (L.M.); <email>francisperez@udec.cl</email> (F.P.); <email>gasaavedra@udec.cl</email> (G.S.)</aff><aff id="af2-sensors-25-05435"><label>2</label>Instituto Universitario de Investigaci&#243;n y Desarrollo Tecnol&#243;gico, Universidad Tecnol&#243;gica Metropolitana, Ignacio Valdivieso 2409, Santiago 8940000, Chile</aff><aff id="af3-sensors-25-05435"><label>3</label>Institute for Technological Development and Innovation in Communications, Universidad de Las Palmas de Gran Canaria, 35001 Las Palmas de Gran Canaria, Spain; <email>vicente.matus@ulpgc.es</email></aff><aff id="af4-sensors-25-05435"><label>4</label>Facultad de Ingenier&#237;a, Universidad San Sebasti&#225;n, Lientur 1457, Concepci&#243;n 4030000, Chile; <email>lien.rodriguez@uss.cl</email></aff><author-notes><corresp id="c1-sensors-25-05435"><label>*</label>Correspondence: <email>lisanbravo@udec.cl</email> (L.B.A.); <email>smontejo@utem.cl</email> (S.M.-S.)</corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5435</elocation-id><history><date date-type="received"><day>31</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>27</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>29</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 15:25:32.480"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05435.pdf"/><abstract><p>The growing number of connected devices has strained traditional radio frequency wireless networks, driving interest in alternative technologies such as optical wireless communications (OWC). Among OWC solutions, optical camera communication (OCC) stands out as a cost-effective option because it leverages existing devices equipped with cameras, such as smartphones and security systems, without requiring specialized hardware. This paper proposes a novel deep learning-based detection and classification model designed to optimize the receiver&#8217;s performance in an OCC system utilizing color-shift keying (CSK) modulation. The receiver was experimentally validated using an <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LED matrix transmitter and a CMOS camera receiver, achieving reliable communication over distances ranging from 30 cm to 3 m under varying ambient conditions. The system employed CSK modulation to encode data into eight distinct color-based symbols transmitted at fixed frequencies. Captured image sequences of these transmissions were processed through a YOLOv8-based detection and classification framework, which achieved <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>98.4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>% accuracy in symbol recognition. This high precision minimizes transmission errors, validating the robustness of the approach in real-world environments. The results highlight OCC&#8217;s potential for low-cost applications, where high-speed data transfer and long-range are unnecessary, such as Internet of Things connectivity and vehicle-to-vehicle communication. Future work will explore adaptive modulation and coding schemes as well as the integration of more advanced deep learning architectures to improve data rates and system scalability.</p></abstract><kwd-group><kwd>convolutional neural network (CNN)</kwd><kwd>deep learning</kwd><kwd>optical camara communication (OCC)</kwd></kwd-group><funding-group><award-group><funding-source>ANID FONDECYT Regular</funding-source><award-id>1231826</award-id></award-group><award-group><funding-source>ANID FONDECYT Regular</funding-source><award-id>1241977</award-id></award-group><funding-statement>This research was funded by ANID FONDECYT Regular 1231826 and ANID FONDECYT Regular 1241977.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05435"><title>1. Introduction</title><p>Every year, the technology and communications landscape undergoes exponential growth, driven by the increasing demand for high-speed Internet services. However, while this surge in demand emphasizes the need to improve network capacity [<xref rid="B1-sensors-25-05435" ref-type="bibr">1</xref>], saturation of the radio frequency (RF) spectrum presents significant challenges [<xref rid="B2-sensors-25-05435" ref-type="bibr">2</xref>]. These factors underscore the imperative need to explore new technologies [<xref rid="B3-sensors-25-05435" ref-type="bibr">3</xref>] that, although not directly competing with RF in terms of speed, can effectively complement its use in various scenarios [<xref rid="B4-sensors-25-05435" ref-type="bibr">4</xref>].</p><p>In this context, Optical Wireless Communication (OWC) has emerged as an innovative and viable alternative to the saturated RF spectrum, distinguished by its operation within an unlicensed spectrum and its potential to deliver significantly greater bandwidth than traditional RF systems [<xref rid="B5-sensors-25-05435" ref-type="bibr">5</xref>]. Within the realm of OWC, Optical Camera Communication (OCC) technology has experienced substantial technological advancements and renewed research interest. OCC is incorporated in the IEEE 802.15.7 standard [<xref rid="B6-sensors-25-05435" ref-type="bibr">6</xref>], which reinforces its feasibility for practical applications and its potential for large-scale adoption in specific environments. This standard introduces Color-Shift Keying (CSK) modulation, employed in this study for its capability to transmit bit streams by varying the colors of a light source.</p><p>OCC utilizes hardware from mobile devices to capture video, which acts as a receiver and provides connectivity [<xref rid="B7-sensors-25-05435" ref-type="bibr">7</xref>]. Employing light-emitting diodes for data transmission capitalizes on pre-existing infrastructure, thereby significantly reducing deployment costs [<xref rid="B8-sensors-25-05435" ref-type="bibr">8</xref>]. Furthermore, OCC derives considerable advantage from continuous advancements in image processing and Deep Learning (DL) technologies to enhance the accuracy and reliability of the receiver [<xref rid="B9-sensors-25-05435" ref-type="bibr">9</xref>]. The application of Deep Learning techniques in OCC has been demonstrated to markedly reduce errors in the identification and tracking in receiver devices, even under challenging conditions such as abrupt lighting changes or the presence of obstacles [<xref rid="B10-sensors-25-05435" ref-type="bibr">10</xref>].</p><p>Several papers in the scientific literature have proposed deep learning-based algorithms to enhance OCC [<xref rid="B11-sensors-25-05435" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05435" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05435" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05435" ref-type="bibr">14</xref>]. In [<xref rid="B11-sensors-25-05435" ref-type="bibr">11</xref>], an OCC technique for smart factory systems is introduced, which employs an LED array as the transmitter and utilizes On-Off keying modulation. Artificial intelligence is incorporated for LED detection, resulting in a significant improvement in performance compared to traditional methods. By optimizing parameters such as shutter speed, camera focal length, and appropriate channel coding, the system enables stable communication links over distances of up to 7 m. Conversely, ref. [<xref rid="B12-sensors-25-05435" ref-type="bibr">12</xref>] proposes the design and implementation of a real-time OCC system capable of operating efficiently under high mobility conditions. For this purpose, the YOLOv8 object detection algorithm is employed, which allows for accurate identification of an LED array used as the emission source. The authors of [<xref rid="B13-sensors-25-05435" ref-type="bibr">13</xref>] proposed a display-to-camera optical communication system that uses complementary color barcodes in conjunction with deep neural networks to achieve seamless transmission and reliable communication during normal video playback. This system employs the YOLO model to continuously detect the barcode region on electronic displays and utilizes convolutional neural networks to accurately identify pilot symbols and data embedded in the received images. Furthermore, ref. [<xref rid="B14-sensors-25-05435" ref-type="bibr">14</xref>] reports a study on an OCC-based vehicle-to-vehicle communication system using LED arrays as transmitters and cameras as receivers. In addition, other works have addressed the design of OCC systems capable of maintaining reliable communication in dynamic conditions, such as vehicular and underwater environments. For instance, ref. [<xref rid="B15-sensors-25-05435" ref-type="bibr">15</xref>] applied deep reinforcement learning to achieve ultra-reliable, low-latency vehicular links, ref. [<xref rid="B16-sensors-25-05435" ref-type="bibr">16</xref>] developed a channel-adaptive decoding method for underwater OCC, and [<xref rid="B17-sensors-25-05435" ref-type="bibr">17</xref>] employed machine learning to meet uRLLC requirements in vehicular networks.</p><p>Faced with the unresolved challenge of ensuring stability and accuracy at practical distances and dynamic conditions in OCC systems, this work proposes an approach based on the efficient use of deep learning that goes beyond detecting signal changes or black-and-white symbols by innovatively leveraging the information contained in color. The main contributions of this paper are as follows:<list list-type="bullet"><list-item><p>The implementation and validation of a novel, high-accuracy deep learning-based receiver architecture for an OCC system. Unlike previous approaches that focus on detecting symbol transitions [<xref rid="B18-sensors-25-05435" ref-type="bibr">18</xref>], our proposal uses color as the primary information carrier, focusing on classification of color symbols through the YOLOv8 model, applied for the first time in this context.</p></list-item><list-item><p>The experimental evaluation over communication distances ranging from 30 cm to 3 m, ensuring the results&#8217; applicability to real-world scenarios and confirming system robustness.</p></list-item><list-item><p>The integration of advanced data augmentation techniques, including noise addition and overlaying real-world environmental images, to improve robustness and generalization.</p></list-item><list-item><p>A comprehensive hyperparameter study assessing whether YOLOv8&#8217;s default settings are optimal, further verifying the model&#8217;s suitability for this specific application.</p></list-item></list></p><p>The remainder of this paper is structured as follows: <xref rid="sec2-sensors-25-05435" ref-type="sec">Section 2</xref> describes the methodology, presenting the proposed Deep Learning-based classifier, detailing the phases of data collection, data preprocessing, model selection and training, as well as validation and adjustment of hyperparameters. <xref rid="sec3-sensors-25-05435" ref-type="sec">Section 3</xref> presents the experiments and discusses the results obtained in the validation of the proposal. Finally, <xref rid="sec4-sensors-25-05435" ref-type="sec">Section 4</xref> summarizes the main conclusions and outlines potential future research directions for further extension.</p></sec><sec id="sec2-sensors-25-05435"><title>2. Deep Learning-Based Classifier</title><p>In this section, the design of the system architecture is presented. The challenges inherent to symbol classification in this specific context are addressed. This involves considering aspects such as ambient illumination, channel distance, symbol variability, and algorithm robustness against possible distortions or interferences.</p><sec id="sec2dot1-sensors-25-05435"><title>2.1. Data Collection</title><p>The experimental setup used to generate the dataset is shown in <xref rid="sensors-25-05435-f001" ref-type="fig">Figure 1</xref>. It consists of a transmitter (LED matrix and microcontroller) and a receiver (camera and processing unit), as described below.</p><p>An LED matrix controlled using a microcontroller is used as a transmitter. As a modulation format, we used 8-Color Shift Keying (8-CSK) with colors: yellow, blue, white, cyan, magenta, orange, red, and green. This follows the IEEE 802.15.7-2011 standard [<xref rid="B19-sensors-25-05435" ref-type="bibr">19</xref>], which sets design rules for the 8-CSK constellation to achieve reliable performance. In 8-CSK, every symbol carries 3 bits of information. All symbols used the same spatial arrangement in the LED matrix. To reduce the blooming effect, common in OCC, each symbol includes white LEDs at the top and bottom rows (see <xref rid="sensors-25-05435-f001" ref-type="fig">Figure 1</xref>c). In addition, the microcontroller applies gamma correction to adjust for the nonlinear behavior of the LEDs, using the following transformation on each RGB channel:<disp-formula id="FD1-sensors-25-05435"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>255</mml:mn><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mn>255</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced><mml:mi>&#947;</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent the output and input intensity values of the red, green, and blue channels, respectively. These values are constrained to the range [0, 255]. In this work, the gamma correction factor is set to <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Each transmission sequence consisted of consecutively displaying the eight color symbols on the LED matrix at a frequency of 10 Hz, with each symbol active for a fixed time duration. In our work, eight colors for CSK modulation were chosen to balance data rate and detection accuracy. While it is possible to further expand the color alphabet, this introduces inherent challenges, particularly in distinguishing between visually similar colors, as shown in <xref rid="sensors-25-05435-t001" ref-type="table">Table 1</xref>.</p><p>The receiver was a V2 camera connected to a Raspberry Pi 4. This camera integrates a Sony IMX219 CMOS image sensor [<xref rid="B20-sensors-25-05435" ref-type="bibr">20</xref>]. The camera was configured to record video at 30 frames per second (FPS), resulting in an oversampling factor of 1.5, considering a transmitter symbol rate of 10 Hz. The captured videos contain frames where each symbol is visible under different conditions.</p><p>Five-second videos of the symbol sequences were recorded under varied experimental conditions to introduce diversity in the dataset. The parameters adjusted in each experiment were:<list list-type="bullet"><list-item><p>Channel distance: 50 cm, 100 cm, 150 cm, 300 cm.</p></list-item><list-item><p>Camera exposure time: 500 &#956;s, 1000 &#956;s, 4000 &#956;s, 6000 &#956;s.</p></list-item><list-item><p>Angle between TX and RX: aligned and unaligned matrix.</p></list-item><list-item><p>Controlled light environment: dark room and illuminated room.</p></list-item></list></p><p>Each video was processed frame by frame. Frames affected by symbol transitions or motion artifacts (such as frame 2 in <xref rid="sensors-25-05435-f002" ref-type="fig">Figure 2</xref>) were manually discarded, retaining only clean frames that represent each class [<xref rid="B21-sensors-25-05435" ref-type="bibr">21</xref>]. Due to oversampling, an average of three usable images per class was extracted from each sequence. In total, 3,247 images were collected for training and validation. A separate test set was later acquired under the same parameter ranges, but at different random combinations of distance, angle, and exposure time within the laboratory. This ensured the test set included unseen samples that still followed the same distribution as the training data.</p><p>To provide a comprehensive overview of the experimental conditions, the key system parameters used for data collection are summarized in <xref rid="sensors-25-05435-t002" ref-type="table">Table 2</xref>.</p></sec><sec id="sec2dot2-sensors-25-05435"><title>2.2. Data Preprocessing</title><p>Data pre-processing consists of four stages: data augmentation, dataset splitting, image resizing, and pixel value normalization. The data augmentation technique was applied directly to the raw data, i.e., to the data extracted from the captured videos, without having received any additional operation beyond the removal of non-useful frames. For this, the Albumentations library was used to first define the transformations and then apply them to all the collected images. In addition to these transformations, two additional components were incorporated: the addition of white Gaussian noise with different standard deviations (<inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula>), and the superimposition of an office image on the images of the training, validation, and test sets.</p><p><xref rid="sensors-25-05435-f003" ref-type="fig">Figure 3</xref> illustrates the two transformations performed, which were applied randomly to the images. Thus, the training and validation samples were duplicated and, subsequently, one of three operations was applied: Albumentations transformations, noise addition (with mean 0 and a standard deviation randomly selected from the values (<inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>1.5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), or office image overlay. All these operations have the same probability of being applied to images duplicated from the original data sets. The use of the overlay image was intended to increase the variability in the dataset, thereby enabling the model to learn to recognize the LED matrix in a wider variety of scenarios. This additional variability was necessary due to the exposure times used.</p><p>The samples intended for model evaluation were also duplicated; however, only the noise addition and image overlay operations were applied to them, excluding the Albumentations transformations, since these were used exclusively to improve model learning during training. The data splitting consisted of allocating <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>80</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the data for training and the remaining <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for validation, from the 6494 samples obtained after data augmentation.</p><p>In the data resizing stage, a hyperparameter, known as <italic toggle="yes">imgsz</italic>, is set with the help of the YOLOv8 network, which is responsible for resizing the images to a predefined size. During the training of the deep learning model, YOLOv8 allows flexibility in the input sizes by automatically performing the resizing [<xref rid="B22-sensors-25-05435" ref-type="bibr">22</xref>]. Normalization is an essential preprocessing technique that adjusts the pixel values of the images to a standard range, thus facilitating faster convergence during training and improving model performance. Normalization is automatically and seamlessly integrated as part of the preprocessing stage in YOLOv8 during model training. This automated preprocessing ensures that the input images are prepared consistently and properly before being processed by the CNN. To evaluate the balance of the generated and split dataset, the number of samples per class in each set is obtained, as shown in <xref rid="sensors-25-05435-t003" ref-type="table">Table 3</xref>.</p></sec><sec id="sec2dot3-sensors-25-05435"><title>2.3. Model Selection and Training</title><p>To effectively address the challenge of classifying CSK symbols in real-world Optical Camera Communication (OCC) environments, this work employs a robust deep learning approach based on the YOLOv8 family of convolutional neural networks (CNNs), renowned for its exceptional balance between speed and accuracy. A central component of our methodology is the use of transfer learning: instead of training a model from scratch, which would require a vast amount of labeled data, we leverage a YOLOv8 model pre-trained on the large-scale ImageNet dataset. This enables the model to retain its powerful, generalized feature extraction capabilities while being fine-tuned for the specific task of CSK symbol classification. For our task, we adapted its main backbone, based on CSPDarknet, as a feature extractor and complemented it with a classification head capable of recognizing the eight color symbol classes in our dataset. This backbone integrates optimized convolutional modules, Cross Stage Partial layers for improved efficiency, and Darknet Bottleneck residual connections. Finally, the Spatial Pyramid Pooling Fast layer was replaced with a dedicated classification layer. This final layer transforms the extracted features into output predictions. Although originally designed to support up to 1000 output classes, the model automatically adjusts its final linear layer to 8 during training, matching the number of CSK symbol classes present in the dataset.</p></sec><sec id="sec2dot4-sensors-25-05435"><title>2.4. Validation and Adjustment of Hyperparameters</title><p>A model was initially trained using the default hyperparameters of YOLOv8, with validation enabled. Subsequently, a manual tuning of the key hyperparameters such as the initial learning rate (lr0), optimizer choice (AdamW, SGD), and dropout rate was performed (see <xref rid="sensors-25-05435-t004" ref-type="table">Table 4</xref>), seeking a balance between accuracy, stability, and generalizability. Two values per parameter were defined, and an exhaustive search was applied to train nine different combinations, including the model with default values. Comparing the results, it was observed that the model with adjusted hyperparameters does not represent an improvement compared with the default hyperparameters. Finally, the best model was retrained for 100 epochs to evaluate its performance robustly on the test set. <xref rid="sensors-25-05435-f004" ref-type="fig">Figure 4</xref> shows the loss and the validation results during training of the models used to adjust the hyperparameters.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05435"><title>3. Results</title><p>This section details the performance of our YOLOv8-based classifier in the OCC system, beginning with a summary of the key parameters used to ensure reproducibility.</p><p><xref rid="sensors-25-05435-t005" ref-type="table">Table 5</xref> presents the loss and accuracy results in the validation set for the various training runs performed during this process. This table facilitates the identification of the best hyperparameter settings for the classification task. In this table, the lowest loss value and the highest level of accuracy achieved during the epochs of the validation stage are presented. The validation curves (accuracy and loss vs epochs) for the reported data are shown in <xref rid="sensors-25-05435-f005" ref-type="fig">Figure 5</xref>.</p><p>In <xref rid="sensors-25-05435-f005" ref-type="fig">Figure 5</xref>a, the blue curve (train 1), corresponds to the best model and presents a low loss in most epochs, starting at <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.4961</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and reaching a minimum of <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.2837</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The curve shows a decline until the sixth epoch, where the loss stabilizes around <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. On the other hand, in <xref rid="sensors-25-05435-f005" ref-type="fig">Figure 5</xref>b, it is observed that the accuracy starts <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>90.71</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and achieves <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.69</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> after 20 epochs. This indicates, at first glance, that the model has outstanding performance even with a reduced number of iterations.</p><p>The best model from <xref rid="sensors-25-05435-f005" ref-type="fig">Figure 5</xref> was selected (train 1), and was re-trained using the best hyperparameter configuration. This process generated the training and validation loss graphs, as well as the validation accuracy, which are shown in <xref rid="sensors-25-05435-f006" ref-type="fig">Figure 6</xref>a and <xref rid="sensors-25-05435-f006" ref-type="fig">Figure 6</xref>b, respectively. Training of the final model, minimum loss of <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.27</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and maximum accuracy of <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.85</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were achieved. This accuracy value will be used by YOLO as the criterion to select the best model, which will be used in the test set.</p><p>When analyzing the learning curves of the best model, trained for 100 epochs, in <xref rid="sensors-25-05435-f006" ref-type="fig">Figure 6</xref>a, a marked discrepancy between training loss and validation loss is observed. The training loss decreases rapidly, reaching very low values, while the validation loss remains significantly higher in comparison. However, it is seen that both losses could continue to decrease by further training the model using more epochs, suggesting that the model is not overfitting. <xref rid="sensors-25-05435-f006" ref-type="fig">Figure 6</xref>b shows a high validation accuracy throughout the epochs. The accuracy increases rapidly during the first epochs and stabilizes at values close to 98&#8211;99%. This high validation accuracy, combined with a high validation loss, could preliminarily indicate that the model makes correct predictions in most cases, but with low confidence in its classifications. That is, the model assigns lower probabilities to the correct classes, which is penalized by the loss function without affecting the overall accuracy. However, when analyzing the probabilities associated with each sample during validation, it was confirmed that this is not the case, as all samples were classified with reliabilities higher than <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To evaluate the performance of the final model, a separate test set was used, different from the training and validation data. This set includes subsequently collected images covering various experimental conditions such as channel distances of 60 cm, 70 cm, 110 cm, and 250 cm, as well as different combinations of exposure time, environment variations, illumination levels, and transmitter positions. Additionally, these images were also processed to duplicate their quantity and. Noise and a different overlay image were added to the duplicated set.</p><p>Predictions were made using the final model, with the results summarized in he confusion matrix shown in <xref rid="sensors-25-05435-f007" ref-type="fig">Figure 7</xref>. From the confusion matrix, it is observed that most predictions are concentrated on the main diagonal, indicating correct classification in the vast majority of cases. Classes such as green, blue, and cyan show near-perfect accuracy, with no errors or only one incorrect prediction. However, some specific confusions are observed, especially in the white class, which was misclassified as yellow in eight cases and as cyan in ten, suggesting some difficulty for the model in distinguishing between light spectrum colors or colors possibly influenced by similar lighting conditions. Minor errors are also recorded in other classes, such as yellow and magenta, although without significantly affecting the overall performance of the system.</p><p>It can be concluded that very good results are obtained, in general; however, some exceptions exist, especially among classes with visually similar samples, such as white and yellow, cyan and white, or blue and magenta. This is primarily due to the phenomenon of channel crosstalk, which causes interference between color channels as a result of spectral overlap in the LED array and the camera&#8217;s limitations to distinguish close tones. Additionally, factors like ambient lighting and noise in the capture system contribute to the camera registering color mixtures rather than pure tones, leading to confusion between these classes when interpreted by the YOLOv8 model.</p><p>The samples in each class were correctly classified with high accuracy and, overall, the model demonstrated a high generalization ability with an accuracy of 98.4% in the test set. The confusion matrix, therefore, not only confirms the strong overall performance of the model but also provides a nuanced understanding of its limitations. The strong concentration of correct predictions along the main diagonal, notably for classes like blue and green (with 294 and 296 correct predictions, respectively), demonstrates effective generalization under real testing conditions. However, the confusion between white, cyan, and yellow highlights the inherent physical constraints of the optical camera communication system, such as channel crosstalk and sensor saturation, when processing colors requiring high intensity across multiple RGB channels. Minor misclassifications, like magenta being confused with blue, further reinforce these.</p><p>The strong overall accuracy and robust performance of our model against varying distances and lighting conditions hold significant implications for its practical deployment. A symbol recognition accuracy of approximately 98.4% translates directly into an extremely low data transmission error rate, which is a critical requirement for safety-sensitive applications where reliability is paramount. In vehicle-to-vehicle (V2V) communication, for example, our system could be used to reliably classify emergency signals, such as the flashing lights of an ambulance or police car, or the red light of a traffic signal, providing a robust communication channel in complex and dynamic environments. Similarly, in industrial IoT scenarios, this high reliability is fundamental for the correct execution of machine-to-machine commands, preventing costly operational errors. Therefore, our results not only confirm the model&#8217;s technical efficacy but also validate its potential as a low-cost, high-reliability solution for real-world applications where low-data-rate, high-precision signaling is required.</p><p>In addition, system robustness in real-world applications could be further improved through specific design improvements. For instance, the use of light diffusers in vehicular and other outdoor scenarios would mitigate channel crosstalk, allowing color separation under conditions such as direct sunlight or car headlight interference. Similarly, adaptive gain control mechanisms in the camera receiver would allow automatic adjustment to varying illumination, thereby ensuring stable detection in dynamic outdoor environments. At the communication layer, advanced error correction coding could compensate for residual symbol misclassifications, while temporal and spatial filtering would stabilize detection in settings with strong motion or background variability. These enhancements, although not implemented in this work, are challenges observed in V2V and industrial IoT scenarios.</p></sec><sec sec-type="conclusions" id="sec4-sensors-25-05435"><title>4. Conclusions</title><p>This paper evaluated the performance of the YOLOv8 model in symbol classification within optical camera communication environments. The results consistently demonstrated strong performance in all scenarios analyzed, achieving an accuracy of <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>98.4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on the test set, indicating the effective generalizability of the model. Incorporating noise transformations and varied data augmentation techniques applied to training and validation images was essential for developing a robust and resilient model. These strategies substantially improved generalizability, allowing the model to reach values above <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>93</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> across all evaluation metrics for each symbol class, even when the test set included noise and previously unseen samples.</p><p>From the outset, the model was designed and trained to handle diverse and complex conditions by simulating realistic environments during the training phase, which is essential for its successful deployment and practical application. Misclassified samples were primarily caused by channel crosstalk; specifically, the RGB color information captured by the image sensor is affected by overlapping signals from the red, green, and blue channels. This interference causes certain colors in the LED matrix, which should ideally appear as distinct hues (e.g., white), to be perceived as different colors, such as yellow or cyan. Such distortions highlight the inherent challenges of color-based modulation schemes in OCC systems.</p><p>For future research, two main directions are recommended. First, explore the implementation of the YOLOv8 model for real-time detection and sensing in OCC applications, which could offer valuable tools for automated monitoring and the management of low-latency optical communication systems. Second, developing a more diverse and comprehensive dataset is essential. This dataset should encompass variations such as different background textures, a wider range of channel distances, diverse ambient lighting conditions, and various indoor/outdoor scenarios, which will allow the model to generalize across a broader spectrum of real-world environments.</p></sec></body><back><ack><title>Acknowledgments</title><p>Gabriel Saavedra thanks Instituto Milenio de Investigaci&#243;n en &#243;ptica ICN17-012 and ANID FONDECYT Regular 1231826, Samuel Montejo-S&#225;nchez thanks ANID FONDECYT Regular 1241977.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>This manuscript was prepared by several authors, with each author&#8217;s specific contributions outlined below. Conceptualization, F.V.V.V., L.B.A. and G.S.; methodology, F.V.V.V., L.M., F.P. and V.M.I.; software, F.V.V.V., L.M., F.P. and L.R.-L.; validation, L.B.A., S.M.-S.,V.M.I. and G.S.; formal analysis, L.B.A., F.V.V.V., and V.M.I.; investigation, F.V.V.V., L.M., F.P., L.B.A. and L.R.-L.; resources, S.M.-S., L.R.-L. and G.S.; data curation, F.V.V.V., L.M. and F.P.; writing&#8212;original draft preparation, L.B.A.; writing&#8212;review and editing, F.V.V.V., S.M.-S., L.R.-L. and G.S.; supervision, L.B.A., S.M.-S., V.M.I. and G.S.; project administration, S.M.-S., L.R.-L. and G.S.; funding acquisition, S.M.-S., L.R.-L. and G.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05435"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Haas</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name></person-group><article-title>What is LiFi?</article-title><source>J. Light. Technol.</source><year>2015</year><volume>34</volume><fpage>1533</fpage><lpage>1544</lpage><pub-id pub-id-type="doi">10.1109/JLT.2015.2510021</pub-id></element-citation></ref><ref id="B2-sensors-25-05435"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ahmed</surname><given-names>T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gayah</surname><given-names>V.V.</given-names></name></person-group><article-title>OCC-MP: A Max-Pressure framework to prioritize transit and high occupancy vehicles</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2024</year><volume>166</volume><fpage>104795</fpage><pub-id pub-id-type="doi">10.1016/j.trc.2024.104795</pub-id></element-citation></ref><ref id="B3-sensors-25-05435"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mohsan</surname><given-names>S.A.H.</given-names></name><name name-style="western"><surname>Amjad</surname><given-names>H.</given-names></name></person-group><article-title>A comprehensive survey on hybrid wireless networks: Practical considerations, challenges, applications and research directions</article-title><source>Opt. Quantum Electron.</source><year>2021</year><volume>53</volume><fpage>523</fpage><pub-id pub-id-type="doi">10.1007/s11082-021-03141-1</pub-id></element-citation></ref><ref id="B4-sensors-25-05435"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bravo Alvarez</surname><given-names>L.</given-names></name><name name-style="western"><surname>Montejo-S&#225;nchez</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rodr&#237;guez-L&#243;pez</surname><given-names>L.</given-names></name><name name-style="western"><surname>Azurdia-Meza</surname><given-names>C.</given-names></name><name name-style="western"><surname>Saavedra</surname><given-names>G.</given-names></name></person-group><article-title>A Review of Hybrid VLC/RF Networks: Features, Applications, and Future Directions</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>7545</elocation-id><pub-id pub-id-type="doi">10.3390/s23177545</pub-id><pub-id pub-id-type="pmid">37688001</pub-id><pub-id pub-id-type="pmcid">PMC10490815</pub-id></element-citation></ref><ref id="B5-sensors-25-05435"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lisandra</surname><given-names>B.A.</given-names></name><name name-style="western"><surname>Samuel</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Lien</surname><given-names>R.L.</given-names></name><name name-style="western"><surname>Jos&#233;</surname><given-names>N.K.</given-names></name><name name-style="western"><surname>David</surname><given-names>R.G.</given-names></name><name name-style="western"><surname>Gabriel</surname><given-names>S.</given-names></name></person-group><article-title>Enhanced Network Selection Algorithms for IoT-Home Environments With Hybrid VLC/RF Systems</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>108942</fpage><lpage>108952</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3440194</pub-id></element-citation></ref><ref id="B6-sensors-25-05435"><label>6.</label><element-citation publication-type="book"><std>IEEE Std 802.16-2004</std><source>IEEE Standard for Local and Metropolitan Area Networks-Part 16: Air Interface for Fixed Broad-Band Wireless Access Systems</source><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2004</year></element-citation></ref><ref id="B7-sensors-25-05435"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Co-Occ: Coupling Explicit Feature Fusion with Volume Rendering Regularization for Multi-Modal 3D Semantic Occupancy Prediction</article-title><source>IEEE Robot. Autom. Lett.</source><year>2024</year><volume>9</volume><fpage>5687</fpage><lpage>5694</lpage><pub-id pub-id-type="doi">10.1109/LRA.2024.3396092</pub-id></element-citation></ref><ref id="B8-sensors-25-05435"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dao</surname><given-names>N.N.</given-names></name><name name-style="western"><surname>Do</surname><given-names>T.H.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dustdar</surname><given-names>S.</given-names></name></person-group><article-title>Information Revealed by Vision: A Review on the Next-Generation OCC Standard for AIoV</article-title><source>IT Prof.</source><year>2022</year><volume>24</volume><fpage>58</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1109/MITP.2022.3180354</pub-id></element-citation></ref><ref id="B9-sensors-25-05435"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Utama</surname><given-names>I.B.K.Y.</given-names></name><name name-style="western"><surname>Jang</surname><given-names>Y.M.</given-names></name></person-group><article-title>Enabling Technologies and New Challenges in IEEE 802.15.7 Optical Camera Communications Standard</article-title><source>IEEE Commun. Mag.</source><year>2023</year><volume>62</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCOM.002.2300289</pub-id></element-citation></ref><ref id="B10-sensors-25-05435"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lyu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>X.</given-names></name></person-group><article-title>Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>8940</fpage><lpage>8950</lpage></element-citation></ref><ref id="B11-sensors-25-05435"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jang</surname><given-names>Y.M.</given-names></name></person-group><article-title>An Experimental Demonstration of 2D-MIMO Based Deep Learning for OCC System</article-title><source>Proceedings of the 2024 Fifteenth International Conference on Ubiquitous and Future Networks (ICUFN)</source><conf-loc>Budapest, Hungary</conf-loc><conf-date>2&#8211;5 July 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>165</fpage><lpage>168</lpage></element-citation></ref><ref id="B12-sensors-25-05435"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sitanggang</surname><given-names>O.S.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>V.L.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pamungkas</surname><given-names>R.F.</given-names></name><name name-style="western"><surname>Faridh</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Jang</surname><given-names>Y.M.</given-names></name></person-group><article-title>Design and implementation of a 2D MIMO OCC system based on deep learning</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>7637</elocation-id><pub-id pub-id-type="doi">10.3390/s23177637</pub-id><pub-id pub-id-type="pmid">37688093</pub-id><pub-id pub-id-type="pmcid">PMC10490714</pub-id></element-citation></ref><ref id="B13-sensors-25-05435"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>M.T.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>B.W.</given-names></name></person-group><article-title>DeepCCB-OCC: Deep Learning-Driven Complementary Color Barcode-Based Optical Camera Communications</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>11239</elocation-id><pub-id pub-id-type="doi">10.3390/app122111239</pub-id></element-citation></ref><ref id="B14-sensors-25-05435"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>An LED Detection and Recognition Method Based on Deep Learning in Vehicle Optical Camera Communication</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>80897</fpage><lpage>80905</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3085117</pub-id></element-citation></ref><ref id="B15-sensors-25-05435"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>A.</given-names></name><name name-style="western"><surname>Thomos</surname><given-names>N.</given-names></name><name name-style="western"><surname>Musavian</surname><given-names>L.</given-names></name></person-group><article-title>Achieving uRLLC with Machine Learning Based Vehicular OCC</article-title><source>Proceedings of the GLOBECOM 2022&#8212;2022 IEEE Global Communications Conference</source><conf-loc>Rio de Janeiro, Brazil</conf-loc><conf-date>4&#8211;8 December 2022</conf-date></element-citation></ref><ref id="B16-sensors-25-05435"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Aberathna</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kumarage</surname><given-names>M.</given-names></name><name name-style="western"><surname>Atthanayake</surname><given-names>N.</given-names></name></person-group><article-title>Channel Adaptive Decoding in Underwater Optical Camera Communication Systems</article-title><source>Proceedings of the 2025 5th International Conference on Advanced Research in Computing (ICARC)</source><conf-loc>Belihuloya, Sri Lanka</conf-loc><conf-date>19&#8211;20 February 2025</conf-date></element-citation></ref><ref id="B17-sensors-25-05435"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>A.</given-names></name><name name-style="western"><surname>Thomos</surname><given-names>N.</given-names></name><name name-style="western"><surname>Musavian</surname><given-names>L.</given-names></name></person-group><article-title>Deep Reinforcement Learning-Based Ultra Reliable and Low Latency Vehicular OCC</article-title><source>IEEE Trans. Commun.</source><year>2025</year><volume>73</volume><fpage>3254</fpage><lpage>3267</lpage><pub-id pub-id-type="doi">10.1109/TCOMM.2024.3478108</pub-id></element-citation></ref><ref id="B18-sensors-25-05435"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fernandes</surname><given-names>D.</given-names></name><name name-style="western"><surname>Matus</surname><given-names>V.</given-names></name><name name-style="western"><surname>Figueiredo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Alves</surname><given-names>L.N.</given-names></name></person-group><article-title>Asynchronous encoding scheme for optical camera communication system using two-dimensional transmitter</article-title><source>Proceedings of the 2023 South American Conference On Visible Light Communications (SACVLC)</source><conf-loc>Santiago, Chile</conf-loc><conf-date>8&#8211;10 November 2023</conf-date></element-citation></ref><ref id="B19-sensors-25-05435"><label>19.</label><element-citation publication-type="book"><std>IEEE Std 802.15.7-2011</std><source>IEEE Standard for Local and Metropolitan Area Networks&#8212;Part 15.7: Short-Range Wireless Optical Communication Using Visible Light</source><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2011</year></element-citation></ref><ref id="B20-sensors-25-05435"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Matus</surname><given-names>V.</given-names></name><name name-style="western"><surname>Teli</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Guerra</surname><given-names>V.</given-names></name><name name-style="western"><surname>Jurado-Verdu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zvanovec</surname><given-names>S.</given-names></name><name name-style="western"><surname>Perez-Jimenez</surname><given-names>R.</given-names></name></person-group><article-title>Evaluation of Fog Effects on Optical Camera Communications Link</article-title><source>Proceedings of the 2020 3rd West Asian Symposium on Optical and Millimeter-wave Wireless Communication (WASOWC)</source><conf-loc>Tehran, Iran</conf-loc><conf-date>24&#8211;25 November 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2020</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B21-sensors-25-05435"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Onodera</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hisano</surname><given-names>D.</given-names></name><name name-style="western"><surname>Maruta</surname><given-names>K.</given-names></name><name name-style="western"><surname>Nakayama</surname><given-names>Y.</given-names></name></person-group><article-title>First Demonstration of 512-Color Shift Keying Signal Demodulation Using Neural Equalization for Optical Camera Communication</article-title><source>Proceedings of the Optical Fiber Communication Conference</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>5&#8211;9 March 2023</conf-date><publisher-name>Optica Publishing Group</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2023</year><fpage>Th3H-7</fpage></element-citation></ref><ref id="B22-sensors-25-05435"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Terven</surname><given-names>J.</given-names></name><name name-style="western"><surname>C&#243;rdova-Esparza</surname><given-names>D.M.</given-names></name><name name-style="western"><surname>Romero-Gonz&#225;lez</surname><given-names>J.A.</given-names></name></person-group><article-title>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS</article-title><source>Mach. Learn. Knowl. Extr.</source><year>2023</year><volume>5</volume><fpage>1680</fpage><lpage>1716</lpage><pub-id pub-id-type="doi">10.3390/make5040083</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05435-f001" orientation="portrait"><label>Figure 1</label><caption><p>System architecture and experimental setup: (<bold>a</bold>) Corresponding block diagram illustrating the system components and data flow; (<bold>b</bold>) experimental setup used for data acquisition; (<bold>c</bold>) colors on the LED matrix.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05435-g001.jpg"/></fig><fig position="float" id="sensors-25-05435-f002" orientation="portrait"><label>Figure 2</label><caption><p>Frames captured from the cyan and magenta classes when oversampling.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05435-g002.jpg"/></fig><fig position="float" id="sensors-25-05435-f003" orientation="portrait"><label>Figure 3</label><caption><p>Transformations performed on the images in the test set.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05435-g003.jpg"/></fig><fig position="float" id="sensors-25-05435-f004" orientation="portrait"><label>Figure 4</label><caption><p>Learning and loss curves during model training. (<bold>a</bold>) Optimizer = AdamW, lr0 = 0.01, dropout = 0.0; (<bold>b</bold>) Optimizer = AdamW, lr0 = 0.01, dropout = 0.2; (<bold>c</bold>) Optimizer = AdamW, lr0 = 0.001, dropout = 0.0; (<bold>d</bold>) Optimizer = AdamW, lr0 = 0.001, dropout = 0.2.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05435-g004.jpg"/></fig><fig position="float" id="sensors-25-05435-f005" orientation="portrait"><label>Figure 5</label><caption><p>(<bold>a</bold>) Loss and (<bold>b</bold>) accuracy validation curves for trained models.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05435-g005.jpg"/></fig><fig position="float" id="sensors-25-05435-f006" orientation="portrait"><label>Figure 6</label><caption><p>(<bold>a</bold>) Loss and (<bold>b</bold>) accuracy of the final model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05435-g006.jpg"/></fig><fig position="float" id="sensors-25-05435-f007" orientation="portrait"><label>Figure 7</label><caption><p>Confounding matrix before adding variability to the test set.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05435-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05435-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05435-t001_Table 1</object-id><label>Table 1</label><caption><p>RGB values of the different classes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Class</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">R</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">G</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">B</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yellow</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Blue</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">White</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cyan</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mangenta</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Orange</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td><td align="center" valign="middle" rowspan="1" colspan="1">165</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Red</td><td align="center" valign="middle" rowspan="1" colspan="1">255</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Green</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">255</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05435-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05435-t002_Table 2</object-id><label>Table 2</label><caption><p>Key System Parameters for Experimental Setup.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter Category</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specifics</th></tr></thead><tbody><tr><td colspan="2" align="left" valign="middle" rowspan="1">
<bold>Transmitter (TX)</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LED Matrix</td><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LED array</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Modulation Scheme</td><td align="left" valign="middle" rowspan="1" colspan="1">Color-Shift Keying (CSK) with 8 levels</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Transmission Frequency</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10 Hz</td></tr><tr><td colspan="2" align="left" valign="middle" rowspan="1">
<bold>Receiver (RX)</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Camera Type</td><td align="left" valign="middle" rowspan="1" colspan="1">CMOS V2 with Sony IMX219 sensor</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frame Rate</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30 fps</td></tr><tr><td colspan="2" align="left" valign="middle" rowspan="1">
<bold>Channel &amp; Environment</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Communication Distance</td><td align="left" valign="middle" rowspan="1" colspan="1">30 cm to 3 m</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Lighting Conditions</td><td align="left" valign="middle" rowspan="1" colspan="1">Dark room, illuminated room</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TX-RX Alignment</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Aligned and unaligned</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05435-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05435-t003_Table 3</object-id><label>Table 3</label><caption><p>Number of samples per class in the datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Class</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Training</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Validation</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Test</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0: Yellow</td><td align="center" valign="middle" rowspan="1" colspan="1">646</td><td align="center" valign="middle" rowspan="1" colspan="1">162</td><td align="center" valign="middle" rowspan="1" colspan="1">288</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1: Blue</td><td align="center" valign="middle" rowspan="1" colspan="1">630</td><td align="center" valign="middle" rowspan="1" colspan="1">158</td><td align="center" valign="middle" rowspan="1" colspan="1">294</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2: White</td><td align="center" valign="middle" rowspan="1" colspan="1">670</td><td align="center" valign="middle" rowspan="1" colspan="1">168</td><td align="center" valign="middle" rowspan="1" colspan="1">290</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3: Cyan</td><td align="center" valign="middle" rowspan="1" colspan="1">657</td><td align="center" valign="middle" rowspan="1" colspan="1">165</td><td align="center" valign="middle" rowspan="1" colspan="1">290</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4: Mangenta</td><td align="center" valign="middle" rowspan="1" colspan="1">646</td><td align="center" valign="middle" rowspan="1" colspan="1">162</td><td align="center" valign="middle" rowspan="1" colspan="1">292</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5: Orange</td><td align="center" valign="middle" rowspan="1" colspan="1">664</td><td align="center" valign="middle" rowspan="1" colspan="1">166</td><td align="center" valign="middle" rowspan="1" colspan="1">290</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6: Red</td><td align="center" valign="middle" rowspan="1" colspan="1">649</td><td align="center" valign="middle" rowspan="1" colspan="1">163</td><td align="center" valign="middle" rowspan="1" colspan="1">292</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7: Green</td><td align="center" valign="middle" rowspan="1" colspan="1">630</td><td align="center" valign="middle" rowspan="1" colspan="1">158</td><td align="center" valign="middle" rowspan="1" colspan="1">296</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5192</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1302</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2332</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05435-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05435-t004_Table 4</object-id><label>Table 4</label><caption><p>Hyperparameters to be adjusted in YOLOv8.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Default</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Values</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Definition</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">lr0</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">Initial learning rate. Adjusting this value is crucial to the optimization process, influencing how quickly the model weights are updated.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">auto</td><td align="center" valign="middle" rowspan="1" colspan="1">AdamW, SGD</td><td align="center" valign="middle" rowspan="1" colspan="1">Choice of optimizer for training. Options include SGD, AdamW, etc. Affects convergence speed and stability.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout rate for regularization in classification tasks, preventing overfitting by randomly omitting units during training.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05435-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05435-t005_Table 5</object-id><label>Table 5</label><caption><p>Validation loss and accuracy results when training with different hyperparameter configurations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Train</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Optimizer</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">lr0</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dropout</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Loss</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">AdamW</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.000714</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.2837</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.69</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">AdamW</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.2918</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.69</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">AdamW</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.2902</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.00</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">AdamW</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.2850</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.39</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">AdamW</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.2862</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.46</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">SGD</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.2852</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.23</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">SGD</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.2838</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.54</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">SGD</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.3046</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>98.46</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SGD</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.3049</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>98.46</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>