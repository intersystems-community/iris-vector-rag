<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431225</article-id><article-id pub-id-type="pmcid-ver">PMC12431225.1</article-id><article-id pub-id-type="pmcaid">12431225</article-id><article-id pub-id-type="pmcaiid">12431225</article-id><article-id pub-id-type="doi">10.3390/s25175432</article-id><article-id pub-id-type="publisher-id">sensors-25-05432</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Vision and 2D LiDAR Fusion-Based Navigation Line Extraction for Autonomous Agricultural Robots in Dense Pomegranate Orchards</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Shi</surname><given-names initials="Z">Zhikang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05432" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Bai</surname><given-names initials="Z">Ziwen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-05432" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yi</surname><given-names initials="K">Kechuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-05432" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Qiu</surname><given-names initials="B">Baijing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af2-sensors-25-05432" ref-type="aff">2</xref><xref rid="af3-sensors-25-05432" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Dong</surname><given-names initials="X">Xiaoya</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af2-sensors-25-05432" ref-type="aff">2</xref><xref rid="af3-sensors-25-05432" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="Q">Qingqing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05432" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jiang</surname><given-names initials="C">Chunxia</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05432" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="X">Xinwei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><xref rid="af1-sensors-25-05432" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Huang</surname><given-names initials="X">Xin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05432" ref-type="aff">1</xref><xref rid="c1-sensors-25-05432" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Chen</surname><given-names initials="X">Xiyuan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05432"><label>1</label>College of Intelligent Manufacturing, Anhui Science and Technology University, Chuzhou 239000, China; <email>shizk1212@163.com</email> (Z.S.); <email>baizw@ahstu.edu.cn</email> (Z.B.); <email>yikc@ahstu.edu.cn</email> (K.Y.); <email>qqwang@ahstu.edu.cn</email> (Q.W.); <email>jiangcx@ahstu.edu.cn</email> (C.J.); <email>zhangxw@ahstu.edu.cn</email> (X.Z.)</aff><aff id="af2-sensors-25-05432"><label>2</label>Key Laboratory of Plant Protection Engineering, Ministry of Agriculture and Rural Affairs, Jiangsu University, Zhenjiang 212013, China; <email>qbj@ujs.edu.cn</email> (B.Q.); <email>dongxiaoya@ujs.edu.cn</email> (X.D.)</aff><aff id="af3-sensors-25-05432"><label>3</label>Key Laboratory of Modern Agricultural Equipment and Technology, Ministry of Education, Jiangsu University, Zhenjiang 212013, China</aff><author-notes><corresp id="c1-sensors-25-05432"><label>*</label>Correspondence: <email>huangxin@ahstu.edu.cn</email>; Tel.: +86-183-5608-0665</corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5432</elocation-id><history><date date-type="received"><day>20</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>21</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>26</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05432.pdf"/><abstract><p>To address the insufficient accuracy of traditional single-sensor navigation methods in dense planting environments of pomegranate orchards, this paper proposes a vision and LiDAR fusion-based navigation line extraction method for orchard environments. The proposed method integrates a YOLOv8-ResCBAM trunk detection model, a reverse ray projection fusion algorithm, and geometric constraint-based navigation line fitting techniques. The object detection model enables high-precision real-time detection of pomegranate tree trunks. A reverse ray projection algorithm is proposed to convert pixel coordinates from visual detection into three-dimensional rays and compute their intersections with LiDAR scanning planes, achieving effective association between visual and LiDAR data. Finally, geometric constraints are introduced to improve the RANSAC algorithm for navigation line fitting, combined with Kalman filtering techniques to reduce navigation line fluctuations. Field experiments demonstrate that the proposed fusion-based navigation method improves navigation accuracy over single-sensor methods and semantic-segmentation methods, reducing the average lateral error to 5.2 cm, yielding an average lateral error RMS of 6.6 cm, and achieving a navigation success rate of 95.4%. These results validate the effectiveness of the vision and 2D LiDAR fusion-based approach in complex orchard environments and provide a viable route toward autonomous navigation for orchard robots.</p></abstract><kwd-group><kwd>orchard navigation</kwd><kwd>sensor fusion</kwd><kwd>object detection</kwd><kwd>DBSCAN algorithm</kwd><kwd>RANSAC algorithm</kwd><kwd>agricultural robots</kwd></kwd-group><funding-group><award-group><funding-source>Talent Introduction Project of Anhui Science and Technology University</funding-source><award-id>JXYJ202204</award-id></award-group><award-group><funding-source>2024 University Research Projects of Anhui Province</funding-source><award-id>2024AH050305</award-id></award-group><funding-statement>This study was funded by the Talent Introduction Project of Anhui Science and Technology University (JXYJ202204), 2024 University Research Projects of Anhui Province (Study on the mechanism of plant protection UAV obstacle avoidance on droplet deposition in the obstacle neighborhoods in rice terraces (2024AH050305)).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05432"><title>1. Introduction</title><p>As an important economic crop, pomegranate cultivation has developed rapidly in many regions [<xref rid="B1-sensors-25-05432" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05432" ref-type="bibr">2</xref>]. However, the level of mechanization in orchard operations significantly lags behind other agricultural sectors. Particularly in densely planted pomegranate orchards, traditional manual operations are not only labor-intensive but also exhibit low efficiency, severely constraining the rapid development of the pomegranate industry [<xref rid="B3-sensors-25-05432" ref-type="bibr">3</xref>].</p><p>Compared with other orchards, pomegranate orchards exhibit distinct environmental characteristics and operational challenges. At the young-tree stage, trunk diameters are only 5&#8211;15 cm, and the planting density is high, with a row spacing of 2&#8211;3 m and an in-row spacing of 1&#8211;2 m, as shown in <xref rid="sensors-25-05432-f001" ref-type="fig">Figure 1</xref>. Beyond the spatial layout, ambient conditions directly affect both experiments and algorithm performance: fluctuations in temperature and humidity broaden the image luminance dynamic range; strong direct sunlight and dappled canopy shadows cause unstable image contrast and textures, increasing the difficulty of trunk detection; the ground surface contains both bare soil and weeds, and the soil is predominantly sandy loam&#8212;under dry conditions dust is raised, whereas under wet conditions water-surface glare and wheel slip further degrade the measurement quality of the camera and LiDAR. These climate, soil, and ground-surface factors, compounded with the high-density planting structure, impose more demanding operating conditions than typical orchards, making autonomous navigation more challenging in object perception, robust fitting, and path tracking [<xref rid="B4-sensors-25-05432" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05432" ref-type="bibr">5</xref>].</p><p>In the development of autonomous navigation technology for orchards, traditional orchard navigation methods primarily rely on Global Navigation Satellite Systems (GNSS). However, in dense canopy-occluded orchard environments, GNSS signals are susceptible to interference or loss, resulting in significantly degraded positioning accuracy [<xref rid="B6-sensors-25-05432" ref-type="bibr">6</xref>]. To overcome this limitation, researchers have developed sensor-based active positioning and navigation technologies, among which Light Detection and Ranging (LiDAR) and machine vision technologies have attracted considerable attention due to their robustness in complex environments [<xref rid="B7-sensors-25-05432" ref-type="bibr">7</xref>].</p><p>In the field of machine vision navigation, Liu et al. [<xref rid="B8-sensors-25-05432" ref-type="bibr">8</xref>] developed a single-stage navigation path extraction network (NPENet) that directly predicts road centerlines through an end-to-end approach, simplifying traditional multi-stage processing workflows. Cao et al. [<xref rid="B9-sensors-25-05432" ref-type="bibr">9</xref>] implemented grape trunk detection based on an improved YOLOv8-Trunk network and fitted navigation lines using the least squares method. Yang et al. [<xref rid="B10-sensors-25-05432" ref-type="bibr">10</xref>] proposed a visual navigation path extraction method based on neural networks and pixel scanning, improving the segmentation performance for orchard road condition information and background environments. Silva et al. [<xref rid="B11-sensors-25-05432" ref-type="bibr">11</xref>] presented a deep learning crop row detection algorithm capable of achieving robust visual navigation under different field conditions. Gai et al. [<xref rid="B12-sensors-25-05432" ref-type="bibr">12</xref>] developed a depth camera-based crop row detection system particularly suitable for robot navigation under tall crop canopies such as corn and sorghum, addressing GPS signal occlusion issues. Winterhalter et al. [<xref rid="B13-sensors-25-05432" ref-type="bibr">13</xref>] proposed a method using pattern Hough transform to detect small plants, improving the accuracy of crop row detection in early growth stages. Gai et al. [<xref rid="B14-sensors-25-05432" ref-type="bibr">14</xref>] introduced a color and depth image fusion method that enhanced crop detection accuracy under high weed density conditions. However, these single vision-based methods experience significant performance degradation under adverse lighting conditions and struggle to directly obtain precise distance information, limiting their application in precision navigation [<xref rid="B15-sensors-25-05432" ref-type="bibr">15</xref>].</p><p>Jiang et al. [<xref rid="B16-sensors-25-05432" ref-type="bibr">16</xref>] developed a 2D LiDAR-based orchard spraying robot navigation system that achieved trunk detection and path planning through DBSCAN clustering and RANSAC algorithms. Li et al. [<xref rid="B17-sensors-25-05432" ref-type="bibr">17</xref>] proposed a 3D LiDAR-based autonomous navigation method for orchard mobile robots, optimizing point cloud processing efficiency through octree data structures. Abanay et al. [<xref rid="B18-sensors-25-05432" ref-type="bibr">18</xref>] developed a ROS-based 2D LiDAR navigation system for strawberry greenhouse environments. Liu et al. [<xref rid="B19-sensors-25-05432" ref-type="bibr">19</xref>] presented a LiDAR-based navigation system for standardized apple trees, but it primarily relied on regularly arranged tree configurations. Malavazi et al. [<xref rid="B20-sensors-25-05432" ref-type="bibr">20</xref>] developed an autonomous agricultural robot navigation algorithm based on 2D LiDAR, achieving crop row line extraction through an improved PEARL method that enables robust navigation without prior crop information. Jiang et al. [<xref rid="B21-sensors-25-05432" ref-type="bibr">21</xref>] proposed a 3D LiDAR SLAM-based orchard spraying robot navigation system, combining NDT and ICP point cloud registration algorithms to improve positioning accuracy. Jiang et al. [<xref rid="B22-sensors-25-05432" ref-type="bibr">22</xref>] developed a 3D LiDAR SLAM-based autonomous navigation system for stacked cage farming, achieving reliable robot localization and mapping. Firkat et al. [<xref rid="B23-sensors-25-05432" ref-type="bibr">23</xref>] proposed FGSeg, a LiDAR-based field ground segmentation algorithm for agricultural robots, achieving high-precision ground detection and obstacle recognition through seed ground point extraction. However, these single LiDAR-based methods still face challenges of point cloud processing efficiency and high computational resource consumption in complex agricultural environments, with detection accuracy significantly declining when trunk diameters are small, limiting their widespread application in low-cost agricultural robots [<xref rid="B24-sensors-25-05432" ref-type="bibr">24</xref>].</p><p>To overcome the limitations of single sensors, multi-sensor fusion technology has gradually become a research hotspot in the field of orchard navigation. Jiang et al. [<xref rid="B25-sensors-25-05432" ref-type="bibr">25</xref>] employed thermal cameras and LiDAR as sensors, utilizing YOLACT (You Only Look At CoefficienTs) deep learning for navigation, object detection, and image segmentation, integrating accurate distance data from LiDAR to achieve real-time navigation based on vehicle position. Ban et al. [<xref rid="B26-sensors-25-05432" ref-type="bibr">26</xref>] proposed a camera-LiDAR-IMU fusion-based navigation line extraction method for corn fields, achieving precise agricultural robot navigation through feature-level fusion. Kang et al. [<xref rid="B27-sensors-25-05432" ref-type="bibr">27</xref>] presented a high-resolution LiDAR and camera fusion method for fruit localization, achieving fusion by projecting point clouds onto image planes. Han et al. [<xref rid="B28-sensors-25-05432" ref-type="bibr">28</xref>] developed a LiDAR and vision-based obstacle avoidance and navigation system, using calibration parameters to transform both data types into the same coordinate system. Shalal et al. [<xref rid="B29-sensors-25-05432" ref-type="bibr">29</xref>] developed an orchard mapping and mobile robot localization system based on onboard camera and laser scanner data fusion, achieving tree detection and precise robot positioning through geometric transformation registration of laser point clouds with visual images. Xue et al. [<xref rid="B30-sensors-25-05432" ref-type="bibr">30</xref>] proposed a trunk detection method based on LiDAR and visual data fusion, employing evidence theory for multi-sensor information fusion and converting laser point clouds and visual images to a unified coordinate system through calibration parameters, improving trunk detection robustness. Yu et al. [<xref rid="B31-sensors-25-05432" ref-type="bibr">31</xref>] noted in their review of agricultural 3D reconstruction technology that multi-sensor fusion technology combines LiDAR&#8217;s precise distance information with cameras&#8217; rich texture information, providing more comprehensive environmental perception capabilities for precision agriculture. Ji et al. [<xref rid="B32-sensors-25-05432" ref-type="bibr">32</xref>] proposed a farmland obstacle detection and recognition method based on fused point cloud data, achieving reliable obstacle detection in complex agricultural environments through multi-sensor data fusion. However, these multi-sensor fusion methods still face challenges in data synchronization, sensor calibration, and real-time processing, requiring further algorithm optimization to reduce computational complexity and improve system practicality [<xref rid="B33-sensors-25-05432" ref-type="bibr">33</xref>]. Currently, specialized research on autonomous navigation for pomegranate orchards is relatively limited, with most studies focusing on mature orchard environments such as apple and citrus orchards [<xref rid="B34-sensors-25-05432" ref-type="bibr">34</xref>], showing limited adaptability for dense planting scenarios.</p><p>In current agricultural robot navigation, many studies adopt conventional algorithm-based navigation line extraction methods, such as RANSAC-based navigation line fitting, simple fusion of sensor data, and SLAM-based global path planning. However, these existing methods often fail to cope effectively with densely planted orchards characterized by complex occlusions and diverse scene changes, especially when tree trunks are thin and the spacing between trunks is narrow. They are susceptible to environmental disturbances, occlusions, and illumination variations, leading to insufficient navigation accuracy and poor adaptability under complex conditions, and they cannot realize real-time orchard autonomous navigation without a prior map. To address these issues, this paper proposes a fusion-based navigation line extraction scheme that combines a YOLOv8-ResCBAM trunk detection model with 2D LiDAR data, aiming to tackle the real-time autonomous navigation challenges in densely planted orchards that current techniques struggle with and to improve navigation accuracy and stability. This research employs a YOLOv8-ResCBAM trunk detection network model based on improved YOLOv8 [<xref rid="B35-sensors-25-05432" ref-type="bibr">35</xref>], enhancing target trunk detection accuracy in dense environments through the introduction of residual connections and dual attention mechanisms. A reverse ray projection fusion algorithm is proposed that differs from traditional point cloud-to-image plane projection methods [<xref rid="B36-sensors-25-05432" ref-type="bibr">36</xref>], projecting visual detection information into three-dimensional space and computing intersections with LiDAR scanning planes to achieve precise data association. A navigation line fitting algorithm based on geometric constraints and directional confidence is constructed, improving RANSAC algorithm fitting accuracy and stability through analysis of inter-tree geometric relationships and construction of angle histograms. The technical contributions of this research provide references for solving autonomous navigation problems in densely planted orchard environments and hold significant practical value for advancing orchard mechanization and intelligent development.</p></sec><sec id="sec2-sensors-25-05432"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-25-05432"><title>2.1. Experimental Platform</title><p>To validate the effectiveness of the proposed navigation system in pomegranate orchard environments, this study constructed an autonomous mobile platform as the experimental setup, as shown in <xref rid="sensors-25-05432-f002" ref-type="fig">Figure 2</xref>. The platform integrates an Intel RealSense D455 camera (Intel Corporation, Santa Clara, CA, USA), a SLAMTEC 2D LiDAR (Slamtec Co., Ltd., Shanghai, China), and an NVIDIA Jetson Xavier NX edge-computing module (NVIDIA Corporation, Santa Clara, CA, USA), and other equipment. The parameters of each component in the experimental platform are presented in <xref rid="sensors-25-05432-t001" ref-type="table">Table 1</xref>.</p><p>The communication architecture of the entire system is based on the ROS Topics mechanism. Sensor data (camera images and LiDAR point clouds) are published as corresponding topics through their respective ROS drivers. The trunk detection module subscribes to the camera image topic and publishes detection results (bounding box center coordinates and confidence scores) as new ROS topics. The navigation line fitting module simultaneously subscribes to both the visual detection results topic and the LiDAR point cloud topic, and after fusion processing, publishes the fitted navigation line parameters to the path tracking control module. The path tracking control module receives the navigation line information, calculates the required linear and angular velocity control commands, and publishes them to the chassis motion controller through ROS Topics, thereby achieving autonomous navigation control of the mobile platform.</p></sec><sec id="sec2dot2-sensors-25-05432"><title>2.2. Monocular Camera and 2D LiDAR Data Fusion</title><p>In the complex autonomous navigation process of pomegranate orchards, accurate fruit tree positioning is the key to achieving orchard navigation. Sensor calibration aims to realize data fusion between vision and LiDAR to obtain accurate orchard information. This study conducted camera intrinsic parameter calibration, camera-LiDAR extrinsic parameter calibration, and joint calibration of multiple sensors to the chassis coordinate system, ensuring accurate alignment of different sensor data within a unified coordinate system [<xref rid="B37-sensors-25-05432" ref-type="bibr">37</xref>].</p><sec id="sec2dot2dot1-sensors-25-05432"><title>2.2.1. Coordinate System Definition and Establishment</title><p>To achieve effective fusion of multi-sensor data, this study established a standardized coordinate system framework. The chassis coordinate system is defined as the reference coordinate system, with its origin located at the chassis center point at a height above the ground. The LiDAR coordinate system has its origin at the LiDAR scanning center at a height above the ground. The camera coordinate system has its origin at the RGB camera optical center at a height above the ground. All proposed coordinate systems follow the convention where the <italic toggle="yes">x</italic>-axis points toward the vehicle&#8217;s forward direction, the <italic toggle="yes">y</italic>-axis points toward the vehicle&#8217;s left side, and the <italic toggle="yes">z</italic>-axis points vertically upward in accordance with the right-hand rule. The LiDAR is mounted directly above the origin of the chassis coordinate frame, while the camera is spatially offset relative to the LiDAR. The precise relative pose between the two sensors is obtained via joint extrinsic calibration. The arrangement of the mobile chassis and the two sensors is shown in <xref rid="sensors-25-05432-f003" ref-type="fig">Figure 3</xref>.</p></sec><sec id="sec2dot2dot2-sensors-25-05432"><title>2.2.2. Spatial Fusion</title><p>Camera intrinsic parameter calibration was performed using Zhang&#8217;s calibration method [<xref rid="B38-sensors-25-05432" ref-type="bibr">38</xref>], employing the Camera Calibrator toolbox in MATLAB (2021b). The checkerboard grid used featured 8 &#215; 5 inner corner points, with each square having a side length of 27 mm. As shown in <xref rid="sensors-25-05432-f004" ref-type="fig">Figure 4</xref>, checkerboard images were captured under different poses, and the camera&#8217;s focal length, principal point coordinates, radial distortion coefficients, and tangential distortion coefficients were calculated using functions provided by the OpenCV (3.3.1) library. The calibration process was repeated multiple times, and average values were taken to improve accuracy. The camera intrinsic parameter matrix is shown in Equation (1).<disp-formula id="FD1-sensors-25-05432"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Extrinsic parameter calibration between the camera and LiDAR is a crucial step for achieving multi-sensor data fusion. As illustrated in <xref rid="sensors-25-05432-f004" ref-type="fig">Figure 4</xref>, this study proposes an extrinsic calibration method suitable for 2D LiDAR and camera systems. The method first obtains the pixel coordinates of the calibration board&#8217;s center point and its coordinates in the LiDAR coordinate system, then employs the PnP (Perspective-n-Point) algorithm to accurately solve the spatial pose relationship of the camera relative to the LiDAR.</p><p>The essence of extrinsic calibration is to determine the rigid body transformation relationship between the LiDAR coordinate system and the camera coordinate system. The proposed 2D LiDAR-camera extrinsic calibration method takes pixel coordinates and radar coordinates as inputs, first converting the calibration board center point from the LiDAR coordinate system to the camera coordinate system. The transformation relationship between the two systems is shown in Equation (2).<disp-formula id="FD2-sensors-25-05432"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a 3 &#215; 3 rotation matrix, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a 3 &#215; 1 translation vector, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the coordinates of the target point in the camera coordinate system, and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the coordinates of the target point in the radar coordinate system.</p><p>The coordinates of the calibration board center point are converted from the camera coordinate system to the pixel coordinate system through the transformation shown in Equation (3). The relationship between the camera coordinate system and pixel coordinate system has been extensively studied [<xref rid="B39-sensors-25-05432" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05432" ref-type="bibr">40</xref>] and will not be elaborated upon in this paper.<disp-formula id="FD3-sensors-25-05432"><label>(3)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The conversion process from the radar coordinate system to the pixel coordinate system is thus expressed as:<disp-formula id="FD4-sensors-25-05432"><label>(4)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>R</mml:mi></mml:mtd><mml:mtd><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>R</mml:mi></mml:mtd><mml:mtd><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the scale factor and K is the camera intrinsic parameter matrix.</p><p>For a given set of n calibration point pairs <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in Equation (5), the objective of the PnP problem is to solve for the optimal rotation matrix <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and translation vector <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD5-sensors-25-05432"><label>(5)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#960;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the i-th pixel point, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the three-dimensional coordinates of the i-th point in the LiDAR coordinate system, and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#960;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the function that projects the i-th point to the pixel coordinate system through transformation parameters <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The position of the camera in the LiDAR coordinate system is calculated as:<disp-formula id="FD6-sensors-25-05432"><label>(6)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Based on the aforementioned mathematical model and algorithm workflow, precise extrinsic calibration between the camera and 2D LiDAR can be achieved. This method establishes a complete coordinate transformation chain from the LiDAR coordinate system to the camera coordinate system and then to the pixel coordinate system. By utilizing the PnP algorithm to solve for the rotation matrix <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and translation vector <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the relative position of the camera with respect to the 2D LiDAR is computed, thereby achieving precise alignment of data from both sensors within a unified coordinate system.</p></sec><sec id="sec2dot2dot3-sensors-25-05432"><title>2.2.3. Temporal Fusion</title><p>In the camera-LiDAR extrinsic calibration process, ensuring temporal consistency between the two sensor data streams is a critical prerequisite for obtaining accurate calibration results. In multi-sensor fusion systems, temporal alignment strategies are primarily categorized into synchronous fusion and asynchronous fusion [<xref rid="B41-sensors-25-05432" ref-type="bibr">41</xref>]. Since pomegranate orchard tree trunks serve as navigation reference targets with fixed positions and the mobile platform experiences minimal positional changes over short time periods, while the navigation system requires rapid response and should avoid excessive data waiting times, an asynchronous fusion strategy is more suitable for pomegranate orchard navigation environments.</p><p>Due to the Intel RealSense D455 camera outputting RGB images at 30 fps and the SLAMTEC 2D LiDAR operating at a scanning frequency of 5&#8211;15 Hz, their respective data arrival intervals are 33.3 ms and 67&#8211;200 ms. Without appropriate temporal fusion processing, this would lead to incorrect calibration point correspondences, thereby affecting the solution accuracy of the PnP algorithm and producing erroneous spatial associations. As shown in <xref rid="sensors-25-05432-f005" ref-type="fig">Figure 5</xref>, a parallel approach is adopted to independently process Intel RealSense D455 camera data and SLAMTEC 2D LiDAR data, where visual features are extracted through YOLO detection algorithms and point cloud data undergoes DBSCAN clustering processing. Each sensor data stream is stored in independent circular buffers that retain timestamp information, eliminating the need for strict synchronization constraints. The system employs a 10 Hz timing-triggered data association mechanism between the two sensors, periodically retrieving data from both buffers within a 5 s time window, ensuring computational efficiency while guaranteeing access to fresh data. The system automatically removes expired data beyond the time window, ensuring the use of consistent stored data and preventing data accumulation.</p><p>Each camera frame is paired with the temporally nearest 2D LiDAR scan. Within each trunk detection bounding box, we compute the nearest distance between the pixel rays and the raw LiDAR point clusters; if both a distance threshold and a directional-consistency check are satisfied, the cluster is accepted and its centroid is projected onto the <italic toggle="yes">Z</italic> = 0 plane of the mobile chassis coordinate frame <italic toggle="yes">B</italic> to obtain a trunk point. All trunk points are then fitted with RANSAC to obtain the row-direction navigation line. The method fuses directly on raw pixels and raw measurements, facilitating real-time operation.</p></sec></sec><sec id="sec2dot3-sensors-25-05432"><title>2.3. Algorithm Framework</title><p>The proposed vision and LiDAR fusion-based orchard navigation system adopts a modular design architecture, primarily comprising three components: trunk detection module, navigation line extraction module, and path tracking control. The overall system architecture is illustrated in <xref rid="sensors-25-05432-f006" ref-type="fig">Figure 6</xref>.</p><p>System architecture showing the data flow between three core modules. The Tree Trunk Detection Module processes camera images and outputs detection results to the Navigation Line Fitting Module, which fuses these with LiDAR data to generate navigation paths. The Path Tracking Control Module then converts these paths into control commands for the robot execution system.</p></sec><sec id="sec2dot4-sensors-25-05432"><title>2.4. YOLOv8-ResCBAM-Based Trunk Detection</title><p>Trunk detection in pomegranate orchard environments faces numerous technical challenges. Pomegranate juvenile trees have relatively small trunk diameters and exhibit low contrast characteristics against complex backgrounds. Simultaneously, branch and leaf occlusion, weed interference, and varying lighting conditions further increase detection difficulty and are prone to causing detection confusion.</p><sec id="sec2dot4dot1-sensors-25-05432"><title>2.4.1. YOLOv8-ResCBAM Model Applicability Analysis</title><p>To select an appropriate detection method for pomegranate orchard environments, this study conducted comparative analysis of mainstream object detection technologies. Traditional image processing methods, while computationally efficient, lack robustness under complex lighting and occlusion conditions. The deep learning-based YOLOv8 model achieves a good balance between speed and accuracy, but still has limitations in detecting small and occluded targets [<xref rid="B42-sensors-25-05432" ref-type="bibr">42</xref>]. Considering the aforementioned issues comprehensively, this study selected the YOLOv8-ResCBAM model proposed by Ju et al. [<xref rid="B43-sensors-25-05432" ref-type="bibr">43</xref>] as the fundamental architecture for trunk detection. The model is an efficient enhancement of the YOLOv8 architecture that integrates residual connections and a Convolutional Block Attention Module (CBAM). In orchard environments where trunks are small and planting density is high, traditional object detection methods often fail under background clutter or illumination changes. Leveraging improved feature extraction and attention, YOLOv8-ResCBAM can effectively discriminate fruit-tree trunks from complex backgrounds and remain robust to occlusions and illumination variations. Consequently, the model substantially enhances feature representation while maintaining computational efficiency, thereby providing high-precision trunk detections for the subsequent navigation line extraction.</p><p>YOLOv8-ResCBAM effectively mitigates the gradient vanishing problem in deep networks through ResBlock structures, improving the retention capability for small target features. In densely planted pomegranate orchard environments, this characteristic helps accurately identify slender tree trunks and reduces missed detections caused by feature loss. The Convolutional Block Attention Module (CBAM) comprises two sub-modules: channel attention and spatial attention. The channel attention mechanism enhances the model&#8217;s perception capability for trunk texture features by learning importance weights of different feature channels, effectively suppressing interference from background noise such as weeds. The spatial attention mechanism improves recognition capability for partially occluded tree trunks by focusing on key regions within feature maps.</p></sec><sec id="sec2dot4dot2-sensors-25-05432"><title>2.4.2. YOLOv8-ResCBAM Training and Implementation</title><p>A large amount of image data was collected in pomegranate orchard environments, totaling 4150 images after data augmentation, with 70% allocated for training, 20% for validation, and 10% for testing. Tree trunks in each image were annotated using the Labelimg (1.7.1) annotation tool. The training parameters were configured as follows:</p><p>(1) SGD optimizer with an initial learning rate of 1 &#215; 10<sup>&#8722;2</sup>, weight decay of 5 &#215; 10<sup>&#8722;4</sup>, and momentum of 0.937.</p><p>(2) Input image size of 640 &#215; 640, batch size of 8, training for 200 epochs. To prevent overfitting, a cosine annealing learning rate scheduling strategy was employed.</p><p>(3) Data augmentation techniques including random scaling, random rotation, color adjustment (HSV), and horizontal flipping were applied during the training process.</p><p>The detection results output by the trained model include bounding box coordinates, confidence scores, and class labels. The center point coordinates of the bounding box <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> are extracted as shown in Equation (6), and detection results with confidence scores above the threshold (<inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">&#964;</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) are published as ROS topics for subscription by downstream modules.<disp-formula id="FD7-sensors-25-05432"><label>(7)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec2dot5-sensors-25-05432"><title>2.5. Vision and LiDAR Fusion-Based Navigation Line Fitting</title><sec id="sec2dot5dot1-sensors-25-05432"><title>2.5.1. DBSCAN Trunk Clustering Algorithm</title><p>In the dense planting environment of pomegranate orchards with trunk diameters of 5&#8211;15 cm and plant spacing of 1&#8211;2 m, point cloud data acquired by 2D LiDAR exhibits characteristics of roughly row-wise arrangement. However, variations in plant spacing and local occlusions may result in incomplete or irregular point clouds. These characteristics render traditional clustering methods based on shape assumptions ineffective, making it difficult to effectively distinguish actual tree trunks from background noise44 [<xref rid="B44-sensors-25-05432" ref-type="bibr">44</xref>]. Therefore, the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm, which performs clustering based on density connectivity, is employed for LiDAR point cloud clustering. This algorithm is suitable for processing noisy data and irregularly shaped clusters, making it extremely effective for trunk detection in orchard environments [<xref rid="B45-sensors-25-05432" ref-type="bibr">45</xref>].</p><p>Raw measurements are first limited to the [1,1.5] m range window according to the orchard row spacing, removing near- and far-field non&#8211;tree-row interference. Following the scan order, a median filter with window width <italic toggle="yes">k</italic> = 5 is applied to the point-cloud ranges <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to obtain <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>. When <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>&#160;</mml:mo><mml:mo>&gt;</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn><mml:mo>&#160;</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> holds, the sample is regarded as a spike and replaced with <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>. This step effectively suppresses non&#8211;tree-row clutter and sporadic outliers, providing a cleaner point set for DBSCAN.</p><p>The DBSCAN algorithm workflow for pomegranate orchard environments is illustrated in <xref rid="sensors-25-05432-f007" ref-type="fig">Figure 7</xref>. The algorithm classifies a given point set <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> into three categories:</p><p>Core points: points that have at least <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> points within their <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> neighborhood radius;</p><p>Border points: points that have fewer than <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> points within their <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> neighborhood radius but are within the <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> neighborhood radius of some core point;</p><p>Noise points: points that are neither core points nor border points.</p><p>The performance of the DBSCAN algorithm is primarily determined by two key parameters: the neighborhood radius <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the minimum number of points <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Parameter selection must comprehensively consider the geometric characteristics of pomegranate orchards, LiDAR specifications, and clustering accuracy requirements.</p><p>The selection of neighborhood radius <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> requires balancing clustering completeness and accuracy. An excessively small <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> value would cause a single tree to be segmented into multiple clusters, while an excessively large <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> value might merge adjacent trees into one cluster. The <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> parameter affects the noise resistance and completeness of clustering. The selection of this parameter is based on the number of trunk reflection points that LiDAR can acquire at different distances. Based on the geometric constraints of pomegranate orchards, the theoretical ranges of <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> values can be determined through the following analysis:</p><p>Pomegranate juvenile trees have diameters of 5&#8211;15 cm, with plant spacing of 1&#8211;2 m, row spacing of 2&#8211;3 m, and LiDAR angular resolution of 0.225&#176;. To ensure that point clouds from individual tree trunks are not over-segmented, the minimum value of <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> should be greater than 1.5 times the minimum trunk diameter, and the maximum value should be less than half the minimum plant spacing, i.e., <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#8712;</mml:mo><mml:mo>&#160;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0.075</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Based on LiDAR technical specifications and trunk geometric characteristics, the theoretical point count estimation for a single trunk with 5 cm diameter at 1.5 m detection distance is shown in Equation (8).<disp-formula id="FD8-sensors-25-05432"><label>(8)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>360</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mi>r</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0.225</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8776;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the trunk diameter, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the radar linear resolution, and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the detection distance.</p><p>Considering practical factors such as occlusion and reflection intensity variations, setting <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> can effectively filter noise points while ensuring clustering completeness. Based on the above, the DBSCAN neighborhood radius <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the minimum number of points <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are derived from the LiDAR&#8217;s angular resolution and the desired linear resolution corresponding to the expected trunk diameter, selecting a target scale suited to densely planted inter-row spacing. This configuration strikes a balance between noise suppression and cluster integrity.</p></sec><sec id="sec2dot5dot2-sensors-25-05432"><title>2.5.2. Vision and LiDAR Data Association</title><sec><title>Line-of-Sight and LiDAR Plane Intersection Calculation</title><p>This study employs a loosely coupled sensor fusion method to associate and fuse visual detection results with 2D LiDAR data. The vision system provides pixel coordinates of tree trunks, while LiDAR provides accurate distance information. Through joint calibration of multiple sensors to the chassis reference coordinate system, this study performs processing and computation for vision and LiDAR data fusion.</p><p>This study proposes a reverse ray projection algorithm that differs from traditional point cloud-to-image plane projection methods. The algorithm projects pixel coordinates from visual detection into three-dimensional rays and computes their intersections with LiDAR scanning planes to achieve data association.</p><p>The pixel point <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> from visual detection is converted to a ray direction in the camera coordinate system:<disp-formula id="FD9-sensors-25-05432"><label>(9)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The camera principal point <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and focal length <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> are obtained through camera intrinsic parameter calibration.</p><p>The ray direction in the reference coordinate system is then calculated as:<disp-formula id="FD10-sensors-25-05432"><label>(10)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The LiDAR plane can be expressed as:<disp-formula id="FD11-sensors-25-05432"><label>(11)</label><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the plane normal vector and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance from the plane to the origin.</p><p>The intersection point between the ray and plane can be obtained by solving the following equation:<disp-formula id="FD12-sensors-25-05432"><label>(12)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the ray origin and <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the ray direction.</p><p>The intersection coordinates are:<disp-formula id="FD13-sensors-25-05432"><label>(13)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec><title>Association Algorithm</title><p>This paper proposes a vision and LiDAR data association algorithm that matches visually detected trunk points with LiDAR-clustered trunk point clouds. Unlike traditional data association methods that employ point cloud projection onto image planes, this study proposes a vision and LiDAR data association algorithm based on reverse ray projection. The core principle involves calculating the minimum distance between visual rays and LiDAR points and setting thresholds for association. The algorithm workflow is illustrated in <xref rid="sensors-25-05432-f008" ref-type="fig">Figure 8</xref>.</p><p>Given a visual ray with origin <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and direction <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">&#8594;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, and a LiDAR point <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the minimum distance from the point to the ray is:<disp-formula id="FD14-sensors-25-05432"><label>(14)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>ray</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>&#8722;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>&#8901;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">&#8594;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">&#8594;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the vector from the ray origin to point <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>&#8901;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">&#8594;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the projection length of <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> along the ray direction, and <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>O</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>&#8901;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">&#8594;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">&#8594;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the projection vector.</p><p>The proposed vision and LiDAR data association algorithm back-projects visual detections in the pixel coordinate system into three-dimensional rays in the reference coordinate system through camera intrinsic parameter matrix and distortion correction. Each ray extends from the camera center toward the detected target. The algorithm calculates the minimum distance from point clouds to rays and treats laser points with distances smaller than the preset threshold <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#964;</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as matching candidate points. To ensure detection effectiveness, spatial neighborhood point density analysis and consistency verification based on expected trunk dimensions are performed. Finally, the DBSCAN clustering algorithm is employed to merge multiple detections belonging to the same trunk, generating precise trunk positions through centroid calculation.</p><p>As shown in <xref rid="sensors-25-05432-f009" ref-type="fig">Figure 9</xref>, <xref rid="sensors-25-05432-f009" ref-type="fig">Figure 9</xref>a illustrates the geometric principle of the association algorithm: the ray casting method projects visual detections into 3D space, and association succeeds when LiDAR points satisfy the distance threshold and density constraints of the projection geometric boundaries. <xref rid="sensors-25-05432-f009" ref-type="fig">Figure 9</xref>b shows the three stages of vision and LiDAR association: the visual detection stage that generates candidate regions with confidence scores, LiDAR point cloud data acquisition, and the final associated point clusters after processing.</p></sec></sec><sec id="sec2dot5dot3-sensors-25-05432"><title>2.5.3. Navigation Line Fitting</title><sec><title>Geometric Constraint Navigation Line Fitting</title><p>After obtaining the associated points, the RANSAC (Random Sample Consensus) algorithm is employed to fit fruit tree rows. The RANSAC algorithm can robustly fit models in the presence of numerous outliers, making it suitable for addressing scenarios with weeds and irregular arrangements in orchard environments [<xref rid="B46-sensors-25-05432" ref-type="bibr">46</xref>]. Considering detection errors and planting irregularities comprehensively, the distance threshold for the RANSAC algorithm is set to <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> = 0.15 m. Given that interference from weeds, support poles, and other factors in orchard environments reduces the inlier ratio, the maximum number of iterations is set to 100 to ensure algorithm robustness.</p><p>Although the conventional RANSAC algorithm exhibits good robustness to noise and outliers in orchard environments, its reliance on random sampling can lead to unstable fitting results under complex conditions. In particular, when tree planting is irregular and interference is heavy, it remains susceptible to errors, making it difficult to ensure fitting accuracy [<xref rid="B47-sensors-25-05432" ref-type="bibr">47</xref>]. To improve fitting stability and accuracy, this paper introduces a Geometric Constraint Algorithm (GCA). Field investigations reveal that fruit tree rows in pomegranate orchards are typically planted along the same direction to facilitate orchard management. As shown in <xref rid="sensors-25-05432-f010" ref-type="fig">Figure 10</xref>, this study analyzes the geometric relationships between fruit trees based on orchard characteristics and constructs angle histograms to identify the main navigation direction for constraining the fitting process. Compared with the traditional RANSAC method, the geometric-constraint method (GCA) introduces a directional-consistency constraint that prioritizes a fitting direction aligned with the orchard&#8217;s planting pattern, thereby effectively improving fitting accuracy and stability. When facing interfering data and environmental variations, it markedly reduces the fitting error. By fully exploiting the regularity of tree rows, the method avoids sampling-induced errors inherent to RANSAC and substantially enhances both robustness and accuracy.</p><p>The directional angles between all trunk point pairs <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> are analyzed to construct a directional histogram:<disp-formula id="FD15-sensors-25-05432"><label>(15)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Angles are normalized to the <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="sans-serif">&#960;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> range to avoid directional differences:<disp-formula id="FD16-sensors-25-05432"><label>(16)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>&#960;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Histogram statistics are performed on the normalized angles with intervals of <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#960;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>/18, and the weight for each interval is:<disp-formula id="FD17-sensors-25-05432"><label>(17)</label><mml:math id="mm79" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mi>&#1013;</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>&#963;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the confidence scores of the points, <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the point pair distance, and <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance weight coefficient.</p><p>The histogram interval with the maximum weight is identified as the main direction:<disp-formula id="FD18-sensors-25-05432"><label>(18)</label><mml:math id="mm84" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Based on the main direction analysis, directional confidence <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is defined to quantify the reliability of the current direction estimation:<disp-formula id="FD19-sensors-25-05432"><label>(19)</label><mml:math id="mm86" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8901;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8901;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of inliers, <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the minimum required number of inliers, <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the fitted line length, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the minimum required length, <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum weight, and <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the total weight.</p><p>When the confidence <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the current estimated direction is used to constrain subsequent fitting:<disp-formula id="FD20-sensors-25-05432"><label>(20)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Under geometric constraints, the line slope is fixed and only the intercept is estimated:<disp-formula id="FD21-sensors-25-05432"><label>(21)</label><mml:math id="mm95" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">tan</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8901;</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where the intercept <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is:<disp-formula id="FD22-sensors-25-05432"><label>(22)</label><mml:math id="mm97" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">tan</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After obtaining the left and right row fitting lines, the center navigation line is generated by calculating the midpoints of the two lines. Assuming the left and right row equations are:<disp-formula id="FD23-sensors-25-05432"><label>(23)</label><mml:math id="mm98" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The center line equation is:<disp-formula id="FD24-sensors-25-05432"><label>(24)</label><mml:math id="mm99" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec><title>Kalman Filter Smoothing</title><p>To reduce navigation line jumps caused by noise, a Kalman filter is employed to smooth the fitting parameters and reduce noise effects [<xref rid="B48-sensors-25-05432" ref-type="bibr">48</xref>]. For each row of fruit trees on the left and right sides, the state vector is <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, representing the slope and intercept parameters of the fitted line for that tree row, respectively.</p><p>Considering the fixed nature of fruit tree rows in orchard environments, navigation line parameters change slowly over short periods, thus a constant velocity model is adopted:<disp-formula id="FD25-sensors-25-05432"><label>(25)</label><mml:math id="mm101" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The observation equation is:<disp-formula id="FD26-sensors-25-05432"><label>(26)</label><mml:math id="mm102" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the process noise and measurement noise, respectively.</p><p>The prediction steps are:<disp-formula id="FD27-sensors-25-05432"><label>(27)</label><mml:math id="mm105" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD28-sensors-25-05432"><label>(28)</label><mml:math id="mm106" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The update steps are:<disp-formula id="FD29-sensors-25-05432"><label>(29)</label><mml:math id="mm107" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mfenced open="|" close="" separators="|"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mfenced open="|" close="" separators="|"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD30-sensors-25-05432"><label>(30)</label><mml:math id="mm108" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mfenced open="|" close="" separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mfenced open="|" close="" separators="|"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mfenced open="|" close="" separators="|"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD31-sensors-25-05432"><label>(31)</label><mml:math id="mm109" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the state covariance matrix, <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the process noise covariance matrix, <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the measurement noise covariance matrix, and <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the Kalman gain. According to the characteristics of pomegranate orchard environments, the process noise covariance is set to <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0.01,0.05</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> to reflect the relative stability of navigation line parameters, where slope changes are smaller than intercept changes. The measurement noise covariance <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0.1,0.2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is determined based on statistical analysis of RANSAC fitting accuracy. From the weighted angle histogram in Section Geometric Constraint Navigation Line Fitting the principal row-direction angle <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained, and the vehicle&#8217;s lateral offset from the row line is computed. The heading and lateral offset, after first-order Kalman filtering, are fed to the Pure Pursuit algorithm to generate linear and angular velocity commands. This interface decouples perception from control and effectively suppresses command jitter under short-term occlusions or abrupt illumination changes.</p></sec></sec></sec><sec id="sec2dot6-sensors-25-05432"><title>2.6. Experimental Methods</title><p>To validate the effectiveness and accuracy of the proposed vision and 2D LiDAR fusion-based orchard navigation system, this paper conducted sensor calibration experiments, visual detection performance evaluation experiments, data association verification experiments, geometric constraint navigation line fitting simulation experiments, and navigation line tracking comparison experiments. Experiments were conducted at the Anhui Pomegranate Germplasm Resource Nursery in Bengbu, Anhui Province, China (117&#176;4&#8242;15.764880&#8243; E, 32&#176;58&#8242;53.022720&#8243; N). The site exhibits typical densely planted pomegranate-orchard characteristics and includes trees at multiple growth stages; this study focused primarily on 1&#8211;3-year-old trees with trunk diameters of 5&#8211;15 cm, an in-row spacing of 1&#8211;2 m, and a row spacing of 2&#8211;3 m. The orchard layout of the test area is shown in <xref rid="sensors-25-05432-f011" ref-type="fig">Figure 11</xref>. Representative operating scenarios included straight rows, slight lateral offset, and cross-slope operation. Weather and illumination conditions were sunny, overcast, dusk, strong direct light, and backlit. In each scenario, one operating row was covered; each single-row segment contained 25&#8211;32 trees, and every scenario was repeated 10 times to assess stability and repeatability. In total, approximately 1400 trees were recorded. Throughout the experiments, the sensor mounting configuration and algorithm parameters were kept uniform.</p><list list-type="simple"><list-item><label>(1)</label><p>The sensor calibration experimental methodology was as follows: Zhang&#8217;s calibration method was employed for camera intrinsic parameter calibration, capturing 20 images of different poses using an 8 &#215; 5 checkerboard (27 mm side length); the PnP algorithm was used for camera and 2D LiDAR extrinsic parameter calibration, obtaining multiple sets of calibration board center point positions in the radar coordinate system and pixel coordinates. Evaluation metrics included reprojection error and calibration accuracy.</p></list-item><list-item><label>(2)</label><p>The visual detection performance evaluation experimental methodology was as follows: 4150 images were collected under different lighting conditions (sunny, cloudy, dusk, strong light), divided into training, validation, and test sets at a 7:2:1 ratio, and the YOLOv8-ResCBAM model was used for trunk detection training and testing. Evaluation metrics included mAP50, recall rate, precision, and inference time.</p></list-item><list-item><label>(3)</label><p>The data association verification experimental methodology was as follows: five groups of 20 m-long inter-row orchard environments were selected, each containing 25&#8211;32 fruit trees, with each group experiment repeated 10 times, using the reverse ray projection algorithm for vision and LiDAR data association. Evaluation metrics included association count and association success rate.</p></list-item><list-item><label>(4)</label><p>The geometric constraint navigation line fitting simulation experimental methodology was as follows: six groups of simulation comparison experiments were designed with fruit tree point quantities ranging from 19 to 27, employing the proposed GCA and traditional RANSAC algorithm, respectively, for navigation line fitting. Evaluation metrics included fitting accuracy RMSE, inlier ratio, and computation time.</p></list-item><list-item><label>(5)</label><p>The navigation-line tracking comparison was designed as follows. Five representative inter-row segments were selected in the orchard. Each segment (20 m) was manually surveyed to obtain the ground-truth centerline and included operating conditions such as straight rows, slight lateral offset, cross-slope, local occlusions, and non-uniform illumination. For each segment, 10 independent navigation trials were conducted at speeds from 0.5 to 2.0 m&#183;s<sup>&#8722;1</sup>. Four navigation line extraction methods were compared: (i) single 2D LiDAR with RANSAC, (ii) single vision detection with RANSAC, (iii) DeepLab v3+ semantic segmentation with RANSAC, and (iv) the proposed fusion-based method. The mobile chassis position was recorded using a &#8220;taut-string&#8211;centerline-marking&#8211;tape-measurement&#8221; protocol. Specifically, a cotton string was stretched taut between the midpoints equidistant from the left and right rows at the two ends of the test segment to define the ground-truth centerline, and sampling stations were marked every 1 m along the string. A marker was mounted at the geometric center of the mobile chassis; after the vehicle passed, a measuring tape was used at each station to measure the perpendicular distance from the trajectory to the centerline, yielding the lateral deviation.</p></list-item></list><p>For the single 2D LiDAR + RANSAC method, the &#8220;2.5.1 DBSCAN trunk clustering algorithm&#8221; was used to obtain candidate points for the left and right rows, followed by separate line fittings with RANSAC to produce the two row lines and the center navigation line. For the single vision detection + RANSAC method, key pomegranate-tree targets were detected in images; low-confidence and out-of-range points were removed, and the remaining pixel points were projected onto the ground plane via the extrinsic parameters to form a point set for RANSAC fitting of the center navigation line. For the DeepLab v3+ semantic segmentation + RANSAC method, DeepLab v3+ (Backbone: ResNet-50, input 640 &#215; 640) segmented ground and trunk background in each frame; the road mask underwent morphological opening/closing and Guo&#8211;Hall thinning to obtain a skeleton, on which RANSAC fitting was performed. The resulting centerline was then projected into the ground coordinate frame using the camera&#8211;LiDAR extrinsics to obtain the center navigation line. Evaluation metrics included the average lateral error, the lateral error RMS, and the navigation success rate (single-sided lateral error &#8804; 10 cm).</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05432"><title>3. Results</title><sec id="sec3dot1-sensors-25-05432"><title>3.1. Calibration Experimental Results</title><p>As shown in <xref rid="sensors-25-05432-f012" ref-type="fig">Figure 12</xref>, camera intrinsic parameter calibration was performed according to the monocular camera and 2D LiDAR data fusion method described in <xref rid="sec2dot2-sensors-25-05432" ref-type="sec">Section 2.2</xref>.</p><p>After calibration, the intrinsic parameter matrix <italic toggle="yes">K</italic> was obtained as shown in Equation (32), with an average reprojection error of 0.18 pixels.<disp-formula id="FD32-sensors-25-05432"><label>(32)</label><mml:math id="mm117" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mo>&#160;</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>639.3178</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>660.2825</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>635.1858</mml:mn></mml:mtd><mml:mtd><mml:mn>363.30748</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In addition to camera intrinsic parameter calibration, it is necessary to determine the relative position of the camera on the mobile platform according to the camera and 2D LiDAR extrinsic parameter calibration method described in <xref rid="sec2dot2-sensors-25-05432" ref-type="sec">Section 2.2</xref> to complete data fusion. The mobile platform is placed on a horizontal plane, with the 2D LiDAR coordinate system origin located 250 mm directly above the platform coordinate system origin. As shown in <xref rid="sensors-25-05432-f013" ref-type="fig">Figure 13</xref>, the radar coordinate system position and pixel coordinates of the calibration board center point are obtained.</p><p>Through calibration calculations on multiple sets of data, the three-dimensional coordinates <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> of the camera relative to the 2D LiDAR were determined to be (0.075, 0.029, &#8722;0.096). Combining the results of camera intrinsic parameter calibration, camera and 2D LiDAR extrinsic parameter calibration, and temporal synchronization, the data fusion between camera and 2D LiDAR was completed, providing a consistent spatial reference for subsequent navigation algorithms.</p></sec><sec id="sec3dot2-sensors-25-05432"><title>3.2. Visual Detection Performance Evaluation</title><p>For different scenarios in pomegranate orchards, the detection results of the YOLOv8-ResCBAM model are shown in <xref rid="sensors-25-05432-f014" ref-type="fig">Figure 14</xref>.</p><p>As shown in <xref rid="sensors-25-05432-t002" ref-type="table">Table 2</xref>, this paper conducted comparative analysis of different YOLOv8 model variants (each integrating different attention mechanisms) to evaluate their performance in detecting tree trunks in densely planted pomegranate orchards.</p><p>The results demonstrate that the YOLOv8-ResCBAM model outperforms the baseline YOLOv8 and other attention-enhanced variants. Comparison of various metrics reveals that YOLOv8-ResCBAM exhibits the highest mAP50 and recall rate, achieving 95.7% and 92.5%, respectively, indicating its superior capability in detecting tree trunks under high-density and occlusion environments. Although its precision (91.0%) is slightly lower than the highest-performing YOLOv8-SA (92.0%), it still maintains a high level, and the balance between precision and recall is more favorable for practical applications. The inference time of YOLOv8-ResCBAM is 15.36 ms, which, though longer than the baseline YOLOv8 (12.96 ms), remains within the acceptable range for real-time processing in autonomous navigation systems. Through comparison of various metrics, it is verified that YOLOv8-ResCBAM demonstrates the best comprehensive performance in enhancing feature extraction and focusing on salient regions. Particularly in complex environments with dense planting and the presence of weed and branch occlusion, the advantage of high recall rate is especially prominent, proving the applicability and superiority of this model in practical orchard application scenarios.</p></sec><sec id="sec3dot3-sensors-25-05432"><title>3.3. Data Association Results</title><p>The experimental results of LiDAR clustering of pomegranate trees and their association with visual rays are shown in <xref rid="sensors-25-05432-f015" ref-type="fig">Figure 15</xref>. In <xref rid="sensors-25-05432-f015" ref-type="fig">Figure 15</xref>a, the red scatter points represent reflection points detected by LiDAR, and the point cloud distribution reflects the spatial structural characteristics of fruit tree rows. <xref rid="sensors-25-05432-f015" ref-type="fig">Figure 15</xref>b shows the results after clustering processing of LiDAR point clouds, where green points represent the identified fruit tree target center points after clustering, demonstrating that the clustering algorithm effectively extracted the positional information of fruit trees. <xref rid="sensors-25-05432-f015" ref-type="fig">Figure 15</xref>c displays the association effect between visual feature points and LiDAR clustered points, where blue rays represent rays emitted from the camera position toward target points, and blue points represent data points where visual rays successfully associated with laser clusters.</p><p>The results of five experimental groups, along with the average number of fruit trees and association counts, are presented in <xref rid="sensors-25-05432-t003" ref-type="table">Table 3</xref>.</p><p>The experimental results demonstrate that the average association success rate reached 92.6%, indicating that the proposed vision and LiDAR data association algorithm possesses high accuracy. The association success rates for the five experimental groups under different fruit tree density environments (25&#8211;32 trees) consistently remained between 91.9 and 94.3%, with a standard deviation of 0.9%, validating the algorithm&#8217;s stability and adaptability, which meets the accuracy requirements for orchard navigation.</p></sec><sec id="sec3dot4-sensors-25-05432"><title>3.4. Geometric Constraint Navigation Line Fitting Simulation Results</title><p>As shown in <xref rid="sensors-25-05432-f016" ref-type="fig">Figure 16</xref>, a comparison of the visualization of fitting performance between the two algorithms in experiments is presented, where the blue line represents the fitting performance of the GCA and the red line represents the fitting performance of the RANSAC algorithm.</p><p>The detailed performance comparison results of the six experimental groups are shown in <xref rid="sensors-25-05432-t004" ref-type="table">Table 4</xref>.</p><p>The experiments demonstrate that the proposed Geometric Constraint Algorithm exhibits significant advantages in orchard navigation line fitting tasks. Compared to the traditional RANSAC algorithm, GCA achieves an overall improvement of 2.4% in fitting accuracy. The average inlier ratio of GCA is 92.03%, which is 5.1% higher than RANSAC, demonstrating excellent noise resistance capability. GCA improves computational efficiency by 47.06%, meeting the requirements for real-time navigation.</p></sec><sec id="sec3dot5-sensors-25-05432"><title>3.5. Navigation Line Tracking Results</title><p>The experimental results are shown in <xref rid="sensors-25-05432-f017" ref-type="fig">Figure 17</xref>, displaying the navigation line fitting results of three algorithms under five experimental conditions.</p><p>The experimental results are presented in <xref rid="sensors-25-05432-t005" ref-type="table">Table 5</xref>, comparing the evaluation metrics of different navigation line extraction methods.</p><p>As shown in <xref rid="sensors-25-05432-f017" ref-type="fig">Figure 17</xref>, representative operating conditions&#8212;including straight rows, slight lateral offset, cross-slope, local occlusions, and non-uniform illumination&#8212;are presented together with the navigation-line fitting results of the four algorithms under the corresponding conditions. The results indicate that the proposed vision and 2D LiDAR fusion with a geometric constraint (GCA) improves both lateral deviation and navigation success rate over conventional methods. Although the gains relative to single-sensor and semantic-segmentation approaches are modest, they are crucial given the high occlusion, illumination variability, and small trunk size in orchards, helping ensure stable and efficient navigation for agricultural robots. Specifically, the proposed fusion method achieved the smallest average lateral error of 5.2 cm, corresponding to reductions of 25.7%, 21.2%, and 24.6% compared with the LiDAR method, the vision method, and the semantic-segmentation method, respectively. In addition, it attained the highest navigation success rate of 95.4% and exhibited better stability in the RMS of the lateral error.</p><p>To assess stability from an operational-task perspective, 95% conservative bounds are reported for the key metrics in <xref rid="sensors-25-05432-t005" ref-type="table">Table 5</xref>: a conservative upper bound for the lateral error and its RMS, and a conservative lower bound for the success rate. Under common approximations, these bounds are equivalent to worst-case robust estimates and can serve as surrogate evidence of long-term or large-scale operational stability [<xref rid="B49-sensors-25-05432" ref-type="bibr">49</xref>]. As shown in <xref rid="sensors-25-05432-t006" ref-type="table">Table 6</xref> (converted from the values in <xref rid="sensors-25-05432-t005" ref-type="table">Table 5</xref>), the proposed method yields a conservative upper bound of 6.18 cm for the lateral error, 7.78 cm for the RMS, and a conservative lower bound of 91.28% for the success rate&#8212;all superior to the two baseline methods. These results indicate that, even under a conservative regimen, the method maintains a lower error upper bound and a higher success-rate lower bound, demonstrating task-level stability for densely planted inter-row operation.</p><p>As shown in <xref rid="sensors-25-05432-f018" ref-type="fig">Figure 18</xref>, a grouped bar chart provides an intuitive comparison of the navigation-line extraction performance of the different methods. The experimental results validate the effectiveness of the proposed vision and 2D LiDAR fusion-based approach in practical applications, offering a more feasible solution for agricultural automation.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05432"><title>4. Discussion</title><p>To improve the autonomous navigation capability of robots in densely planted pomegranate orchard environments, this study proposed a vision and 2D LiDAR fusion-based navigation line extraction method. The method integrates three core technical modules: YOLOv8-ResCBAM trunk detection, reverse ray projection data association, and geometric constraint navigation line fitting, achieving high-precision navigation line extraction in complex orchard environments.</p><p>In densely planted pomegranate orchards with challenging illumination and occlusions, our system achieves trunk detection with a current precision of 91.0%. To further improve performance, we will act on dataset collection/processing and model optimization: continuously add hard cases&#8212;strong backlighting, shadows, wet-ground glare, slight wind-induced sway, and season-dependent canopy density&#8212;apply targeted data augmentation and rigorous annotation to reduce small-object boundary noise; modestly improve image sharpness; jointly select, on the same validation set, suitable confidence thresholds and overlap-removal (NMS) thresholds to balance recall and precision; and apply simple temporal smoothing to detections across consecutive frames to avoid jitter from frame-wise flicker. These strategies are expected to raise the effective accuracy under complex outdoor conditions while maintaining stability across terrains and seasons, without noticeably increasing computational overhead.</p><p>The reverse ray projection algorithm proposed in this study demonstrates significant advantages in vision and LiDAR data association. Unlike traditional methods that project point clouds onto image planes, this algorithm projects visual detection results into three-dimensional rays and computes their intersections with LiDAR scanning planes, achieving more precise spatial correspondence. Experimental results show that in five groups of orchard environments with different densities (25&#8211;32 trees), the average association success rate reached 92.6% with a standard deviation of 0.9%, validating the algorithm&#8217;s stability and adaptability. The level of association success rate directly affects the accuracy of subsequent navigation line fitting. Association failures typically occur due to the following situations: first, sparse point clouds when LiDAR encounters small trunk diameters at long distances, leading to DBSCAN clustering failure; second, visual detection errors under complex lighting conditions, producing incorrect ray directions; third, irregular obstacles in orchards such as support poles and irrigation equipment interfering with data association.</p><p>To address the problem of traditional RANSAC algorithms being susceptible to noise in orchard environments, this study introduced a geometric constraint algorithm that guides the line fitting process by analyzing geometric relationships between fruit trees and constructing angle histograms. Experimental results show that compared to traditional RANSAC algorithms, the geometric constraint algorithm improved fitting accuracy by 2.4%, inlier ratio by 5.1%, and computational efficiency by 47.06%. The primary reason for performance improvement lies in the geometric constraint mechanism&#8217;s full utilization of prior knowledge about pomegranate orchards. Fruit tree rows in pomegranate orchards are typically planted along the same direction, and this regularity provides powerful constraints for the algorithm. By constructing direction histograms and extracting principal direction angles, the algorithm can preferentially select candidate lines that conform to orchard planting patterns during the fitting process, thereby improving real-time efficiency and stability of fitting.</p><p>In the orchard navigation-line tracking trials, the proposed fusion method delivered strong overall performance: the average lateral error was 5.2 cm, the RMS was 6.6 cm, and the navigation success rate reached 95.4%. Relative to the single-LiDAR method, the average lateral error and RMS decreased by 25.7% and 26.7%, respectively, while the success rate increased by 19.5%. Relative to the single detection-based vision method, the average lateral error and RMS decreased by 21.2% and 19.5%, with a 5.6% gain in success rate. Compared with the DeepLab v3+ semantic-segmentation method, the average lateral error and RMS decreased by 24.6% and 23.3%, and the success rate improved by 8.4%. For single LiDAR, when trunks are thin or the ground has soil cover or glare, near-ground returns become sparse and contain outliers, so the RANSAC-fitted centerline is prone to abrupt jumps. For single vision, strong backlighting and local occlusions easily cause misses/false detections and unstable localization of trees. The DeepLab v3+ method yields a smooth curve under sunny straight-row conditions, but under cross-slope, backlit, and occluded scenes its robustness is lower than the detection-based vision baseline; mask fragmentation and S-shaped skeletons are also common, making the fitting sensitive to noise. By using vision to provide semantic candidates that constrain geometric search, then weighting the fit with LiDAR&#8217;s absolute scale and directional consistency&#8212;and further applying a directional histogram and Kalman smoothing to suppress transient outliers&#8212;the proposed method markedly reduces line jumps and polyline jaggedness. Fusing the two sensors enables stable navigation across varying environmental conditions.</p><p>In densely planted inter-row scenarios, the method still exhibits a lower error upper bound and a higher success-rate lower bound under the conservative-bounds regimen, indicating task-level stability. The approach relies on three keys: a geometric prior of row-direction consistency in tree rows, the detectability of trunk-type targets, and the clusterability of sparse LiDAR returns. As shown by the derivation of the number of LiDAR hits on a trunk during navigation in <xref rid="sec2dot5dot1-sensors-25-05432" ref-type="sec">Section 2.5.1</xref> (Equation (8)), the number of points increases when trunks are thicker or the range is smaller (e.g., in apple or citrus orchards). Within reasonable parameter ranges across crops and tree forms, the return density is sufficient to support stable clustering and the subsequent geometrically constrained fitting, thereby providing clear parameter bounds and theoretical feasibility for cross-orchard transferability. To verify transferability, future work will add small-scale cross-scene tests&#8212;e.g., one test plot each in an apple and a citrus orchard&#8212;using few-shot fine-tuning only for the detection head while keeping the fusion and geometric-constraint fitting modules unchanged. We will also add task-level metrics (long-term navigation stability and obstacle-avoidance success rate) and report them alongside the average lateral error, RMS, and success rate.</p><p>Based on our data and field experience, we draw the following conclusions about environmental and meteorological impacts on measurement quality. Preferred conditions: clear days with stable, moderate illumination; no precipitation or fog; dry ground; ambient wind speed &#8804; 3 m&#183;s<sup>&#8722;1</sup>. Under these conditions, image illumination and shadow boundaries are most stable, ground-surface glare is minimal, spurious LiDAR returns are markedly reduced, and measurement quality is optimal. Adverse conditions: strong direct sunlight, higher wind speeds, rain, fog, or standing water. Strong direct sunlight produces long, sharp shadows and highlights that cause fluctuations in vision; LiDAR returns become mixed and outliers increase. Higher winds induce sway of branches and slender trunks, creating short-term occlusions and unstable tree localization. Rain/fog can fog the camera lens and LiDAR window, further degrading SNR. Recommended acquisition seasons and times: when compatible with production, prioritize concentrated data collection after pruning and before budbreak or in the early fruit-set stage (sparser canopy, less occlusion), and during dry or low-rainfall periods; avoid extended rainy or foggy periods whenever possible.</p><p>The results indicate that the proposed method can improve robot autonomous navigation capability in complex orchard environments and provide new insights for the popularization and promotion of orchard mechanized operations. In future work, the application will be extended to more types of orchard environments and different planting patterns to verify the algorithm&#8217;s generalizability and portability, aiming to provide effective technical solutions for robot navigation in densely planted orchard environments.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05432"><title>5. Conclusions</title><p>This study proposed a vision and LiDAR fusion-based inter-row navigation line extraction method for orchards, addressing the path extraction problem in densely planted pomegranate orchard environments. This research integrated a YOLOv8-ResCBAM trunk detection model, vision and LiDAR data fusion, and navigation line extraction algorithms under geometric constraints. In detection performance evaluation experiments, the YOLOv8-ResCBAM model improved trunk detection accuracy, achieving an mAP50 of 95.7% and a recall rate of 92.5%. The proposed reverse ray projection algorithm achieved precise data association with an association success rate of 92.6%. The proposed geometric constraint algorithm improved fitting accuracy by 2.4%, inlier ratio by 5.1%, and computational efficiency by 47.06% compared to traditional RANSAC algorithms. In the orchard navigation-line tracking trials, the vision and 2D LiDAR fusion-based navigation line extraction method reduced the average lateral error by 25.7%, 21.2%, and 24.6% relative to the single 2D LiDAR, single detection-based vision, and DeepLab v3+ semantic-segmentation methods, respectively, lowering the average lateral error to 5.2 cm and achieving a navigation-line extraction success rate of 95.4%. In addition, 95% conservative bounds were reported: a conservative upper bound of 6.18 cm for the lateral error, 7.78 cm for the lateral-error RMS, and a conservative lower bound of 91.28% for the success rate&#8212;all superior to the three baselines&#8212;demonstrating task-level stability for densely planted inter-row operation.</p><p>This study provides research concepts and solutions for robot autonomous navigation computation in densely planted orchard environments, offering reference value for advancing orchard mechanization and intelligent development. With the continuous improvement and optimization of related technologies, this method is expected to find broader applications in modern agriculture.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, Z.B.; Software, X.Z.; Formal analysis, K.Y.; Investigation, X.D.; Resources, B.Q.; Writing&#8212;original draft, Z.S.; Writing&#8212;review &amp; editing, X.H.; Visualization, C.J.; Project administration, Q.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author. The data are not publicly available due to privacy and permissions re-strictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05432"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ghasemi-Soloklui</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Kordrostami</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gharaghani</surname><given-names>A.</given-names></name></person-group><article-title>Environmental and geographical conditions influence color, physical properties, and physiochemical composition of pomegranate fruits</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>15447</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-42749-z</pub-id><pub-id pub-id-type="pmid">37723234</pub-id><pub-id pub-id-type="pmcid">PMC10507014</pub-id></element-citation></ref><ref id="B2-sensors-25-05432"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yazdanpanah</surname><given-names>P.</given-names></name><name name-style="western"><surname>Jonoubi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zeinalabedini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rajaei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ghaffari</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Vazifeshenas</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Abdirad</surname><given-names>S.</given-names></name></person-group><article-title>Seasonal metabolic investigation in pomegranate (<italic toggle="yes">Punica granatum</italic> L.) highlights the role of amino acids in genotype- and organ-specific adaptive responses to freezing stress</article-title><source>Front. Plant Sci.</source><year>2021</year><volume>12</volume><elocation-id>699139</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2021.699139</pub-id><pub-id pub-id-type="pmid">34456940</pub-id><pub-id pub-id-type="pmcid">PMC8397415</pub-id></element-citation></ref><ref id="B3-sensors-25-05432"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name></person-group><article-title>Design and development of orchard autonomous navigation spray system</article-title><source>Front. Plant Sci.</source><year>2022</year><volume>13</volume><elocation-id>960686</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2022.960686</pub-id><pub-id pub-id-type="pmid">35979071</pub-id><pub-id pub-id-type="pmcid">PMC9376256</pub-id></element-citation></ref><ref id="B4-sensors-25-05432"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Blok</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>van Boheemen</surname><given-names>K.</given-names></name><name name-style="western"><surname>van Evert</surname><given-names>F.K.</given-names></name><name name-style="western"><surname>IJsselmuiden</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>G.H.</given-names></name></person-group><article-title>Robot navigation in orchards with localization based on Particle filter and Kalman filter</article-title><source>Comput. Electron. Agric.</source><year>2019</year><volume>157</volume><fpage>261</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2018.12.046</pub-id></element-citation></ref><ref id="B5-sensors-25-05432"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Noguchi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ahamed</surname><given-names>T.</given-names></name></person-group><article-title>Tree Trunk Recognition in Orchard Autonomous Operations under Different Light Conditions Using a Thermal Camera and Faster R-CNN</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>2065</elocation-id><pub-id pub-id-type="doi">10.3390/s22052065</pub-id><pub-id pub-id-type="pmid">35271214</pub-id><pub-id pub-id-type="pmcid">PMC8914652</pub-id></element-citation></ref><ref id="B6-sensors-25-05432"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lyu</surname><given-names>X.</given-names></name></person-group><article-title>Research on orchard navigation method based on fusion of 3D SLAM and point cloud positioning</article-title><source>Front. Plant Sci.</source><year>2023</year><volume>14</volume><elocation-id>1207742</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2023.1207742</pub-id><pub-id pub-id-type="pmid">37434606</pub-id><pub-id pub-id-type="pmcid">PMC10330707</pub-id></element-citation></ref><ref id="B7-sensors-25-05432"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Diao</surname><given-names>Z.</given-names></name></person-group><article-title>Vision-based navigation and guidance for agricultural autonomous vehicles and robots: A review</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>205</volume><fpage>107724</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.107584</pub-id></element-citation></ref><ref id="B8-sensors-25-05432"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>Z.</given-names></name></person-group><article-title>A Single-Stage Navigation Path Extraction Network for agricultural robots in orchards</article-title><source>Comput. Electron. Agric.</source><year>2025</year><volume>229</volume><fpage>109687</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.109687</pub-id></element-citation></ref><ref id="B9-sensors-25-05432"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>C.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Rao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>W.</given-names></name></person-group><article-title>Orchard Vision Navigation Line Extraction Based on YOLOv8-Trunk Detection</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>89156</fpage><lpage>89168</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3422422</pub-id></element-citation></ref><ref id="B10-sensors-25-05432"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name></person-group><article-title>Visual navigation path extraction of orchard hard pavement based on scanning method and neural network</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>197</volume><fpage>106964</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.106964</pub-id></element-citation></ref><ref id="B11-sensors-25-05432"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>De Silva</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cielniak</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>J.</given-names></name></person-group><article-title>Deep learning-based crop row detection for infield navigation of agri-robots</article-title><source>J. Field Robot.</source><year>2024</year><volume>41</volume><fpage>2299</fpage><lpage>2321</lpage><pub-id pub-id-type="doi">10.1002/rob.22238</pub-id></element-citation></ref><ref id="B12-sensors-25-05432"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>L.</given-names></name></person-group><article-title>Using a depth camera for crop row detection and mapping for under-canopy navigation of agricultural robotic vehicle</article-title><source>Comput. Electron. Agric.</source><year>2021</year><volume>188</volume><fpage>106301</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2021.106301</pub-id></element-citation></ref><ref id="B13-sensors-25-05432"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Winterhalter</surname><given-names>W.</given-names></name><name name-style="western"><surname>Fleckenstein</surname><given-names>F.V.</given-names></name><name name-style="western"><surname>Dornhege</surname><given-names>C.</given-names></name><name name-style="western"><surname>Burgard</surname><given-names>W.</given-names></name></person-group><article-title>Crop row detection on tiny plants with the pattern hough transform</article-title><source>IEEE Robot. Autom. Lett.</source><year>2018</year><volume>3</volume><fpage>3394</fpage><lpage>3401</lpage><pub-id pub-id-type="doi">10.1109/LRA.2018.2852841</pub-id></element-citation></ref><ref id="B14-sensors-25-05432"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Steward</surname><given-names>B.L.</given-names></name></person-group><article-title>Automated crop plant detection based on the fusion of color and depth images for robotic weed control</article-title><source>J. Field Robot.</source><year>2020</year><volume>37</volume><fpage>35</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1002/rob.21897</pub-id></element-citation></ref><ref id="B15-sensors-25-05432"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>T.H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name></person-group><article-title>Applications of machine vision in agricultural robot navigation: A review</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>198</volume><fpage>107085</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.107085</pub-id></element-citation></ref><ref id="B16-sensors-25-05432"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ahamed</surname><given-names>T.</given-names></name></person-group><article-title>Navigation of an Autonomous Spraying Robot for Orchard Operations Using LiDAR for Tree Trunk Detection</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>4808</elocation-id><pub-id pub-id-type="doi">10.3390/s23104808</pub-id><pub-id pub-id-type="pmid">37430726</pub-id><pub-id pub-id-type="pmcid">PMC10223392</pub-id></element-citation></ref><ref id="B17-sensors-25-05432"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>X.</given-names></name></person-group><article-title>An autonomous navigation method for orchard mobile robots based on octree 3D point cloud optimization</article-title><source>Front. Plant Sci.</source><year>2025</year><volume>15</volume><elocation-id>1510683</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2024.1510683</pub-id><pub-id pub-id-type="pmid">39840350</pub-id><pub-id pub-id-type="pmcid">PMC11746033</pub-id></element-citation></ref><ref id="B18-sensors-25-05432"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abanay</surname><given-names>A.</given-names></name><name name-style="western"><surname>Masmoudi</surname><given-names>L.</given-names></name><name name-style="western"><surname>El Ansari</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gonzalez-Jimenez</surname><given-names>J.</given-names></name><name name-style="western"><surname>Moreno</surname><given-names>F.-A.</given-names></name></person-group><article-title>LIDAR-based autonomous navigation method for an agricultural mobile robot in strawberry greenhouse: AgriEco Robot</article-title><source>AIMS Electron. Electr. Eng.</source><year>2022</year><volume>6</volume><fpage>317</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.3934/electreng.2022019</pub-id></element-citation></ref><ref id="B19-sensors-25-05432"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>X.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>Overall integrated navigation based on satellite and LiDAR in the standardized tall spindle apple orchards</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>216</volume><fpage>108489</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2023.108489</pub-id></element-citation></ref><ref id="B20-sensors-25-05432"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Malavazi</surname><given-names>F.B.</given-names></name><name name-style="western"><surname>Guyonneau</surname><given-names>R.</given-names></name><name name-style="western"><surname>Fasquel</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>Lagrange</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mercier</surname><given-names>F.</given-names></name></person-group><article-title>LiDAR-only based navigation algorithm for an autonomous agricultural robot</article-title><source>Comput. Electron. Agric.</source><year>2018</year><volume>154</volume><fpage>71</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2018.08.034</pub-id></element-citation></ref><ref id="B21-sensors-25-05432"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Han</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name></person-group><article-title>Navigation system for orchard spraying robot based on 3D LiDAR SLAM with NDT_ICP point cloud registration</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>220</volume><fpage>108870</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.108870</pub-id></element-citation></ref><ref id="B22-sensors-25-05432"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name></person-group><article-title>LiDAR-based 3D SLAM for autonomous navigation in stacked cage farming houses: An evaluation</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>230</volume><fpage>109885</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.109885</pub-id></element-citation></ref><ref id="B23-sensors-25-05432"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Firkat</surname><given-names>E.</given-names></name><name name-style="western"><surname>An</surname><given-names>F.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mijit</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ahat</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hamdulla</surname><given-names>A.</given-names></name></person-group><article-title>FGSeg: Field-ground segmentation for agricultural robot based on LiDAR</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>210</volume><fpage>107923</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2023.107965</pub-id></element-citation></ref><ref id="B24-sensors-25-05432"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ringdahl</surname><given-names>O.</given-names></name><name name-style="western"><surname>Hohnloser</surname><given-names>P.</given-names></name><name name-style="western"><surname>Hellstr&#246;m</surname><given-names>T.</given-names></name><name name-style="western"><surname>Holmgren</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lindroos</surname><given-names>O.</given-names></name></person-group><article-title>Enhanced Algorithms for Estimating Tree Trunk Diameter Using 2D Laser Scanner</article-title><source>Remote Sens.</source><year>2013</year><volume>5</volume><fpage>4839</fpage><lpage>4856</lpage><pub-id pub-id-type="doi">10.3390/rs5104839</pub-id></element-citation></ref><ref id="B25-sensors-25-05432"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ahamed</surname><given-names>T.</given-names></name></person-group><article-title>Development of an autonomous navigation system for orchard spraying robots integrating a thermal camera and LiDAR using a deep learning algorithm under low- and no-light conditions</article-title><source>Comput. Electron. Agric.</source><year>2025</year><volume>235</volume><fpage>110359</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2025.110359</pub-id></element-citation></ref><ref id="B26-sensors-25-05432"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ban</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Su</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name></person-group><article-title>A Camera-LiDAR-IMU fusion method for real-time extraction of navigation line between maize field rows</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>223</volume><fpage>109114</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.109114</pub-id></element-citation></ref><ref id="B27-sensors-25-05432"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>H.W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>C.</given-names></name></person-group><article-title>Accurate fruit localisation using high resolution LiDAR-camera fusion and instance segmentation</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>203</volume><fpage>107450</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2022.107450</pub-id></element-citation></ref><ref id="B28-sensors-25-05432"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>Visual Navigation and Obstacle Avoidance Control for Agricultural Robots via LiDAR and Camera</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>5402</elocation-id><pub-id pub-id-type="doi">10.3390/rs15225402</pub-id></element-citation></ref><ref id="B29-sensors-25-05432"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shalal</surname><given-names>N.</given-names></name><name name-style="western"><surname>Low</surname><given-names>T.</given-names></name><name name-style="western"><surname>McCarthy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hancock</surname><given-names>N.</given-names></name></person-group><article-title>Orchard mapping and mobile robot localisation using on-board camera and laser scanner data fusion&#8212;Part A: Tree detection</article-title><source>Comput. Electron. Agric.</source><year>2015</year><volume>119</volume><fpage>254</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2015.09.025</pub-id></element-citation></ref><ref id="B30-sensors-25-05432"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xue</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>B.W.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>S.X.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>Q.S.</given-names></name></person-group><article-title>Trunk detection based on laser radar and vision data fusion</article-title><source>Int. J. Agric. Biol. Eng.</source><year>2018</year><volume>11</volume><fpage>20</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.25165/j.ijabe.20181106.3725</pub-id></element-citation></ref><ref id="B31-sensors-25-05432"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name></person-group><article-title>Sensors, systems and algorithms of 3D reconstruction for smart agriculture and precision farming: A review</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>224</volume><fpage>109229</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.109229</pub-id></element-citation></ref><ref id="B32-sensors-25-05432"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ji</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name></person-group><article-title>Obstacle detection and recognition in farmland based on fusion point cloud data</article-title><source>Comput. Electron. Agric.</source><year>2021</year><volume>189</volume><fpage>106409</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2021.106409</pub-id></element-citation></ref><ref id="B33-sensors-25-05432"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>K.P.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Z.</given-names></name></person-group><article-title>Review of the field environmental sensing methods based on multi-sensor information fusion technology</article-title><source>Int. J. Agric. Biol. Eng.</source><year>2024</year><volume>17</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.25165/j.ijabe.20241702.8596</pub-id></element-citation></ref><ref id="B34-sensors-25-05432"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Geng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name></person-group><article-title>A deep-learning extraction method for orchard visual navigation lines</article-title><source>Agriculture</source><year>2022</year><volume>12</volume><elocation-id>1650</elocation-id><pub-id pub-id-type="doi">10.3390/agriculture12101650</pub-id></element-citation></ref><ref id="B35-sensors-25-05432"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chien</surname><given-names>C.T.</given-names></name><name name-style="western"><surname>Ju</surname><given-names>R.Y.</given-names></name><name name-style="western"><surname>Chou</surname><given-names>K.Y.</given-names></name><name name-style="western"><surname>Xieerke</surname><given-names>E.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>J.-S.</given-names></name></person-group><article-title>YOLOv8-AM: YOLOv8 Based on Effective Attention Mechanisms for Pediatric Wrist Fracture Detection</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2402.09329</pub-id><pub-id pub-id-type="doi">10.1109/ACCESS.2025.3549839</pub-id></element-citation></ref><ref id="B36-sensors-25-05432"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name></person-group><article-title>Real time object detection using LiDAR and camera fusion for autonomous driving</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>8056</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-35170-z</pub-id><pub-id pub-id-type="pmid">37198255</pub-id><pub-id pub-id-type="pmcid">PMC10192255</pub-id></element-citation></ref><ref id="B37-sensors-25-05432"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>B.</given-names></name></person-group><article-title>The Improved A* Obstacle Avoidance Algorithm for the Plant Protection UAV with Millimeter Wave Radar and Monocular Camera Data Fusion</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>3364</elocation-id><pub-id pub-id-type="doi">10.3390/rs13173364</pub-id></element-citation></ref><ref id="B38-sensors-25-05432"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>A flexible new technique for camera calibration</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2000</year><volume>22</volume><fpage>1330</fpage><lpage>1334</lpage><pub-id pub-id-type="doi">10.1109/34.888718</pub-id></element-citation></ref><ref id="B39-sensors-25-05432"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhuang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><article-title>A Review of Computer Vision-Based Structural Deformation Monitoring in Field Environments</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>3789</elocation-id><pub-id pub-id-type="doi">10.3390/s22103789</pub-id><pub-id pub-id-type="pmid">35632197</pub-id><pub-id pub-id-type="pmcid">PMC9144850</pub-id></element-citation></ref><ref id="B40-sensors-25-05432"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>K.-Y.</given-names></name><name name-style="western"><surname>Tseng</surname><given-names>Y.-H.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>K.-W.</given-names></name></person-group><article-title>Interpretation and Transformation of Intrinsic Camera Parameters Used in Photogrammetry and Computer Vision</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>9602</elocation-id><pub-id pub-id-type="doi">10.3390/s22249602</pub-id><pub-id pub-id-type="pmid">36559969</pub-id><pub-id pub-id-type="pmcid">PMC9787778</pub-id></element-citation></ref><ref id="B41-sensors-25-05432"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Duan</surname><given-names>J.</given-names></name></person-group><article-title>Study on Multi-Heterogeneous Sensor Data Fusion Method Based on Millimeter-Wave Radar and Camera</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>6044</elocation-id><pub-id pub-id-type="doi">10.3390/s23136044</pub-id><pub-id pub-id-type="pmid">37447893</pub-id><pub-id pub-id-type="pmcid">PMC10346902</pub-id></element-citation></ref><ref id="B42-sensors-25-05432"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Iqra</surname></name><name name-style="western"><surname>Giri</surname><given-names>K.J.</given-names></name></person-group><article-title>SO-YOLOv8: A novel deep learning-based approach for small object detection with YOLO beyond COCO</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>280</volume><fpage>127447</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2025.127447</pub-id></element-citation></ref><ref id="B43-sensors-25-05432"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ju</surname><given-names>R.Y.</given-names></name><name name-style="western"><surname>Chien</surname><given-names>C.T.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>J.S.</given-names></name></person-group><article-title>YOLOv8-ResCBAM: YOLOv8 Based on an Effective Attention Module for Pediatric Wrist Fracture Detection</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2409.18826</pub-id></element-citation></ref><ref id="B44-sensors-25-05432"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Onda</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hashimoto</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gomi</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chiu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Inokoshi</surname><given-names>S.</given-names></name></person-group><article-title>A tree detection method based on trunk point cloud section in dense plantation forest using drone LiDAR data</article-title><source>For. Ecosyst.</source><year>2023</year><volume>10</volume><fpage>100088</fpage><pub-id pub-id-type="doi">10.1016/j.fecs.2023.100088</pub-id></element-citation></ref><ref id="B45-sensors-25-05432"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>An</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Namkung</surname><given-names>H.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.E.</given-names></name></person-group><article-title>Grid-Based DBSCAN Clustering Accelerator for LiDAR&#8217;s Point Cloud</article-title><source>Electronics</source><year>2024</year><volume>13</volume><elocation-id>3395</elocation-id><pub-id pub-id-type="doi">10.3390/electronics13173395</pub-id></element-citation></ref><ref id="B46-sensors-25-05432"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Diao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name></person-group><article-title>Row detection based navigation and guidance for agricultural robots and autonomous vehicles in row-crop fields: Methods and applications</article-title><source>Agronomy</source><year>2023</year><volume>13</volume><elocation-id>1780</elocation-id><pub-id pub-id-type="doi">10.3390/agronomy13071780</pub-id></element-citation></ref><ref id="B47-sensors-25-05432"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name></person-group><article-title>Research on Global Navigation Operations for Rotary Burying of Stubbles Based on Machine Vision</article-title><source>Agronomy</source><year>2025</year><volume>15</volume><elocation-id>114</elocation-id><pub-id pub-id-type="doi">10.3390/agriculture15010114</pub-id></element-citation></ref><ref id="B48-sensors-25-05432"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lv</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name></person-group><article-title>A loosely coupled extended Kalman filter algorithm for agricultural scene-based multi-sensor fusion</article-title><source>Front. Plant Sci.</source><year>2022</year><volume>13</volume><elocation-id>849260</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2022.849260</pub-id><pub-id pub-id-type="pmid">35548311</pub-id><pub-id pub-id-type="pmcid">PMC9082075</pub-id></element-citation></ref><ref id="B49-sensors-25-05432"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Francq</surname><given-names>B.G.</given-names></name><name name-style="western"><surname>Berger</surname><given-names>M.</given-names></name><name name-style="western"><surname>Boachie</surname><given-names>C.</given-names></name></person-group><article-title>To Tolerate or to Agree: A Tutorial on Tolerance Intervals in Method Comparison Studies with BivRegBLS R Package</article-title><source>Stat. Med.</source><year>2020</year><volume>39</volume><fpage>4334</fpage><lpage>4349</lpage><pub-id pub-id-type="doi">10.1002/sim.8709</pub-id><pub-id pub-id-type="pmid">32964501</pub-id><pub-id pub-id-type="pmcid">PMC7756677</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05432-f001" orientation="portrait"><label>Figure 1</label><caption><p>Pomegranate orchard environment: (<bold>a</bold>) Regular planting pattern; (<bold>b</bold>) Slope covering planting pattern.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g001.jpg"/></fig><fig position="float" id="sensors-25-05432-f002" orientation="portrait"><label>Figure 2</label><caption><p>Experimental platform.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g002.jpg"/></fig><fig position="float" id="sensors-25-05432-f003" orientation="portrait"><label>Figure 3</label><caption><p>Positional relationship between chassis and sensors.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g003.jpg"/></fig><fig position="float" id="sensors-25-05432-f004" orientation="portrait"><label>Figure 4</label><caption><p>Schematic diagram of extrinsic parameter calibration.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g004.jpg"/></fig><fig position="float" id="sensors-25-05432-f005" orientation="portrait"><label>Figure 5</label><caption><p>Temporal fusion model for monocular camera and 2D LiDAR.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g005.jpg"/></fig><fig position="float" id="sensors-25-05432-f006" orientation="portrait"><label>Figure 6</label><caption><p>Vision-LiDAR Fusion Orchard Navigation System Architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g006.jpg"/></fig><fig position="float" id="sensors-25-05432-f007" orientation="portrait"><label>Figure 7</label><caption><p>DBSCAN algorithm flowchart.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g007.jpg"/></fig><fig position="float" id="sensors-25-05432-f008" orientation="portrait"><label>Figure 8</label><caption><p>Association algorithm flowchart.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g008.jpg"/></fig><fig position="float" id="sensors-25-05432-f009" orientation="portrait"><label>Figure 9</label><caption><p>Schematic diagram of the association algorithm: (<bold>a</bold>) Geometric principle&#8212;the camera ray intersects the LiDAR height plane ({z=h}_{lidar}); items within the distance threshold \delta are shown in green (successful association), whereas those beyond \delta are in red (failed association). (<bold>b</bold>) Association process&#8212;the blue dashed line denotes the reverse ray projection from the camera optical center through each 2D trunk detection; LiDAR points close to this ray are highlighted in red as candidates, gray dots indicate background returns, and the finally associated trunks are shown in green.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g009.jpg"/></fig><fig position="float" id="sensors-25-05432-f010" orientation="portrait"><label>Figure 10</label><caption><p>Geometric constraint navigation line fitting: (<bold>a</bold>) Fruit tree angle analysis; (<bold>b</bold>) Angle histogram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g010.jpg"/></fig><fig position="float" id="sensors-25-05432-f011" orientation="portrait"><label>Figure 11</label><caption><p>Experimental scenarios.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g011.jpg"/></fig><fig position="float" id="sensors-25-05432-f012" orientation="portrait"><label>Figure 12</label><caption><p>Camera intrinsic parameter calibration: (<bold>a</bold>) Partial images used for calibration; (<bold>b</bold>) Checkerboard and camera positions; (<bold>c</bold>) Image errors for calibration.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g012.jpg"/></fig><fig position="float" id="sensors-25-05432-f013" orientation="portrait"><label>Figure 13</label><caption><p>Camera and radar extrinsic parameter calibration.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g013.jpg"/></fig><fig position="float" id="sensors-25-05432-f014" orientation="portrait"><label>Figure 14</label><caption><p>Tree trunk detection results under different environmental conditions: (<bold>a</bold>) Sunny; (<bold>b</bold>) Dusk; (<bold>c</bold>) Cloudy; (<bold>d</bold>) Strong light.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g014.jpg"/></fig><fig position="float" id="sensors-25-05432-f015" orientation="portrait"><label>Figure 15</label><caption><p>Data-association results. (<bold>a</bold>) 2D LiDAR point cloud&#8212;red dots are LiDAR returns. (<bold>b</bold>) LiDAR clustering&#8212;green solid squares mark cluster centroids (trunk hypotheses). (<bold>c</bold>) Visual&#8211;LiDAR as-sociation&#8212;blue solid lines are reverse ray projections from the camera optical center; blue solid squares denote LiDAR clusters that are successfully associated to the rays.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g015.jpg"/></fig><fig position="float" id="sensors-25-05432-f016" orientation="portrait"><label>Figure 16</label><caption><p>Experimental comparison between GCA and RANSAC algorithm.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g016.jpg"/></fig><fig position="float" id="sensors-25-05432-f017" orientation="portrait"><label>Figure 17</label><caption><p>Experimental results of three navigation line extraction algorithms across five scenarios: (<bold>a</bold>) original camera images; (<bold>b</bold>) LiDAR-only fitting&#8212;red dots denote 2D LiDAR returns, green solid squares indicate LiDAR cluster centroids, and the cyan and red lines are the RANSAC-fitted left/right trunk-row lines and the navigation line; (<bold>c</bold>) vision-only fitting&#8212;blue squares mark 2D trunk detections, the green rectangle indicates the ROI, and the cyan and red lines are the RANSAC-fitted left/right trunk-row lines and the navigation line; (<bold>d</bold>) DeepLab v3+ semantic segmentation&#8212;black is background, white is the segmented ground region, and the red curve is the skeletonized center-line; (<bold>e</bold>) proposed fusion&#8212;blue solid lines with arrows denote reverse ray projections from the camera optical center; blue solid squares indicate LiDAR clusters successfully associated with the rays; green squares are the remaining unassociated clusters within the ROI; the cyan and red lines are as defined above.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g017a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g017b.jpg"/></fig><fig position="float" id="sensors-25-05432-f018" orientation="portrait"><label>Figure 18</label><caption><p>Comparison of different navigation line fusion methods.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05432-g018.jpg"/></fig><table-wrap position="float" id="sensors-25-05432-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05432-t001_Table 1</object-id><label>Table 1</label><caption><p>Hardware components.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Component</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" colspan="1">Intel RealSense D455</td><td align="center" valign="middle" rowspan="1" colspan="1">RGB Resolution</td><td align="center" valign="middle" rowspan="1" colspan="1">Up to 1280 &#215; 800</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RGB Frame Rate/fps</td><td align="center" valign="middle" rowspan="1" colspan="1">Up to 90</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RGB FOV/(&#176;)</td><td align="center" valign="middle" rowspan="1" colspan="1">86 &#215; 57 (&#177;3)</td></tr><tr><td rowspan="6" align="center" valign="middle" colspan="1">SLAMTEC<break/>2D LiDAR</td><td align="center" valign="middle" rowspan="1" colspan="1">Measurement Radius/m</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2&#8211;12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sampling Frequency/k</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Scanning Frequency/Hz</td><td align="center" valign="middle" rowspan="1" colspan="1">5&#8211;15</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Angular Resolution/(&#176;)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.225</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Scanning Range/(&#176;)</td><td align="center" valign="middle" rowspan="1" colspan="1">360</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ranging Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">2% of actual distance (&#8804;5 m)</td></tr><tr><td rowspan="5" align="center" valign="middle" colspan="1">NVIDIA Jetson XAVIER NX</td><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">6-core NVIDIA Carmel ARM v8.2 64-bit CPU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">384-core NVIDIA Volta&#8482; GPU with 48 Tensor Cores</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Acceleration Unit</td><td align="center" valign="middle" rowspan="1" colspan="1">2 &#215; NVIDIA<break/>(NVIDIA Deep Learning Accelerator)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TOPS</td><td align="center" valign="middle" rowspan="1" colspan="1">21</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Memory</td><td align="center" valign="middle" rowspan="1" colspan="1">8 GB 128-bit LPDDR4</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Tracked Mobile Platform</td><td align="center" valign="middle" rowspan="1" colspan="1">Dimensions/mm</td><td align="center" valign="middle" rowspan="1" colspan="1">750 &#215; 550 &#215; 850</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Motion Controller</td><td align="center" valign="middle" rowspan="1" colspan="1">STM32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Motors</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Rated Power/w</td><td align="center" valign="middle" rowspan="1" colspan="1">322</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rated Speed/r&#183;min<sup>&#8722;1</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05432-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05432-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance comparison of different detection methods in pomegranate trunk detection.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Detection Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP50 (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall Rate (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8</td><td align="center" valign="middle" rowspan="1" colspan="1">95.5</td><td align="center" valign="middle" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" rowspan="1" colspan="1">12.96</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8-ECA</td><td align="center" valign="middle" rowspan="1" colspan="1">95.5</td><td align="center" valign="middle" rowspan="1" colspan="1">90.9</td><td align="center" valign="middle" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" rowspan="1" colspan="1">13.04</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8-GAM</td><td align="center" valign="middle" rowspan="1" colspan="1">95.2</td><td align="center" valign="middle" rowspan="1" colspan="1">91.1</td><td align="center" valign="middle" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" rowspan="1" colspan="1">15.64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8-SA</td><td align="center" valign="middle" rowspan="1" colspan="1">95.6</td><td align="center" valign="middle" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" rowspan="1" colspan="1">92.0</td><td align="center" valign="middle" rowspan="1" colspan="1">13.71</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv8-ResCBAM (Proposed)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.36</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05432-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05432-t003_Table 3</object-id><label>Table 3</label><caption><p>Data association statistics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Fruit Trees</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Association Count</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Association Success Rate (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Test 1</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">23.0</td><td align="center" valign="middle" rowspan="1" colspan="1">92.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Test 2</td><td align="center" valign="middle" rowspan="1" colspan="1">27</td><td align="center" valign="middle" rowspan="1" colspan="1">24.8</td><td align="center" valign="middle" rowspan="1" colspan="1">91.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Test 3</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">27.7</td><td align="center" valign="middle" rowspan="1" colspan="1">92.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Test 4</td><td align="center" valign="middle" rowspan="1" colspan="1">28</td><td align="center" valign="middle" rowspan="1" colspan="1">26.4</td><td align="center" valign="middle" rowspan="1" colspan="1">94.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Test 5</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td><td align="center" valign="middle" rowspan="1" colspan="1">29.6</td><td align="center" valign="middle" rowspan="1" colspan="1">92.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05432-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05432-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance Comparison of GCA and RANSAC Algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Case</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Tree Points</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RMSE (m)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inlier Ratio (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Runtime (s)</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" colspan="1">Test 1</td><td rowspan="2" align="center" valign="middle" colspan="1">19</td><td align="center" valign="middle" rowspan="1" colspan="1">GCA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1469</td><td align="center" valign="middle" rowspan="1" colspan="1">79.44</td><td align="center" valign="middle" rowspan="1" colspan="1">0.001</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1395</td><td align="center" valign="middle" rowspan="1" colspan="1">79.44</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0087</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Test 2</td><td rowspan="2" align="center" valign="middle" colspan="1">21</td><td align="center" valign="middle" rowspan="1" colspan="1">GCA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">95.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.002</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1062</td><td align="center" valign="middle" rowspan="1" colspan="1">85.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.005</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Test 3</td><td rowspan="2" align="center" valign="middle" colspan="1">27</td><td align="center" valign="middle" rowspan="1" colspan="1">GCA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0982</td><td align="center" valign="middle" rowspan="1" colspan="1">96.43</td><td align="center" valign="middle" rowspan="1" colspan="1">0.009</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0947</td><td align="center" valign="middle" rowspan="1" colspan="1">89.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.004</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Test 4</td><td rowspan="2" align="center" valign="middle" colspan="1">23</td><td align="center" valign="middle" rowspan="1" colspan="1">GCA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0756</td><td align="center" valign="middle" rowspan="1" colspan="1">95.83</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0017</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0794</td><td align="center" valign="middle" rowspan="1" colspan="1">91.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.005</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Test 5</td><td rowspan="2" align="center" valign="middle" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">GCA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0775</td><td align="center" valign="middle" rowspan="1" colspan="1">95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0012</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0786</td><td align="center" valign="middle" rowspan="1" colspan="1">95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.003</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Test 6</td><td rowspan="2" align="center" valign="middle" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">GCA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1018</td><td align="center" valign="middle" rowspan="1" colspan="1">90</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0013</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1059</td><td align="center" valign="middle" rowspan="1" colspan="1">85</td><td align="center" valign="middle" rowspan="1" colspan="1">0.005</td></tr><tr><td rowspan="2" align="center" valign="middle" colspan="1">Average</td><td rowspan="2" align="center" valign="middle" colspan="1">21.7</td><td align="center" valign="middle" rowspan="1" colspan="1">GCA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0983</td><td align="center" valign="middle" rowspan="1" colspan="1">92.03</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0027</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1007</td><td align="center" valign="middle" rowspan="1" colspan="1">87.58</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0051</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improvement</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GCA vs. RANSAC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.40%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.10%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.06%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05432-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05432-t005_Table 5</object-id><label>Table 5</label><caption><p>Performance comparison of different navigation line extraction methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Lateral Error &#177; Standard Deviation/cm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Lateral Error RMS &#177; Standard Deviation/cm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Success Rate &#177; Standard Deviation (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">LiDAR RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">7.0 &#177; 0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">9.0 &#177; 0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">75.9 &#177; 4.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Vision RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">6.6 &#177; 0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2 &#177; 0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">89.8 &#177; 2.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeepLab v3+</td><td align="center" valign="middle" rowspan="1" colspan="1">6.9 &#177; 0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">8.6 &#177; 0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">87.0 &#177; 2.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.2 &#177; 0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.6 &#177; 0.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.4 &#177; 2.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05432-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05432-t006_Table 6</object-id><label>Table 6</label><caption><p>95% Conservative Bounds at the Task Level.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Conservative Upper Bound of Lateral Error/cm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Conservative Upper Bound of Lateral Error RMS/cm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Conservative Lower Bound of Success Rate (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">LiDAR RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">8.37</td><td align="center" valign="middle" rowspan="1" colspan="1">10.37</td><td align="center" valign="middle" rowspan="1" colspan="1">67.68</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Vision RANSAC</td><td align="center" valign="middle" rowspan="1" colspan="1">7.78</td><td align="center" valign="middle" rowspan="1" colspan="1">9.18</td><td align="center" valign="middle" rowspan="1" colspan="1">84.90</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeepLab v3+</td><td align="center" valign="middle" rowspan="1" colspan="1">8.27</td><td align="center" valign="middle" rowspan="1" colspan="1">9.78</td><td align="center" valign="middle" rowspan="1" colspan="1">81.51</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.28</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Calculation convention: conservative upper bound = mean + 1.96 &#215; SD; conservative lower bound = mean &#8722; 1.96 &#215; SD.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>