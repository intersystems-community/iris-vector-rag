<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431548</article-id><article-id pub-id-type="pmcid-ver">PMC12431548.1</article-id><article-id pub-id-type="pmcaid">12431548</article-id><article-id pub-id-type="pmcaiid">12431548</article-id><article-id pub-id-type="doi">10.3390/s25175330</article-id><article-id pub-id-type="publisher-id">sensors-25-05330</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Enhancing LiDAR&#8211;IMU SLAM for Infrastructure Monitoring via Dynamic Coplanarity Constraints and Joint Observation</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Feng</surname><given-names initials="Z">Zhaosheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05330" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Chen</surname><given-names initials="J">Jun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af2-sensors-25-05330" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liang</surname><given-names initials="Y">Yaofeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af3-sensors-25-05330" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3981-3693</contrib-id><name name-style="western"><surname>Liu</surname><given-names initials="W">Wenli</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af3-sensors-25-05330" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Peng</surname><given-names initials="Y">Yongfeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af3-sensors-25-05330" ref-type="aff">3</xref><xref rid="c1-sensors-25-05330" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Previtali</surname><given-names initials="M">Mattia</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05330"><label>1</label>China Harbour Engineering Company Limited, Beijing 100027, China; <email>fengzs@chec.bj.cn</email></aff><aff id="af2-sensors-25-05330"><label>2</label>China Communications Second Navigation Bureau First Engineering Co., Ltd., Wuhan 430416, China; <email>chenj@chec.bj.cn</email></aff><aff id="af3-sensors-25-05330"><label>3</label>School of Civil and Hydraulic Engineering, Huazhong University of Science and Technology, Wuhan 430074, China; <email>m202271514@hust.edu.cn</email> (Y.L.); <email>liu_wenli@hust.edu.cn</email> (W.L.)</aff><author-notes><corresp id="c1-sensors-25-05330"><label>*</label>Correspondence: <email>17809259776@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>27</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5330</elocation-id><history><date date-type="received"><day>07</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>15</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>27</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 15:25:32.480"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05330.pdf"/><abstract><p>Real-time acquisition of high-precision 3D spatial information is critical for intelligent maintenance of urban infrastructure. While SLAM technology based on LiDAR&#8211;IMU sensor fusion has become a core approach for infrastructure monitoring, its accuracy remains limited by vertical pose estimation drift. To address this challenge, this paper proposes a LiDAR&#8211;IMU fusion SLAM algorithm incorporating a dynamic coplanarity constraint and a joint observation model within an improved error-state Kalman filter framework. A threshold-driven ground segmentation method is developed to robustly extract planar features in structured environments, enabling dynamic activation of ground constraints to suppress vertical drift. Extensive experiments on a self-collected long-corridor dataset and the public M2DGR dataset demonstrate that the proposed method significantly improves pose estimation accuracy. In structured environments, the method reduces z-axis endpoint errors by 85.8% compared with Fast-LIO2, achieving an average z-axis RMSE of 0.0104 m. On the M2DGR Hall04 sequence, the algorithm attains a z-axis RMSE of 0.007 m, outperforming four mainstream LiDAR-based SLAM methods. These results validate the proposed approach as an effective solution for high-precision 3D mapping in infrastructure monitoring applications.</p></abstract><kwd-group><kwd>SLAM</kwd><kwd>error-state Kalman filter</kwd><kwd>LiDAR</kwd><kwd>LiDAR&#8211;IMU fusion</kwd><kwd>pose estimation</kwd></kwd-group><funding-group><award-group><funding-source>National Key Research and Development Program</funding-source><award-id>2023YFC3805800</award-id></award-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>72171094</award-id><award-id>52192664</award-id><award-id>U21A20151</award-id></award-group><award-group><funding-source>Intergovernmental Cooperation in International Science and Technology Innovation</funding-source><award-id>2024YFE0114400</award-id></award-group><funding-statement>This work is supported by the National Key Research and Development Program (Grant No. 2023YFC3805800), [National Natural Science Foundation of China] grant numbers [Grant No. 72171094 and No. 52192664, and No. U21A20151] and [Intergovernmental Cooperation in International Science and Technology Innovation] grant number [2024YFE0114400]. The APC was funded by the National Natural Science Foundation of China (No. 72171094).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05330"><title>1. Introduction</title><p>With the ongoing expansion of urban infrastructure, the efficient and accurate acquisition of its three-dimensional spatial information has become a critical factor in promoting intelligent upgrades in urban infrastructure maintenance [<xref rid="B1-sensors-25-05330" ref-type="bibr">1</xref>]. Traditional manual inspection-based maintenance models are facing efficiency bottlenecks, as intelligent mobile robot systems, leveraging their autonomy and all-weather operational capabilities, are gradually becoming the core tools for monitoring the full lifecycle of infrastructure. In this context, simultaneous localization and mapping (SLAM) technology, based on the fusion of heterogeneous sensors such as LiDAR and inertial measurement units (IMU), can construct high-precision three-dimensional spatial representations of dynamic environments. The localization accuracy and map quality of SLAM directly impact the reliability of geometric modeling, thereby offering a theoretical and technical framework for the digital transformation of urban infrastructure maintenance [<xref rid="B2-sensors-25-05330" ref-type="bibr">2</xref>].</p><p>However, mainstream SLAM algorithms inevitably suffer from pose estimation drift, whether they are based on nonlinear optimization [<xref rid="B3-sensors-25-05330" ref-type="bibr">3</xref>] or Kalman filtering [<xref rid="B4-sensors-25-05330" ref-type="bibr">4</xref>]. In six-degree-of-freedom (6-DoF) pose estimation tasks dominated by LiDAR sensors, the drift issues along the roll, pitch, and z-axes are particularly pronounced. This phenomenon stems from two critical factors. First, data-acquisition robots typically mount mechanical LiDARs (e.g., Velodyne VLP-16) in horizontal configurations, resulting in inherently sparse vertical measurements. For instance, the VLP-16 sensor has a vertical resolution of only 2&#176; and a limited vertical field of view (FoV) of 30&#176;, leading to insufficient geometric constraints for vertical-axis pose estimation in SLAM systems. Second, according to J. Laconte et al. [<xref rid="B5-sensors-25-05330" ref-type="bibr">5</xref>], LiDAR measurement biases caused by large incident angles at ground regions further degrade ranging accuracy. As incident angles increase, the measurement precision deteriorates systematically, causing cumulative errors during point cloud registration. This error propagation ultimately manifests as significant pose estimation drift.</p><p>In urban infrastructure scenarios dominated by rigid structures, ground regions typically exhibit strong coplanarity within local areas. Capitalizing on this attribute to address the aforementioned challenges, this paper presents a multi-sensor fusion localization and mapping algorithm based on an improved error-state Kalman filter, designed for application on data acquisition robots in infrastructure maintenance scenarios. Specifically, our contributions include the following:<list list-type="bullet"><list-item><p>Design of a ground point cloud extraction algorithm based on angular thresholding, which effectively distinguishes ground from non-ground points through vertical angular analysis of LiDAR point clouds frame-by-frame. This significantly enhances the accuracy and robustness of ground point cloud extraction, thereby improving practical applicability in engineering applications.</p></list-item><list-item><p>Development of a ground constraint module that exploits the local planar consistency prior inherent in urban infrastructure environments, incorporating coplanarity assessment functionality. By conditionally activating ground constraints, this approach effectively filters outliers, mitigates pose estimation drift, and enhances system robustness and reliability.</p></list-item><list-item><p>Integration of ground constraints with traditional LiDAR point cloud registration constraints through joint optimization to obtain optimal pose estimates, enabling construction of high-precision point cloud maps.</p></list-item></list></p><p>The remainder of this article is organized as follows. <xref rid="sec2-sensors-25-05330" ref-type="sec">Section 2</xref> reviews relevant academic research. <xref rid="sec3-sensors-25-05330" ref-type="sec">Section 3</xref> presents the proposed system. <xref rid="sec4-sensors-25-05330" ref-type="sec">Section 4</xref> analyzes and discusses the experimental results. Finally, <xref rid="sec5-sensors-25-05330" ref-type="sec">Section 5</xref> summarizes the entire text and outlines future research directions.</p></sec><sec id="sec2-sensors-25-05330"><title>2. Related Work</title><p>Since its conceptual introduction at the 1986 IEEE International Conference on Robotics and Automation (ICRA) [<xref rid="B6-sensors-25-05330" ref-type="bibr">6</xref>], SLAM algorithms have evolved into two major technical paradigms: filter-based and optimization-based approaches. In this section, we provide a brief review of the academic research related to these two paradigms.</p><sec id="sec2dot1-sensors-25-05330"><title>2.1. Filter-Based SLAM Approaches</title><p>Filtering-based SLAM algorithms emerged during the formative years of SLAM technology. In the era of limited arithmetic power, this type of method became mainstream due to its high computational efficiency, and its core idea is to estimate the amount of state by recursion, for example, using the state of the previous moment to deduce the state of the next moment. Early SLAM research primarily focused on filter frameworks, commonly employing the extended Kalman filter (EKF) to solve state estimation problems in SLAM [<xref rid="B7-sensors-25-05330" ref-type="bibr">7</xref>]. In 2001, J. Neira et al. demonstrated the sensitivity of the EKF algorithm to prediction association errors, highlighting the limitations of linearization assumptions in motion and observation models [<xref rid="B8-sensors-25-05330" ref-type="bibr">8</xref>]. To overcome this bottleneck, Montemerlo et al. proposed FastSLAM [<xref rid="B9-sensors-25-05330" ref-type="bibr">9</xref>], which combined particle filtering with Bayesian estimation for the first time, enhancing robustness in nonlinear scenarios by decoupling state estimation from map updates. Its improved version, FastSLAM 2.0 [<xref rid="B10-sensors-25-05330" ref-type="bibr">10</xref>], further incorporated EKF-based pose updates, effectively mitigating drift caused solely by kinematic recursion. Recently, Qin et al. introduced LINS [<xref rid="B4-sensors-25-05330" ref-type="bibr">4</xref>], a lightweight laser-IMU fusion framework utilizing the error-state Kalman filter (ESKF) to significantly reduce computational costs, marking a breakthrough in lightweight SLAM system development.</p><p>Filter-based SLAM approaches update system states recursively, offering low-latency performance but remaining susceptible to accumulated errors. This limitation has driven researchers toward optimization-based methods with global perspective advantages.</p></sec><sec id="sec2dot2-sensors-25-05330"><title>2.2. Optimization-Based SLAM Approaches</title><p>In the modern SLAM period, optimization methods have become mainstream, especially for applications in vision SLAM. Optimization methods bundle the globally accumulated information into an offline estimation of the robot&#8217;s entire trajectory and waypoints. Thanks to the increasing maturity of computer vision research and the significant improvement of computer performance, optimization methods based on visual sensors have become the mainstream direction of modern SLAM research. In 1997, Lu et al. formulated the SLAM problem as a maximum a posteriori probability estimation through graph optimization theory [<xref rid="B11-sensors-25-05330" ref-type="bibr">11</xref>], marking the emergence of optimization methods by reducing trajectory drift through historical sensor data fusion. Subsequently, Gutmann et al. proposed an efficient loop closure detection method, establishing an incremental graph optimization-based SLAM framework [<xref rid="B12-sensors-25-05330" ref-type="bibr">12</xref>]. Kschischang et al. introduced factor graph models to infer intrinsic variable dependencies, where each factor serves as a constraint relative to system poses, advancing optimization methods through constraint relationships among variables [<xref rid="B13-sensors-25-05330" ref-type="bibr">13</xref>]. Thereafter, optimization-based approaches experienced sustained development, gradually forming modern optimization-centric SLAM architectures [<xref rid="B14-sensors-25-05330" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05330" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05330" ref-type="bibr">16</xref>].</p><p>In this context, significant progress has also been made in a series of optimization methods using LiDAR as the main sensor. LOAM [<xref rid="B17-sensors-25-05330" ref-type="bibr">17</xref>] is a classical real-time LiDAR odometry and map-building method, which decomposes the complex SLAM problem into two algorithms running in parallel: a high-frequency, low-accuracy odometry estimation algorithm and a low-frequency, high-accuracy map-building algorithm. This divide-and-conquer strategy effectively balances computational efficiency and accuracy. To cope with the localization challenges in large-scale environments, SegMap [<xref rid="B18-sensors-25-05330" ref-type="bibr">18</xref>] proposes a map representation method based on 3D point cloud segmentation. The method achieves robust relocation and closed-loop detection by extracting structured segments in the scene and matching them using data-driven descriptors.</p><p>To enhance map density while achieving real-time mapping operations, SuMa [<xref rid="B19-sensors-25-05330" ref-type="bibr">19</xref>] adopts a surfel-based map representation and realizes fast alignment of the current frame to the map model by correlating the projected data. In addition, SuMa utilizes the rendered virtual map view for closed-loop inspection and validation to build globally consistent dense maps. For dynamic urban scenes, SuMa++ [<xref rid="B20-sensors-25-05330" ref-type="bibr">20</xref>] introduces semantic information on top of SuMa. By extracting the semantic labels of point clouds through a fully convolutional neural network, SuMa++ can effectively filter out dynamic objects and utilize semantic constraints to improve the robustness of position estimation, and ultimately generate dense 3D maps with rich semantic information.</p><p>Introduced in 2016, Google&#8217;s Cartographer algorithm [<xref rid="B21-sensors-25-05330" ref-type="bibr">21</xref>] integrated laser scan matching within submaps, loop closure detection, and graph optimization, emerging as a cornerstone 2D SLAM solution widely deployed in engineering practice. LeGO-LOAM [<xref rid="B22-sensors-25-05330" ref-type="bibr">22</xref>], proposed by Tixiao Shan et al., achieved efficiency&#8211;accuracy balance in complex terrains through a two-step optimization strategy leveraging ground and corner constraints. With growing multi-sensor fusion demands, the authors of LeGO-LOAM subsequently developed LIO-SAM [<xref rid="B23-sensors-25-05330" ref-type="bibr">23</xref>] and LVI-SAM [<xref rid="B24-sensors-25-05330" ref-type="bibr">24</xref>], both of which construct pose constraints using factor graphs. The latter augmented LIO-SAM with visual data via a tightly coupled architecture that merged LiDAR-inertial and visual-inertial subsystems. Koide et al. proposed hdl_graph_slam [<xref rid="B3-sensors-25-05330" ref-type="bibr">3</xref>], innovatively fusing multi-source constraints including GPS, ground planes, LiDAR odometry, and loop closure constraints to suppress long-term drift errors. Recently, Lin et al. introduced BALM [<xref rid="B25-sensors-25-05330" ref-type="bibr">25</xref>], a SLAM framework that incorporated bundle adjustment (BA), a technique from visual SLAM, into LiDAR-based SLAM, effectively mitigating localization drift. Xu&#8217;s team developed Fast-LIO [<xref rid="B26-sensors-25-05330" ref-type="bibr">26</xref>] and Fast-LIO2 [<xref rid="B27-sensors-25-05330" ref-type="bibr">27</xref>], where Fast-LIO utilized ESKF for tight integration of LiDAR and IMU data to reduce mapping errors, while Fast-LIO2 introduced an incremental k-d tree for global map maintenance alongside direct point cloud-map registration. Lin et al. further proposed R3LIVE [<xref rid="B28-sensors-25-05330" ref-type="bibr">28</xref>], a multi-sensor fusion system combining LiDAR, inertial measurements, and visual camera data to achieve real-time localization, mapping, and colorization. Zheng&#8217;s team advanced Fast-LIVO2 [<xref rid="B29-sensors-25-05330" ref-type="bibr">29</xref>] through ESKF-based efficient fusion of IMU, LiDAR, and image measurements, employing a sequential update strategy to address synchronization challenges in heterogeneous sensor networks. In recent years, researchers have proposed several innovations to address the challenges of LiDAR&#8211;IMU fusion algorithms. To tackle the vertical, pose drift problem, SDV-LOAM [<xref rid="B30-sensors-25-05330" ref-type="bibr">30</xref>] introduced an adaptive optimization strategy that dynamically adjusts the degrees of freedom based on geometric constraints, effectively suppressing the drift. Performance in challenging environments has also been a key focus. To improve performance in feature-sparse environments, D-LIOM [<xref rid="B31-sensors-25-05330" ref-type="bibr">31</xref>] adopted a tightly-coupled direct method, enhancing efficiency and robustness by directly registering raw point clouds. For scenarios with aggressive and unsmooth motion, such as those encountered by quadruped robots, Zhou et al. [<xref rid="B32-sensors-25-05330" ref-type="bibr">32</xref>] proposed a tightly coupled SLAM algorithm using a Normal Distribution Transform (NDT)-based registration method to improve robustness. Furthermore, fusing different sensor modalities has become an effective approach to boost system performance. One strategy involves integrating heterogeneous LiDARs; for instance, Li et al. [<xref rid="B33-sensors-25-05330" ref-type="bibr">33</xref>] developed a system that fuses the wide field of view (FoV) of a spinning LiDAR with the dense, high-resolution measurements of a solid-state LiDAR to achieve both robust ego-estimation and detailed maps. A more common approach is the fusion of visual information with LiDAR-inertial data. LVIO-fusion [<xref rid="B34-sensors-25-05330" ref-type="bibr">34</xref>], for example, achieves high-precision state estimation in degenerate environments, while R3LIVE++ [<xref rid="B35-sensors-25-05330" ref-type="bibr">35</xref>] combines state estimation with radiance map reconstruction, demonstrating the latest advancements. To handle the complexities of such fusion, recent works have targeted specific challenges. In dynamic environments, frameworks like LVI-fusion [<xref rid="B36-sensors-25-05330" ref-type="bibr">36</xref>] and another by Cai et al. [<xref rid="B37-sensors-25-05330" ref-type="bibr">37</xref>] incorporate object-detection networks to identify and remove transient objects such as pedestrians and vehicles, which significantly improves mapping quality. Additionally, to achieve a more robust fusion, LVI-fusion also proposes a method to assign reliable depth to visual features using a local LiDAR point cloud map. Despite these advancements, existing methods still face challenges, including insufficient utilization of geometric priors, misalignment of dynamic scene data, and computational efficiency bottlenecks.</p><p>Although filter-based SLAM approaches and optimization-based SLAM approaches have achieved significant progress, current localization and mapping methodologies share common limitations. Most studies underutilize structural geometric information in scenes, with existing algorithms lacking sufficient positioning accuracy for urban infrastructure scenarios. Future research demands scene geometry-aware multi-sensor fusion strategies for enhanced localization and mapping performance.</p></sec></sec><sec id="sec3-sensors-25-05330"><title>3. Proposed Method</title><p>This section details the LiDAR-inertial odometry framework developed in this study. The framework is built upon and deeply improves the advanced FAST-LIO2 algorithm. First, in <xref rid="sec3dot1-sensors-25-05330" ref-type="sec">Section 3.1</xref>, we provide an overview of the core architecture of FAST-LIO2, which serves as the theoretical foundation for the subsequent improvements. Subsequently, in <xref rid="sec3dot2-sensors-25-05330" ref-type="sec">Section 3.2</xref>, inspired by and optimized from the ground point cloud extraction methodology in LeGO-LOAM, we design a more robust ground segmentation algorithm. Next, to mitigate the drift in pose estimation, under the assumption of local ground plane consistency, we develop a ground constraint module in <xref rid="sec3dot3-sensors-25-05330" ref-type="sec">Section 3.3</xref>. Finally, in <xref rid="sec3dot4-sensors-25-05330" ref-type="sec">Section 3.4</xref>, we integrate this ground constraint with the inherent LiDAR point cloud registration constraints of FAST-LIO2 into a unified optimization framework, forming a novel joint observation model. This results in the final improved SLAM algorithm framework (improved FAST-LIO2 LiDAR SLAM). The overall system architecture is illustrated in <xref rid="sensors-25-05330-f001" ref-type="fig">Figure 1</xref>.</p><sec id="sec3dot1-sensors-25-05330"><title>3.1. Overview of FAST-LIO2</title><p>FAST-LIO2, as an advanced LiDAR-inertial odometry system, is fundamentally built upon an error-state Kalman filter framework, enabling tightly coupled multi-sensor data integration. Compared to conventional solutions, the algorithm achieves significant performance improvements through two breakthrough design innovations: first, it introduces an incremental dynamic indexing structure (ikd-Tree) for map management, enabling millisecond-level node insertion, deletion, and topological restructuring, effectively addressing the challenge of real-time map updating in large-scale environments; second, by abandoning traditional preprocessing pipelines reliant on handcrafted feature extraction, it employs a raw point cloud matching strategy that preserves high-dimensional geometric information within the environment, providing richer constraints for pose estimation.</p><p>From an engineering perspective, this framework demonstrates three notable advantages: (1) computational efficiency&#8212;within outdoor environments spanning hundreds of meters, the system achieves real-time pose tracking and 3D mapping at up to 100 Hz; (2) environmental adaptability&#8212;the framework has been validated under extreme motion scenarios with angular velocities exceeding 1000 degrees per second, confirming its strong robustness in complex dynamic environments; (3) measurement accuracy&#8212;in evaluations across multiple public datasets, its localization and mapping precision consistently outperforms various current state-of-the-art approaches. Importantly, these performance gains are achieved without reliance on hardware acceleration or distributed computing architectures, underscoring the superiority of its algorithmic design.</p><p>Given the outstanding performance and robust architecture of FAST-LIO2, this study adopts it as the foundational framework. Targeting typical urban infrastructure environments characterized by rich planar features, we conduct in-depth algorithmic optimization and functional enhancement.</p></sec><sec id="sec3dot2-sensors-25-05330"><title>3.2. Ground Extraction</title><p>To effectively establish ground constraints within a LiDAR-inertial odometry (LIO) framework, it is essential to accurately and robustly segment ground points from each LiDAR scan. This study proposes an improved ground extraction algorithm inspired by the ground segmentation principle of the LeGO-LOAM framework [<xref rid="B16-sensors-25-05330" ref-type="bibr">16</xref>], enhanced through the introduction of a multi-stage outlier removal strategy. This addresses the issue of false positives and missed detections commonly encountered by conventional methods in complex environments.</p><p>For mechanical spinning LiDAR sensors mounted in a horizontal orientation&#8212;such as the Velodyne VLP-16&#8212;the ground point cloud exhibits the following characteristics in the LiDAR coordinate system: (1) z-coordinates are predominantly low; (2) the points form a concentric circular pattern; and (3) locally, they can be approximated as planar surfaces. Based on these observations, the algorithm identifies ground points by examining vertical angle differences between point pairs from adjacent scan lines within the same horizontal azimuth.</p><p>Specifically, for each pair of points from adjacent scan lines sharing the same horizontal direction, the vertical angle between them is computed as:<disp-formula id="FD1-sensors-25-05330"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">arctan</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mo>&#8710;</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>&#8710;</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8804;</mml:mo><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mo>&#8710;</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mo>&#8710;</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent the coordinate differences between qualifying point pairs, and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes a predefined angular threshold. Point pairs satisfying the above inequality are classified as belonging to the ground region. By iterating through all points and evaluating this condition, continuous clusters of points that meet the angular constraint are extracted as candidate ground point sets.</p><p>In practical scenarios, LiDAR point clouds are susceptible to interference from dynamic obstacles, sensor noise, and self-occlusion. To enhance the robustness of the algorithm, a three-stage outlier suppression strategy is designed:<list list-type="bullet"><list-item><p>Dynamic search truncation: Points that are too close to or too far from the LiDAR center (&lt;0.3 m or &gt;50 m) are skipped to avoid ego-body interference and long-range measurement noise.</p></list-item><list-item><p>Cross-obstacle detection: A radial distance ratio threshold <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
is introduced. When the ratio of radial distances between adjacent points exceeds this threshold, the pair is considered to span an obstacle, and the current column search is terminated, as formulated below:<disp-formula id="FD2-sensors-25-05330"><label>(2)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">mix</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8804;</mml:mo><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item></list>
<list list-type="bullet"><list-item><p>Invalid point tolerance: Points with NaN (Not a Number) values are automatically skipped to prevent computational failures during processing.</p></list-item></list></p></sec><sec id="sec3dot3-sensors-25-05330"><title>3.3. Ground Constraint</title><p>In urban infrastructure environments dominated by rigid structures, the ground typically exhibits a high degree of local planarity. To exploit this characteristic, this study introduces the local ground plane consistency assumption, which models the ground as an infinitely extended plane with fixed slope and height within local regions during the SLAM process. By integrating real-time ground observations with historical plane parameters, this constraint effectively suppresses pose estimation drift. The key mathematical formulation is as follows.</p><sec id="sec3dot3dot1-sensors-25-05330"><title>3.3.1. Plane Parameterization and Residual Definition</title><p>Let the plane parameters fitted from the ground point cloud in a given LiDAR scan be denoted as <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the normalized normal vector of the plane, and d is the intercept, indicating the distance from the origin to the plane along the normal direction. The local dominant plane parameters <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are calibrated at the initial stage within the local region. The parameters of the initial plane are obtained by fitting the ground point cloud from the first radar frame at the initial time to a plane. The process of plane fitting is as follows:</p><p>Given a set of radar point clouds consisting of <italic toggle="yes">N</italic> three-dimensional spatial points, their three-dimensional Euclidean coordinates are represented as <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Find a suitable set of plane parameters <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to construct the following least squares problem:<disp-formula id="FD3-sensors-25-05330"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mstyle displaystyle="true" mathsize="120%"><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the normalized normal vector, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the intercept.</p><p>The ground constraint residual is then defined as:<disp-formula id="FD4-sensors-25-05330"><label>(4)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the ground plane parameters at the current time step, and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the result of transforming the initial plane parameter <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> into the current LiDAR coordinate frame <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3dot3dot2-sensors-25-05330"><title>3.3.2. Coordinate Transformation and Observation Model</title><p>To realize the real-time computation of <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, a three-stage coordinate transformation chain is constructed for <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (as illustrated in <xref rid="sensors-25-05330-f002" ref-type="fig">Figure 2</xref>):</p><list list-type="order"><list-item><p>Initial calibration:</p></list-item></list><p>In the initial LiDAR coordinate frame <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the dominant plane <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is fitted;</p><list list-type="simple"><list-item><label>2.</label><p>Transformation to world coordinate system:</p></list-item></list><p><disp-formula id="FD5-sensors-25-05330"><label>(5)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mtext>&#160;</mml:mtext><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the extrinsic rotation and translation from the LiDAR to the IMU at the initial time;</p><list list-type="simple"><list-item><label>3.</label><p>Transformation to current frame LiDAR coordinate system:</p></list-item></list><p><disp-formula id="FD6-sensors-25-05330"><label>(6)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">R</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the extrinsic parameters between the LiDAR and IMU at the current time; and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote the current system pose.</p><p>Thus, the nonlinear observation equation is established:<disp-formula id="FD7-sensors-25-05330"><label>(7)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the system state to be estimated; <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the observation function; and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the observation noise.</p></sec><sec id="sec3dot3dot3-sensors-25-05330"><title>3.3.3. Transformation to Current Frame LiDAR Coordinate System</title><p>Given the geometric characteristics of the planar constraint, the ground constraint can only restrict the pitch angle, roll angle, and normal translation (z), but it cannot observe the yaw angle or horizontal translations (x, y). Therefore, the corresponding dimensions in the Jacobian matrix <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8706;</mml:mo><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> need to be explicitly set to zero to ensure the physical validity of the optimization problem.</p><p>Additionally, the ground constraint relies on the assumption of local ground planar consistency, which assumes that the current ground plane is the same as the main plane at the current time step. To avoid incorrect constraints, a dual coplanarity check condition is introduced:<disp-formula id="FD8-sensors-25-05330"><label>(8)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>&#952;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">arccos</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8804;</mml:mo><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The ground constraint is only activated when both of the above conditions are satisfied simultaneously.</p></sec></sec><sec id="sec3dot4-sensors-25-05330"><title>3.4. Joint Observation</title><p>To address the weak horizontal pose constraints provided by ground point clouds, this paper proposes a differentiated processing strategy: ground point clouds are specifically used for planar constraint observations (<xref rid="sec3dot3-sensors-25-05330" ref-type="sec">Section 3.3</xref>), while non-ground point clouds participate in point cloud registration. The two types of constraints are fused through a joint observation model as follows:<disp-formula id="FD9-sensors-25-05330"><label>(9)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Specifically, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the observation model corresponding to point cloud registration, while <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the observation model associated with the ground constraint.</p><p>Due to the difference in noise characteristics between the ground constraints and point cloud registration, the Kalman gain computation method used in FAST-LIO2 cannot be directly applied. To address this, an improved gain is derived based on the principle of block matrices:<disp-formula id="FD10-sensors-25-05330"><label>(10)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mo stretchy="true">&#711;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>&#183;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8942;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> matrix and noise coefficient for the point-to-plane ICP component of point cloud registration, while <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> matrix and noise coefficient for the ground constraint component, respectively. This formulation avoids inverting high-dimensional matrices, thereby preserving the computational efficiency advantages of Fast-LIO2.</p><p>So far, the system&#8217;s posterior error state has been optimally updated, completing one iteration. Finally, the distortion-corrected feature points are projected into the world coordinate system based on the optimized pose, enabling incremental updates to the global map.</p><p>In summary, building upon FAST-LIO2 and incorporating improvements through three key modules&#8212;ground segmentation, ground constraint, and joint observation&#8212;we propose a SLAM system that achieves significantly enhanced localization accuracy and mapping quality in typical planar scenarios such as urban infrastructure environments.</p></sec></sec><sec id="sec4-sensors-25-05330"><title>4. Experiment and Results</title><p>This section conducts a comprehensive experimental evaluation of the proposed GC-LIO algorithm. To thoroughly validate its performance and robustness, we designed two distinct sets of experiments. First, to address the need for repeatable, small-scale validation in structured environments, we conducted tests using a self-built mobile robot platform on a self-collected dataset from a long corridor scene. Second, to demonstrate the algorithm&#8217;s accuracy and mapping capabilities in large-scale, complex scenarios, we utilized the public M2DGR dataset [<xref rid="B38-sensors-25-05330" ref-type="bibr">38</xref>]. In <xref rid="sec4dot1-sensors-25-05330" ref-type="sec">Section 4.1</xref>, we will introduce the hardware platforms and sensor configurations for both experimental setups. Then, <xref rid="sec4dot2-sensors-25-05330" ref-type="sec">Section 4.2</xref> defines the error metrics used for performance evaluation. Finally, in <xref rid="sec4dot3-sensors-25-05330" ref-type="sec">Section 4.3</xref>, we will show and discuss the experimental results in detail, including accuracy analysis and ablation studies.</p><sec id="sec4dot1-sensors-25-05330"><title>4.1. Experimental Setups</title><p>This section will provide a detailed overview of the hardware platform and sensor configuration used in the experiment. Additionally, prior to the experiment, we determined the six-degree-of-freedom external transformation between the LiDAR and IMU for both the self-built platform and the M2DGR dataset platform. We adopted the open-source LI-Calib toolbox [<xref rid="B39-sensors-25-05330" ref-type="bibr">39</xref>], which is based on a continuous-time batch estimation method for target-free calibration. This method achieves joint optimization by simultaneously minimizing the residuals between the raw IMU measurements and the laser radar point-to-surface distance in a B-spline parameterized trajectory. This target-free method is highly suitable for practical deployment and can accurately handle high-rate asynchronous data. The calibration sequence was executed under sufficient rotational and translational excitation to ensure observability, and the obtained external parameters were kept fixed in all subsequent experiments.</p><sec id="sec4dot1dot1-sensors-25-05330"><title>4.1.1. Self-Built Platform and Self-Collected Dataset</title><p>The mobile robot system platform used in this study is shown in <xref rid="sensors-25-05330-f003" ref-type="fig">Figure 3</xref>. This platform integrates four core modules: mechanical structure, actuation, control, and sensing. At the perception level, the robot is equipped with a LiDAR (Light Detection and Ranging) sensor and an Inertial Measurement Unit (IMU), which serve as the primary sensors for localization and mapping. The LiDAR has a data output rate of approximately 300 kHz and communicates with the main computer via the TCP/IP protocol. The IMU operates at a frequency of 100 Hz and interfaces with the host through USB. Detailed specification parameters of the LiDAR and IMU are listed in <xref rid="sensors-25-05330-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-05330-t002" ref-type="table">Table 2</xref>, respectively.</p><p><xref rid="sensors-25-05330-t001" ref-type="table">Table 1</xref> lists the key specification parameters of the LiDAR used in this study. The LiDAR model features a 360&#176; horizontal field of view (FoV), enabling comprehensive, blind-spot-free perception of the surrounding environment. It has a ranging accuracy of &#177;3 cm and a maximum detection range of up to 100 m, which is sufficient for mapping large-scale environments. Notably, its vertical field of view is 30&#176; with a vertical angular resolution of 2&#176;. This relatively sparse scanning pattern in the vertical direction is one of the key factors contributing to vertical drift in traditional SLAM algorithms, highlighting the necessity of the ground constraint method proposed in this paper.</p><p><xref rid="sensors-25-05330-t002" ref-type="table">Table 2</xref> presents the specification parameters of the IMU. The IMU supports a maximum sampling rate of 1 kHz, allowing it to provide high-frequency measurements of angular velocity and acceleration. Its accelerometer and gyroscope offer resolutions of 0.001 g and 0.001&#176;/s, respectively. The high-precision IMU data serves as the foundation for high-frequency motion prediction within the ESKF framework, offering reliable support for accurate pose estimation during the low-frequency scanning intervals of the LiDAR.</p></sec><sec id="sec4dot1dot2-sensors-25-05330"><title>4.1.2. Public M2DGR Dataset</title><p>To evaluate the localization accuracy advantage of the proposed method, this paper uses the Hall04 and Street04 sequences from the publicly available dataset M2DGR for testing. The M2DGR dataset is captured by a set of multi-sensor fusion mobile robot platform that integrates a variety of sensing sensors. The core sensing devices of the platform and their mounting positions on the robot are shown in <xref rid="sensors-25-05330-f004" ref-type="fig">Figure 4</xref>, including LiDAR, IMU, GNSS-IMU, fisheye camera, infrared camera, event camera, VI sensor, etc. The detailed parameters of various sensors are shown in <xref rid="sensors-25-05330-t003" ref-type="table">Table 3</xref>. Among them, LiDAR and IMU are used as the main sensors of the SLAM algorithm, and their high-precision data provide the basis for this study.</p><p>The sensors of the platform are distributed in two layers, where the LiDAR is located in the upper part of the lower layer and is capable of providing 360&#176; panoramic scanning for generating highly accurate 3D point cloud maps. Inertial sensors such as IMU and GNSS-IMU, on the other hand, provide high-frequency motion information to support position estimation. In addition, a variety of cameras (e.g., fisheye, infrared, and event cameras) integrated on the platform provide visual information for future studies, but in this paper, we mainly utilize LiDAR and IMU data for experiments.</p><p>Among them, the 360&#176; horizontal FoV of the Velodyne VLP-32C LiDAR is able to realize a comprehensive perception of the surrounding environment, and its maximum detection distance is up to 200 m, which is sufficient to meet the needs of large-scale environment mapping. It is worth noting that its vertical FoV ranges from &#8722;30&#176; to +10&#176;, and the vertical angular resolution is relatively sparse, which is one of the key factors of the vertical drift problem in traditional SLAM algorithms, thus highlighting the necessity of the ground constraint method proposed in this paper. The high-frequency motion measurements provided by sensors such as GNSS-IMU, IMU, etc., provide reliable support for high-frequency motion prediction in the ESKF framework, which ensures accurate position estimation during the LiDAR low-frequency scanning interval.</p></sec></sec><sec id="sec4dot2-sensors-25-05330"><title>4.2. Evaluation Metrics and Baseline Algorithms</title><p>This study used absolute position error (APE) as the primary evaluation metric in both datasets. APE serves as the quantitative performance metric for evaluating SLAM algorithms in this study, it is a widely used metric. APE measures the global trajectory accuracy by aligning the algorithm-estimated trajectories with the true-value trajectories provided by the dataset, and then calculating the difference between the corresponding bit position points. APE is precisely defined as the root mean square error (RMSE) across all trajectory points. The calculation formula is as follows:<disp-formula id="FD11-sensors-25-05330"><label>(11)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For the corridor dataset collected independently, in order to verify its effectiveness in improving positioning accuracy, this section uses its baseline algorithm, Fast-LIO2, for direct comparison. The dataset is based on a long corridor in an office. During the data collection process, the robot made two turns and eventually returned to the starting point via the same route. The experimental scenes are shown in <xref rid="sensors-25-05330-f005" ref-type="fig">Figure 5</xref>.</p><p>For the M2DGR dataset, the Hall04 and Street04 sequences are adopted to evaluate the localization accuracy and map-building performance of GC-LIO against multiple mainstream SLAM algorithms. The Hall04 sequence is captured in an indoor hall with a large flat floor, which is an ideal scenario to examine the validity of the ground constraint. The Street04 sequence is collected in a campus road environment containing slopes and more complex structures, and is used to test the robustness and mapping performance in a real urban scenario. The experimental scenes are shown in <xref rid="sensors-25-05330-f006" ref-type="fig">Figure 6</xref>.</p><p>In the M2DGR dataset, we compared the proposed algorithm with four representative open-source SLAM algorithms based on lidar, which are the following:<list list-type="bullet"><list-item><p>A-LOAM [<xref rid="B17-sensors-25-05330" ref-type="bibr">17</xref>]: A-LOAM is a code implementation and optimized version of the original LOAM algorithm, which mainly improves the readability and implementation efficiency of the code;</p></list-item><list-item><p>LeGO-LOAM [<xref rid="B22-sensors-25-05330" ref-type="bibr">22</xref>]: A lightweight LiDAR odometry and mapping algorithm optimized for ground-based applications;</p></list-item><list-item><p>LIO-SAM [<xref rid="B23-sensors-25-05330" ref-type="bibr">23</xref>]: A tightly coupled LiDAR-inertial odometry approach based on smoothing and mapping;</p></list-item><list-item><p>Fast-LIO2 [<xref rid="B27-sensors-25-05330" ref-type="bibr">27</xref>]: The baseline algorithm on which our proposed method is based is an advanced, tightly coupled LIO (LiDAR-inertial odometry) system.</p></list-item></list></p><p>All algorithms were executed using their default parameter configurations from the open-source code and ran on the same computer.</p></sec><sec id="sec4dot3-sensors-25-05330"><title>4.3. Results and Discussion</title><p>This section characterizes the GC-LIO algorithm through comprehensive benchmarking and analytical validation on both a self-collected corridor dataset and the public M2DGR dataset. The self-collected dataset is used to verify the algorithm&#8217;s ability to suppress vertical and attitude drift in a structured environment with strong planar constraints, in direct comparison with the baseline Fast-LIO2. The M2DGR dataset is used to further evaluate localization accuracy and mapping quality in large-scale, complex scenarios against multiple mainstream LiDAR-based SLAM algorithms. Furthermore, an ablation experiment is conducted to analyze the practical effectiveness and contribution of the coplanarity checking module in the proposed method.</p><sec id="sec4dot3dot1-sensors-25-05330"><title>4.3.1. Analysis of Accuracy</title><p>To evaluate the effectiveness of the proposed ground constraint in environments with strong planar geometry, we first conducted experiments on the self-collected long-corridor dataset described in <xref rid="sec4dot1dot1-sensors-25-05330" ref-type="sec">Section 4.1.1</xref>. Only Fast-LIO2 was used as the baseline for direct comparison, as it is the underlying framework of GC-LIO and ensures that performance differences are solely attributable to the proposed improvements.</p><p><xref rid="sensors-25-05330-f007" ref-type="fig">Figure 7</xref> shows the experimental scene and the top-down XY trajectories generated by both algorithms. The two trajectories almost completely overlap in the horizontal plane, indicating that the proposed ground constraint does not adversely affect horizontal pose estimation.</p><p>As shown in <xref rid="sensors-25-05330-f008" ref-type="fig">Figure 8</xref>, compared to Fast-LIO2, GC-LIO has a terminal cumulative error closer to zero in the roll, pitch, and z dimensions. Additionally, it can be observed that the GC-LIO algorithm&#8217;s curve has fewer &#8220;spikes&#8221; in the figure and a smoother trajectory. This is because GC-LIO introduces more robust ground plane observations in these three dimensions, resulting in less noise compared to the radar point cloud registration observations relied upon by Fast-LIO2.</p><p>In addition, in order to more accurately quantify the performance of the ground constraint module in mitigating drift issues, more data sequences were collected within the above corridor scene, and the error values of the z-axis estimates at the endpoints were compared between the two algorithms (since the data collection was closed, the true value can be considered 0), resulting in <xref rid="sensors-25-05330-t004" ref-type="table">Table 4</xref>.</p><p><xref rid="sensors-25-05330-t004" ref-type="table">Table 4</xref> summarizes the z-axis estimation errors of the two algorithms at the end of the self-collected corridor dataset. The baseline algorithm Fast-LIO2 exhibits a noticeable vertical offset relative to the ground truth endpoints, with an average error of 0.0734 m, reflecting the cumulative effect of drift in the absence of explicit planar constraints. In contrast, the proposed GC-LIO algorithm achieves an average z-axis estimation error of 0.0104 m, reducing endpoint errors by approximately 85.83%. This result further confirms that ground constraints can effectively suppress vertical drift in environments with strong planar geometric structures.</p><p>To intuitively compare the localization performance of different algorithms, we first conducted tests on the Hall04 sequence of the M2DGR dataset and visualized the trajectories generated by each algorithm against the ground truth. <xref rid="sensors-25-05330-f009" ref-type="fig">Figure 9</xref> presents the test results for each algorithm.</p><p><xref rid="sensors-25-05330-f009" ref-type="fig">Figure 9</xref>a,b, respectively, show the trajectory comparison results of different algorithms on the Hall04 sequence in the xy-plane and along the z-axis. As clearly seen in <xref rid="sensors-25-05330-f009" ref-type="fig">Figure 9</xref>a, the trajectories of our method (GC-LIO) and the baseline method (Fast-LIO2) on the xy-plane are almost completely overlapping. This is fully consistent with our design expectation, as the core improvement of the algorithm proposed in this chapter&#8212;the ground constraint module&#8212;primarily constrains the three vertical degrees of freedom in the pose estimation: roll, pitch, and z-axis. It does not directly affect the pose estimation in the xy-plane. Additionally, we observe that although the trajectories of all algorithms generally match the shape of the ground truth in the xy direction, there are still differences in terms of scale and extent of the trajectory, leading to discrepancies between the estimated and true trajectories. This is likely due to the corridor-like structure of the Hall04 environment, which lacks sufficient geometric constraints in the horizontal direction, causing a certain degree of degradation in the performance of all algorithms.</p><p>In contrast, the trajectory plot along the z-axis (<xref rid="sensors-25-05330-f009" ref-type="fig">Figure 9</xref>b) clearly demonstrates the superiority of the proposed algorithm. Traditional algorithms such as LIO-SAM and LEGO-LOAM exhibit severe accumulated drift, with their z-axis estimates increasingly deviating from the ground truth (represented by the red solid line) over time. Although the baseline algorithm Fast-LIO2 performs relatively well, it still shows noticeable drift to the naked eye. In sharp contrast to all the comparison algorithms, the z-axis trajectory of the proposed GC-LIO algorithm closely follows the true trajectory throughout the entire operation period. This intuitively proves that our method significantly mitigates the drift issue in pose estimation along the z-axis.</p><p>Vertical positioning precision is quantified via the z-axis component of APE across evaluated SLAM methods. The detailed statistical data of the absolute pose errors along the z-axis for all algorithms are shown in <xref rid="sensors-25-05330-t005" ref-type="table">Table 5</xref>, and a visualization of these results is presented in <xref rid="sensors-25-05330-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-05330-f011" ref-type="fig">Figure 11</xref>.</p><p>As can be seen from the detailed data in <xref rid="sensors-25-05330-t005" ref-type="table">Table 5</xref> and the visual results in <xref rid="sensors-25-05330-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-05330-f011" ref-type="fig">Figure 11</xref>, the proposed method GC-LIO significantly outperforms all other algorithms across all statistical metrics. Specifically, as shown in <xref rid="sensors-25-05330-t005" ref-type="table">Table 5</xref>, the z-axis APE root mean square error (RMSE) of GC-LIO is only 0.007 m, while the corresponding error of the baseline algorithm Fast-LIO2 is 0.027 m. This set of quantitative results irrefutably demonstrates that the ground constraint method proposed in this paper effectively mitigates the vertical pose drift commonly present in traditional SLAM algorithms, thereby achieving higher localization accuracy. Additionally, the improvement in localization accuracy directly contributes to enhanced mapping quality. To effectively evaluate the mapping performance of our algorithm, we conducted mapping tests on the Street04 sequence, which contains loop closures. As shown in <xref rid="sensors-25-05330-f012" ref-type="fig">Figure 12</xref>, based on the high-precision localization results, the map constructed by our algorithm (GC-LIO) exhibits a flat ground surface and consistent loop closure. In contrast, LEGO-LOAM accumulated significant vertical pose drift during operation, leading to a noticeable &#8220;double-ground&#8221; artifact at the loop closure in the generated map, indicating poor closure performance. This further validates the overall practical advantages of our algorithm in real-world applications.</p><p>To holistically validate GC-LIO&#8217;s performance in expansive outdoor environments, we geographically align its point cloud maps and trajectories with satellite imagery using the Street04 dataset, as visualized in <xref rid="sensors-25-05330-f013" ref-type="fig">Figure 13</xref>. From the figure, we can clearly see that the point cloud map generated by the algorithm (the colored part in the figure) is highly consistent with the roads, building outlines, and vegetation areas in the satellite image, which indicates that the map is georeferenced accurately. Meanwhile, the black motion trajectory closes the loop well, and the global path is accurate. This result intuitively proves that the algorithm proposed in this study can effectively suppress the cumulative drift under long-time operation, and construct high-quality 3D maps that are globally consistent and accurately aligned with the real world, which is of great application value for urban infrastructure inspection and digital modeling.</p></sec><sec id="sec4dot3dot2-sensors-25-05330"><title>4.3.2. Ablation Experiment</title><p>To independently verify the effectiveness and necessity of the coplanarity judgment module in our proposed method, we designed an ablation experiment. In this experiment, we ran three versions of the algorithm on the Street04 sequence:<list list-type="order"><list-item><p>The complete version of our proposed method (GC-LIO);</p></list-item><list-item><p>A variant of our method with the coplanarity judgment module disabled (referred to as GC-LIO w/o CPJ);</p></list-item><list-item><p>The baseline algorithm Fast-LIO2.</p></list-item></list></p><p>We then compared the trajectories of these three algorithms along the z-axis against the ground truth. The results are shown in <xref rid="sensors-25-05330-f014" ref-type="fig">Figure 14</xref>.</p><p>As observed from the trajectory curves in <xref rid="sensors-25-05330-f014" ref-type="fig">Figure 14</xref>, the complete GC-LIO algorithm, when operating for approximately 800 s, brings the robot back to a position close to the starting point. At this moment, the algorithm successfully identifies that the current ground plane is coplanar with the reference main plane established at the initial stage, and accordingly activates the ground constraint. As shown in the inset magnified view, the introduction of this constraint rapidly pulls the accumulated z-axis error back toward zero, achieving an almost perfect vertical loop closure.</p><p>In contrast, the GC-LIO w/o CPJ variant, which disables the coplanarity judgment module, blindly applies ground constraints throughout the entire operation. This erroneous constraint not only fails to correct the estimation errors but also introduces harmful information into the filter. Consequently, the resulting pose closure error becomes significantly larger, and its performance even deteriorates compared to the unmodified baseline algorithm Fast-LIO2.</p><p>The results of this ablation study strongly demonstrate that the proposed coplanarity judgment module is both critical and effective. It ensures that ground constraints are introduced only under safe conditions where the geometric assumption holds true, thereby preventing erroneous constraints from interfering with the system. This significantly enhances the robustness and reliability of the algorithm in real-world, dynamic environments.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05330"><title>5. Conclusions and Future Work</title><p>This paper proposes a multi-sensor fusion localization and mapping algorithm based on an improved error-state Kalman filter to address the demand for high-precision positioning and mapping in intelligent maintenance of urban infrastructure. Specifically, by introducing a ground point cloud segmentation method based on angular threshold and a dynamic coplanar constraint module, this work effectively integrates local planar consistency of the scene, significantly suppressing pose estimation drift in the vertical direction commonly observed in traditional SLAM algorithms. This study validates the effectiveness of scene prior knowledge combined with multi-sensor fusion strategies in complex environments, providing technical support for high-precision geometric modeling of urban infrastructure.</p><p>Although the proposed method demonstrates satisfactory performance in typical urban infrastructure scenarios, the following limitations require further investigation:<list list-type="order"><list-item><p>In scenarios with abrupt ground elevation changes or continuously varying slopes, the current static ground constraint module may experience degraded positioning accuracy due to failure of the local planar assumption.</p></list-item><list-item><p>The global map consistency optimization capability of the existing algorithm requires further improvement, along with enhanced robustness against dynamic obstacles.</p></list-item><list-item><p>In addition, it should be noted that hardware selection has a non-negligible influence on the experimental results. The LiDAR sensors used in this work have relatively sparse vertical resolution, which directly affects the strength of vertical geometric constraints and may limit performance in certain environments. While our proposed ground constraint mitigates this weakness, sensors with denser vertical channels could further improve accuracy. Moreover, the IMU precision and synchronization quality also contribute to the overall stability of the system. Regarding field tests, the evaluation scenarios&#8212;although representative&#8212;mainly feature structured environments with sufficient planar regions. More diverse and unstructured field conditions should be considered in the future to comprehensively assess robustness and generalization.</p></list-item></list></p><p>Future research could develop multi-ground principal plane constraints based on sliding windows to enhance the algorithm&#8217;s practicality in complex scenarios.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Writing&#8212;review and editing, Z.F.; writing&#8212;original draft preparation, Y.P., Z.F. and Y.L.; methodology&#8212;Y.L.; formal analysis, J.C.; supervision, W.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study were derived from the following resources available in the public domain: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/SJTU-ViSYS/M2DGR">https://github.com/SJTU-ViSYS/M2DGR</uri> (accessed on 10 February 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Zhaosheng Feng and Jun Chen are from Company. All authors declare no conflicts of interest. The research presented in this paper was conducted independently, and the authors have no financial interests or personal relationships that could potentially influence this study or its findings.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05330"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Stilla</surname><given-names>U.</given-names></name></person-group><article-title>Toward Building and Civil Infrastructure Reconstruction from Point Clouds: A Review on Data and Key Techniques</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2021</year><volume>14</volume><fpage>2857</fpage><lpage>2885</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2021.3060568</pub-id></element-citation></ref><ref id="B2-sensors-25-05330"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>Y.K.</given-names></name></person-group><article-title>SLAM-Driven Robotic Mapping and Registration of 3D Point Clouds</article-title><source>Autom. Constr.</source><year>2018</year><volume>89</volume><fpage>38</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2018.01.009</pub-id></element-citation></ref><ref id="B3-sensors-25-05330"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koide</surname><given-names>K.</given-names></name><name name-style="western"><surname>Miura</surname><given-names>J.</given-names></name><name name-style="western"><surname>Menegatti</surname><given-names>E.</given-names></name></person-group><article-title>A Portable Three-Dimensional LIDAR-Based System for Long-Term and Wide-Area People Behavior Measurement</article-title><source>Int. J. Adv. Robot. Syst.</source><year>2019</year><volume>16</volume><fpage>1729881419841532</fpage><pub-id pub-id-type="doi">10.1177/1729881419841532</pub-id></element-citation></ref><ref id="B4-sensors-25-05330"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pranata</surname><given-names>C.E.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name></person-group><article-title>LINS: A Lidar-Inertial State Estimator for Robust and Efficient Navigation</article-title><source>Proceedings of the 2020 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Paris, France</conf-loc><conf-date>31 May&#8211;31 August 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Paris, France</publisher-loc><year>2020</year><fpage>8899</fpage><lpage>8906</lpage></element-citation></ref><ref id="B5-sensors-25-05330"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Laconte</surname><given-names>J.</given-names></name><name name-style="western"><surname>Desch&#234;nes</surname><given-names>S.-P.</given-names></name><name name-style="western"><surname>Labussi&#232;re</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pomerleau</surname><given-names>F.</given-names></name></person-group><article-title>Lidar Measurement Bias Estimation via Return Waveform Modelling in a Context of 3D Mapping</article-title><source>Proceedings of the 2019 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>20&#8211;24 May 2019</conf-date><fpage>8100</fpage><lpage>8106</lpage></element-citation></ref><ref id="B6-sensors-25-05330"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>R.C.</given-names></name><name name-style="western"><surname>Cheeseman</surname><given-names>P.</given-names></name></person-group><article-title>On the Representation and Estimation of Spatial Uncertainty</article-title><source>Int. J. Robot. Res.</source><year>1986</year><volume>5</volume><fpage>56</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1177/027836498600500404</pub-id></element-citation></ref><ref id="B7-sensors-25-05330"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chatila</surname><given-names>R.</given-names></name><name name-style="western"><surname>Laumond</surname><given-names>J.</given-names></name></person-group><article-title>Position Referencing and Consistent World Modeling for Mobile Robots</article-title><source>Proceedings of the 1985 IEEE International Conference on Robotics and Automation</source><conf-loc>St. Louis, MO, USA</conf-loc><conf-date>25&#8211;28 March 1985</conf-date><publisher-name>Institute of Electrical and Electronics Engineers</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>1985</year><volume>Volume 2</volume><fpage>138</fpage><lpage>145</lpage></element-citation></ref><ref id="B8-sensors-25-05330"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Neira</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tardos</surname><given-names>J.D.</given-names></name></person-group><article-title>Data Association in Stochastic Mapping Using the Joint Compatibility Test</article-title><source>IEEE Trans. Robot. Autom.</source><year>2001</year><volume>17</volume><fpage>890</fpage><lpage>897</lpage><pub-id pub-id-type="doi">10.1109/70.976019</pub-id></element-citation></ref><ref id="B9-sensors-25-05330"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Montemerlo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Thrun</surname><given-names>S.</given-names></name><name name-style="western"><surname>Koller</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wegbreit</surname><given-names>B.</given-names></name></person-group><article-title>FastSLAM: A Factored Solution to the Simultaneous Localization and Mapping Problem</article-title><source>Proceedings of the AAAI National Conference on Artificial Intelligence</source><conf-loc>Edmonton, AB, Canada</conf-loc><conf-date>28 July&#8211;1 August 2002</conf-date><publisher-name>AAAI Press</publisher-name><publisher-loc>Edmonton, AB, Canada</publisher-loc><year>2002</year><fpage>593</fpage><lpage>598</lpage></element-citation></ref><ref id="B10-sensors-25-05330"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Montemerlo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Thrun</surname><given-names>S.</given-names></name><name name-style="western"><surname>Koller</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wegbreit</surname><given-names>B.</given-names></name></person-group><article-title>FastSLAM 2.0: An Improved Particle Filtering Algorithm for Simultaneous Localization and Mapping That Provably Converges</article-title><source>IJCAI&#8217;03, Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco, Mexico, 9&#8211;15 August 2003</source><publisher-name>Morgan Kaufmann Publishers Inc.</publisher-name><publisher-loc>San Francisco, CA, USA</publisher-loc><year>2003</year></element-citation></ref><ref id="B11-sensors-25-05330"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Milios</surname><given-names>E.</given-names></name></person-group><article-title>Globally Consistent Range Scan Alignment for Environment Mapping</article-title><source>Auton. Robot.</source><year>1997</year><volume>4</volume><fpage>333</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1023/A:1008854305733</pub-id></element-citation></ref><ref id="B12-sensors-25-05330"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gutmann</surname><given-names>J.-S.</given-names></name><name name-style="western"><surname>Konolige</surname><given-names>K.</given-names></name></person-group><article-title>Incremental Mapping of Large Cyclic Environments</article-title><source>Proceedings of the 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation</source><conf-loc>Monterey, CA, USA</conf-loc><conf-date>8&#8211;9 November 1999</conf-date><comment>CIRA&#8217;99 (Cat. No.99EX375)</comment><publisher-name>IEEE</publisher-name><publisher-loc>Monterey, CA, USA</publisher-loc><year>1999</year><fpage>318</fpage><lpage>325</lpage></element-citation></ref><ref id="B13-sensors-25-05330"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kschischang</surname><given-names>F.R.</given-names></name><name name-style="western"><surname>Loeliger</surname><given-names>H.-A.</given-names></name></person-group><article-title>Factor Graphs and the Sum-Product Algorithm</article-title><source>IEEE Trans. Inf. Theory</source><year>2001</year><volume>47</volume><fpage>498</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1109/18.910572</pub-id></element-citation></ref><ref id="B14-sensors-25-05330"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kretzschmar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Stachniss</surname><given-names>C.</given-names></name><name name-style="western"><surname>Grisetti</surname><given-names>G.</given-names></name></person-group><article-title>Efficient Information-Theoretic Graph Pruning for Graph-Based SLAM with Laser Range Finders</article-title><source>Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>25&#8211;30 September 2011</conf-date></element-citation></ref><ref id="B15-sensors-25-05330"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sunderhauf</surname><given-names>N.</given-names></name><name name-style="western"><surname>Protzel</surname><given-names>P.</given-names></name></person-group><article-title>Towards a Robust Back-End for Pose Graph SLAM</article-title><source>Proceedings of the 2012 IEEE International Conference on Robotics and Automation</source><conf-loc>St Paul, MN, USA</conf-loc><conf-date>14&#8211;18 May 2012</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>St Paul, MN, USA</publisher-loc><year>2012</year><fpage>1254</fpage><lpage>1261</lpage></element-citation></ref><ref id="B16-sensors-25-05330"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Saeedi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Paull</surname><given-names>L.</given-names></name><name name-style="western"><surname>Trentini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Multiple Robot Simultaneous Localization and Mapping</article-title><source>Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>25&#8211;30 September 2011</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>San Francisco, CA, USA</publisher-loc><year>2011</year><fpage>853</fpage><lpage>858</lpage></element-citation></ref><ref id="B17-sensors-25-05330"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>S.</given-names></name></person-group><article-title>LOAM: Lidar Odometry and Mapping in Real-time</article-title><source>Proceedings of the Robotics: Science and Systems (RSS), University of California</source><conf-loc>Berkeley, CA, USA</conf-loc><conf-date>12&#8211;16 July 2014</conf-date><volume>Volume 2</volume><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B18-sensors-25-05330"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dub&#233;</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cramariuc</surname><given-names>A.</given-names></name><name name-style="western"><surname>Dugas</surname><given-names>D.</given-names></name><name name-style="western"><surname>Nieto</surname><given-names>J.</given-names></name><name name-style="western"><surname>Siegwart</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cadena</surname><given-names>C.</given-names></name></person-group><article-title>SegMap: 3D Segment Mapping Using Data-Driven Descriptors</article-title><source>Proceedings of the Robotics: Science and Systems XIV; Robotics: Science and Systems Foundation</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>26&#8211;30 June 2018</conf-date><comment>Paper 3</comment><pub-id pub-id-type="doi">10.15607/RSS.2018.XIV.003</pub-id></element-citation></ref><ref id="B19-sensors-25-05330"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Behley</surname><given-names>J.</given-names></name><name name-style="western"><surname>Stachniss</surname><given-names>C.</given-names></name></person-group><article-title>Efficient Surfel-Based SLAM Using 3D Laser Range Data in Urban Environments</article-title><source>Proceedings of the Robotics: Science and Systems XIV; Robotics: Science and Systems Foundation</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>26&#8211;30 June 2018</conf-date><comment>Paper 16</comment><pub-id pub-id-type="doi">10.15607/RSS.2018.XIV.016</pub-id></element-citation></ref><ref id="B20-sensors-25-05330"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Milioto</surname><given-names>A.</given-names></name><name name-style="western"><surname>Palazzolo</surname><given-names>E.</given-names></name><name name-style="western"><surname>Giguere</surname><given-names>P.</given-names></name><name name-style="western"><surname>Behley</surname><given-names>J.</given-names></name><name name-style="western"><surname>Stachniss</surname><given-names>C.</given-names></name></person-group><article-title>SuMa++: Efficient LiDAR-Based Semantic SLAM</article-title><source>Proceedings of the 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), The Venetian Macao</source><conf-loc>Macau</conf-loc><conf-date>3&#8211;8 November 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Macau, China</publisher-loc><year>2019</year><fpage>4530</fpage><lpage>4537</lpage></element-citation></ref><ref id="B21-sensors-25-05330"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hess</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kohler</surname><given-names>D.</given-names></name><name name-style="western"><surname>Rapp</surname><given-names>H.</given-names></name><name name-style="western"><surname>Andor</surname><given-names>D.</given-names></name></person-group><article-title>Real-Time Loop Closure in 2D LIDAR SLAM</article-title><source>Proceedings of the 2016 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>16&#8211;21 May 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Stockholm, Sweden</publisher-loc><year>2016</year><fpage>1271</fpage><lpage>1278</lpage></element-citation></ref><ref id="B22-sensors-25-05330"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Englot</surname><given-names>B.</given-names></name></person-group><article-title>LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain</article-title><source>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Madrid, Spain</conf-loc><conf-date>1&#8211;5 October 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Madrid, Spain</publisher-loc><year>2018</year><fpage>4758</fpage><lpage>4765</lpage></element-citation></ref><ref id="B23-sensors-25-05330"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Englot</surname><given-names>B.</given-names></name><name name-style="western"><surname>Meyers</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ratti</surname><given-names>C.</given-names></name><name name-style="western"><surname>Rus</surname><given-names>D.</given-names></name></person-group><article-title>LIO-SAM: Tightly-Coupled Lidar Inertial Odometry via Smoothing and Mapping</article-title><source>Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>24 October 2020&#8211;24 January 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Las Vegas, NV, USA</publisher-loc><year>2020</year><fpage>5135</fpage><lpage>5142</lpage></element-citation></ref><ref id="B24-sensors-25-05330"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Shan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Englot</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ratti</surname><given-names>C.</given-names></name><name name-style="western"><surname>Rus</surname><given-names>D.</given-names></name></person-group><article-title>LVI-SAM: Tightly-Coupled Lidar-Visual-Inertial Odometry via Smoothing and Mapping</article-title><source>Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA), Xi&#8217;an, China</source><conf-date>30 May&#8211;5 June 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Xi&#8217;an, China</publisher-loc><year>2021</year><fpage>5692</fpage><lpage>5698</lpage></element-citation></ref><ref id="B25-sensors-25-05330"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>BALM: Bundle Adjustment for Lidar Mapping</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>6</volume><fpage>3184</fpage><lpage>3191</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3062815</pub-id></element-citation></ref><ref id="B26-sensors-25-05330"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>FAST-LIO: A Fast, Robust LiDAR-Inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>6</volume><fpage>3317</fpage><lpage>3324</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3064227</pub-id></element-citation></ref><ref id="B27-sensors-25-05330"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>FAST-LIO2: Fast Direct LiDAR-Inertial Odometry</article-title><source>IEEE Trans. Robot.</source><year>2022</year><volume>38</volume><fpage>2053</fpage><lpage>2073</lpage><pub-id pub-id-type="doi">10.1109/TRO.2022.3141876</pub-id></element-citation></ref><ref id="B28-sensors-25-05330"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>R<sup>3</sup> LIVE: A Robust, Real-Time, RGB-Colored, LiDAR-Inertial-Visual Tightly-Coupled State Estimation and Mapping Package</article-title><source>Proceedings of the 2022 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>23&#8211;27 May 2022</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Philadelphia, PA, USA</publisher-loc><day>23</day><month>May</month><year>2022</year><fpage>10672</fpage><lpage>10678</lpage></element-citation></ref><ref id="B29-sensors-25-05330"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>C.</given-names></name><name name-style="western"><surname>He</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>F.</given-names></name><etal/></person-group><article-title>FAST-LIVO2: Fast, Direct LiDAR&#8211;Inertial&#8211;Visual Odometry</article-title><source>IEEE Trans. Robot.</source><year>2025</year><volume>41</volume><fpage>326</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1109/TRO.2024.3502198</pub-id></element-citation></ref><ref id="B30-sensors-25-05330"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>SDV-LOAM: Semi-Direct Visual&#8211;LiDAR Odometry and Mapping</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>11203</fpage><lpage>11220</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3262817</pub-id><pub-id pub-id-type="pmid">37030871</pub-id></element-citation></ref><ref id="B31-sensors-25-05330"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group><article-title>D-LIOM: Tightly-Coupled Direct LiDAR-Inertial Odometry and Mapping</article-title><source>IEEE Trans. Multimed.</source><year>2023</year><volume>25</volume><fpage>3905</fpage><lpage>3920</lpage><pub-id pub-id-type="doi">10.1109/TMM.2022.3168423</pub-id></element-citation></ref><ref id="B32-sensors-25-05330"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><article-title>A Tightly-Coupled LIDAR-IMU SLAM Method for Quadruped Robots</article-title><source>Meas. Control</source><year>2024</year><volume>57</volume><fpage>1004</fpage><lpage>1013</lpage><pub-id pub-id-type="doi">10.1177/00202940231224593</pub-id></element-citation></ref><ref id="B33-sensors-25-05330"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Queralta</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Westerlund</surname><given-names>T.</given-names></name></person-group><article-title>Robust Multi-Modal Multi-LiDAR-Inertial Odometry and Mapping for Indoor Environments</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2303.02684</pub-id></element-citation></ref><ref id="B34-sensors-25-05330"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Du</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S.</given-names></name></person-group><article-title>LVIO-Fusion:Tightly-Coupled LiDAR-Visual-Inertial Odometry and Mapping in Degenerate Environments</article-title><source>IEEE Robot. Autom. Lett.</source><year>2024</year><volume>9</volume><fpage>3783</fpage><lpage>3790</lpage><pub-id pub-id-type="doi">10.1109/LRA.2024.3371383</pub-id></element-citation></ref><ref id="B35-sensors-25-05330"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>R3LIVE++: A Robust, Real-Time, Radiance Reconstruction Package With a Tightly-Coupled LiDAR-Inertial-Visual State Estimator</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>11168</fpage><lpage>11185</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2024.3456473</pub-id><pub-id pub-id-type="pmid">39250361</pub-id></element-citation></ref><ref id="B36-sensors-25-05330"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>LVI-Fusion: A Robust Lidar-Visual-Inertial SLAM Scheme</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>1524</elocation-id><pub-id pub-id-type="doi">10.3390/rs16091524</pub-id></element-citation></ref><ref id="B37-sensors-25-05330"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>T.</given-names></name></person-group><article-title>Improving SLAM Techniques with Integrated Multi-Sensor Fusion for 3D Reconstruction</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>2033</elocation-id><pub-id pub-id-type="doi">10.3390/s24072033</pub-id><pub-id pub-id-type="pmid">38610245</pub-id><pub-id pub-id-type="pmcid">PMC11014387</pub-id></element-citation></ref><ref id="B38-sensors-25-05330"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>D.</given-names></name></person-group><article-title>M2DGR: A Multi-Sensor and Multi-Scenario SLAM Dataset for Ground Robots</article-title><source>IEEE Robot. Autom. Lett.</source><year>2022</year><volume>7</volume><fpage>2266</fpage><lpage>2273</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3138527</pub-id></element-citation></ref><ref id="B39-sensors-25-05330"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lv</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zuo</surname><given-names>X.</given-names></name></person-group><article-title>Targetless Calibration of LiDAR-IMU System Based on Continuous-Time Batch Estimation</article-title><source>Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>24 October 2020&#8211;24 January 2021</conf-date></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05330-f001" orientation="portrait"><label>Figure 1</label><caption><p>Framework diagram of a radar-inertial odometry algorithm based on improved FAST-LIO2.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g001.jpg"/></fig><fig position="float" id="sensors-25-05330-f002" orientation="portrait"><label>Figure 2</label><caption><p>Three-stage coordinate transformation chain for ground constraint enforcement in GC-LIO.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g002.jpg"/></fig><fig position="float" id="sensors-25-05330-f003" orientation="portrait"><label>Figure 3</label><caption><p>Robotic system architecture overview, including data flow (<bold>left</bold>) and robot schematic (<bold>right</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g003.jpg"/></fig><fig position="float" id="sensors-25-05330-f004" orientation="portrait"><label>Figure 4</label><caption><p>M2DGR multi-sensor fusion mobile robot platform.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g004.jpg"/></fig><fig position="float" id="sensors-25-05330-f005" orientation="portrait"><label>Figure 5</label><caption><p>Scene diagram of a long corridor.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g005.jpg"/></fig><fig position="float" id="sensors-25-05330-f006" orientation="portrait"><label>Figure 6</label><caption><p>Algorithm test scenarios for the M2DGR dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g006.jpg"/></fig><fig position="float" id="sensors-25-05330-f007" orientation="portrait"><label>Figure 7</label><caption><p>Trajectory diagrams of each algorithm in the xy direction in the corridor dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g007.jpg"/></fig><fig position="float" id="sensors-25-05330-f008" orientation="portrait"><label>Figure 8</label><caption><p>Corridor dataset: diagrams showing the changes in pose estimated by each algorithm over time. (<bold>a</bold>) roll angle variation over time graph. (<bold>b</bold>) pitch angle variation over time graph. (<bold>c</bold>) Z-axis variation over time graph.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g008.jpg"/></fig><fig position="float" id="sensors-25-05330-f009" orientation="portrait"><label>Figure 9</label><caption><p>Comparison of trajectories among algorithms. (<bold>a</bold>) The trajectories of all algorithms on the Hall04 sequence compared with the ground truth in the xy direction. (<bold>b</bold>) The trajectory of the algorithm compared with the ground truth on the Hall04 sequence in terms of z-axis variation over time.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g009.jpg"/></fig><fig position="float" id="sensors-25-05330-f010" orientation="portrait"><label>Figure 10</label><caption><p>Absolute pose errors of all algorithms in the z-axis direction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g010.jpg"/></fig><fig position="float" id="sensors-25-05330-f011" orientation="portrait"><label>Figure 11</label><caption><p>Statistical metrics of absolute pose errors along the z-axis for all algorithms.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g011.jpg"/></fig><fig position="float" id="sensors-25-05330-f012" orientation="portrait"><label>Figure 12</label><caption><p>Side view of point cloud maps constructed by different algorithms.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g012.jpg"/></fig><fig position="float" id="sensors-25-05330-f013" orientation="portrait"><label>Figure 13</label><caption><p>Satellite overhead view of the sequence Street04 with generated point cloud map and robot trajectory.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g013.jpg"/></fig><fig position="float" id="sensors-25-05330-f014" orientation="portrait"><label>Figure 14</label><caption><p>The variation of trajectories along the z-axis over time for all algorithms compared with the ground truth.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05330-g014.jpg"/></fig><table-wrap position="float" id="sensors-25-05330-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05330-t001_Table 1</object-id><label>Table 1</label><caption><p>LiDAR specification parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specifications</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Dimensions</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8709;</mml:mo><mml:mn>103</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>72</mml:mn><mml:mtext>&#160;</mml:mtext><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Rotation Rate</td><td align="left" valign="middle" rowspan="1" colspan="1">10 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#177;</mml:mo><mml:mn>3</mml:mn><mml:mtext>&#160;</mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Horizontal field of view</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mn>360</mml:mn></mml:mrow><mml:mrow><mml:mo>&#176;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Vertical field of view</td><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mrow><mml:mo>&#176;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (+<inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mrow><mml:mo>&#176;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to &#8722;<inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mrow><mml:mo>&#176;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Vertical angular resolution</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>&#176;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Horizontal angular resolution</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mn>0.1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#176;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>0.4</mml:mn></mml:mrow><mml:mrow><mml:mo>&#176;</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Measurement range</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Up to 100 m</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05330-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05330-t002_Table 2</object-id><label>Table 2</label><caption><p>IMU specification parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specifications</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Dimensions</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8709;</mml:mo><mml:mn>103</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>72</mml:mn><mml:mtext>&#160;</mml:mtext><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Sampling Rate</td><td align="left" valign="middle" rowspan="1" colspan="1">Up to 1 kHz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Accelerometer resolution</td><td align="left" valign="middle" rowspan="1" colspan="1">0.001 g</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Gyroscope resolution</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow><mml:mrow><mml:mo>&#176;</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Weight</td><td align="left" valign="middle" rowspan="1" colspan="1">78 g</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interface</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">USB, RS-232</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05330-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05330-t003_Table 3</object-id><label>Table 3</label><caption><p>M2DGR dataset platform sensor parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Transducers</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Key Parameters</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">LiDAR</td><td align="left" valign="middle" rowspan="1" colspan="1">Velodyne VLP-32C</td><td align="left" valign="middle" rowspan="1" colspan="1">Horizontal Field of View (H-FoV): 360&#176;, Vertical Field of View (V-FoV): &#8722;30&#176; to +10&#176;, Rotation Rate: 10 Hz, Max Range: 200 m, Ranging Accuracy: 3 cm, Horizontal Angular Resolution: 0.2&#176;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RBG Camera</td><td align="left" valign="middle" rowspan="1" colspan="1">FLIR Pointgrey CM3-U3-13Y3C-CS</td><td align="left" valign="middle" rowspan="1" colspan="1">Resolution: 1280 &#215; 1024, H-FoV: 190&#176;, V-FoV: 190&#176;, Frame Rate: 15 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="left" valign="middle" rowspan="1" colspan="1">Ublox M8T</td><td align="left" valign="middle" rowspan="1" colspan="1">System: GPS/BeiDou, Sampling Rate: 1 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Infrared Camera</td><td align="left" valign="middle" rowspan="1" colspan="1">PLUG 617</td><td align="left" valign="middle" rowspan="1" colspan="1">Resolution: 640 &#215; 512, H-FoV: 90.2&#176;, V-FoV: 70.6&#176;, Frame Rate: 25 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VI Sensor</td><td align="left" valign="middle" rowspan="1" colspan="1">Realsense d435i</td><td align="left" valign="middle" rowspan="1" colspan="1">RGB/Depth Resolution: 640 &#215; 480, H-FoV: 69&#176;, V-FoV: 42.5&#176;, Frame Rate: 15 Hz, IMU: 6-axis, 200 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Event Camera</td><td align="left" valign="middle" rowspan="1" colspan="1">Inivation DVXplorer</td><td align="left" valign="middle" rowspan="1" colspan="1">Resolution: 640 &#215; 480, Frame Rate: 15 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IMU</td><td align="left" valign="middle" rowspan="1" colspan="1">Handsfree A9</td><td align="left" valign="middle" rowspan="1" colspan="1">Axes: 9-axis, Sampling Rate: 150 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNSS-IMU</td><td align="left" valign="middle" rowspan="1" colspan="1">Xsens Mti 680 G</td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS-RTK, Localization Precision: 2 cm, Sampling Rate: 100 Hz, IMU: 9-axis, 100 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Laser Scanner</td><td align="left" valign="middle" rowspan="1" colspan="1">Leica MS60</td><td align="left" valign="middle" rowspan="1" colspan="1">Localization Precision: 1 mm + 1.5 ppm</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Motion-capture System</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vicon Vero 2.2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Localization Accuracy: 1 mm, Sampling Rate: 50 Hz</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05330-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05330-t004_Table 4</object-id><label>Table 4</label><caption><p>Z-axis estimation error values in meters at the endpoints of each algorithm in the corridor dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Algorithm</th><th colspan="5" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Sequence</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">01</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">02</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">03</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">04</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">05</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fast-LIO2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.027</td><td align="center" valign="middle" rowspan="1" colspan="1">0.068</td><td align="center" valign="middle" rowspan="1" colspan="1">0.094</td><td align="center" valign="middle" rowspan="1" colspan="1">0.076</td><td align="center" valign="middle" rowspan="1" colspan="1">0.102</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GC-LIO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.007</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;0.002</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.013</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;0.003</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.027</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05330-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05330-t005_Table 5</object-id><label>Table 5</label><caption><p>Experimental results of the absolute trajectory error in the z-axis direction for each algorithm in the Hall04 sequence.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Algorithm</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Error Type</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Median</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSE</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fast-LIO2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.027</td><td align="center" valign="middle" rowspan="1" colspan="1">0.024</td><td align="center" valign="middle" rowspan="1" colspan="1">0.023</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" rowspan="1" colspan="1">0.055</td><td align="center" valign="middle" rowspan="1" colspan="1">0.128</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">A-LOAM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.036</td><td align="center" valign="middle" rowspan="1" colspan="1">0.034</td><td align="center" valign="middle" rowspan="1" colspan="1">0.033</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" rowspan="1" colspan="1">0.075</td><td align="center" valign="middle" rowspan="1" colspan="1">0.234</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LIO-SAM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.082</td><td align="center" valign="middle" rowspan="1" colspan="1">0.063</td><td align="center" valign="middle" rowspan="1" colspan="1">0.037</td><td align="center" valign="middle" rowspan="1" colspan="1">0.004</td><td align="center" valign="middle" rowspan="1" colspan="1">0.167</td><td align="center" valign="middle" rowspan="1" colspan="1">0.088</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LEGO-LOAM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.045</td><td align="center" valign="middle" rowspan="1" colspan="1">0.035</td><td align="center" valign="middle" rowspan="1" colspan="1">0.029</td><td align="center" valign="middle" rowspan="1" colspan="1">0.001</td><td align="center" valign="middle" rowspan="1" colspan="1">0.115</td><td align="center" valign="middle" rowspan="1" colspan="1">0.044</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GC-LIO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.007</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.005</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.003</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.007</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>