<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431609</article-id><article-id pub-id-type="pmcid-ver">PMC12431609.1</article-id><article-id pub-id-type="pmcaid">12431609</article-id><article-id pub-id-type="pmcaiid">12431609</article-id><article-id pub-id-type="doi">10.3390/s25175217</article-id><article-id pub-id-type="publisher-id">sensors-25-05217</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Temporal Video Segmentation Approach for Peruvian Sign Language</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-2518-1108</contrib-id><name name-style="western"><surname>Farfan</surname><given-names initials="S">Summy</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05217" ref-type="aff">1</xref><xref rid="c1-sensors-25-05217" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6868-2322</contrib-id><name name-style="western"><surname>Choquehuanca-Zevallos</surname><given-names initials="JJ">Juan J.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05217" ref-type="aff">1</xref><xref rid="af2-sensors-25-05217" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0726-1759</contrib-id><name name-style="western"><surname>Aguilera</surname><given-names initials="A">Ana</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af3-sensors-25-05217" ref-type="aff">3</xref><xref rid="af4-sensors-25-05217" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4859-0428</contrib-id><name name-style="western"><surname>Dongo</surname><given-names initials="I">Irvin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05217" ref-type="aff">1</xref><xref rid="af5-sensors-25-05217" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5966-0113</contrib-id><name name-style="western"><surname>Cardinale</surname><given-names initials="Y">Yudith</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af6-sensors-25-05217" ref-type="aff">6</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Lee</surname><given-names initials="EC">Eui Chul</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05217"><label>1</label>Electrical and Electronics Engineering Department, Universidad Cat&#243;lica San Pablo, Arequipa 04001, Peru; <email>jchoquehuancaz@ucsp.edu.pe</email> (J.J.C.-Z.); <email>ifdongo@ucsp.edu.pe</email> (I.D.)</aff><aff id="af2-sensors-25-05217"><label>2</label>Department of Signal Theory and Communications, Universidad Carlos III de Madrid, 28911 Leganes, Spain</aff><aff id="af3-sensors-25-05217"><label>3</label>Escuela de Ingenier&#237;a Inform&#225;tica, Facultad de Ingenier&#237;a, Universidad de Valpara&#237;so, Valpara&#237;so 2340000, Chile; <email>ana.aguilera@uv.cl</email></aff><aff id="af4-sensors-25-05217"><label>4</label>MEDING Interdisciplinary Center, Facultad de Ingenier&#237;a, Universidad de Valpara&#237;so, Valpara&#237;so 2340000, Chile</aff><aff id="af5-sensors-25-05217"><label>5</label>ESTIA Institute of Technology, University of Bordeaux, 64210 Bidart, France</aff><aff id="af6-sensors-25-05217"><label>6</label>Centro de Estudios en Ciencia de Datos e Inteligencia Artificial (ESenCIA), Universidad Internacional de Valencia, 46002 Val&#232;ncia, Spain; <email>ycardinale@universidadviu.com</email></aff><author-notes><corresp id="c1-sensors-25-05217"><label>*</label>Correspondence: <email>summy.farfan@ucsp.edu.pe</email></corresp></author-notes><pub-date pub-type="epub"><day>22</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5217</elocation-id><history><date date-type="received"><day>05</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>01</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>22</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05217.pdf"/><abstract><p>Continuous Sign Language Recognition is a task that involves the recognition of complete sequences of Sign Language videos and translating them into spoken or written language. This task is particularly challenging due to the complex temporal structure and variability in signing styles. In this context, temporal video segmentation emerges to distinguish individual signs from transitions in continuous video streams. Despite its importance, temporal segmentation for Sign Language has not yet fully benefited from recent advancements in Machine Learning. Many existing approaches still rely on outdated architecture. This study aims to learn the characteristics that distinguish signs from transitions, which are fundamental elements of Sign Language. To this end, we adapt two current temporal segmentation models, DiffAct and MS-TCN, and apply them to our own precisely annotated datasets for Peruvian Sign Language. We explore three training strategies&#8212;baseline, data augmentation, and multi-dataset. Results suggest that they can enhance the scores for both models but at the cost of increased variability across splits. Notably, the diffusion-based model showcased its ability to generalize to unseen sequences through higher scores for sign and transition identification on the test set, reaching a median value of 71.89% for mF1S and 72.68% for mF1B.</p></abstract><kwd-group><kwd>temporal segmentation</kwd><kwd>Peruvian Sign Language</kwd><kwd>diffusion network</kwd><kwd>deep learning</kwd></kwd-group><funding-group><award-group><funding-source>PROCIENCIA</funding-source><award-id>PE501084828-2023-PROCIENCIA</award-id></award-group><award-group><funding-source>ANID, Chile</funding-source><award-id>1250344</award-id></award-group><funding-statement>This research was supported by PROCIENCIA as executing entity of CONCYTEC under grant agreement no. PE501084828-2023-PROCIENCIA in the project Temporal Segmentation for Continuous Recognition of Peruvian Sign Language using Deep Learning Techniques. A. Aguilera thanks to ANID, Chile, Regular Fondecyt N&#176; 1250344.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05217"><title>1. Introduction</title><p>Disability inclusion has become a global priority, aiming to create fairer societies where all individuals can fully participate. This is critical for the 1.3 billion people worldwide who experience some form of disability [<xref rid="B1-sensors-25-05217" ref-type="bibr">1</xref>]. Despite advancements in terms of policy, barriers still persist, including poverty resulting from limited access to education and social exclusion due to prejudice and discrimination. In Peru, the 2017 national census reports that 10.3% of the population (3.2 million individuals) deals with disability, with the majority being women [<xref rid="B2-sensors-25-05217" ref-type="bibr">2</xref>]. Within this group, 60.4% of the working-age disabled population is economically inactive, and many do not have access to insurance coverage. Educational disparities aggravate the issue: 13.4% of individuals with disabilities have no formal education and 16.8% remain illiterate, with only 0.5% accessing specialized&#160;education.</p><p>In 2017, deafness was the third most common disability in Peru, affecting 7.6% of the disabled population (243,486 individuals) [<xref rid="B2-sensors-25-05217" ref-type="bibr">2</xref>]. Although Peruvian Sign Language was officially recognized as a national language back in 2010 by Law No. 29535, it is not considered a true language by many linguistic groups, so it is not widely accepted or utilized [<xref rid="B3-sensors-25-05217" ref-type="bibr">3</xref>]. As a consequence, many deaf children are pressured to learn oral language and lip-reading, which are ineffective for those unable to associate words with sounds. This lack of acceptance, coupled with limited exposure to Peruvian Sign Language during critical developmental years, often results in language deprivation, which can negatively impact cognitive development, mental health, and overall well-being [<xref rid="B4-sensors-25-05217" ref-type="bibr">4</xref>].</p><p>Recently, many efforts have been focused on creating digital services to support education and social inclusion of people with hearing impairments [<xref rid="B5-sensors-25-05217" ref-type="bibr">5</xref>]. In particular, automatic and Continuous Sign Language Recognition has attracted the interest of the research community [<xref rid="B6-sensors-25-05217" ref-type="bibr">6</xref>]. To do so, Machine Learning has been increasingly applied to Sign Language processing [<xref rid="B7-sensors-25-05217" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05217" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05217" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05217" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05217" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05217" ref-type="bibr">12</xref>]. At first, studies focused on Isolated Sign Language Recognition (ISLR), a task that classifies individual signs from pre-cropped video segments and for which various methods have been proposed [<xref rid="B13-sensors-25-05217" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05217" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05217" ref-type="bibr">15</xref>]. More recently, the focus has moved towards Continuous Sign Language Recognition, which aims to recognize entire sentences and provide full translations [<xref rid="B8-sensors-25-05217" ref-type="bibr">8</xref>,<xref rid="B10-sensors-25-05217" ref-type="bibr">10</xref>,<xref rid="B16-sensors-25-05217" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05217" ref-type="bibr">17</xref>]. In this context, temporal video segmentation emerges as a way to distinguish individual signs from transitions in continuous video streams. As stated by Koprinska and Carrato [<xref rid="B18-sensors-25-05217" ref-type="bibr">18</xref>], the main objective of Continuous Sign Language Recognition is to divide a video stream into meaningful and manageable segments, making it the first step toward the automatic annotation of digital video sequences.</p><p>In this regard, by learning differences between signs and transitions, temporal segmentation models can improve the flexibility and accuracy of Sign Language recognition systems [<xref rid="B19-sensors-25-05217" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05217" ref-type="bibr">20</xref>]. Moreover, this characteristic can speed up the labeling process, as spotting the temporal limits of signs is the most complex and time-consuming task.</p><p>Despite its importance, temporal segmentation models for Sign Language have not yet benefited from recent developments in Machine Learning (many existing approaches still rely on foundational models such as the work shown in Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>]).</p><p>While temporal segmentation has been extensively studied in the context of general actions, adapting these models to Sign Language presents unique challenges. Unlike actions, which benefit from rich contextual and environmental cues such as full-body motion and object interaction, Sign Language conveys meaning primarily through facial expressions and hand movements. This narrower visual scope limits the available information for segmentation, requiring more precise modeling of subtle gestures and transitions [<xref rid="B22-sensors-25-05217" ref-type="bibr">22</xref>].</p><p>Building upon the outlined challenges, this work is the first to apply temporal segmentation specifically to Peruvian Sign Language. It addresses a significant gap in the literature and seeks to inspire related future research through its key contributions:<list list-type="bullet"><list-item><p>Precise temporal annotation of three Peruvian Sign Language datasets. We annotated 546 videos across three distinct datasets, with a total duration of 57.64 min of continuous video stream. Special emphasis was placed on accurately identifying the temporal boundaries between signs and transitions, addressing a major gap in existing Peruvian Sign Language datasets. These high-quality annotations provide a valuable benchmark for training and evaluating temporal segmentation models, supporting future research in the field.</p></list-item><list-item><p>Development of a Binary Temporal Segmentation Framework for Peruvian Sign Language datasets. A customized Deep Learning pipeline was designed to detect transitions and signs within continuous video streams, addressing the challenges posed by limited annotated data.</p></list-item><list-item><p>This work marks the first known application and adaptation of two state-of-the-art models&#8212;originally developed for different domains and tasks&#8212;to the context of Peruvian Sign Language. By retraining these models on the newly annotated Peruvian Sign Language datasets, we enable a comparative evaluation of their performance and capacity to generalize in a low-resource Sign Language setting.</p></list-item></list></p><p>In summary, this paper aims to advance the field of Peruvian Sign Language temporal segmentation by developing a new dataset, adapting state-of-the-art models from temporal action segmentation, and assessing their effectiveness.</p><p>This paper is organized as follows. A summary of recent research is given in <xref rid="sec2-sensors-25-05217" ref-type="sec">Section 2</xref>, with an emphasis on action temporal segmentation and Sign Language processing. <xref rid="sec3-sensors-25-05217" ref-type="sec">Section 3</xref> describes our methodology, detailing the dataset creation, model adaptations, and experimental setup. The results on the test and validation sets are analyzed both quantitatively and qualitatively in <xref rid="sec4-sensors-25-05217" ref-type="sec">Section 4</xref>. We conclude and propose future perspectives of our work in <xref rid="sec5-sensors-25-05217" ref-type="sec">Section 5</xref>.</p></sec><sec id="sec2-sensors-25-05217"><title>2. Related Work</title><p>In this section, we describe both early approaches based on thresholding and more recent Deep Learning methods for temporal sign segmentation. Additionally, we review temporal segmentation techniques applied in other domains&#8212;such as actions and Sign Language comparison&#8212;to provide a broader context for methodological advances relevant to our study.</p><sec id="sec2dot1-sensors-25-05217"><title>2.1. Threshold-Based Methods</title><p>Early work on temporal segmentation was addressed using thresholds. Mocialov et al. [<xref rid="B23-sensors-25-05217" ref-type="bibr">23</xref>] propose the use of static thresholds to differentiate small, medium, and large movements by calculating hand centroid trajectories, assuming that acceleration is lower during transitions, making it possible to differentiate transitions from signs. Similarly, Choudhury et al. [<xref rid="B24-sensors-25-05217" ref-type="bibr">24</xref>] classify small movements as transitions, while Mocialov et al. [<xref rid="B23-sensors-25-05217" ref-type="bibr">23</xref>] also include medium movements in the same category.</p><p>However, centroids consider only global hand movement, excluding the motion of fingers, which is crucial for accurately capturing sign configuration required to differentiate signs. To address this limitation, Nayan et al. [<xref rid="B25-sensors-25-05217" ref-type="bibr">25</xref>] propose adaptive thresholds, which calculate optical flow to estimate the velocity of hands and fingers, and then two-norm values of the magnitude matrix are used to differentiate signs from transitions. This strategy is applied to finger-spelling in Indian Sign Language, based on the assumption that sign frames are quasi-stationary, which is true only in this specific scenario of spelling. Another strategy is proposed by Farag and Brock [<xref rid="B26-sensors-25-05217" ref-type="bibr">26</xref>], who base their proposal on 41 three-dimensional articulation positions as data input. Then, geometric descriptors are used to model the spatial relationship and translation movement, and temporal information is added through sliding windows. A balanced binary random forest, enhanced with bootstrapping, is employed to classify sequences of frames as either a sign or a non-sign.</p><p>Although these studies demonstrate the utility of threshold-based methods, they also highlight their limitations. These methods tend to oversimplify the movements in Sign Language or make assumptions that fail to capture its full complexity.</p></sec><sec id="sec2dot2-sensors-25-05217"><title>2.2. Deep Learning Methods</title><p>Recent advances have focused on Deep Learning techniques capable of modeling complex temporal dependencies. Yang et al. [<xref rid="B27-sensors-25-05217" ref-type="bibr">27</xref>] identify that the reduction in temporal resolution, a common issue in sequential data processing, is associated with the use of pooling layers, as strides are often greater than 1. Therefore, their study suggests modifications to the architecture of Shou et al. [<xref rid="B28-sensors-25-05217" ref-type="bibr">28</xref>] by replacing convolutional filters with Temporal Preservation Convolution (TPC)&#8212;dilated convolutions&#8212;along the temporal dimension while maintaining the same parameters for the spatial dimension.</p><p>P&#233;rez et al. [<xref rid="B22-sensors-25-05217" ref-type="bibr">22</xref>] adapted ASFormer, a Transformer-based architecture originally proposed by Yi et al. [<xref rid="B29-sensors-25-05217" ref-type="bibr">29</xref>] for action segmentation, to the task of Sign Language segmentation using five-fold cross-validation. ASFormer models temporal sequences through encoder blocks that combine self-attention mechanisms with dilated temporal convolutions, allowing the capture of complex temporal patterns within predefined windows. The decoder comprises multiple cross-attention blocks that progressively refine predictions by integrating external information, thereby reducing error accumulation.</p><p>In their adaptation, P&#233;rez et al. [<xref rid="B22-sensors-25-05217" ref-type="bibr">22</xref>] reported lower performance compared to action segmentation, primarily due to the removal of positional encoding and the intrinsic differences between the two tasks. While action sequences typically involve slower, goal-oriented movements and interactions with the environment, Sign Language is characterized by rapid manual articulations and non-manual cues such as facial expressions. These features introduce additional complexity in temporal modeling, which poses challenges for effective&#160;segmentation.</p><p>Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] retrained the MS-TCN architecture, originally proposed by Farha and Gall [<xref rid="B30-sensors-25-05217" ref-type="bibr">30</xref>], on the BSL-Corpus and PHOENIX14 datasets, achieving higher results over ASFormer. The MS-TCN model consists of four stages with ten dilated convolutional layers each, where the dilation factor doubles at every layer using 64 filters of size 3. This structure enables the model to capture increasingly long temporal dependencies without resorting to pooling or fully connected layers, thereby preserving temporal resolution and avoiding parameter inflation. Importantly, each stage outputs only per-frame class probabilities, as incorporating additional features was found to degrade performance. To enhance temporal consistency, the model integrates two loss functions during training: cross-entropy loss for frame-wise classification and a smoothing loss to penalize abrupt transitions. However, the qualitative analysis revealed over-segmentation issues, especially for short signs and fingerspelling, as each stage introduced incremental segmentation errors that negatively impacted the F1-score.</p><p>Temporal segmentation has also been widely explored in the context of action recognition, which Ding et al. [<xref rid="B31-sensors-25-05217" ref-type="bibr">31</xref>] define as the task of segmenting untrimmed videos and assigning each segment a label from a predefined set of actions. Bahrami et al. [<xref rid="B32-sensors-25-05217" ref-type="bibr">32</xref>] propose LTContext, a model that combines local windowed attention with long-term sparse attention to effectively capture both short-term and long-range dependencies. The architecture employs blocks composed of temporal convolutions followed by attention mechanisms, demonstrating the importance of balancing local and global context, particularly in lengthy and complex action sequences.</p><p>Liu et al. [<xref rid="B33-sensors-25-05217" ref-type="bibr">33</xref>] introduce DiffAct, a generative model based on denoising diffusion processes. It consists of an encoder that progressively adds noise to the video sequence and a decoder trained to reconstruct the original sequence, following a generative refinement paradigm. In contrast, Wen et al. [<xref rid="B34-sensors-25-05217" ref-type="bibr">34</xref>] develop a real-time segmentation framework that operates in a single stage. Their model incorporates a temporal convolutional network with a memory cache, a spatial&#8211;temporal feature extractor, and multimodal inputs by integrating language prompts with image features. This approach achieves competitive accuracy while maintaining computational efficiency and reducing over-segmentation.</p><p>While models used in temporal action segmentation have leveraged recent advancements in Machine Learning, those applied to temporal segmentation for Sign Language often rely on outdated architectures. These approaches could benefit significantly from state-of-the-art techniques that are specifically designed to capture long-range temporal dependencies in video data.</p><p>However, it is important to note that temporal segmentation is not the only approach to transitioning from isolated to Continuous Sign Language Recognition. Several studies have addressed this challenge using alternative strategies that do not rely on explicit segmentation [<xref rid="B8-sensors-25-05217" ref-type="bibr">8</xref>,<xref rid="B35-sensors-25-05217" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05217" ref-type="bibr">36</xref>].</p><p>Huang et al. [<xref rid="B35-sensors-25-05217" ref-type="bibr">35</xref>] argue that temporal segmentation is a source of errors that propagate through subsequent stages of a recognition model. Therefore, they propose a Hierarchical Attention Network with Latent Space (LS-HAN). For feature extraction, two-stream 3D Convolutional Neural Network (CNN) is employed, where one stream captures hand location and motion while the other focuses on local characteristics of local hand characteristics, LS maps video representations and annotations into a shared space, and the HAN predicts the sentences word by word. To achieve this, the authors use two large-scale datasets, the Modern Chinese Sign Language dataset and the PHOENIX14 dataset, both of which are limited to a given number of signs.</p><p>A recent model proposed by Feng et al. [<xref rid="B8-sensors-25-05217" ref-type="bibr">8</xref>] on Continuous Sign Language Recognition (CSLR), focuses not only on the correct classification of the sentences but also on the semantic coherence of the output. The proposed model is composed of four main modules: a dynamic feature module, a sequence module, a cross-modal contrastive learning module, and a classifier. The dynamic feature module employs dynamic trajectory capture and a key enhancer in order to highlight motion trajectories and relevant Sign Language elements. Next, temporal convolution is combined with Bi-LSTM to model temporal dependencies. The third module aligns visual and textual representations at the gloss level. For the classification module, ResNet34, pre-trained on ImageNet, is used asthe backbone.</p><p>A main drawback is that CSLR requires access to large-scale annotated datasets to achieve high performance. However, such datasets are scarce and costly to produce, which is why most existing studies rely on a limited number of publicly available benchmarks. While effective within their constrained settings, these approaches face significant limitations when applied to other Sign Languages. In particular, they often depend on a fixed vocabulary and require multiple repetitions per word or sentence, making them less suitable for low-resource languages or real-world deployment where such data is not&#160;available.</p><p>A summary of the related works discussed in this section is provided in <xref rid="sensors-25-05217-t001" ref-type="table">Table 1</xref>. Notably, the majority of these studies are based on the PHOENIX14 dataset, which offers multiple sentence-level annotations and serves as a widely accepted benchmark in the CSLR field.</p></sec></sec><sec id="sec3-sensors-25-05217"><title>3. Temporal Segmentation for Peruvian Sign Language: Our Proposal</title><p>As shown in <xref rid="sensors-25-05217-f001" ref-type="fig">Figure 1</xref>, our proposed framework for temporal segmentation for Peruvian Sign Language consists of a four-step approach: data collection, preprocessing, feature extraction, and classification. Our approach involves the use and comparison of two Machine Learning models: DiffAct and MS-TCN.</p><p>DiffAct is a diffusion-based model with an encoder&#8211;decoder architecture, originally developed for action temporal segmentation. It models the data distribution through iterative denoising and has demonstrated robust performance across different datasets, outperforming transformer-based models [<xref rid="B33-sensors-25-05217" ref-type="bibr">33</xref>]. These results motivated its selection to investigate whether its architectural advances can improve temporal segmentation in Sign Language. Therefore, in this work, we adapt DiffAct for the first time to Peruvian Sign Language. MS-TCN is a multistage model, related to temporal segmentation for Sign Language, which has demonstrated state-of-the-art performance, specifically on British and German datasets. It was selected as the baseline in our work, since it is the first and only model formally adapted for temporal segmentation in Sign Language, offering a strong reference to evaluate cross-language transferability. We excluded other potentially beneficial models due to practical constraints, as many lack reproducible implementations or accessible pretrained weights, making fair and controlled comparisons unfeasible. In contrast, MS-TCN and DiffAct provide publicly available, well-documented codebases and pretrained models, enabling reliable evaluation on our dataset.</p><p>The following sections describe in detail each step of our proposed approach.</p><sec id="sec3dot1-sensors-25-05217"><title>3.1. Data Acquisition</title><p>We evaluated several available Peruvian Sign Language datasets, and we realized that they do not provide accurate boundaries of individual signs within continuous video streams, limiting their applicability to this research. In the following, we describe the limitations of the Peruvian datasets evaluated.</p><p>The <monospace>AEC PUCP</monospace> dataset [<xref rid="B38-sensors-25-05217" ref-type="bibr">38</xref>] was developed by the Pontificia Universidad Cat&#243;lica del Per&#250; (PUCP) and created from videos of the educational program <italic toggle="yes">Aprendo en casa</italic> (learning at home), a national TV program that started its transmission during the pandemic, around 2020, which allowed children with access to TV or internet to keep studying the basic subjects, as online teaching was not possible at the time. The program&#8217;s topics are very varied and depend on children&#8217;s ages. The videos consist of a teacher explaining a certain topic, while an interpreter, located at the lower corner, is translating it into Peruvian Sign Language. An example frame from the dataset is shown in <xref rid="sensors-25-05217-f002" ref-type="fig">Figure 2</xref>. The <monospace>AEC PUCP</monospace> is the largest Peruvian dataset; it contains two videos partially annotated by three volunteers with intermediate knowledge of Peruvian Sign Language. Two levels of annotations are used: word and sentence. However, when reviewing the dataset, the boundaries of words were imprecise, making the dataset not useful for temporal segmentation.</p><p><monospace>PUCP-305</monospace> [<xref rid="B39-sensors-25-05217" ref-type="bibr">39</xref>] is a very varied dataset, as five interpreters participated in the video filming process. Each video features a signer standing in front of the camera, dressed in black, as illustrated in <xref rid="sensors-25-05217-f003" ref-type="fig">Figure 3</xref>, signing predefined sentences, with only one repetition per sentence. However, their participation was not even, resulting in an imbalanced dataset. Also, the boundaries of words were poorly annotated.</p><p>It is important to point out that <monospace>AEC PUCP</monospace> and <monospace>PUCP-305</monospace> were annotated using ELAN, a software tool that allows assigning labels to specific time segments.</p><p>Lastly, <monospace>LSP10</monospace> dataset [<xref rid="B40-sensors-25-05217" ref-type="bibr">40</xref>] contains videos of 25 people standing in front of varied backgrounds, facing a Kinect sensor v1 and wearing different types of clothing. The participants perform 10 short sentences, used on a daily basis, with each sentence repeated 60 times. Nevertheless, there is no level of annotation, as the video names are used as the only way of labeling.</p><p>A summary of the characteristics of the state-of-the-art Peruvian datasets are presented in <xref rid="sensors-25-05217-t002" ref-type="table">Table 2</xref>, highlighting their limitations for training Machine Learning models in temporal segmentation. To bridge this gap, we introduce three refined datasets, <monospace>manejar_conflictos</monospace>&#160;(manage_conflicts), <monospace>ira_alegria_RE</monospace>&#160;(anger_joy), and <monospace>PUCP-305_RE</monospace>, where &#8220;RE&#8221; stands for refined. The annotations for the three datasets were newly created in this work, but the video content was obtained from existing sources. Frame-level annotation was conducted to precisely mark the boundaries between signs and transitions. The following section provides a detailed description of these datasets.</p><sec id="sec3dot1dot1-sensors-25-05217"><title>3.1.1. Datasets</title><p>The <monospace>manejar_conflictos</monospace> and <monospace>ira_alegria_RE</monospace> datasets originate from two 28-second-long videos produced by <italic toggle="yes">the Aprendo en casa</italic> TV program, which are publicly available on YouTube. The <monospace>PUCP-305_RE</monospace> dataset was curated from the original <monospace>PUCP-305</monospace> dataset [<xref rid="B39-sensors-25-05217" ref-type="bibr">39</xref>] and includes a selection of 174 videos. This selection was made by a Peruvian Sign Language interpreter, who identified the most relevant and frequently used sentences to construct a more focused and representative dataset. Although we did not record the videos ourselves, they were originally produced under controlled conditions, ensuring consistency in their characteristics:<list list-type="bullet"><list-item><p>Interpreters wear dark clothing.</p></list-item><list-item><p>The background is uniformly white.</p></list-item><list-item><p>Videos were recorded at an approximate frame rate of 30 fps.</p></list-item><list-item><p>The camera is positioned directly in front of the signer, capturing their faces and keeping the signing space within the frame.</p></list-item></list></p><p>The annotation process was conducted manually by a certified Peruvian Sign Language interpreter using ELAN software, ensuring high-quality and consistent labels throughout the datasets. Each sentence in the datasets is unique, with no repetitions. A detailed summary of the dataset characteristics is provided in <xref rid="sensors-25-05217-t003" ref-type="table">Table 3</xref>.</p></sec><sec id="sec3dot1dot2-sensors-25-05217"><title>3.1.2. Annotation Process</title><p>The annotation process involves assigning a specific label to a recorded time span, where a sign has been spotted. We follow the guidelines proposed by Cormier and Fenlon [<xref rid="B41-sensors-25-05217" ref-type="bibr">41</xref>], where the end of a sign is marked when the hands begin to move away from the previous sign, while we consider the start of a sign when the hands reach the minimal position and configuration necessary to perform the sign.</p><p>Following this approach, every sign is labeled with its corresponding word in Spanish. Based on the recommendations of Bejarano et al. [<xref rid="B42-sensors-25-05217" ref-type="bibr">42</xref>], the words are written in the present tense and in their masculine form. Additionally, we ensure that the same sign is represented by a single word. It is important to point out that the annotation process is a very slow and costly task to perform, as every minute of the video takes 1 h to annotate.</p><p>Beyond the two primary categories&#8212;<italic toggle="yes">sign</italic> and <italic toggle="yes">transition</italic>&#8212;other categories such as <italic toggle="yes">rest</italic>, <italic toggle="yes">gestural signs</italic>, <italic toggle="yes">fillers</italic>, and <italic toggle="yes">NN</italic> (incorrectly or incompletely performed signs) are also identified in the datasets, which naturally occur in the flow of a conversation. These categories are detailed in <xref rid="sensors-25-05217-t004" ref-type="table">Table 4</xref>. <italic toggle="yes">Fillers</italic> and <italic toggle="yes">NN</italic> often lack clear semantic content, are challenging even for human annotators, and can negatively affect the model&#8217;s ability to learn meaningful temporal boundaries.</p><p>Identifying these categories in continuous video streams is crucial, as ignoring them could hinder sign recognition. However, using a multiclass approach is not possible, because they occur less frequently than the two main classes, as stated in <xref rid="sensors-25-05217-t005" ref-type="table">Table 5</xref>. Thus, we maintained a binary classification approach by changing labels. To this end, <italic toggle="yes">fillers</italic>, <italic toggle="yes">NN</italic>, and <italic toggle="yes">gestural signs</italic> were grouped under the <italic toggle="yes">sign</italic> label due to their similarity to this class, while <italic toggle="yes">rest</italic> was labeled as <italic toggle="yes">transition</italic>. Additionally, <xref rid="sensors-25-05217-t005" ref-type="table">Table 5</xref> shows that transitions are inherently fewer than sign frames, resulting in a class imbalance at a ratio of approximately 1:2. To mitigate this skewed distribution, we adopted a cost-sensitive learning strategy by leveraging the <monospace>pos_weight</monospace> parameter in the binary cross-entropy loss. As recommended in the PyTorch documentation, this weight was computed as the ratio between the majority (<italic toggle="yes">sign</italic>) and minority (<italic toggle="yes">transition</italic>) class frequencies, effectively rebalancing the loss function to penalize misclassifications of the shorter transition segments more heavily. This adjustment enhances gradient contributions for the underrepresented transition class.</p><p>As a demonstration, <xref rid="sensors-25-05217-f004" ref-type="fig">Figure 4</xref> provides a comparison between the annotations of the dataset <monospace>AEC PUCP</monospace> for the <monospace>ira_alegria</monospace> video with those in the dataset <monospace>ira_alegria_RE</monospace>, highlighting key differences that significantly impact the quality of the data used to train the Machine Learning model, because, depending on the precision of annotation, we either include or exclude a given number of frames, which affects the delimitation between closely related classes.</p></sec></sec><sec id="sec3dot2-sensors-25-05217"><title>3.2. Preprocessing</title><p>Firstly, the videos were cropped to isolate the Region of Interest (ROI) surrounding the Peruvian Sign Language interpreter, ensuring the full signing space was included. To ensure compatibility with the feature extraction models, the video frames were resized to a fixed resolution of <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>220</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>220</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels before being fed into the models. Moreover, the videos were temporally trimmed to last between 4 and 8 s, encompassing 7 to 14&#160;signs per video. As a result, we obtained 171 videos for the dataset <monospace>manejar_conflictos</monospace>, 201 for <monospace>ira_alegria_RE</monospace>, and 174 sentences were already clipped for <monospace>PUCP-305_RE</monospace>.</p><sec><title>Data Augmentation</title><p>The goal of data augmentation is to enhance the quality, volume, and diversity of the training data, as collecting sufficient data for real-world applications is difficult and costly [<xref rid="B43-sensors-25-05217" ref-type="bibr">43</xref>]. To address this, data augmentation techniques were applied to the datasets by introducing random variations in rotation, zoom, and translation. Their specific variation ranges are detailed in <xref rid="sensors-25-05217-t006" ref-type="table">Table 6</xref>.</p><p>These techniques fall under the category of affine transformations. Affine transformations are geometric modifications that alter the position of pixels without changing their values [<xref rid="B43-sensors-25-05217" ref-type="bibr">43</xref>]. They are considered traditional data augmentation techniques and are widely used to make training data more representative of real-world variations. Despite their simplicity, these methods have proven effective in improving the performance of Machine Learning models, so that they are usually applied before more advanced techniques.</p><p>The selected transformations expanded the dataset while preserving the linguistic and gestural integrity of the signs, making them suitable for training without introducing unrealistic variations. Other common augmentation techniques, such as frame sampling or inserting artificial intermediate frames, were avoided because they could alter the natural sequence of movements, leading to unrealistic transitions that do not accurately represent real-world Sign Language patterns. By focusing on spatial augmentations rather than temporal modifications, the approach ensured that the model learned from data that remained true to actual signing behavior while still benefiting from additional variability.</p><p>An ablation study was conducted to assess the impact of these augmentation techniques on segmentation performance. The experiments conducted consist in assessing the results obtained in the validation dataset when adding augmented versions of the data to the training phase. To identify the most effective one, all possible combinations of rotation, zoom, and translation transformations were evaluated. Notably, the transformations were carried out by using openCV functions.</p></sec></sec><sec id="sec3dot3-sensors-25-05217"><title>3.3. Feature Extraction</title><p>Since videos cannot be processed directly, feature extractors were required to obtain the most representative information, reducing the amount of data the Machine Learning models need to process. In this study, we employed the I3D feature extractors proposed by Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] and Iashin et al. [<xref rid="B37-sensors-25-05217" ref-type="bibr">37</xref>], leveraging the prior knowledge embedded in these&#160;architectures.</p><p>These feature extractors&#8217; selection depends on the I3D input shape required by the models used in this study, MS-TCN and DiffAct. The key difference between them lies in the shape of their output feature vectors:<list list-type="bullet"><list-item><p>The extractor of Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] produces a feature vector of size <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>,</p></list-item><list-item><p>The extractor of Iashin et al. [<xref rid="B37-sensors-25-05217" ref-type="bibr">37</xref>] generates a feature vector of size <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>2048</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>,</p></list-item></list>where <italic toggle="yes">N</italic> represents the number of temporal windows in the video, as the feature extractors compute RGB and optical flow features using sliding windows, with each resulting vector being assigned to the middle frame. This process is illustrated in <xref rid="sensors-25-05217-f005" ref-type="fig">Figure 5</xref>, where a simplified example is shown using a window size of <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and a stride of 1.</p><p>To ensure that predictions align with the ground truth labels for effective model comparison, videos were padded by repeating the first and last frames. The number of repetitions is determined by the window size <italic toggle="yes">w</italic>, which varies depending on the feature extractor. Specifically, Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] use a fixed window size of <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, as it is hardcoded in the implementation, while Iashin et al. [<xref rid="B37-sensors-25-05217" ref-type="bibr">37</xref>] employ the default setting of <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. In both models, the stride is set to 1.</p></sec><sec id="sec3dot4-sensors-25-05217"><title>3.4. Classification</title><p>As we mentioned, MS-TCN [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] is the most recent work in temporal segmentation for Sign Language and serves as the state-of-the-art reference model for our study. Given its strong performance and its origins as an adaptation of an action segmentation model, we adopt a similar methodology by leveraging transfer learning. In the case of DiffAct, knowledge acquired from action segmentation is repurposed to improve temporal segmentation in Peruvian Sign Language, allowing for an assessment of both its effectiveness and limitations in this specific context.</p><p>Instead of adopting a Continuous Sign Language Recognition approach, this study focuses on temporal segmentation due to the characteristics of the Peruvian Sign Language dataset. Unlike large-scale international datasets, such as PHOENIX14T [<xref rid="B44-sensors-25-05217" ref-type="bibr">44</xref>] or BSL-Corpus [<xref rid="B45-sensors-25-05217" ref-type="bibr">45</xref>], which are for German and British Sign Language, respectively, the datasets used in this study are significantly smaller, limiting the feasibility of training deep models from scratch. Additionally, the datasets contain unique sentences, meaning that the sequence of signs is never repeated, making the segmentation task inherently more&#160;challenging.</p><p>Furthermore, this study addresses the common challenge of limited annotated data, since the goal is to assist interpreters in accelerating the annotation process. The models are designed to generalize from the available data, predicting the location of signs and transitions even when encountering unseen sign sequences. Although the predictions may not be perfectly accurate, they can significantly reduce the time required for manual annotation, making the process more efficient.</p><sec id="sec3dot4dot1-sensors-25-05217"><title>3.4.1. Model A: MS-TCN</title><p>Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] state that the MS-TCN model was retrained on the BSLCorpus and PHOENIX14 datasets. Originally designed for temporal action segmentation, MS-TCN employs a multi-stage architecture in which initial predictions are iteratively refined through successive stages. This recursive approach provides higher-level layers with extended contextual information, enabling the capture of dependencies between actions.</p><p>Each stage outputs per-frame class probabilities without incorporating additional features, as including extra information was found to degrade performance. Furthermore, the model omits pooling and fully connected layers to preserve temporal resolution and limit the number of parameters. Instead, every stage consists solely of temporal convolution layers, with an increased receptive field achieved through 1D dilated convolutions. Also, there is a residual connection between the input and output of each dilated convolution&#160;layers.</p><p>During training, a combination of two loss functions is used:<list list-type="bullet"><list-item><p>Cross-entropy loss (<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) is applied for classification.</p></list-item><list-item><p>Smoothing loss (<inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>)&#8212;defined as the truncated mean squared error over the logarithmic per-frame probabilities&#8212;is employed to mitigate the problem of over-segmentation. The contribution of the smoothing loss is regulated by the <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> factor.</p></list-item></list></p><p>MS-TCN [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] adopts the architecture and hyperparameters used by Farha and Gall [<xref rid="B30-sensors-25-05217" ref-type="bibr">30</xref>], that is, an architecture composed of four stages, each one containing 10 dilated convolution layers. The dilation factor doubles with each subsequent layer, and each layer uses 64 filters of size 3. <xref rid="sensors-25-05217-f006" ref-type="fig">Figure 6</xref> shows the overall architecture with its four stages of dilated temporal convolutions and residual connections. These connections progressively refine predictions at each stage. The input represents feature embeddings from video frames, while the output corresponds to frame-wise class probabilities.</p></sec><sec id="sec3dot4dot2-sensors-25-05217"><title>3.4.2. Model B: DiffAct</title><p>DiffAct, proposed by Liu et al. [<xref rid="B33-sensors-25-05217" ref-type="bibr">33</xref>], is a diffusion-based generative model, structured as an encoder&#8211;decoder architecture and designed for temporal action segmentation. Unlike conventional discriminative models that perform frame-wise classification, it incorporates a denoising diffusion process, enabling the model to learn the underlying distribution of action sequences. This approach enhances temporal consistency and boundary alignment, addressing key challenges in action segmentation.</p><p>Moreover, the model follows a two-step generative framework, as pictured in <xref rid="sensors-25-05217-f007" ref-type="fig">Figure 7</xref>. The forward process gradually corrupts input features by adding Gaussian noise at each step with a predefined variance, while the reverse process iteratively removes noise, reconstructing the original action sequence. This iterative refinement helps the model to generate more coherent predictions while reducing frame-wise inconsistencies.</p><p>The encoder processes I3D features (<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>2048</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), extracted from the input video, and generates a temporally enriched representation. It consists of 12 convolutional blocks, each containing 256 feature maps and a kernel size of 5. To enhance long-term dependencies, dilated convolutions are incorporated, effectively expanding the model&#8217;s receptive field. A key design choice is the multi-scale feature extraction, where the outputs of blocks 7, 8, and 9 are concatenated before being passed to the decoder. This approach retains information at multiple temporal resolutions, improving segmentation accuracy.</p><p>Before entering the decoder, the feature representations undergo masking and noise injection. Specifically, Gaussian noise is added to simulate a diffusion process, which helps the model learn to denoise and refine predictions. Additionally, random feature masking is applied, forcing the model to reconstruct missing information. This strategy improves generalization by making the model more robust to variations in input data.</p><p>The decoder follows a structure similar to the encoder but is optimized for sequence reconstruction. It consists of 8 convolutional blocks, each with 128 feature maps and a kernel size of 7, which improves smoothing and feature refinement. Additionally, self-attention layers are incorporated to enhance the model&#8217;s ability to focus on relevant temporal dependencies, ensuring that predictions align with the true structure of the action sequence. The decoder removes noise iteratively, restoring the original sequence from a noise-corrupted input, which results in more coherent and temporally consistent action segmentation.</p><p>On the other hand, DiffAct optimizes a weighted multi-term loss function, ensuring accurate classification, smooth transitions, and well-defined boundaries. The total loss is defined as in Equation&#160;(<xref rid="FD1-sensors-25-05217" ref-type="disp-formula">1</xref>), where</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (Cross-entropy loss) supervises multi-class classification, ensuring accurate action predictions.</p></list-item><list-item><p><inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (Binary cross-entropy loss) detects class transitions, assigning 1 to boundary frames and 0 otherwise.</p></list-item><list-item><p><inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (Mean squared error loss) enforces temporal smoothness by minimizing variations between neighboring frames.</p></list-item></list><disp-formula id="FD1-sensors-25-05217"><label>(1)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#215;</mml:mo><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#215;</mml:mo><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>&#947;</mml:mi><mml:mo>&#215;</mml:mo><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><p>Initial weight settings are <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3dot4dot3-sensors-25-05217"><title>3.4.3. Performance Metrics</title><p>In order to evaluate the performance of the resulting models, we use the performance metrics proposed by Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>]. These metrics assess how accurately the models identify contiguous segments of video frames corresponding to either <italic toggle="yes">signs</italic> or <italic toggle="yes">transitions</italic>, within a continuous sequence, as detailed below:<list list-type="bullet"><list-item><p>mF1B (mean F1 Boundary): This metric evaluates the accuracy of predicted boundaries in action segmentation. A boundary prediction is considered correct if its midpoint-to-true-boundary distance is within a specified threshold (from 1 to 4 frames). The final mF1B score is obtained by averaging the F1B scores computed for each threshold in this range, providing a comprehensive measure of boundary detection accuracy.</p></list-item><list-item><p>mF1S (mean F1 Sign): This metric measures the quality of sign segments in temporal action segmentation. A predicted segment is considered correct if its Intersection over Union (IoU) with the ground truth exceeds a threshold (ranging from 0.45 to 0.75). The final mF1S score is calculated as the mean F1-Score across all thresholds, ensuring a robust evaluation of both temporal alignment and segment completeness.</p></list-item></list></p><p>The practical implications of these metrics extend beyond simple per-frame classification as they reflect the model&#8217;s capability to accurately capture complete temporal segments rather than merely predicting isolated frames.</p></sec></sec></sec><sec id="sec4-sensors-25-05217"><title>4. Experiments and Analysis</title><p>This section presents the experimental framework used to compare the MS-TCN and DiffAct models, detailing the implementation setup, training procedures, and evaluation metrics. We provide a comprehensive analysis of the results obtained, including ablation studies that explore the contribution of individual components and design choices within the model. Through these experiments, we aim to validate the effectiveness of our approach and gain deeper insights into the factors influencing performance.</p><p><bold>Datasets</bold>. This study utilizes three datasets (depicted in <xref rid="sensors-25-05217-t003" ref-type="table">Table 3</xref>): <monospace>manejar_conflictos</monospace>, <monospace>ira_alegria_RE</monospace>, and <monospace>PUCP-305_RE</monospace>. For the initial training phase, only the <monospace>manejar_ conflictos</monospace> dataset was used, as it provides the most suitable data for model training in terms of clarity and class separability. Specifically, this dataset contains fewer instances of ambiguous or non-discriminative categories&#8212;such as <italic toggle="yes">filler</italic> and <italic toggle="yes">NN</italic> segments. The <monospace>ira_alegria_RE</monospace> dataset, which includes a high proportion of these categories, was initially excluded for this reason, but was later incorporated to explore its potential in enhancing model robustness. The <monospace>PUCP-305_RE</monospace> dataset was omitted from training due to an imbalanced distribution of videos among signers, which could lead to underrepresentation and hinder generalization.</p><p>For both validation and testing, only the <monospace>manejar_conflictos</monospace> dataset was employed. The data were split using a hold-out strategy, allocating 60% for training, 20% for validation, and 20% for testing to ensure robust generalization.</p><p><bold>Preprocessing</bold>. In brief, each model uses a distinct feature extractor: DiffAct utilizes I3D features (2048-dimensional) with the extractor proposed by Iashin et al. [<xref rid="B37-sensors-25-05217" ref-type="bibr">37</xref>], while MS-TCN also employs I3D features but with a 1024-dimensional representation, using the extractor proposed by Carreira and Zisserman [<xref rid="B46-sensors-25-05217" ref-type="bibr">46</xref>] and retrained by Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>], but both require frames with size <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>220</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>220</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels as input. Additionally, data augmentation transformations, such as rotation, zoom, and translation, were applied to improve generalization.</p><p><bold>Implementation Details</bold>. The models used in this study were retrained with distinct configurations to optimize their performance. DiffAct was retrained using a batch size of 1 and a learning rate of 0.0005, while MS-TCN used a batch size of 4 and a learning rate of 0.0005. Both models were optimized using Adam. Additionally, early stopping was applied based on mF1B to prevent overfitting. The comparison between the implementation details per model is shown in <xref rid="sensors-25-05217-t007" ref-type="table">Table 7</xref>.</p><p>These implementation choices reflect the specific requirements of each model&#8212;DiffAct, with its high-dimensional feature representation and generative nature, required individualized sequence processing, while MS-TCN, focused on temporal segmentation, benefited from a larger batch size and structured feature extraction.</p><p>Moreover, the models&#8217; architectures were modified to better fit the task at hand, such that different architectural variants were tested twice and the best results on the validation set were taken as the baseline architecture for future experiments. MS-TCN was set to 4&#160;stages, 8 blocks per stage, and 128 feature maps, while DiffAct was configured with 14&#160;blocks and 64 feature maps for the encoder, and the decoder used 10 blocks and 64&#160;feature&#160;maps.</p><p>Having selected the appropriate architecture for the models, we employed data augmentation techniques and multi-dataset training to improve their performance. They were applied across 10 different splits. These results were compared to those obtained without augmentation. Finally, the best-performing models for both MS-TCN and DiffAct were evaluated on the test set.</p><p><bold>Hardware and software</bold>. MS-TCN was trained on Google Colaboratory using an NVIDIA Tesla T4 GPU with 12.7 GB of RAM, running in the Linux-based Google Colab environment, selected for compatibility reasons. The training setup included Python 3.11.12, PyTorch 2.6.0, and CUDA 12.4. In contrast, DiffAct was trained locally on a workstation equipped with an NVIDIA RTX 2050 GPU and 64 GB of RAM, using Python 3.9.19, PyTorch 2.0.1, and CUDA 11.</p><sec id="sec4dot1-sensors-25-05217"><title>4.1. Architectural Modifications</title><p>DiffAct model has to be adapted to binary classification, which includes adjustments to the loss functions and output layer.</p><p><bold>Loss Function Adjustments.</bold> Initially, two Binary Cross-Entropy (BCE) losses were considered:<list list-type="bullet"><list-item><p>Classification BCE Loss&#8212;applied to distinguish between the two classes.</p></list-item><list-item><p>Boundary Detection BCE Loss&#8212;intended to detect transition frames between classes.</p></list-item></list></p><p>However, since the task involves only two classes, boundary detection was found to be redundant and harmful to the performance, and this second BCE loss was removed. Additionally, the Mean Squared Error (MSE) loss calculation was improved. The MSE formula proposed by Liu et al. [<xref rid="B33-sensors-25-05217" ref-type="bibr">33</xref>] was adapted to better suit the binary nature of the problem. Specifically, a probability-based transformation using the cross-entropy formula (see Equation (<xref rid="FD2-sensors-25-05217" ref-type="disp-formula">2</xref>)), where <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the decoder&#8217;s output probability for the <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>th</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> video frame, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the corresponding output for the <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mi>th</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> frame. It was introduced to determine whether predictions belonged to the same class before computing MSE. This modification significantly enhanced the performance of the&#160;model.<disp-formula id="FD2-sensors-25-05217"><label>(2)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>cross</mml:mi><mml:mspace width="4.pt"/><mml:mi>entropy</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Furthermore, the masking techniques originally proposed by DiffAct were discarded, as they neither improved nor degraded performance. In the original setting, these techniques helped the model learn logical sequences of actions, but in the context of binary classification, where no sequential structure between signs is enforced, their contribution was insignificant.</p></sec><sec id="sec4dot2-sensors-25-05217"><title>4.2. Ablation Study</title><p>In this section, through a series of controlled experiments, we aim to evaluate the contribution of the employed training strategies&#8212;such as baseline, data augmentation, and multi dataset-training&#8212;on boundary and sign detection. By analyzing the results under different conditions, we gain insights into the strengths and limitations of each model and better understand which factors most significantly affect performance.</p><sec id="sec4dot2dot1-sensors-25-05217"><title>4.2.1. Models&#8217; Performance with Pretrained Weights</title><p>Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] provide pretrained weights for various model configurations trained on different Sign Language datasets. Given the similarity among Sign Languages, particularly in terms of motion patterns, this study initially aimed to leverage these pretrained weights to improve performance on the Peruvian Sign Language datasets. The first approach involved using the model with its default weights without any modifications. However, the results were significantly poor, reaching up to 36.76% for mF1B and 32.30% for mF1S.</p><p>A gradual fine-tuning technique was used to examine the effects of pretrained representations, as shown in <xref rid="sensors-25-05217-f008" ref-type="fig">Figure 8</xref>a. From the last stage to the first, the four-stage MS-TCN model was gradually unfrozen, with each unfreezing step representing a different experimental environment. When the complete model was unfrozen and retrained on the Peruvian Sign Language datasets, the model performed at its peak, as seen in <xref rid="sensors-25-05217-f008" ref-type="fig">Figure 8</xref>b, with mF1B and mF1S values of 51.45% and 57.21%, respectively. This implies that the higher-level temporal dependencies and language-specific structures contained in the later stages of the model are not directly reusable, even though certain low-level visual features&#8212;like hand forms and motion cues&#8212;are somewhat transferable between Sign Languages.</p><p>These findings highlight an important consideration in Sign Language processing: while Sign Languages share commonalities, their grammatical structures and execution vary significantly and are heavily influenced by cultural context. Moreover, factors such as the signer&#8217;s hand movement speed, articulation style, and experience influence model performance. Some signers perform signs clearly and concisely, whereas others introduce fillers or fail to complete signs properly. These variations suggest that direct transfer learning from one Sign Language dataset to another is not always effective and must be supplemented with domain-specific fine-tuning.</p><p>On the other hand, DiffAct, being a generative model, required full retraining from scratch. Since generative models rely on the statistical properties of the training domain, applying a pretrained DiffAct model to a new dataset with different visual and motion characteristics would result in poor generalization. However, to facilitate training and leverage available knowledge, layers that matched in size with the variations in the model&#8217;s architecture were initialized with the weights from the available pretrained files. This approach aimed to accelerate convergence while ensuring the model adapted properly to the new domain.</p></sec><sec id="sec4dot2dot2-sensors-25-05217"><title>4.2.2. Baseline Performance and Tuning</title><p>Based on the performance results presented in the previous subsection, both DiffAct and MS-TCN underwent systematic architectural tuning. Multiple configurations of architectural hyperparameters were tested, and each combination was evaluated twice to ensure reliable selection. The final architectures were chosen based on the mean performance across evaluations.</p><p>As a result of this process,</p><list list-type="bullet"><list-item><p>MS-TCN was set to 4 stages, with 8 blocks per stage and 128 feature maps.</p></list-item><list-item><p>DiffAct was configured with 14 blocks and 64 feature maps for the encoder, while the decoder used 10 blocks and 64 feature maps.</p></list-item></list><p>These architectures served as the baseline configuration for further experiments, where we introduced additional training strategies to evaluate which approach yields improved performance over the baseline.</p></sec><sec id="sec4dot2dot3-sensors-25-05217"><title>4.2.3. Data Augmentation for Performance Improvement</title><p>The effectiveness of each augmentation technique was evaluated by training models under different augmentation configurations and comparing their impact on performance, which are summarized in <xref rid="sensors-25-05217-t008" ref-type="table">Table 8</xref>.</p><p>Initially, applying all available augmentations&#8212;rotation, zoom, and translation&#8212;at the same time resulted in overfitting and no relevant improvement in the metrics. The mF1B and mF1S values were not the highest for either of the two models. The high redundancy in the transformed data likely caused the model to memorize variations instead of learning robust features, limiting its ability to generalize to unseen sequences.</p><p>For the MS-TCN model, the best metric values were achieved when applying zoom and translation transformations, resulting in an mF1B of 65.44% and an mF1S of 65.65%, along with the lowest validation loss value of 2.95.</p><p>In the case of DiffAct, the highest mF1B value of 62.68%&#8212;indicating the best performance for transition recognition&#8212;was obtained when using only zoom transformation. However, the corresponding value of mF1S, 58.59%, was significantly lower, resulting in a loss value of 1. In contrast, the combination of rotation and zoom achieved a balanced performance for both signs and transitions, with an mF1B of 60.62% and an mF1S of 60.10%, and a lower loss of 0.93.</p><p>Consequently, the combination of zoom and translation transformations will be applied to MS-TCN, while DiffAct will use the combination of rotation and zoom in subsequent experiments.</p></sec><sec id="sec4dot2dot4-sensors-25-05217"><title>4.2.4. Multi-Dataset Training</title><p>The previous experiments were conducted exclusively using the <monospace>manejar_conflictos</monospace> dataset. However, under this training strategy, additional videos from the <monospace>ira_alegria_RE</monospace> dataset were incorporated to enhance the generalization of the model. However, these samples were initially excluded due to a significant presence of the category <italic toggle="yes">filler</italic>, a phenomenon in Sign Language where movements between signs do not correspond to specific lexical signs. The excessive presence of fillers can disrupt the understanding of sign sequences, making these videos function more as noise samples rather than clear training&#160;data.</p><p>Thus, this experiment aimed not only to increase the diversity of samples but also to evaluate the effect of different interpretation styles on model performance. The <monospace>manejar_conflictos</monospace> dataset follows a concise signing approach, where the signer prioritizes the most relevant words to convey meaning efficiently. In contrast, the videos from <monospace>ira_alegria</monospace> reflect a word-by-word translation style, where the interpreter attempts to represent each spoken word with a sign. This approach sometimes results in incomplete or unnatural transitions, as certain words in Peruvian Sign Language do not have a direct equivalent or require additional context to be fully articulated.</p><p>By introducing both datasets, the experiment assessed whether the model could learn from both structured and less structured signing styles while maintaining accurate segmentation. The key challenge was to determine if the presence of filler-heavy samples negatively impacted boundary detection (mF1B) and sign recognition (mF1S) or if the increased variability in signing styles provided additional robustness to the model.</p></sec></sec><sec id="sec4dot3-sensors-25-05217"><title>4.3. Selecting Optimal Models via
Validation Performance of Training Strategies</title><p>The performance of both models under the three different training strategies&#8212;baseline, data augmentation, and multi-dataset&#8212;was evaluated using the Monte Carlo cross-validation technique with a fixed hold-out rate, a variation of the method originally proposed by Picard and Cook [<xref rid="B47-sensors-25-05217" ref-type="bibr">47</xref>]. In our implementation, we generated 10 random data splits. For each split, samples were randomly selected without replacement to ensure that no sample appeared in more than one set within the same split. However, across different splits, samples could assume different roles (training, validation, or test). This evaluation setup enabled a robust estimation of model performance and facilitated the identification of the most effective training strategy.</p><p>The evaluation using validation sets and testing sets is detailed as follows in the next&#160;subsection.</p><sec id="sec4dot3dot1-sensors-25-05217"><title>4.3.1. MS-TCN: Training Technique Evaluation</title><p>The boxplots in <xref rid="sensors-25-05217-f009" ref-type="fig">Figure 9</xref> reveal that across all settings, mF1S scores are consistently higher than mF1B, indicating that MS-TCN is more proficient at recognizing signs than detecting transitions. This suggests that the temporal structure of the model favors stable, repetitive patterns like signs over short, ambiguous ones like transitions.</p><p>Training on multiple datasets introduces substantial variability, as indicated by the highest standard deviations among all configurations: 11.18 for mF1B (%) and 12.44 for mF1S (%). Although this setup exhibits higher metric values, it also leads to instability across splits. This is visually confirmed by the wide performance ranges, from 59.76% to 87.50% for mF1B and 55.89% to 98.44% for mF1S. Additionally, the boxplots for this configuration are right-skewed, meaning that better-performing splits also exhibit greater dispersion. While some dataset combinations enhance the generalization capability of the MS-TCN model, others degrade its ability to reliably detect signs and transitions.</p><p>The configuration without data augmentation is the most stable, with the lowest standard deviations (1.20 for mF1B and 1.48 for mF1S).Yet, an outlier with an mF1S of 59.42% is located below the mean (62.61%). This could be due to an unbalanced or particularly challenging split that lacks sufficient variation in signs, emphasizing the risk of underrepresentation in small datasets, such as in this work, where the video sequences are never repeated.</p><p>Data augmentation leads to higher performance in terms of median (63.06% mF1S, 61.12% mF1B) compared to the baseline (62.87% mF1S, 60.70% mF1B), confirming its usefulness in enhancing model robustness. However, it also increases variability (standard deviation: 1.86 for mF1B and 1.67 for mF1S), suggesting that augmented data helps the model to achieve better results in some splits than in others.</p><p>Therefore, the most balanced and reliable training configuration is data augmentation, as it consistently enhances performance in both sign (mF1S) and transition detection (mF1B) compared to the baseline, while maintaining relatively low variance. This stability is crucial in practical applications, as it ensures the model performs reliably across different data splits and scenarios. In contrast, although multi-dataset training occasionally achieves high scores, the substantial variability it introduces&#8212;stemming from divergent data distributions and inconsistent generalization&#8212;renders it less dependable. Thus, data augmentation strikes an optimal balance between performance and robustness, making it the most suitable configuration for future experiments and deployments.</p></sec><sec id="sec4dot3dot2-sensors-25-05217"><title>4.3.2. DiffAct: Training Technique Evaluation</title><p><xref rid="sensors-25-05217-f010" ref-type="fig">Figure 10</xref> compares the performance of the Diffact model under the three training strategies: baseline, data augmentation, and multi-dataset training. In all cases, the model demonstrates better results for transition detection, as reflected in higher mF1B scores compared to mF1S, indicating the strength of the model in identifying boundaries rather than stable sign segments.</p><p>The baseline strategy exhibits the lowest variation, with standard deviations of 1.15 for mF1B and 1.21 for mF1S. However, this consistency comes at the cost of lower performance, with median scores of 60.75% for mF1B and 62.88% for mF1S. Moreover, the values are almost evenly distributed, with only one outlier in mF1S.</p><p>The use of data augmentation leads to an improvement in performance. The median for mF1B increases from 57.50% to 59.39%, and for mF1S from 56.67% to 58.58%, compared to the baseline. Standard deviations increase slightly to 1.31 for mF1B and 1.26 for mF1S.</p><p>The multi-dataset training strategy achieves the highest performance across both metrics. The median mF1B reaches 61.11%, accompanied by the highest standard deviation of 3.32. For mF1S, the median also surpasses the baseline, reaching 57.57% with a standard deviation of 2.58.</p><p>Outliers are present because a particular split comprises a sample distribution that is negatively impacted by the addition of noisy or heterogeneous data, especially when learning via data augmentation and multi-dataset training methodologies. This is probably because the distribution of the enhanced data and the original distribution of this particular split do not match. This effect is much more noticeable in the multi-dataset training situation, where the introduction of varied samples enhances the distributional shift and produces more extreme outlier values, even while data augmentation already introduces significant variability. These outliers show how poorly the model can generalize to some divides that might not be well-represented in the enhanced or merged data.</p><p>Despite its higher variability, the multi-dataset training strategy emerges as the most effective approach for training the DiffAct model. It achieves the highest median and maximum scores, particularly in transition detection (mF1B)&#8212;the most challenging class due to its brief duration and dependence on contextual cues. By exposing the model to a wider range of signing styles and scenarios, this strategy significantly enhances its generalization ability and improves its accuracy in identifying subtle transitions. While data augmentation offers a stable performance boost and the baseline ensures consistency, only multi-dataset training fully leverages the generative nature of DiffAct, enabling it to leverage noisy and diverse data for improved generalization. Therefore, despite its inherent variance, multi-dataset training is recommended as the most promising strategy for developing robust and generalizable segmentation models.</p></sec></sec><sec id="sec4dot4-sensors-25-05217"><title>4.4. Testing Evaluation</title><p>After selecting the best-performing configurations for MS-TCN and DiffAct based on training and validation results, we evaluated both models on the test set. As shown in <xref rid="sensors-25-05217-f011" ref-type="fig">Figure 11</xref>, DiffAct clearly outperforms MS-TCN in both mF1S and mF1B metrics, achieving higher medians (71.89% for mF1S and 72.68% for mF1B) and a broader upper range. In contrast, the performance of the MS-TCN model is clustered around lower medians (63.10% for mF1S and 61.17% for mF1B).</p><p>MS-TCN exhibits narrower interquartile ranges (1.94 for mF1S and 1.27 for mF1B), reflecting stable but limited performance. DiffAct, while more variable (IQRs of 5.92 for mF1S and 5.01 for mF1B), demonstrates greater potential to generalize, particularly in handling complex temporal dependencies. The presence of outliers in both models highlights occasional instability, yet the higher overall performance of DiffAct suggests that its variability is a worthwhile trade-off.</p><p>Interestingly, MS-TCN displays more outliers in mF1B, indicating inconsistent boundary detection, whereas its sign recognition (mF1S) remains stable. These findings differ from validation trends, where MS-TCN showed more favorable results, suggesting that DiffAct generalizes better to unseen data.</p><p>Despite improvements from data augmentation and multi-dataset strategies, performance variability persists, particularly in DiffAct. The small size of the dataset likely limits generalization in both models. MS-TCN appears more prone to overfitting, potentially hindering its ability to detect subtle transitions in unfamiliar sequences.</p></sec><sec id="sec4dot5-sensors-25-05217"><title>4.5. Qualitative Results</title><p><xref rid="sensors-25-05217-f012" ref-type="fig">Figure 12</xref> presents a general example of how the results obtained in this study translate into the practical recognition of the boundaries between signs and transitions, which represent the temporal segmentation points within each video. This is demonstrated through the sample frames shown in the figure.</p><p>Specifically, <xref rid="sensors-25-05217-f013" ref-type="fig">Figure 13</xref>, <xref rid="sensors-25-05217-f014" ref-type="fig">Figure 14</xref>, <xref rid="sensors-25-05217-f015" ref-type="fig">Figure 15</xref> and <xref rid="sensors-25-05217-f016" ref-type="fig">Figure 16</xref> present qualitative results for videos 26, 35, 78, and 100, from the <monospace>manejar_conflictos</monospace> dataset, respectively. These examples correspond to the split one, where DiffAct achieved some of its best and worst test set performance. Video 78 achieved 100% for both mF1B and mF1S, as illustrated in <xref rid="sensors-25-05217-f015" ref-type="fig">Figure 15</xref>, where the predicted boundaries between <italic toggle="yes">signs</italic> and <italic toggle="yes">transitions</italic>&#8212;visually distinguishable by their lighter shading&#8212;are almost perfectly aligned to the ground truth. Upon reviewing the corresponding annotations, we found that this video contains two words, out of eight, and a gestural sign that have never been encountered by the model during training.</p><p>Similarly, for video 26, we obtained 100% for mF1B and 92.71% for mF1S despite having a considerable number of signs and very brief transitions. The successful recognition of these tiny transitions may be attributed to the distinct hand configurations between consecutive signs, making the transitions more visually separable. Also, for video 35, we obtained 100% for mF1B and 92.19% for mF1S. As shown in <xref rid="sensors-25-05217-f014" ref-type="fig">Figure 14</xref>, the model tends to vary in a few frames when compared to the ground truth labels. However, this discrepancy is minor, as the precise start and end frames of a sign can vary slightly depending on the subjective interpretation of the annotator.</p><p>Moreover, video 100 is the sample with the lower scores for mF1B and mF1S, with 43.18% and 45.45%, accordingly. <xref rid="sensors-25-05217-f016" ref-type="fig">Figure 16</xref> shows evidence of oversegmentation, where the model incorrectly predicts transitions within segments that, according to the ground truth, correspond only to continuous signs. However, upon reviewing both the annotations and the video, we found that the model correctly identified a repeated sign as two separate instances, whereas the annotator had labeled it as a single sign. Additionally, this video presents fingerspelling, but due to the low resolution and the high speed required for real-time translation, it was challenging to accurately identify the boundaries between individual letters, so the annotator considered the segments as a large sign. Despite these challenges, the model was able to detect some distinct movements that are visually similar to isolated signs.</p><p>On the other hand, to provide a qualitative comparison between DiffAct and MS-TCN, we selected split 8&#8212;the split in which MS-TCN achieved its highest overall performance&#8212;and analyzed both its best and worst cases. As shown in <xref rid="sensors-25-05217-f017" ref-type="fig">Figure 17</xref>, MS-TCN performs well in sequences with fewer class transitions and longer, more stable segments. In contrast, <xref rid="sensors-25-05217-f018" ref-type="fig">Figure 18</xref> presents a challenging case involving short transitions and frequent class changes, where MS-TCN struggles, achieving 53.12% for mF1B and 35.94% for mF1S, it fails to detect several transitions, highlighted by red rectangles, where hand configurations between consecutive signs change only slightly, making boundaries difficult to identify. In the same scenario, DiffAct demonstrates superior robustness to rapid temporal variations, with significantly higher scores of 78.57% for mF1B and 53.57% for mF1S.</p><p>This improvement can be attributed to DiffAct&#8217;s diffusion-based architecture, which models temporal dynamics as a denoising process across the entire sequence. This global modeling allows it to better capture subtle transitions and adapt to rapid changes in class boundaries. Unlike MS-TCN, which relies on stage-wise refinement that may overly smooth frequent transitions, DiffAct&#8217;s iterative refinement of latent representations enables more precise boundary localization in complex temporal patterns.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05217"><title>5. Conclusions and Future Work</title><p>This study evaluated the effectiveness of two temporal segmentation models&#8212;MS-TCN [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>] and DiffAct [<xref rid="B33-sensors-25-05217" ref-type="bibr">33</xref>]&#8212;in the context of Peruvian Sign Language video segmentation. Through experiments involving fine-tuning, data augmentation, and multi-dataset training, we found that diffusion-based models like DiffAct, despite their higher variability, offer strong potential for handling complex temporal patterns in sign segmentation tasks.</p><p>As part of this work, we introduced three novel Peruvian Sign Language datasets: <monospace>manejar_conflictos</monospace>, <monospace>ira_alegria_RE</monospace>, and <monospace>PUCP-305_RE</monospace>, based on publicly available videos and annotated in collaboration with a certified Peruvian Sign Language interpreter. The annotation process was resource-intensive&#8212;requiring roughly one hour per minute of video&#8212;due to the need for linguistic and cultural precision. In addition to the core classes of <italic toggle="yes">sign</italic> and <italic toggle="yes">transition</italic>, the data revealed other important categories, including <italic toggle="yes">fillers</italic>, <italic toggle="yes">NN</italic> (incomplete signs), and <italic toggle="yes">gestural signs</italic>.</p><p>We publicly share our datasets and code to support reproducibility and encourage integration into active learning pipelines. Our models can serve as pre-annotation tools, potentially assisting human annotators and advancing sentence-level translation in Continuous Sign Language Recognition tasks.</p><p>For future work, we propose exploring hybrid architectures that combine the temporal stability of MS-TCN with the generative strengths of DiffAct. Transformer-based models also present a promising direction due to their capacity to model long-range dependencies. Further improvements could include designing data augmentation strategies that maintain temporal coherence, incorporating linguistic features such as facial expressions and handshape transitions and refining loss functions for better boundary detection.</p><p>Domain adaptation techniques&#8212;such as adversarial learning and distribution alignment&#8212;could help adapt pretrained models to new datasets and improve generalization across different Sign Languages. Moreover, evaluating the practical impact of these models on annotation efficiency by involving interpreters in user studies will be essential for assessing real-world benefits.</p><p>Finally, expanding national datasets is crucial, as Sign Languages vary widely across regions. We plan to extend our datasets with a greater number of signers, more diverse sentence structures, and better representation of minority classes. This will support signer-independent evaluation and open avenues for multiclass classification, ultimately enhancing the robustness and applicability of segmentation models in real-world scenarios.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>S.F.: Conceptualization, Methodology, Investigation, Funding acquisition, Project administration, Software, Data curation, Formal analysis, Visualization, Writing&#8212;original draft. J.J.C.-Z.: Conceptualization, Methodology, Validation, Visualization, Supervision, Formal analysis. A.A.: Validation, Formal analysis, Visualization, Writing&#8212;review and editing. I.D.: Funding acquisition, Supervision, Writing&#8212;review &amp; editing. Y.C.: Validation, Formal analysis, Visualization, Supervision, Writing&#8212;review and editing. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data supporting the findings of this study will be made available upon reasonable request to the corresponding author. <bold>Code Availability</bold>: The code used for this study can be accessed at the following link: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/Roml68/Temporal-segmentation-for-Peruvian-Sign-language.git">https://github.com/Roml68/Temporal-segmentation-for-Peruvian-Sign-language.git</uri> (accessed on 6 February 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05217"><label>1.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Banco Mundial</collab></person-group><article-title>Disability Inclusion Overview. Banco Mundial</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.worldbank.org/en/topic/disability" ext-link-type="uri">https://www.worldbank.org/en/topic/disability</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-06-15">(accessed on 15 June 2024.)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-05217"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>INEI</collab></person-group><source>Perfil Sociodemogr&#225;fico de la Poblaci&#243;n con Discapacidad [Sociodemographic Profile of the Disabled Population]</source><comment>Technical Report</comment><publisher-name>National Institute of Statistics and Informatics</publisher-name><publisher-loc>Lima, Peru</publisher-loc><year>2017</year></element-citation></ref><ref id="B3-sensors-25-05217"><label>3.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sansoni Toso</surname><given-names>C.A.</given-names></name></person-group><article-title>Las Personas Sordas en el Per&#250;: En B&#250;squeda de una Educaci&#243;n Biling&#252;e Bicultural [Deaf People in Peru: In Search of a Bilingual Bicultural Education]</article-title><source>Bachelor&#8217;s Thesis</source><publisher-name>Pontifica Universidad Cat&#243;lica del Per&#250;</publisher-name><publisher-loc>Lima, Peru</publisher-loc><year>2019</year></element-citation></ref><ref id="B4-sensors-25-05217"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kushalnagar</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ryan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Paludneviciene</surname><given-names>R.</given-names></name><name name-style="western"><surname>Spellun</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gulati</surname><given-names>S.</given-names></name></person-group><article-title>Adverse Childhood Communication Experiences Associated With an Increased Risk of Chronic Diseases in Adults Who Are Deaf</article-title><source>Am. J. Prev. Med.</source><year>2020</year><volume>59</volume><fpage>548</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1016/j.amepre.2020.04.016</pub-id><pub-id pub-id-type="pmid">32636047</pub-id><pub-id pub-id-type="pmcid">PMC7508773</pub-id></element-citation></ref><ref id="B5-sensors-25-05217"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leki&#263;</surname><given-names>S.</given-names></name></person-group><article-title>The Impact of Digital Technologies on the Cultural Rights of D/Deaf and Hard-of-Hearing People</article-title><source>Eudaimonia-J. Leg. Political Soc. Theory Philos.</source><year>2024</year><volume>8</volume><fpage>55</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.51204/IVRS_24103A</pub-id></element-citation></ref><ref id="B6-sensors-25-05217"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alyami</surname><given-names>S.</given-names></name><name name-style="western"><surname>Luqman</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hammoudeh</surname><given-names>M.</given-names></name></person-group><article-title>Reviewing 25 Years of Continuous Sign Language Recognition Research: Advances, Challenges, and Prospects</article-title><source>Inf. Process. Manag.</source><year>2024</year><volume>61</volume><fpage>103774</fpage><pub-id pub-id-type="doi">10.1016/j.ipm.2024.103774</pub-id></element-citation></ref><ref id="B7-sensors-25-05217"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Min</surname><given-names>W.</given-names></name><name name-style="western"><surname>Han</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name></person-group><article-title>VTAN: A Novel Video Transformer Attention-Based Network for Dynamic Sign Language Recognition</article-title><source>Comput. Mater. Contin.</source><year>2025</year><volume>82</volume><fpage>2793</fpage><lpage>2812</lpage><pub-id pub-id-type="doi">10.32604/cmc.2024.057456</pub-id></element-citation></ref><ref id="B8-sensors-25-05217"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>N.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>DFCNet+: Cross-Modal Dynamic Feature Contrast Net for Continuous Sign Language Recognition</article-title><source>Image Vis. Comput.</source><year>2024</year><volume>151</volume><fpage>105260</fpage><pub-id pub-id-type="doi">10.1016/j.imavis.2024.105260</pub-id></element-citation></ref><ref id="B9-sensors-25-05217"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Han</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>W.</given-names></name></person-group><article-title>A large-scale Combinatorial Benchmark for Sign Language recognition</article-title><source>Pattern Recognit.</source><year>2025</year><volume>161</volume><fpage>111246</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2024.111246</pub-id></element-citation></ref><ref id="B10-sensors-25-05217"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rastogi</surname><given-names>U.</given-names></name><name name-style="western"><surname>Mahapatra</surname><given-names>R.P.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>S.</given-names></name></person-group><article-title>Advancements in Machine Learning Techniques for Hand Gesture-Based Sign Language Recognition: A Comprehensive Review</article-title><source>Arch. Comput. Methods Eng.</source><year>2025</year><fpage>1</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1007/s11831-025-10258-z</pub-id></element-citation></ref><ref id="B11-sensors-25-05217"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Renjith</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sumi Suresh</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rashmi</surname><given-names>M.</given-names></name></person-group><article-title>An Effective Skeleton-Based Approach for Multilingual Sign Language Recognition</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>143</volume><fpage>109995</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.109995</pub-id></element-citation></ref><ref id="B12-sensors-25-05217"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zuo</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>F.</given-names></name><name name-style="western"><surname>Mak</surname><given-names>B.</given-names></name></person-group><article-title>Natural Language-Assisted Sign Language Recognition</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><publisher-name>IEEE Computer Society</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2023</year><fpage>14890</fpage><lpage>14900</lpage><pub-id pub-id-type="doi">10.1109/CVPR52729.2023.01430</pub-id></element-citation></ref><ref id="B13-sensors-25-05217"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Laines</surname><given-names>D.</given-names></name><name name-style="western"><surname>Gonzalez-Mendoza</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ochoa-Ruiz</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bejarano</surname><given-names>G.</given-names></name></person-group><article-title>Isolated Sign Language Recognition Based on Tree Structure Skeleton Images</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>276</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1109/CVPRW59228.2023.00033</pub-id></element-citation></ref><ref id="B14-sensors-25-05217"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lim</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>A.W.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>S.C.</given-names></name></person-group><article-title>A Feature Covariance Matrix with Serial Particle Filter for Isolated Sign Language Recognition</article-title><source>Expert Syst. Appl.</source><year>2016</year><volume>54</volume><fpage>208</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2016.01.047</pub-id></element-citation></ref><ref id="B15-sensors-25-05217"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mercanoglu Sincan</surname><given-names>O.</given-names></name><name name-style="western"><surname>Tur</surname><given-names>A.O.</given-names></name><name name-style="western"><surname>Yalim Keles</surname><given-names>H.</given-names></name></person-group><article-title>Isolated Sign Language Recognition with Multi-Scale Features Using LSTM</article-title><source>Proceedings of the 27th Signal Processing and Communications Applications Conference (SIU)</source><conf-loc>Sivas, Turkey</conf-loc><conf-date>24&#8211;26 April 2019</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/SIU.2019.8806467</pub-id></element-citation></ref><ref id="B16-sensors-25-05217"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cihan Camgoz</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hadfield</surname><given-names>S.</given-names></name><name name-style="western"><surname>Koller</surname><given-names>O.</given-names></name><name name-style="western"><surname>Bowden</surname><given-names>R.</given-names></name></person-group><article-title>SubUNets: End-To-End Hand Shape and Continuous Sign Language Recognition</article-title><source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><pub-id pub-id-type="doi">10.1109/ICCV.2017.332</pub-id></element-citation></ref><ref id="B17-sensors-25-05217"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ke</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name></person-group><article-title>Fine-Grained Cross-Modality Consistency Mining for Continuous Sign Language Recognition</article-title><source>Pattern Recognit. Lett.</source><year>2025</year><volume>191</volume><fpage>23</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2025.02.017</pub-id></element-citation></ref><ref id="B18-sensors-25-05217"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koprinska</surname><given-names>I.</given-names></name><name name-style="western"><surname>Carrato</surname><given-names>S.</given-names></name></person-group><article-title>Temporal video segmentation: A survey</article-title><source>Signal Process. Image Commun.</source><year>2001</year><volume>16</volume><fpage>477</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1016/S0923-5965(00)00011-4</pub-id></element-citation></ref><ref id="B19-sensors-25-05217"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>Dual-Stage Temporal Perception Network for Continuous Sign Language Recognition</article-title><source>Vis. Comput.</source><year>2025</year><volume>41</volume><fpage>1971</fpage><lpage>1986</lpage><pub-id pub-id-type="doi">10.1007/s00371-024-03516-x</pub-id></element-citation></ref><ref id="B20-sensors-25-05217"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name></person-group><article-title>StepNet: Spatial-Temporal Part-Aware Network for Isolated Sign Language Recognition</article-title><source>ACM Trans. Multimed. Comput. Commun. Appl.</source><year>2024</year><volume>20</volume><fpage>226</fpage><pub-id pub-id-type="doi">10.1145/3656046</pub-id></element-citation></ref><ref id="B21-sensors-25-05217"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Renz</surname><given-names>K.</given-names></name><name name-style="western"><surname>Stache</surname><given-names>N.C.</given-names></name><name name-style="western"><surname>Albanie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Varol</surname><given-names>G.</given-names></name></person-group><article-title>Sign Language Segmentation with Temporal Convolutional Networks</article-title><source>Proceedings of the ICASSP 2021&#8212;2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>6&#8211;11 June 2021</conf-date><fpage>2135</fpage><lpage>2139</lpage><pub-id pub-id-type="doi">10.1109/ICASSP39728.2021.9413817</pub-id></element-citation></ref><ref id="B22-sensors-25-05217"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>P&#233;rez</surname><given-names>L.F.</given-names></name><name name-style="western"><surname>Valladares</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Santos</surname><given-names>O.C.</given-names></name></person-group><article-title>Sign Language Segmentation Using a Transformer-Based Approach</article-title><source>Master&#8217;s Thesis</source><publisher-name>Universidad Nacional de Educaci&#243;n a Distancia (Espa&#241;a), Escuela T&#233;cnica Superior de Ingenier&#237;a Inform&#225;tica, Departamento de Inteligencia Artificial</publisher-name><publisher-loc>Madrid, Spain</publisher-loc><year>2022</year></element-citation></ref><ref id="B23-sensors-25-05217"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mocialov</surname><given-names>B.</given-names></name><name name-style="western"><surname>Turner</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lohan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hastie</surname><given-names>H.</given-names></name></person-group><article-title>Towards Continuous Sign Language Recognition With Deep Learning</article-title><source>Proceedings of the Workshop on the Creating Meaning with Robot Assistants: The Gap Left by Smart Devices</source><conf-loc>Birmingham, UK</conf-loc><conf-date>15&#8211;17 November 2017</conf-date><volume>Volume 5525834</volume></element-citation></ref><ref id="B24-sensors-25-05217"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Choudhury</surname><given-names>A.</given-names></name><name name-style="western"><surname>Talukdar</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Bhuyan</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Sarma</surname><given-names>K.K.</given-names></name></person-group><article-title>Movement Epenthesis Detection for Continuous Sign Language Recognition</article-title><source>J. Intell. Syst.</source><year>2017</year><volume>26</volume><fpage>471</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1515/jisys-2016-0009</pub-id></element-citation></ref><ref id="B25-sensors-25-05217"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nayan</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ghosh</surname><given-names>D.</given-names></name><name name-style="western"><surname>Pradhan</surname><given-names>P.M.</given-names></name></person-group><article-title>An Optical Flow Based Approach to Detect Movement Epenthesis in Continuous Fingerspelling of Sign Language</article-title><source>Proceedings of the National Conference on Communications (NCC)</source><conf-loc>Kanpur, India</conf-loc><conf-date>27&#8211;30 July 2021</conf-date><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/NCC52529.2021.9530076</pub-id></element-citation></ref><ref id="B26-sensors-25-05217"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Farag</surname><given-names>I.</given-names></name><name name-style="western"><surname>Brock</surname><given-names>H.</given-names></name></person-group><article-title>Learning Motion Disfluencies for Automatic Sign Language Segmentation</article-title><source>Proceedings of the ICASSP&#8212; IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Brighton, UK</conf-loc><conf-date>12&#8211;17 May 2019</conf-date><pub-id pub-id-type="doi">10.1109/icassp.2019.8683523</pub-id></element-citation></ref><ref id="B27-sensors-25-05217"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dou</surname><given-names>Y.</given-names></name></person-group><article-title>TPC: Temporal Preservation Convolutional Networks for Precise Temporal Action Localization</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1708.03280</pub-id></element-citation></ref><ref id="B28-sensors-25-05217"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zareian</surname><given-names>A.</given-names></name><name name-style="western"><surname>Miyazawa</surname><given-names>K.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>S.F.</given-names></name></person-group><article-title>CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>5734</fpage><lpage>5743</lpage></element-citation></ref><ref id="B29-sensors-25-05217"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>T.</given-names></name></person-group><article-title>Asformer: Transformer for Action Segmentation</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2110.08568</pub-id><pub-id pub-id-type="arxiv">2110.08568</pub-id></element-citation></ref><ref id="B30-sensors-25-05217"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Farha</surname><given-names>Y.A.</given-names></name><name name-style="western"><surname>Gall</surname><given-names>J.</given-names></name></person-group><article-title>MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1903.01945</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TPAMI.2020.3021756</pub-id><pub-id pub-id-type="pmid">32886607</pub-id></element-citation></ref><ref id="B31-sensors-25-05217"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sener</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>A.</given-names></name></person-group><article-title>Temporal Action Segmentation: An Analysis of Modern Techniques</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>1011</fpage><lpage>1030</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3327284</pub-id><pub-id pub-id-type="pmid">37874699</pub-id></element-citation></ref><ref id="B32-sensors-25-05217"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bahrami</surname><given-names>E.</given-names></name><name name-style="western"><surname>Francesca</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gall</surname><given-names>J.</given-names></name></person-group><article-title>How Much Temporal Long-Term Context is Needed for Action Segmentation?</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Paris, France</conf-loc><conf-date>1&#8211;6 October 2023</conf-date><fpage>10317</fpage><lpage>10327</lpage><pub-id pub-id-type="doi">10.1109/ICCV51070.2023.00950</pub-id></element-citation></ref><ref id="B33-sensors-25-05217"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Dinh</surname><given-names>A.D.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Shah</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name></person-group><article-title>Diffusion Action Segmentation</article-title><source>Proceedings of the International Conference on Computer Vision (ICCV)</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date></element-citation></ref><ref id="B34-sensors-25-05217"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Streaming Video Temporal Action Segmentation in Real Time</article-title><source>Proceedings of the 18th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)</source><conf-loc>Fuzhou, China</conf-loc><conf-date>17&#8211;19 November 2023</conf-date><fpage>316</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1109/ISKE60036.2023.10481438</pub-id></element-citation></ref><ref id="B35-sensors-25-05217"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name></person-group><article-title>Video-based Sign Language Recognition without Temporal Segmentation</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1801.10111</pub-id><pub-id pub-id-type="doi">10.1609/aaai.v32i1.11903</pub-id></element-citation></ref><ref id="B36-sensors-25-05217"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Varol</surname><given-names>G.</given-names></name><name name-style="western"><surname>Momeni</surname><given-names>L.</given-names></name><name name-style="western"><surname>Albanie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Afouras</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Read and Attend: Temporal Localisation in Sign Language Videos</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2103.16481</pub-id><pub-id pub-id-type="arxiv">2103.16481</pub-id></element-citation></ref><ref id="B37-sensors-25-05217"><label>37.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Iashin</surname><given-names>V.</given-names></name><name name-style="western"><surname>Murai</surname><given-names>R.</given-names></name><name name-style="western"><surname>Korbar</surname><given-names>B.</given-names></name><name name-style="western"><surname>Georgievski</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ho</surname><given-names>J.</given-names></name></person-group><article-title>Video Features</article-title><year>2020</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/v-iashin/video_features" ext-link-type="uri">https://github.com/v-iashin/video_features</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-02-06">(accessed on 6 February 2025)</date-in-citation></element-citation></ref><ref id="B38-sensors-25-05217"><label>38.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Bejarano</surname><given-names>G.</given-names></name><name name-style="western"><surname>Huamani-Malca</surname></name><name name-style="western"><surname>Cerna-Herrera</surname></name><name name-style="western"><surname>Alva-Manchego</surname></name><name name-style="western"><surname>Rivas</surname><given-names>P.</given-names></name></person-group><article-title>Lengua de Se&#241;as Peruana (LSP)&#8212;Aprendo En Casa 2020. Portal de Datos Abiertos de la Pontificia Universidad Cat&#243;lica del Per&#250;</article-title><year>2022</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://hdl.handle.net/20.500.12534/HDOAGH" ext-link-type="uri">https://hdl.handle.net/20.500.12534/HDOAGH</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-01-15">(accessed on 15 January 2024)</date-in-citation></element-citation></ref><ref id="B39-sensors-25-05217"><label>39.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Rodr&#237;guez-Mondo&#241;edo</surname><given-names>M.</given-names></name><name name-style="western"><surname>P&#233;rez-Silva</surname><given-names>J.</given-names></name><name name-style="western"><surname>Velasquez</surname><given-names>H.</given-names></name><name name-style="western"><surname>Oporto</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cerna</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vasquez</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bejarano</surname><given-names>G.</given-names></name></person-group><article-title>Lengua de Se&#241;as Peruana (LSP)&#8212;PUCP 305 (Glosas). Portal de Datos Abiertos de la Pontificia Universidad Cat&#243;lica del Per&#250;</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://hdl.handle.net/20.500.12534/JU4OLG" ext-link-type="uri">https://hdl.handle.net/20.500.12534/JU4OLG</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-01-15">(accessed on 15 January 2024)</date-in-citation></element-citation></ref><ref id="B40-sensors-25-05217"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vargas</surname><given-names>Y.V.H.</given-names></name><name name-style="western"><surname>Ccasa</surname><given-names>N.N.D.</given-names></name><name name-style="western"><surname>Rodas</surname><given-names>L.E.</given-names></name></person-group><article-title>Peruvian Sign Language Recognition Using a Hybrid Deep Neural Network</article-title><source>Proceedings of the Information Management and Big Data</source><conf-loc>Lima, Peru</conf-loc><conf-date>21&#8211;23 August 2020</conf-date><fpage>165</fpage><lpage>172</lpage></element-citation></ref><ref id="B41-sensors-25-05217"><label>41.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cormier</surname><given-names>K.</given-names></name><name name-style="western"><surname>Fenlon</surname><given-names>J.</given-names></name></person-group><source>BSL Corpus Annotation Guidelines</source><publisher-name>Deafness Cognition and Language (DCAL) Research Centre, University College London</publisher-name><publisher-loc>London, UK</publisher-loc><year>2014</year></element-citation></ref><ref id="B42-sensors-25-05217"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bejarano</surname><given-names>G.</given-names></name><name name-style="western"><surname>Huamani-Malca</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cerna-Herrera</surname><given-names>F.</given-names></name><name name-style="western"><surname>Alva-Manchego</surname><given-names>F.</given-names></name><name name-style="western"><surname>Rivas</surname><given-names>P.</given-names></name></person-group><article-title>PeruSIL: A Framework to Build a Continuous Peruvian Sign Language Interpretation Dataset</article-title><source>Proceedings of the LREC 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources</source><conf-loc>Marseille, France</conf-loc><conf-date>25 June 2022</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B43-sensors-25-05217"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mumuni</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mumuni</surname><given-names>F.</given-names></name></person-group><article-title>Data augmentation: A Comprehensive Survey of Modern Approaches</article-title><source>Array</source><year>2022</year><volume>16</volume><fpage>100258</fpage><pub-id pub-id-type="doi">10.1016/j.array.2022.100258</pub-id></element-citation></ref><ref id="B44-sensors-25-05217"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Forster</surname><given-names>J.</given-names></name><name name-style="western"><surname>Schmidt</surname><given-names>C.</given-names></name><name name-style="western"><surname>Koller</surname><given-names>O.</given-names></name><name name-style="western"><surname>Bellgardt</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ney</surname><given-names>H.</given-names></name></person-group><article-title>Extensions of the Sign Language Recognition and Translation Corpus RWTH-PHOENIX-Weather</article-title><source>Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&#8217;14)</source><conf-loc>Reykjavik, Iceland</conf-loc><conf-date>26&#8211;31 May 2014</conf-date></element-citation></ref><ref id="B45-sensors-25-05217"><label>45.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Schembri</surname><given-names>A.</given-names></name><name name-style="western"><surname>Fenlon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Rentelis</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cormier</surname><given-names>K.</given-names></name></person-group><source>British Sign Language Corpus Project: A Corpus of Digital Video Data and Annotations of British Sign Language 2008</source><edition>2nd ed.</edition><publisher-name>BSL Corpus Project</publisher-name><publisher-loc>London, UK</publisher-loc><year>2014</year></element-citation></ref><ref id="B46-sensors-25-05217"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Carreira</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</article-title><source>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2017</year><fpage>4724</fpage><lpage>4733</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.502</pub-id></element-citation></ref><ref id="B47-sensors-25-05217"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Picard</surname><given-names>R.R.</given-names></name><name name-style="western"><surname>Cook</surname><given-names>R.D.</given-names></name></person-group><article-title>Cross-Validation of Regression Models</article-title><source>J. Am. Stat. Assoc.</source><year>1984</year><volume>79</volume><fpage>575</fpage><lpage>583</lpage><pub-id pub-id-type="doi">10.1080/01621459.1984.10478083</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05217-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall pipeline of the proposed framework. Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>], Iashin et al. [<xref rid="B37-sensors-25-05217" ref-type="bibr">37</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g001.jpg"/></fig><fig position="float" id="sensors-25-05217-f002" orientation="portrait"><label>Figure 2</label><caption><p>Example frame from AEC PUCP dataset [<xref rid="B38-sensors-25-05217" ref-type="bibr">38</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g002.jpg"/></fig><fig position="float" id="sensors-25-05217-f003" orientation="portrait"><label>Figure 3</label><caption><p>Example frame from PUCP-305 dataset [<xref rid="B39-sensors-25-05217" ref-type="bibr">39</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g003.jpg"/></fig><fig position="float" id="sensors-25-05217-f004" orientation="portrait"><label>Figure 4</label><caption><p>Comparison between the original annotation of the video <monospace>ira_alegria</monospace> in the dataset <monospace>AEC PUCP</monospace> (upper image) and the refined annotation for the new dataset <monospace>ira_alegria_RE</monospace> (lower image).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g004.jpg"/></fig><fig position="float" id="sensors-25-05217-f005" orientation="portrait"><label>Figure 5</label><caption><p>Illustration of the sliding window mechanism used for feature extraction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g005.jpg"/></fig><fig position="float" id="sensors-25-05217-f006" orientation="portrait"><label>Figure 6</label><caption><p>Architecture of MS-TCN.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g006.jpg"/></fig><fig position="float" id="sensors-25-05217-f007" orientation="portrait"><label>Figure 7</label><caption><p>Overview of the DiffAct model presented in Liu et al. [<xref rid="B33-sensors-25-05217" ref-type="bibr">33</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g007.jpg"/></fig><fig position="float" id="sensors-25-05217-f008" orientation="portrait"><label>Figure 8</label><caption><p>Visual representation of the progressive unfreezing of MS-TCN stages. (<bold>a</bold>) Illustration of the cumulative unfreezing process across the model&#8217;s four stages, where U indicates an unfrozen stage and F a frozen one. (<bold>b</bold>) Comparison of validation metrics (mF1B and mF1S) obtained in each experiment corresponding to different unfreezing configurations.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g008.jpg"/></fig><fig position="float" id="sensors-25-05217-f009" orientation="portrait"><label>Figure 9</label><caption><p>Boxplot visualization of training strategy effects (baseline, augmented, multi-Dataset) on the mF1S and mF1B metrics for the MS-TCN model, evaluated on the validation sets across ten splits.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g009.jpg"/></fig><fig position="float" id="sensors-25-05217-f010" orientation="portrait"><label>Figure 10</label><caption><p>Boxplot visualization of training strategy effects (baseline, augmented, multi-dataset) on mF1S and mF1B metrics for the DiffAct model, evaluated on the validation sets across ten splits.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g010.jpg"/></fig><fig position="float" id="sensors-25-05217-f011" orientation="portrait"><label>Figure 11</label><caption><p>Performance of MS-TCN and the DiffAct model on the testing set across ten splits.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g011.jpg"/></fig><fig position="float" id="sensors-25-05217-f012" orientation="portrait"><label>Figure 12</label><caption><p>Example of model output and its correspondence to video frames.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g012.jpg"/></fig><fig position="float" id="sensors-25-05217-f013" orientation="portrait"><label>Figure 13</label><caption><p>Comparison between ground truth labels (upper bar) and predicted labels (lower bar) for Video 26 from the <monospace>manejar_conflictos</monospace> dataset. Transitions are shown in light blue, and signs in dark blue.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g013.jpg"/></fig><fig position="float" id="sensors-25-05217-f014" orientation="portrait"><label>Figure 14</label><caption><p>Comparison between ground truth labels (upper bar) and predicted labels (lower bar) for Video 35 from the <monospace>manejar_conflictos</monospace> dataset. Transitions are shown in light blue, and signs in dark blue.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g014.jpg"/></fig><fig position="float" id="sensors-25-05217-f015" orientation="portrait"><label>Figure 15</label><caption><p>Comparison between ground truth labels (upper bar) and predicted labels (lower bar) for Video 78 from the <monospace>manejar_conflictos</monospace> dataset. Transitions are shown in light blue, and signs in dark blue.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g015.jpg"/></fig><fig position="float" id="sensors-25-05217-f016" orientation="portrait"><label>Figure 16</label><caption><p>Comparison between ground truth labels (upper bar) and predicted labels (lower bar) for Video 100 from the <monospace>manejar_conflictos</monospace> dataset. Transitions are shown in light blue, and signs in dark blue.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g016.jpg"/></fig><fig position="float" id="sensors-25-05217-f017" orientation="portrait"><label>Figure 17</label><caption><p>Comparison between ground truth labels (upper bar) and predicted labels (lower bar) for Video 120 from the <monospace>manejar_conflictos</monospace> dataset. Transitions are shown in light blue, and signs in dark blue.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g017.jpg"/></fig><fig position="float" id="sensors-25-05217-f018" orientation="portrait"><label>Figure 18</label><caption><p>Comparison between ground truth labels (upper bar) and predicted labels for DiffAct (middle bar) and MS-TCN (lower bar) for Video 3 from the <monospace>manejar_conflictos</monospace> dataset. Transitions are shown in light blue, and signs in dark blue.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05217-g018.jpg"/></fig><table-wrap position="float" id="sensors-25-05217-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05217-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of related work on temporal Sign Language segmentation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Approach</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Work</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Main Method</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th></tr></thead><tbody><tr><td rowspan="6" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Sign Language temporal segmentation</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Mocialov et al. [<xref rid="B23-sensors-25-05217" ref-type="bibr">23</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Calculation of hand centroids acceleration and motion classification</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">NGT corpus (Sign Language of the Netherlands)</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Choudhury et al. [<xref rid="B24-sensors-25-05217" ref-type="bibr">24</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Calculation of hand centroids acceleration and motion classification</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Homemade videos</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Nayan et al. [<xref rid="B25-sensors-25-05217" ref-type="bibr">25</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Optical flow for velocity estimation</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Indian Sign Language (fingerspelling)</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Farag and Brock [<xref rid="B26-sensors-25-05217" ref-type="bibr">26</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">3D joint modeling + balanced Random Forest</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">DJLSC, CMU Mocap</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Renz et al. [<xref rid="B21-sensors-25-05217" ref-type="bibr">21</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Hierarchical model</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">BSL-Corpus, PHOENIX14</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">P&#233;rez et al. [<xref rid="B22-sensors-25-05217" ref-type="bibr">22</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Retrained transformer with attention mechanism</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">PHOENIX14</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Action temporal segmentation</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Bahrami et al. [<xref rid="B32-sensors-25-05217" ref-type="bibr">32</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Combines local and long-term attention mechanisms</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Assembly101, 50Salads and Breakfast</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Liu et al. [<xref rid="B33-sensors-25-05217" ref-type="bibr">33</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Diffusion model + dilated convolutions</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">GTEA, 50Salads and Breakfast</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Yang et al. [<xref rid="B27-sensors-25-05217" ref-type="bibr">27</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">3D Convolutional Network</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">THUMOS14 and UCF101</td></tr><tr><td rowspan="2" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Continuous Sign Language Recognition</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Feng et al. [<xref rid="B8-sensors-25-05217" ref-type="bibr">8</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Dynamic module + Bi-LSTM + contrastive loss</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">PHOENIX14-T, PHOENIX14 and CSL-Daily</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Huang et al. [<xref rid="B35-sensors-25-05217" ref-type="bibr">35</xref>]</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Hierarchical Attention Network with Latent Space</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">CSL, PHOENIX14</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05217-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05217-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison between Peruvian datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># Videos</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># Unique Sentences</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Annotated Time (minutes)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># Signers</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Level of Annotation</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AEC PUCP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">270</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Word and sentence</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PUCP-305</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">272</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">272</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8776;18.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Word and sentence</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSP-10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05217-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05217-t003_Table 3</object-id><label>Table 3</label><caption><p>Summary of datasets used in this study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Datasets</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># Sentences</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Annotated Time (Minutes)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Mean Sentence <break/>Duration (Seconds)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># Signers</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">manejar_conflictos</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">171</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ira_alegria_RE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">201</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PUCP-305_RE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">174</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05217-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05217-t004_Table 4</object-id><label>Table 4</label><caption><p>Summary of the notation used during the manual annotation process.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Categories</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Sign</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">A meaningful gesture formed by a combination of manual and non-manual features</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Transition or movement epenthesis (ME)</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">A transitional movement that occurs between two signs</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Rest</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Resting position</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Fillers</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Involuntary and/or repetitive hand movements that lack meaning</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">NN</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Signs that are performed incorrectly or incompletely</td></tr><tr><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Gestural signs</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">Movements that accompany a sentence to express non-linguistic aspects</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05217-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05217-t005_Table 5</object-id><label>Table 5</label><caption><p>Number of video frames per category for every dataset considered in this study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Rest</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fillers</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">NN</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Gestural <break/>Signs</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Signs</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Transitions</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">manejar_conflictos</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">160</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32,261</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14,978</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ira_alegria_RE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">366</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35,740</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13,687</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PUCP-305_RE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">366</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11,296</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18,553</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05217-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05217-t006_Table 6</object-id><label>Table 6</label><caption><p>Data augmentation methods and the range of variation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Augmentation Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Range</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rotation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;10&#176; to 10&#176;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zoom</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8% to 1.2%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Translation in X and Y axis</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8722;15 to 15</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05217-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05217-t007_Table 7</object-id><label>Table 7</label><caption><p>Implementation details of the retrained models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DiffAct</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MS-TCN</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feature Extractor</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I3D (2048D)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I3D (1024D)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning Rate</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0005</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0005</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Batch Size</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimizer</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adam</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adam</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Early Stopping</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes (Metric: mF1B)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes (Metric: mF1B)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Function Loss</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE and BCE</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSE and BCE</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05217-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05217-t008_Table 8</object-id><label>Table 8</label><caption><p>Performance results using different combinations of data augmentation techniques for DiffAct and MS-TCN models. Best results are in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Augmentation Techniques</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">MS-TCN</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">DiffAct</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rotation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zoom</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Translation</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mF1B</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mF1S</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mF1B</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mF1S</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.53</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.68</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.59</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.41</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>60.62</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>60.10</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>65.44</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>65.65</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.01</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.11</td></tr></tbody></table><table-wrap-foot><fn><p>mF1B: mean F1 Boundary; mF1S: mean F1 Sign.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>