<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431076</article-id><article-id pub-id-type="pmcid-ver">PMC12431076.1</article-id><article-id pub-id-type="pmcaid">12431076</article-id><article-id pub-id-type="pmcaiid">12431076</article-id><article-id pub-id-type="doi">10.3390/s25175384</article-id><article-id pub-id-type="publisher-id">sensors-25-05384</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi-Manifold Learning Fault Diagnosis Method Based on Adaptive Domain Selection and Maximum Manifold Edge</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Zhao</surname><given-names initials="L">Ling</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="c1-sensors-25-05384" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ding</surname><given-names initials="J">Jiawei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="P">Pan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Chi</surname><given-names initials="X">Xin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Lai</surname><given-names initials="Z">Zhihui</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05384">School of Information Science and Engineering, Chongqing Jiaotong University, Chongqing 400074, China; <email>622-200070037@mails.cqjtu.edu.cn</email> (J.D.); <email>631-607010505@mails.cqjtu.edu.cn</email> (P.L.); <email>622-200070038@mails.cqjtu.edu.cn</email> (X.C.)</aff><author-notes><corresp id="c1-sensors-25-05384"><label>*</label>Correspondence: <email>zhaoling@cqjtu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>01</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5384</elocation-id><history><date date-type="received"><day>09</day><month>4</month><year>2025</year></date><date date-type="rev-recd"><day>07</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>23</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>01</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05384.pdf"/><abstract><p>The vibration signal of rotating machinery is usually nonlinear and non-stationary, and the feature set has information redundancy. Therefore, a high-dimensional feature reduction method based on multi-manifold learning is proposed for rotating machinery fault diagnosis. Firstly, considering the non-uniformity of multi-fault feature distribution and the sensitivity of domain selection in traditional manifold learning methods, the neighborhood size of each data point is selected adaptively by using the relationship between neighborhood size and sample density. Then, the between-manifold graph and within-manifold graph are constructed adaptively by the class information, and the divergence matrix and edge distance corresponding to the manifold graph are calculated. Feature fusion reduction is achieved by maximizing edge distance and minimizing within-class differences. Finally, the multi-manifold theoretical dataset and several rotating machinery fault datasets are selected for testing. The results show that the proposed algorithm has higher fault identification accuracy than traditional manifold learning methods.</p></abstract><kwd-group><kwd>fault diagnosis</kwd><kwd>multi-manifold learning</kwd><kwd>adaptive neighborhood selection</kwd><kwd>feature fusion reduction</kwd><kwd>manifold edge distance</kwd></kwd-group><funding-group><award-group><funding-source>Key Scientific Research Project of Chongqing Education Commission</funding-source><award-id>KJZD-M202400706</award-id></award-group><funding-statement>This paper was supported by the following project: Key Scientific Research Project of Chongqing Education Commission (KJZD-M202400706).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05384"><title>1. Introduction</title><p>In recent years, advanced sensor technology, data acquisition equipment, and computer storage equipment have enabled us to accumulate massive amounts of industrial big data. If all this multi-sensor big data can be processed and analyzed effectively, potential faults can be discovered early on, and reasonable operation and maintenance programs can be followed to ensure that rotating machinery operates safely [<xref rid="B1-sensors-25-05384" ref-type="bibr">1</xref>]. However, vibration signals from rotating machinery are often characterized by both nonlinearity and non-stationarity, which makes them difficult to analyze using traditional methods. These signals exhibit time-varying statistical properties (e.g., mean, variance) and nonlinear relationships between the signal and mechanical component interactions. On the other hand, large-scale data analysis has introduced new challenges to the fault diagnosis of rotating machinery. For example, the process of data mining and pattern recognition using high-dimensional data is often plagued with problems such as &#8220;data explosion&#8221; and &#8220;dimension disaster&#8221;. Thus, mining valuable information accurately and efficiently has become a central problem in research related to rotating machinery fault diagnosis.</p><p>Feature reduction is an effective method to analyze and deal with the above-mentioned problems. Feature reduction essentially reduces the complexity and number of features in the original high-dimensional space to improve the performance of the data mining system. It simplifies the space by selecting redundant and unimportant features or combining the original feature changes into fewer features. Some scholars have explored many feature reduction methods [<xref rid="B2-sensors-25-05384" ref-type="bibr">2</xref>].</p><p>Through manifold learning, researchers have been able to curate the nonlinear structure of original data in high-dimensional space and find the advantage of its intrinsic dimension [<xref rid="B3-sensors-25-05384" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05384" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05384" ref-type="bibr">5</xref>]. Confronted with the nonlinear nature of most datasets, linear dimension reduction algorithms such as principal component analysis, linear discriminant analysis, and multidimensional scaling are poor for high-dimensional nonlinear data. Manifold learning has proven its effectiveness by unearthing prominent fault characteristics from the convolution of high-dimensional datasets; the most well-known ones are isometric feature mapping (ISOMAP), local linear embedding (LLE), and Laplacian Eigenmap [<xref rid="B6-sensors-25-05384" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05384" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05384" ref-type="bibr">8</xref>]. Manifold learning-based feature reduction methods have revealed novel diagnostic methodologies which not only pare down feature dimensions but also sift out the extraneous ones [<xref rid="B9-sensors-25-05384" ref-type="bibr">9</xref>].</p><p>But these methods assume that the data is distributed on the same manifold and use it as a modeling basis to ensure the internal structure remains unchanged when the data is mapped from high-dimensional space to a low-dimensional subspace. But previous studies have shown that rotating machinery has complex fault mechanisms, and not all faults exist in the same manifold which inevitably affects the further improvement of identification accuracy [<xref rid="B10-sensors-25-05384" ref-type="bibr">10</xref>]. So multi-manifold learning methods have become the new direction for dimensionality reduction research on fault datasets.</p><p>Methods like Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) have been developed to prove strong diagnostic capabilities even in high noise conditions [<xref rid="B11-sensors-25-05384" ref-type="bibr">11</xref>]. And the integration of discriminant local preservation projection with sparse autoencoder represents a leap in capturing both the macro- and microstructures within data [<xref rid="B12-sensors-25-05384" ref-type="bibr">12</xref>].</p><p>Although these methods marked strides in classification, the critical task of discerning important differentiated information in classification still needs enhancement. The introduction of label information and refined similarity metrics, employing cosine distance for neighbor graph construction and label-centric weighting, has shown potential in more nuanced feature dimension reduction within high-dimensional spaces [<xref rid="B13-sensors-25-05384" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05384" ref-type="bibr">14</xref>]. The challenge of constructing manifold neighbor graphs spurred further innovation, as seen with the local sparse discriminant preserving projection technique. This approach prioritizes relevant features and uses class labels to build comprehensive within and between-manifold graphs [<xref rid="B15-sensors-25-05384" ref-type="bibr">15</xref>]. Similarly, the refinement of optimization functions to emphasize within-class and between-class variance accounts for multi-manifold relationships and has improved the fault feature extraction accuracy [<xref rid="B16-sensors-25-05384" ref-type="bibr">16</xref>].</p><p>The scarcity of annotations in real datasets has prompted semi-supervised methodologies such as SDMM, which use graph-based multi-manifold modeling and convex optimization improvements for tasks like video dataset action recognition [<xref rid="B17-sensors-25-05384" ref-type="bibr">17</xref>]. Statistical significance and the preservation of local data geometries address neighborhood parameter selection, and unsupervised subspace learning (USFN) added the concept of flexible neighborhoods, circumventing noise and neighborhood selection issues [<xref rid="B18-sensors-25-05384" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05384" ref-type="bibr">19</xref>]. Furthermore, aided by adaptive neighborhood selection, algorithms like LTSA for adaptive sample point neighborhood selection enhance classification accuracy [<xref rid="B20-sensors-25-05384" ref-type="bibr">20</xref>].</p><p>As commendable as these advancements are, they still have their vulnerabilities. The existing multi-manifold analysis methods only emphasize bringing similar samples closer and dissimilar samples farther away, but do not consider the aliasing problem that occurs between different manifolds. When different manifolds overlap or lie close to each other in high-dimensional space without being effectively separated, simply pulling features of different classes apart may not prevent boundary confusion. This often results in ambiguous class boundaries and increased distortion after dimensionality reduction. So, between-manifold modeling and selection of optimal neighborhood parameters for manifold learning remain elusive tasks. The upgrades are about choosing neighborhood parameters while preserving the integrity of data geometry, whilst simultaneously minimizing the impact of noise and parameter selection on the outcome.</p><p>Based on this, this paper proposes an ALLRMM model, which can adaptively adjust the neighborhood selection strategy based on local density, and count the multi-manifold edge margin, thereby dealing with the general situation that the data contains linear and nonlinear structures and overlaps with each other in real situations. Finally, it achieves the fault diagnosis of rotating machinery. Through this attempt, the integrity of data geometry can be preserved and the impact of noise and parameter selection minimized. The contributions of this paper are as follows:</p><list list-type="simple"><list-item><label>(1)</label><p>Most of the fault diagnosis methods for rolling bearings focus on the mean value of all samples or the mean value of a certain class of samples, while the description of local information is ignored. Therefore, based on modeling the multi-manifold subspace and using within-class and between-class differences, this paper innovatively applies the multi-manifold learning dimensionality reduction algorithm to realize the efficient classification of fault modes of rolling bearings.</p></list-item><list-item><label>(2)</label><p>To solve the problem of sensitive parameter selection in the field of multi-manifold learning, the relationship between local sample density and neighborhood parameter values was tried, and the local neighborhood parameter size of each sample point was adjusted adaptively by defining the density scaling factor, so as to build the within- and between-manifold graphs adaptively. This helps to recognize boundaries between multiple manifolds, making the data samples more distinguishable.</p></list-item><list-item><label>(3)</label><p>Two different types of datasets are selected for experimental analysis, and multiple nonlinear dimension reduction algorithms (KPCA, LLE, LLRMM, and UMAP) are compared to verify the proposed algorithm&#8217;s advantages in dimension reduction effect and recognition rate. The results show that using a manifold space for feature fusion reduction can lead to better classification.</p></list-item></list></sec><sec id="sec2-sensors-25-05384"><title>2. Manifold Learning</title><p>Manifold learning is a class of nonlinear dimensionality reduction methods based on the core assumption that &#8220;high dimensional observed data often lie or approximately lie on a low dimensional smooth manifold embedded in the ambient space&#8221;, aiming to map the samples to a visual or compact low-dimensional representation while preserving the intrinsic geometry, topology, or statistical structure of the data. ISOMAP uses the geodesic distance calculated by Floyd-Warshall and achieves global isometric embedding through multidimensional scaling (MDS); Kernel Principal Component Analysis (Kernel PCA) implicitly maps the data to a high-dimensional reproducing kernel Hilbert space and then performs linear PCA, which can be regarded as the kernelized generalization of linear manifold learning; Local Tangent Space Alignment (LTSA) estimates the tangent space in each neighborhood first, and then globally aligns to minimize the tangent space projection error; Random Neighbor Embedding (SNE) and its improved version T-SNE map the high-dimensional neighborhood probabilities to low dimensions using Gaussian or Student-t distributions, minimizing the KL divergence through gradient descent, and are particularly excellent in visualization tasks; UMAP characterizes neighborhood relationships using fuzzy simplex complexes, combining Riemannian geometry and algebraic topology ideas, achieving fast speed and good global structure preservation in visualization and low-dimensional representation; LLE minimizes the linear reconstruction error of the neighborhood to maintain local linear relationships and then solves the sparse feature problem to obtain the low-dimensional coordinates. The most classic one is the LLE model.</p><p>LLE is a classical algorithm of manifold learning. It regards the linear reconstruction representation relationship between sample points and their local neighbors as the characterization of local geometric properties of manifolds. Its fundamental concept is to start with the local linearity of nonlinear data, based on which the global nonlinearity of the dataset is transformed into local linearity. According to the global structure information provided by the overlapping local neighborhoods, the global optimal low-dimensional embedding representation is sought while the local geometric relation of the data is fully preserved. The local linear embedding algorithm can learn the local structure of any dimension and is widely used in many applications such as fault detection and image recognition. Consequently, the application and improvement of the local linear embedding algorithm have been widely researched [<xref rid="B6-sensors-25-05384" ref-type="bibr">6</xref>]. The LLE algorithm, as shown in <xref rid="sensors-25-05384-f001" ref-type="fig">Figure 1</xref>, includes the following three steps:</p><list list-type="simple"><list-item><label>(1)</label><p>Search for nearest neighbor points: The first step in the LLE algorithm is determining the nearest neighbors of each data point. Given a dataset <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> is the original dimensionality of the data and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is the number of samples. For each sample <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the neighbors are determined by either choosing the <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> closest Euclidean distance points or by selecting points within a fixed radius around the sample. These neighbors <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are then used to form the neighborhood index set <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> ensuring that the local relationships between data points are preserved.</p></list-item><list-item><label>(2)</label><p>Calculate the optimal reconstruction weight matrix: each sample point is linearly reconstructed from its nearest neighbor data points <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, and the reconstruction error of all data points can be expressed in the following mathematical form:<disp-formula id="FD1-sensors-25-05384"><label>(1)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#949;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true">&#8214;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">&#8214;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight between sample points <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, if <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8713;</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and the rows of the weight matrix <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> sum to one, that is <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>(3)</label><p>Represent low-dimensional embedding information: The spatial position relationship between all sample points and the nearest neighbor set is kept unchanged in the mapping from high to low dimensions. Namely <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is kept unchanged after linear reconstruction, and the low-dimensional reconstruction error is minimized.<disp-formula id="FD2-sensors-25-05384"><label>(2)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>min</mml:mi><mml:mi>&#981;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mfenced close="&#x2016;" open="&#x2016;"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mstyle><mml:mo>&#160;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mi>Y</mml:mi><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> is the identity matrix, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the embedding representation matrix of the sample point in the low-dimensional space, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the <italic toggle="yes">i</italic>th column of the matrix <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>, which represents the coordinate vector of the <italic toggle="yes">i</italic>th sample point in the low-dimensional space of dimension <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula>. The constraint <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> is to ensure that the embedding coordinates are centered around the origin. This ensures that the low-dimensional embedding does not have a translation bias, and the data points are distributed around the origin in the embedding space. The constraint <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mi>Y</mml:mi><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;ensures that the low-dimensional coordinates are scaled appropriately along each axis, and the variance of each axis is equal, preventing any one axis from disproportionately capturing the structure of the data. This is crucial for maintaining the intrinsic structure of the data after dimensionality reduction. The optimal solution <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> of this object can be obtained by calculating the minimum <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> eigenvectors corresponding to the non-zero eigenvalues of the sparse, symmetric, semi-positive definite matrix <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the optimal reconstruction weight matrix and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mi>W</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list></sec><sec id="sec3-sensors-25-05384"><title>3. ALLRMM Model</title><p>LLE is a local processing method, which has the advantages of simplicity and high operational efficiency. In the process of sample dimension reduction, the neighborhood value is set as a fixed value for calculation. Nevertheless, the actual data are distributed on a heterogeneous manifold; the fixed neighborhood size is obviously unreasonable because it is only suitable for the homogeneous manifold structure. In addition, as a conventional manifold learning method, LLE neglects the modeling of data within and between manifolds and cannot deal with the problem of multi-manifold recognition of high-dimensional data. So considering the sensitive problem of neighborhood selection, this study takes a comprehensive view of the relationship between neighborhood size and sample density to further refine the criteria for distance selection. This approach considers the compactness and relative positional relationships of data points within the neighborhood, allowing for a more precise description of the local structure of the data points, thereby making neighborhood selection more accurate and reasonable. To solve the problem that multi-manifolds cannot be processed, the within-manifold graph and between-manifold graph are constructed by combining the data labels, based on which the spacing of multi-manifolds is measured. By maximizing the spacing of manifolds and minimizing the within-class differences, the feature dimension reduction extraction of high-dimensional feature datasets is realized. The improvement of LLE mainly includes the following two aspects:</p><sec id="sec3dot1-sensors-25-05384"><title>3.1. Optimization of Local Neighborhood Selection Method</title><p>The first step of the manifold learning algorithm is domain selection, and appropriate domain selection is key to the algorithm&#8217;s success. The algorithm assumes that each data point and its nearest neighbor are in the same linear hyperplane. Accordingly, it takes the linear reconstruction representation relationship between the sample point and its local nearest neighbor as the description of the local geometric property of the manifold. Fundamentally, the algorithm starts with the local linearity of nonlinear data and converts the global nonlinearity of the dataset into local linearity. Global structure information is provided according to overlapping local fields, and a global optimal low-dimensional embedding representation is sought while the local geometric relation of data is fully maintained [<xref rid="B21-sensors-25-05384" ref-type="bibr">21</xref>]. Furthermore, a density scaling factor algorithm is proposed to solve the sensitive problem of neighborhood selection in conventional manifold learning. The scaling factor is used to adjust any initial domain value to the ideal neighborhood value of each sample point according to the neighborhood iteration factor, taking full account of the relationship between high-dimensional data density distribution and neighborhood value.</p><list list-type="simple"><list-item><label>(1)</label><p><bold>Sensitivity analysis of neighborhood selection:</bold> Generally, an optimal domain size should be taken. If the domain size is too small, the continuous topological space will be constructed into multiple separated subgraphs, resulting in the loss of connections between certain points and the failure to reflect the global characteristics. If it is too large, the neighborhood of certain sample data points will come from other folding planes.</p><p>Next, the influence of the size of the neighborhood parameter <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> on data dimensionality reduction will be elaborated in detail through <xref rid="sensors-25-05384-f002" ref-type="fig">Figure 2</xref> and <xref rid="sensors-25-05384-f003" ref-type="fig">Figure 3</xref>.</p></list-item></list><p><xref rid="sensors-25-05384-f002" ref-type="fig">Figure 2</xref> shows the classic Swiss Roll dataset, a two-dimensional manifold dataset often used in manifold learning research. Its representation in three-dimensional space resembles a rolled-up paper scroll. <xref rid="sensors-25-05384-f002" ref-type="fig">Figure 2</xref> is a three-dimensional scatter plot, with the X-axis, Y-axis, and Z-axis representing the coordinates in three-dimensional space. The figure uses color coding to display the distribution of data points in three-dimensional space, where colors represent different labels, helping us intuitively understand the global structure and local features of the data.</p><p><xref rid="sensors-25-05384-f003" ref-type="fig">Figure 3</xref> further explores the sensitivity of the Local Linear Embedding (LLE) algorithm to the neighborhood parameter <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>. LLE is a popular manifold learning method that aims to achieve low-dimensional embedding of high-dimensional data by preserving the local geometric relationships between data points. <xref rid="sensors-25-05384-f003" ref-type="fig">Figure 3</xref> shows the results of the LLE algorithm reducing the Swiss Roll dataset from three dimensions to two dimensions under different <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> values. Each subgraph corresponds to a specific <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> value, and the X-axis and Y-axis represent the coordinates in the two-dimensional space. By checking the dimensionality reduction results under different <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> values, one can observe the significant impact of the <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> value selection on the performance of the LLE algorithm. If <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is too small, the linearity of the nearest neighbor region is guaranteed, but the global property of the data cannot be reflected. If <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is too large, the premise that the algorithm can learn local linearity of the manifold does not hold. Therefore, it can be seen that the selection of the nearest neighbor number significantly affects the dimensionality reduction effect of the LLE algorithm.</p><p>Thus, the linear processing method is adopted for most of the nonlinear distributed data, resulting in damage to the nonlinear structure between the original data.</p><list list-type="simple"><list-item><label>(2)</label><p><bold>Sample density analysis:</bold> As shown in <xref rid="sensors-25-05384-f004" ref-type="fig">Figure 4</xref>, owing to the unevenly distributed manifold structure, overlapped neighborhoods are conducive to information transmission and the protection of the local linearity hypothesis. Therefore, to maximize overlap, four places with high local density of sample points and more sample points in the neighborhood are selected to make the neighborhood larger, resulting in neighborhood overlap.</p></list-item></list><p>For places with low local density of sample points, a few neighborhood sample points are picked to make the neighborhood smaller and prevent too many local neighborhoods from destroying the local linearity hypothesis of manifold learning. In <xref rid="sensors-25-05384-f004" ref-type="fig">Figure 4</xref>, assuming that the initial fixed value is 6, it can be found that the local density of sample points a and b is large, while the sample density of point c is small. Selecting a fixed field parameter value of 6 will make the neighborhood of points a and b with high local density too small, and the manifold will be divided into multiple discontinuous subgraphs. The neighborhood of point c with small local density is too large, which destroys the assumption of local linearity. So after adaptive neighborhood iteration, the value <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> changed from the original 6 to <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the neighborhood number of points a and b is increased, and that of point c is reduced, so that different neighborhoods overlap on the premise of maintaining local linearity, and information is transmitted between neighborhoods.</p><list list-type="simple"><list-item><label>(3)</label><p><bold>Adaptive neighborhood selection:</bold> The density scaling factor of each sample point was calculated according to the density scaling factor algorithm proposed in the previous section, and the initial neighborhood value was adjusted adaptively according to the size of the density scaling factor. Consequently, the neighborhood value parameter of the high-density sample point increased while that of the low-density sample point decreased. Considering that the extreme value of the initial neighborhood value significantly affects the adaptive results, the initial extreme neighborhood value is preliminarily adjusted, followed by the adaptive iteration to get the ideal neighborhood value. The initial neighborhood value <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is based on the features of the datasets. The <italic toggle="yes">k</italic> value adaptation is performed using the density scaling factor algorithm. The density scaling factor <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is computed based on the local density of sample points in the high-dimensional space. As the local density of a sample point increases, <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> becomes larger; conversely, as the inter-sample density decreases, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> becomes smaller. The adaptive iteration process is determined as follows:</p></list-item></list><p>According to the distance between the sample points, calculate the local density of each sample point, and the local density calculation formula:<disp-formula id="FD3-sensors-25-05384"><label>(3)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:mi>&#951;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the set cut-off distance. If <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#8804;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>&#951;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, if <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>&#951;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. That is, the number of data points less than the cut-off distance <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is used to represent the local density <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The density scaling factor <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> depends on the local density of the sample point <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>, and the calculation formula is as follows:<disp-formula id="FD4-sensors-25-05384"><label>(4)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-05384"><label>(5)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05384"><label>(6)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the average density of the sample points, and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> is the sample variance.</p><p>Using a formula, we calculate a value <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> to determine whether the initial neighborhood value <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is an extreme value. <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is calculated as<disp-formula id="FD7-sensors-25-05384"><label>(7)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi>&#961;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>&#8722;</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>&gt;</mml:mo><mml:mo>&gt;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the initial neighborhood value <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is too small, and the value of <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> needs to be increased:<disp-formula id="FD8-sensors-25-05384"><label>(8)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the initial neighborhood value <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is too large, and the value of <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> needs to be reduced:<disp-formula id="FD9-sensors-25-05384"><label>(9)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the (<italic toggle="yes">j</italic> &#8722; 1)th initial iteration of the initial neighborhood value <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> of the sample point <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the scaling factor for the average density.</p><p>Subsequently, the adjusted neighborhood value <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is calculated. If the iteration condition <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mi>&#948;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is not met, the extreme value is adjusted until the adjusted neighborhood value is no longer the extreme value, following which iterations are made based on the density scaling factor. The neighborhood value <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> of each sample point can be adjusted adaptively as follows:<disp-formula id="FD10-sensors-25-05384"><label>(10)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>&#8727;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mi>&#948;</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the <italic toggle="yes">j</italic>th adaptive iteration of the initial neighborhood value <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> of the sample point <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula> is the limiting condition for judging whether the adaptive adjusted neighborhood value is ideal.</p><p>The adaptive neighborhood value iteration algorithm adapts the globally consistent initial neighborhood value to obtain the ideal neighborhood value of each sample point. According to the above calculation and adaptive iteration process based on the density scaling factor, the flow chart of the adaptive neighborhood iteration algorithm is shown in <xref rid="sensors-25-05384-f005" ref-type="fig">Figure 5</xref>.</p><p>The steps are as follows:</p><p>Step 1: Calculate the local density and density scaling factor of each sample point according to the formula.</p><p>Step 2: Determine whether the initial neighborhood value is an extreme value using (7); if it is an extreme value, adjust the neighborhood value using (8) or (9) and proceed to step 3; if it is not an extreme value, directly proceed to step 3.</p><p>Step 3: Adaptively adjust the neighborhood value size of each sample point <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> according to the density scaling factor of each sample point <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> according to (10), ensuring that the neighborhood value parameter of the high-density sample point increases and that of the low-density sample point decreases. Consequently, the ideal neighborhood value <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained.</p></sec><sec id="sec3dot2-sensors-25-05384"><title>3.2. Improved Edge Margin Algorithm for Local Linear Embedded Manifolds</title><p>The conventional manifold learning method neglects the modeling of data within and between manifolds and fails to deal with the problem of multi-manifold recognition of high-dimensional data. Therefore, this paper proposes an improved Locally Linear Representation of Manifold Margins (LLRMM) [<xref rid="B22-sensors-25-05384" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05384" ref-type="bibr">23</xref>] algorithm that constructs within-manifold and between-manifold graphs based on data labels, measuring the spacing of multi-manifolds on this basis. The algorithm achieves feature dimensionality reduction extraction for high-dimensional feature datasets by maximizing the spacing of manifolds and minimizing within-class differences. The core of the algorithm is built on LLE ALLRMM first fits the local linear model and then aligns the global coordinates. The global nonlinear manifold is divided into some small local linear sub-blocks, and then these sub-blocks are globally arranged together through information transfer to obtain a consistent low-dimensional manifold representation of the data. Compared to other algorithms, the proposed algorithm can adaptively divide into different sizes of sub-blocks, offering greater flexibility and adaptability while better accounting for the inhomogeneity of real data. As a result, it performs exceptionally well when handling rotating machinery fault datasets.</p><list list-type="simple"><list-item><label>(1)</label><p><bold>The construction of manifold graphs:</bold> The within-manifold graphs reflect the local relations in the manifold comprising similar data. For any sample point <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (the <italic toggle="yes">i</italic>-th column of dataset <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>) in the within-manifold graph, sample points with the same category and minimum distance <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are selected to form the local neighborhood of sample point <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Therefore, in this neighborhood, similar to that in LLE, the minimum linear error is expressed by the closest neighbor points of the same class of sample point <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD11-sensors-25-05384"><label>(11)</label><mml:math id="mm90" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#949;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:msup><mml:mrow><mml:mfenced close="&#x2016;" open="&#x2016;"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the nearest neighbor of the sample point <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in the within-manifold graph; <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the adaptive neighborhood selection result of sample point <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the minimum linear representation weight vector of the same kind as the nearest neighbor of sample point <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p>Since the local weights <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> sum to one, the formula can be rewritten as follows:<disp-formula id="FD12-sensors-25-05384"><label>(12)</label><mml:math id="mm98" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>&#949;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:msup><mml:mfenced close="&#x2016;" open="&#x2016;"><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:msup><mml:mfenced close="&#x2016;" open="&#x2016;"><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="10.em"/><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mfenced close="}" open="{"><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mo>&#183;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="1.4em"/><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represents the local term of the Gram matrix <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which can be expressed as<disp-formula id="FD13-sensors-25-05384"><mml:math id="mm101" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#8230;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#8945;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#8230;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Using the Lagrange multiplier method, the locally linear representation weights in the within-manifold graph can be obtained.</p><p>Firstly, a Lagrange function can be formed as<disp-formula id="FD14-sensors-25-05384"><label>(13)</label><mml:math id="mm102" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#949;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mi>&#955;</mml:mi><mml:mfenced><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then let <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8706;</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the locally linear representation weights in the within-manifold graph can be deduced to the following formulation.<disp-formula id="FD15-sensors-25-05384"><label>(14)</label><mml:math id="mm104" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="" open="{"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="7.5.em"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the within-manifold graph neighborhood of the manifold of sample point <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the local term of the inverse Gram matrix <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In the within-manifold graph, the minimum linear representation weight matrix <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained by repeating the same steps for each point. The within-manifold and between-manifold graphs are constructed in a similar manner; the process is repeated except that the neighborhood points should be selected from different manifolds. The weight formula of the local minimum linear representation is shown in (12) and (14). The minimum linear representation weight matrix <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of a between-manifold graph can also be obtained.<disp-formula id="FD16-sensors-25-05384"><label>(15)</label><mml:math id="mm111" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="" open="{"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="7.5.em"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the between-manifold graph neighborhood of sample point <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight matrix in the between-manifold graph.</p><p>In summary, the within-manifold and between-manifold graphs can be constructed by corresponding minimum linear representation weights between point pairs. In a manifold diagram, any sample can be linearly represented by the nearest neighbor points of its within-manifold graph, and the manifold divergence matrix represents the degree of aggregation between data. Therefore, the minimum linear representation error of the manifold graph can be used to represent the manifold divergence matrix of the within-manifold graph, which can be defined as shown below:<disp-formula id="FD17-sensors-25-05384"><label>(16)</label><mml:math id="mm115" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-sensors-25-05384"><label>(17)</label><mml:math id="mm116" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the minimum representation error weight matrices of the within-manifold and the between-manifold graphs, respectively; <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the graph divergence matrix within and between manifolds, respectively; and <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the original high-dimensional data matrix and <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><list list-type="simple"><list-item><label>(2)</label><p><bold>Manifold margin definition:</bold> To describe the degree of dispersion between manifolds with different class labels, a new manifold margin is defined. The manifold margin is the distance between points on the manifold <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and other manifolds <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> minus the within-manifold graph distance of the manifold <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The within-manifold graph distance of the manifold can be described as the degree of convergence and dispersion inside the manifold and can be expressed by the within-manifold graph divergence matrix of the manifold. Therefore, the definition of manifold margin is as shown below:<disp-formula id="FD19-sensors-25-05384"><label>(18)</label><mml:math id="mm127" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> stands for manifold margin; <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the distance between manifold <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and manifold <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is taken to represent the scale of manifold <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>; and <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as the sum of the minimum distance between every point on the ith manifold <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to other manifolds <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8800;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD20-sensors-25-05384"><label>(19)</label><mml:math id="mm137" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mi>min</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is any sample point on the <italic toggle="yes">i</italic>th manifold <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p>The minimum distance between the sample point <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and other manifolds can be expressed by the square of the weighted average of any sample point <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and its nearest neighbors of the between-manifold graph. On repeating this process for all sample points, the following formula for <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained.<disp-formula id="FD21-sensors-25-05384"><label>(20)</label><mml:math id="mm143" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mfenced close="&#x2016;" open="&#x2016;"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>Q</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The above formula is actually the divergence matrix of the graph between manifolds. Thus, the spacing between manifolds can be expressed as<disp-formula id="FD22-sensors-25-05384"><label>(21)</label><mml:math id="mm144" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Furthermore, <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mi>&#949;</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, so <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can be written as follows:<disp-formula id="FD23-sensors-25-05384"><label>(22)</label><mml:math id="mm147" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><list list-type="simple"><list-item><label>(3)</label><p><bold>Optimization solution:</bold> According to the within-manifold and between-manifold graph divergence matrices, and the previously defined manifold edge distance, the purpose of the proposed algorithm is to find a subspace on which data of different manifolds can be more easily distinguished. That is, the edge distance of manifolds should be maximized in the low-dimensional subspace, and the within-class difference should be minimized.</p></list-item></list><p>Therefore, to solve the low-dimensional subspaces, the above two objective functions must be satisfied, which gives the following formula:<disp-formula id="FD24-sensors-25-05384"><label>(23)</label><mml:math id="mm148" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="" open="{"><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi>max</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>{</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>min</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>{</mml:mo><mml:mi>Y</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mo>&#183;</mml:mo></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the trace of a matrix, which is the sum of its diagonal elements.</p><p>Subsequently, a single-objective optimization problem was obtained, whose form is shown in the following formula:<disp-formula id="FD25-sensors-25-05384"><label>(24)</label><mml:math id="mm150" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>{</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>{</mml:mo><mml:mi>Y</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Generally, the conventional manifold learning method encounters out-of-sample problems. Thus, it refers to linear change, <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula>, between the original data and the embedded data, such as <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This linear transformation is orthogonally restricted, that is <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, (20) is rewritten as<disp-formula id="FD26-sensors-25-05384"><label>(25)</label><mml:math id="mm154" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="" open="{"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>max</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>{</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>A</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>{</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>X</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>A</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>{</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mi>A</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="3.5.em"/><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> is the linear transformation matrix. The objective of this optimization is to find a low-dimensional subspace that maximizes the manifold edge distance between different categories of data <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> after dimensionality reduction and minimizes the global manifold error. The Lagrange multiplier method is used to solve the above constraint function, and the feature decomposition (22) is obtained. <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> comprises the feature vectors corresponding to the <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> largest eigenvalues before the feature decomposition.<disp-formula id="FD27-sensors-25-05384"><label>(26)</label><mml:math id="mm159" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the eigenvalue of the generalized characteristic equation, <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the manifold margin, and <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a characteristic vector.</p><p>Finally, the characteristic data after dimensionality reduction can be obtained as follows.<disp-formula id="FD28-sensors-25-05384"><label>(27)</label><mml:math id="mm163" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, considering the contribution to the classification effect of the algorithm, a subspace will be sought to make it easier to distinguish the data of different manifolds on this subspace, maximize the distance between different manifolds, and minimize the within-class distance within manifolds. That is, the edge distance of manifolds is maximized, and the divergence matrix of graphs within manifolds is minimized in the low-dimensional subspace. <xref rid="sensors-25-05384-f006" ref-type="fig">Figure 6</xref> is the overall flowchart of the ALLRMM algorithm.</p><p>The steps to improve the algorithm (Algorithm 1) are as follows:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold> Algorithm for ALLRMM </td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Input:</bold> original data <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>}</mml:mo><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8477;</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, data category <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, initial nearest neighbor number <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>, low-dimensional space dimension <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula>;<break/><bold>Output:</bold> Transform matrix <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> and data <inline-formula><mml:math id="mm169" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> after dimensionality reduction.<break/><italic toggle="yes">Step 1:</italic> Determine the domain size of each data sample point <inline-formula><mml:math id="mm170" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> adaptively through density scaling factor <inline-formula><mml:math id="mm171" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>;<break/><italic toggle="yes">Step 2:</italic> Construct the within-manifold and between-manifold graphs by adaptive selection of <inline-formula><mml:math id="mm172" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>.<break/><italic toggle="yes">Step 3:</italic> Calculate the weight matrix <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the corresponding between-manifold and within-manifold graphs using the formula and calculate the corresponding divergence matrix <inline-formula><mml:math id="mm174" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>;<break/><italic toggle="yes">Step 4:</italic> Calculate the manifold edge distance <inline-formula><mml:math id="mm175" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> according to the manifold divergence matrix;<break/><italic toggle="yes">Step 5:</italic> Solve the feature decomposition equation <inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The eigenvectors corresponding to the first <inline-formula><mml:math id="mm177" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> maximum eigenvalues are selected to form the transformation matrix <inline-formula><mml:math id="mm178" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> and obtain the data after dimensionality reduction by solving <inline-formula><mml:math id="mm179" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</td></tr></tbody></array></p></sec><sec id="sec3dot3-sensors-25-05384"><title>3.3. Analysis of the Initial Dimensionality Reduction Effect of Multi-Manifolds</title><p>The Swiss roll data can be used to evaluate the effectiveness of dimensionality reduction algorithms, as its locally Euclidean manifold structure enables meaningful computation of Euclidean distances between neighboring points. However, the existing nonlinear manifold dimensionality reduction methods can deal with nonlinear structure data well to a certain extent, but they are limited to well-separated structures. How to deal with the general situation where the data contains linear and nonlinear structures and overlaps with each other in the real situation, this paper constructs an overlapping dataset of Swiss Roll data and Sigmoid data for practice. Under the condition of an ideal initial neighborhood value, the dimensionality reduction effects of multi-manifold, KPCA, LLE, and ALLRMM were compared.</p><p><xref rid="sensors-25-05384-f007" ref-type="fig">Figure 7</xref> shows the mixed manifold structure dataset and the effects after being processed by different dimensionality reduction methods. The original dataset needs to be better presented in a three-dimensional space to show the separation and distribution of the data. Two-dimensional graphs are used to display the results after dimensionality reduction, as most dimensionality reduction methods aim to map high-dimensional data to a two-dimensional space for visualization and analysis. Through these graphs, the effects of different dimensionality reduction methods in handling the mixed manifold structure dataset can be intuitively compared, especially their ability to distinguish different manifolds or categories. <xref rid="sensors-25-05384-f007" ref-type="fig">Figure 7</xref>a presents the original dataset of the mixed manifold structure; different colors in the dataset represent different manifolds. The X-axis, Y-axis, and Z-axis, respectively, represent the three original features of the data, which are the natural attributes of the data generation process and are used to describe the position of the samples in the three-dimensional space. This figure is used to show the distribution of the data before any dimensionality reduction processing. The following three subfigures, respectively, show the results after using KPCA, LLE, and the ALLRMM method proposed in this paper. The coordinate axes represent the two main components after dimensionality reduction. The figure shows the distribution of the data points after dimensionality reduction, and different colors represent different manifolds.</p><p>The results show that on the multi-manifold dataset, the original topology structure is damaged after the dimensionality reduction using KPCA, and multi-manifolds cannot be recognized. Similarly, after dimension reduction using the conventional manifold learning LLE algorithm, multi-manifolds cannot be recognized. In contrast, the ALLRMM algorithm preserves the structure of the multi-manifold.</p></sec><sec id="sec3dot4-sensors-25-05384"><title>3.4. Complexity</title><p>The computational complexity of the ALLRMM is composed of five parts. Let the sample size be <inline-formula><mml:math id="mm180" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, the original feature dimension be <inline-formula><mml:math id="mm181" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula>, the maximum neighborhood number be <inline-formula><mml:math id="mm182" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm183" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#8810;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), and <inline-formula><mml:math id="mm184" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#8901;</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8901;</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be the total complexity. The first step is to calculate the local density: Firstly, a distance matrix of size <inline-formula><mml:math id="mm185" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> needs to be calculated. For each pair of distances, <inline-formula><mml:math id="mm186" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> floating-point operations are required, so the time complexity is <inline-formula><mml:math id="mm187" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#8901;</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Next is adaptive adjusting neighborhood: Scan each sample&#8217;s <inline-formula><mml:math id="mm188" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> distance records and update <inline-formula><mml:math id="mm189" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>. The overall complexity remains <inline-formula><mml:math id="mm190" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The third step of constructing intra-class and inter-class graphs: After extracting <inline-formula><mml:math id="mm191" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> neighbors for each of the <inline-formula><mml:math id="mm192" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> samples, solve the <inline-formula><mml:math id="mm193" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> linear equation system to obtain the reconstructed weights. The single-time complexity is <inline-formula><mml:math id="mm194" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the total complexity is <inline-formula><mml:math id="mm195" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8901;</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The fourth step of generating a <inline-formula><mml:math id="mm196" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> divergence matrix: Each sample participates in the external accumulation addition, resulting in <inline-formula><mml:math id="mm197" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8901;</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> operations. The last step of generalized eigenvalue decomposition: perform eigenvalue calculation on the <inline-formula><mml:math id="mm198" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> matrix. The standard algorithm complexity is <inline-formula><mml:math id="mm199" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Compared to traditional manifold learning methods such as LLE and KPCA, ALLRMM requires additional storage for the between-manifold edge distances and the disparity matrices between manifolds, resulting in higher spatial demands. However, these additional storage requirements significantly enhance the representational capacity of the data structure, enabling the algorithm to better capture the differences between manifolds and the intrinsic structure of the data. Although the ALLRMM&#8217;s spatial complexity increases when handling large-scale data, adaptive neighborhood selection and optimization of between-manifold distances effectively reduce the storage of redundant data, thereby improving spatial utilization efficiency.</p></sec></sec><sec id="sec4-sensors-25-05384"><title>4. Experimental Design</title><sec id="sec4dot1-sensors-25-05384"><title>4.1. Dataset</title><p>In order to verify the applicability of the fault diagnosis method based on multi-manifold learning and its diagnostic performance under variable working conditions, the bearing fault data of Professor Lei Yaguo&#8217;s team from Xi&#8217;an Jiaotong University and the Ottawa variable working condition dataset were selected for experiments. The test bearing for the XJTU-SY dataset was LDK UER204. A total of 15 bearings were designed under 3 working conditions, and the speed of the selected working conditions was 2400 (r/min). Bearing life data under a 10 kN load, sampling frequency is 25.6 kHz, sampling interval is 1min, sampling time is 1.28 s, and the number of samples each time is 32,768. The fault data includes seven states: normal, early inner ring fault, moderate inner ring fault, severe inner ring fault, early outer ring fault, moderate outer ring fault, and severe outer ring fault. The Ottawa dataset is shown in <xref rid="sensors-25-05384-t001" ref-type="table">Table 1</xref>. The dataset is designed under four variable conditions: increasing speed, decreasing speed, increasing then decreasing speed, and decreasing then increasing speed. The data sampling frequency is 20 kHz, sampling time is 10s, and the data includes bearing normal, inner ring failure, and outer ring failure, three states. Variable working conditions 1 and 4 were selected for experiments. According to the 1024 data points of each sample, samples in different states were divided into 30 samples, and 20 samples were randomly selected as the training set and 10 samples as the test set. The 30-dimensional features shown in <xref rid="sensors-25-05384-t002" ref-type="table">Table 2</xref>, <xref rid="sensors-25-05384-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-05384-t004" ref-type="table">Table 4</xref> are also selected.</p></sec><sec id="sec4dot2-sensors-25-05384"><title>4.2. Data Preprocessing</title><p>Per sample has 1024 vibration signal data points; samples from different states were divided into 30 segments, with 20 randomly selected as the training set and 10 as the test set. To enhance fault features and suppress noise, the segments were processed using Minimum Entropy Deconvolution (MED) filtering. The 30-dimensional features shown in <xref rid="sensors-25-05384-t002" ref-type="table">Table 2</xref>, <xref rid="sensors-25-05384-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-05384-t004" ref-type="table">Table 4</xref> are also selected. <xref rid="sensors-25-05384-t002" ref-type="table">Table 2</xref> presents 16 statistical features extracted from the time domain, including mean and peak value, which reflect the amplitude distribution, waveform symmetry, and variation trends of the vibration signals. <xref rid="sensors-25-05384-t003" ref-type="table">Table 3</xref> describes five frequency-domain features, such as the frequency center, to characterize the spectral energy distribution. Prior to computing frequency-domain features, each signal segment undergoes a Fast Fourier Transform (FFT) to determine its frequency spectrum. <xref rid="sensors-25-05384-t004" ref-type="table">Table 4</xref> provides nine multifractal features that capture signal complexity and multi-scale variation through the singularity spectrum and Holder exponents. These features, derived from the MED-filtered signal segments, serve as the foundation for subsequent feature selection and classification modeling.</p></sec></sec><sec id="sec5-sensors-25-05384"><title>5. Analysis of Experimental Result</title><sec id="sec5dot1-sensors-25-05384"><title>5.1. Analysis of the Dimension Reduction Effect of Multi-Manifold</title><p>First, the density proportion factor algorithm is employed to determine the optimal neighborhood parameter values <inline-formula><mml:math id="mm200" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>In <xref rid="sensors-25-05384-f008" ref-type="fig">Figure 8</xref>, it is the distance matrix of sample points and the density scaling factor <inline-formula><mml:math id="mm201" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> calculated according to (3)&#8211;(6). Each element represents the Euclidean distance between sample points <inline-formula><mml:math id="mm202" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm203" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To simplify processes and ensure robustness, set the truncation distance <inline-formula><mml:math id="mm204" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the neighborhood restriction condition <inline-formula><mml:math id="mm205" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula> to 2, and calculate the density scaling factor of all sample points as shown. The initial neighborhood value is 12, taking sample point <inline-formula><mml:math id="mm206" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as an example, <inline-formula><mml:math id="mm207" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6695</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and the average density scaling factor <inline-formula><mml:math id="mm208" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3.3749</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The condition <inline-formula><mml:math id="mm209" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8804;</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>&#948;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is judged by the extreme value of the initial neighborhood value. So adjust the initial neighborhood value to 6 according to (9), repeat the above steps, and finally adjust the initial neighborhood value to 3. Repeat the above steps for each sample point to obtain the ideal neighborhood value of each sample point.</p><p>As shown in <xref rid="sensors-25-05384-f009" ref-type="fig">Figure 9</xref>, <xref rid="sensors-25-05384-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-05384-f011" ref-type="fig">Figure 11</xref>, three different datasets are selected, and the classical dimensionality reduction algorithms T-SNE, KPCA, LLE, LLRMM, and UMAP are used to compare with the algorithm in this paper. The data is normalized, and the coordinates are inconsistent because KPCA is mapped by a kernel function.</p><p>In order to prove the effectiveness of the method, the XJTU-SY bearing fault dataset was selected. <xref rid="sensors-25-05384-f009" ref-type="fig">Figure 9</xref> shows the XJTU-SY dataset, and six types of fault data are selected for experiments. Blue represents normal data, red represents an early inner ring fault, and black represents a moderate inner ring fault. Green indicates a severe fault in the inner ring, yellow indicates an early fault in the outer ring, purple-red indicates a moderate fault in the outer ring, and light blue indicates a severe fault in the outer ring. It can be clearly seen from <xref rid="sensors-25-05384-f009" ref-type="fig">Figure 9</xref> that the proposed ALLRMM algorithm distinguishes the fault data obviously and has less overlap. As shown in <xref rid="sensors-25-05384-f009" ref-type="fig">Figure 9</xref>a&#8211;c, for the T-SNE, KPCA, and LLE algorithms, the blue, red, and yellow patterns have obvious overlap, which makes it difficult to distinguish early faults. As shown in <xref rid="sensors-25-05384-f009" ref-type="fig">Figure 9</xref>e&#8211;f, the blue, red, and yellow patterns have less overlap, which is more obvious than that in <xref rid="sensors-25-05384-f009" ref-type="fig">Figure 9</xref>b&#8211;d, and <xref rid="sensors-25-05384-f009" ref-type="fig">Figure 9</xref>f is more obvious than that in <xref rid="sensors-25-05384-f009" ref-type="fig">Figure 9</xref>e in distinguishing moderate and severe faults.</p><p><xref rid="sensors-25-05384-f010" ref-type="fig">Figure 10</xref> shows experimental results under a single working condition. In order to verify the applicability of the proposed algorithm, for variable working condition data, the Ottawa variable working condition bearing dataset is selected to conduct experiments with different dimensionality reduction algorithms.</p><p>The results are shown in <xref rid="sensors-25-05384-f011" ref-type="fig">Figure 11</xref>, where blue represents normal data, red and black represent bearing inner ring failure and outer ring failure faults, respectively. It can be seen from the figures that after dimension reduction, the multi-manifold learning dimensionality reduction algorithm has a better effect on distinguishing faults than the traditional KPCA and LLE.</p><p>Observing <xref rid="sensors-25-05384-f010" ref-type="fig">Figure 10</xref>a&#8211;c and <xref rid="sensors-25-05384-f011" ref-type="fig">Figure 11</xref>a&#8211;c of the two figures, it can be found that under the re-varying operating conditions, the patterns of the traditional manifold learning algorithms T-SNE, KPCA, and LLE overlap obviously, which cannot effectively distinguish different faults. By comparing <xref rid="sensors-25-05384-f010" ref-type="fig">Figure 10</xref>d&#8211;f and <xref rid="sensors-25-05384-f011" ref-type="fig">Figure 11</xref>d&#8211;f, it can be found that ALLRMM calculated by multi-manifold learning can effectively distinguish normal data from fault data, but there is obvious overlap between different fault data. By improving the optimization formula and neighborhood selection, different faults can be distinguished significantly.</p><p>In summary, after three kinds of actual data verification, the superiority of the proposed algorithm is proved.</p></sec><sec id="sec5dot2-sensors-25-05384"><title>5.2. Analysis of Algorithm Recognition Accuracy</title><p>To further compare the dimensionality reduction effect of these four algorithms, Support Vector Machine (SVM) is employed to perform pattern recognition of the features extracted by the above algorithms, and its recognition performance is shown according to the experiment. In this experiment, two-thirds of the samples of rolling bearing data were randomly selected to form the training set, and the remaining one-third formed the test set. To measure the recognition rate of the ALLRMM algorithm under different initial neighborhood values and dimensions, the initial neighborhood value <inline-formula><mml:math id="mm210" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is assigned values of 2, 5, 10, 15, 20, and 25, and the dimensionality reduction dimension <inline-formula><mml:math id="mm211" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> is assigned values of 3, 5, 10, 15, and 20. The experimental results are shown in <xref rid="sensors-25-05384-f012" ref-type="fig">Figure 12</xref>.</p><p>As shown in <xref rid="sensors-25-05384-f012" ref-type="fig">Figure 12</xref>a, when the initial neighborhood value changes from 2 to 25, the recognition rate is negatively correlated with the initial neighborhood value, showing a downward trend. That is, a larger initial neighborhood value results in a lower recognition rate, which further demonstrates that the LLRMM algorithm improved by multi-manifold learning is appropriately sensitive to the selection of the initial neighborhood value. When changes from 3 to 20, the recognition rate is positively correlated with dimension. The figure shows that the average recognition rate of the improved LLRMM algorithm peaks at 97.50%. To further compare the influence of different initial neighborhood values on the algorithm, the LLE, LLRMM, and ALLRMM algorithms were selected for comparison under the ideal feature dimension of different initial neighborhood values. For the comparison, the average neighborhood values of all sample points under different initial neighborhood values were calculated after applying the adaptive neighborhood value iteration algorithm.</p><p>As shown in <xref rid="sensors-25-05384-t005" ref-type="table">Table 5</xref>, the adaptive adjusted neighborhood value <inline-formula><mml:math id="mm212" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the proposed ALLRMM algorithm approaches the ideal neighborhood value of 2. When the initial neighborhood value <inline-formula><mml:math id="mm213" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is set to 20 and 25, it deviates considerably from its ideal value. However, the ALLRMM algorithm can also adjust the neighborhood value to 3 or 2, close to its ideal value. Clearly, the adaptive neighborhood value iteration algorithm is adequately stable and reliable. As shown in <xref rid="sensors-25-05384-f011" ref-type="fig">Figure 11</xref> and <xref rid="sensors-25-05384-t005" ref-type="table">Table 5</xref>, the proposed ALLRMM achieves the highest average recognition rate for various initial neighborhood values, whereas the LLE and ALLRMM achieve the highest average recognition rates of 90.75% and 97.5% only at ideal neighborhood values of <inline-formula><mml:math id="mm214" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm215" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. However, as shown in <xref rid="sensors-25-05384-f011" ref-type="fig">Figure 11</xref>, both the LLRMM and LLE are sensitive to the initial neighborhood value <inline-formula><mml:math id="mm216" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>. In contrast, even when the initial neighborhood value deviates significantly from its ideal value, ALLRMM continues to exhibit a good recognition rate.</p><p>The experiments were conducted under the Ottawa Condition 1 and Condition 4 datasets. The experimental results are shown in <xref rid="sensors-25-05384-t006" ref-type="table">Table 6</xref> and <xref rid="sensors-25-05384-t007" ref-type="table">Table 7</xref>, as well as <xref rid="sensors-25-05384-f013" ref-type="fig">Figure 13</xref> and <xref rid="sensors-25-05384-f014" ref-type="fig">Figure 14</xref>. In the figures, the vertical axis represents the bearing health status, where Class 1 denotes the healthy condition, Class 2 indicates an inner race fault, and Class 3 corresponds to an outer race fault. The horizontal axis represents the test samples. As shown in <xref rid="sensors-25-05384-t007" ref-type="table">Table 7</xref>, the fault diagnosis algorithm proposed based on multi-manifold learning ALLRMM has the same obvious effect under variable conditions. Under condition 1, the highest average recognition accuracy is 96.67%, and the highest recognition accuracy can reach 100%. As shown in <xref rid="sensors-25-05384-t007" ref-type="table">Table 7</xref>, under working condition 4, the average recognition rate of the four algorithms is also the highest, which is 92.33%, and the highest recognition accuracy is 93.33%. Compared with KPCA and LLE, which are more sensitive to variable working condition data, the fault diagnosis accuracy is low. The algorithm based on ALLRMM proposed in this paper also has higher recognition accuracy under changing working conditions.</p><p>To further verify the superiority of the proposed ALLRMM algorithm, this paper conducts statistical significance and stability analyses. The statistical significance of the algorithm in improving recognition accuracy was evaluated using paired <italic toggle="yes">t</italic>-tests, while one-way ANOVA was employed to test the significance of differences in overall performance among various algorithms and to verify the reliability of the improvement results. The analysis results indicate that the accuracy improvement of the ALLRMM is statistically significant, and it exhibits low performance variance across multiple experiments, demonstrating good robustness and stability.</p><p>In summary, although mixed domain features can distinguish the running state of the rolling bearing well, the recognition rate remains low. The proposed algorithm can adjust the neighborhood value of each sample and bring it close to the ideal value, which reduces the sensitivity of conventional manifold learning algorithms to neighborhood value. In addition, it resolves the optimization problem and improves recognition accuracy by referring to the definition of manifold edge distance and the calculation of the manifold divergence matrix. Furthermore, the feature dimensionality is reduced to a certain extent after the features are extracted.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05384"><title>6. Conclusions</title><p>Considering its correlation to the selection of the nearest neighbor points, the sample density of each data point is used to calculate the density scaling factor, which is derived from the adaptive selection of the sample data neighborhood size. Subsequently, the within-manifold and between-manifold graphs are constructed adaptively, and the divergence matrix and edge distance of manifolds are defined. Finally, the feature extraction of the fault dataset is realized by maximizing the edge margin of the manifold and minimizing the within-class difference. The analysis results show the following: (1) The single feature in the time, frequency, and multiple fractal domains cannot completely distinguish the running state. In contrast, the mixed domain feature can better distinguish the running state; however, the recognition rate is not good enough. (2) The conventional learning method only preserves the local structural relationship between the data and does not directly contribute to data classification. Therefore, it cannot identify the distribution problem of multi-manifolds. (3) By improving the construction of manifold graphs through adaptive neighborhood selection, the sensitive problem of domain value selection of LLRMM is solved. Combining the definition of the manifold edge margin of LLRMM and the calculation of the manifold divergence matrix, an ALLRMM is proposed. The algorithm exhibits a remarkable dimensionality reduction effect and improved recognition accuracy.</p><p>Although the proposed ALLRMM shows significant improvement in fault diagnosis accuracy, particularly for complex manifold structures, certain limitations still exist. One notable limitation is its inability to recover manifolds with equidistant properties. This issue arises due to the algorithm&#8217;s focus on local structure preservation and edge margin processing, which inherently overlooks global equidistant relationships between data points.</p><p>Deep learning has been extensively applied in the field of fault diagnosis and has demonstrated remarkable efficacy. However, the manifold learning methodology adopted in this work fundamentally integrates linear projection with local structure preservation. Compared to deep learning approaches, it exhibits inherent limitations in representational capacity. Nevertheless, manifold learning offers distinct advantages in interpretability and model compactness, rendering it particularly suitable for resource-constrained industrial scenarios such as small-sample settings and edge computing. Future research may explore the integration of deep feature extraction with manifold margin discrimination mechanisms to further enhance accuracy while maintaining computational efficiency.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, L.Z. and J.D.; methodology, L.Z. and J.D.; software, J.D. and P.L.; validation, J.D. and X.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05384"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>X.</given-names></name><name name-style="western"><surname>Mi</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>Incipient fault diagnosis on active disturbance rejection control</article-title><source>Sci. China Inf. Sci.</source><year>2021</year><volume>65</volume><fpage>199202</fpage><pub-id pub-id-type="doi">10.1007/s11432-020-3154-5</pub-id></element-citation></ref><ref id="B2-sensors-25-05384"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name></person-group><article-title>A dual-attention feature fusion network for imbalanced fault diagnosis with two-stream hybrid generated data</article-title><source>J. Intell. Manuf.</source><year>2024</year><volume>35</volume><fpage>1707</fpage><lpage>1719</lpage><pub-id pub-id-type="doi">10.1007/s10845-023-02131-2</pub-id></element-citation></ref><ref id="B3-sensors-25-05384"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>L.</given-names></name><name name-style="western"><surname>He</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Y.</given-names></name></person-group><article-title>Fast time-frequency manifold learning and its reconstruction for transient feature extraction in rotating machinery fault diagnosis</article-title><source>Measurement</source><year>2019</year><volume>141</volume><fpage>380</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1016/j.measurement.2019.04.030</pub-id></element-citation></ref><ref id="B4-sensors-25-05384"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>He</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>D.</given-names></name></person-group><article-title>Tool Wear Prediction Based on Multi-Information Fusion and Genetic Algorithm-Optimized Gaussian Process Regression in Milling</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>72</volume><fpage>2516716</fpage><pub-id pub-id-type="doi">10.1109/TIM.2023.3280531</pub-id></element-citation></ref><ref id="B5-sensors-25-05384"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Y.</given-names></name></person-group><article-title>A new supervised multi-head self-attention autoencoder for health indicator construction and similarity-based machinery RUL prediction</article-title><source>Adv. Eng. Inform.</source><year>2023</year><volume>56</volume><fpage>101973</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2023.101973</pub-id></element-citation></ref><ref id="B6-sensors-25-05384"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tenenbaum</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>de Silva</surname><given-names>V.</given-names></name><name name-style="western"><surname>Langford</surname><given-names>J.C.</given-names></name></person-group><article-title>A Global Geometric Framework for Nonlinear Dimensionality Reduction</article-title><source>Science</source><year>2000</year><volume>290</volume><fpage>2319</fpage><lpage>2323</lpage><pub-id pub-id-type="doi">10.1126/science.290.5500.2319</pub-id><pub-id pub-id-type="pmid">11125149</pub-id></element-citation></ref><ref id="B7-sensors-25-05384"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roweis</surname><given-names>S.T.</given-names></name><name name-style="western"><surname>Saul</surname><given-names>L.K.</given-names></name></person-group><article-title>Nonlinear Dimensionality Reduction by Locally Linear Embedding</article-title><source>Science</source><year>2000</year><volume>290</volume><fpage>2323</fpage><lpage>2326</lpage><pub-id pub-id-type="doi">10.1126/science.290.5500.2323</pub-id><pub-id pub-id-type="pmid">11125150</pub-id></element-citation></ref><ref id="B8-sensors-25-05384"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Belkin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Niyogi</surname><given-names>P.</given-names></name></person-group><article-title>Laplacian eigenmaps for dimensionality reduction and data representation</article-title><source>Neural Comput.</source><year>2003</year><volume>15</volume><fpage>1373</fpage><lpage>1396</lpage><pub-id pub-id-type="doi">10.1162/089976603321780317</pub-id></element-citation></ref><ref id="B9-sensors-25-05384"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name></person-group><article-title>Fault detection and quantitative assessment method for process industry based on feature fusion</article-title><source>Measurement</source><year>2022</year><volume>197</volume><fpage>111267</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2022.111267</pub-id></element-citation></ref><ref id="B10-sensors-25-05384"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>M.</given-names></name></person-group><article-title>Rotating machinery fault diagnosis based on multiple fault manifolds</article-title><source>J. Vib. Eng.</source><year>2015</year><volume>28</volume><fpage>309</fpage><lpage>315</lpage></element-citation></ref><ref id="B11-sensors-25-05384"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>W.</given-names></name></person-group><article-title>Sparse discriminant manifold projections for bearing fault diagnosis</article-title><source>J. Sound Vib.</source><year>2017</year><volume>399</volume><fpage>330</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1016/j.jsv.2017.03.029</pub-id></element-citation></ref><ref id="B12-sensors-25-05384"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Y.-L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Q.-X.</given-names></name></person-group><article-title>Fault Diagnosis Using Improved Discrimination Locality Preserving Projections Integrated With Sparse Autoencoder</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2021</year><volume>70</volume><fpage>3527108</fpage><pub-id pub-id-type="doi">10.1109/TIM.2021.3125975</pub-id></element-citation></ref><ref id="B13-sensors-25-05384"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Fault Diagnosis Method Based on a New Manifold Learning Framework</article-title><source>J. Intell. Fuzzy Syst.</source><year>2018</year><volume>34</volume><fpage>3413</fpage><lpage>3427</lpage><pub-id pub-id-type="doi">10.3233/JIFS-169522</pub-id></element-citation></ref><ref id="B14-sensors-25-05384"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.-W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Q.-X.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.-L.</given-names></name></person-group><article-title>Novel Bootstrap-Based Discriminant NPE Integrated With Orthogonal LPP for Fault Diagnosis</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>72</volume><fpage>3506309</fpage><pub-id pub-id-type="doi">10.1109/TIM.2023.3239649</pub-id></element-citation></ref><ref id="B15-sensors-25-05384"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name></person-group><article-title>Local-Dictionary Sparsity Discriminant Preserving Projections for Rotating Machinery Fault Diagnosis Based on Pre-Selected Multi-Domain Features</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>22</volume><fpage>8781</fpage><lpage>8794</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3161596</pub-id></element-citation></ref><ref id="B16-sensors-25-05384"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S.</given-names></name><name name-style="western"><surname>Han</surname><given-names>Q.</given-names></name></person-group><article-title>Unified discriminant manifold learning for rotating machinery fault diagnosis</article-title><source>J. Intell. Manuf.</source><year>2022</year><volume>34</volume><fpage>3483</fpage><lpage>3494</lpage><pub-id pub-id-type="doi">10.1007/s10845-022-02011-1</pub-id></element-citation></ref><ref id="B17-sensors-25-05384"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>Z.M.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.Y.</given-names></name></person-group><article-title>Semisupervised Discriminant Multi-manifold Analysis for Action Recognition</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2019</year><volume>30</volume><fpage>2951</fpage><lpage>2962</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2018.2886008</pub-id><pub-id pub-id-type="pmid">30762568</pub-id></element-citation></ref><ref id="B18-sensors-25-05384"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Carlini</surname><given-names>L.P.</given-names></name><name name-style="western"><surname>Junior</surname><given-names>G.F.M.</given-names></name><name name-style="western"><surname>Giraldi</surname><given-names>G.A.</given-names></name><name name-style="western"><surname>Thomaz</surname><given-names>C.E.</given-names></name></person-group><article-title>A new method of selecting safe neighbors for the Riemannian Manifold Learning algorithm</article-title><source>IEEE Lat. Am. Trans.</source><year>2021</year><volume>19</volume><fpage>89</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1109/TLA.2021.9423851</pub-id></element-citation></ref><ref id="B19-sensors-25-05384"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Bian</surname><given-names>J.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Unsupervised Subspace Learning With Flexible Neighboring</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2023</year><volume>34</volume><fpage>2043</fpage><lpage>2056</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2021.3105813</pub-id><pub-id pub-id-type="pmid">34478380</pub-id></element-citation></ref><ref id="B20-sensors-25-05384"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>N.</given-names></name></person-group><article-title>EEG fading data classification based on improved manifold learning with adaptive neighborhood selection</article-title><source>Neurocomputing</source><year>2022</year><volume>482</volume><fpage>186</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2021.11.039</pub-id></element-citation></ref><ref id="B21-sensors-25-05384"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>D.R.</given-names></name><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name></person-group><article-title>Fault Diagnosis Method Based on Fractal Theory and Its Application in Wind Power Systems</article-title><source>Acta Armamentarii</source><year>2012</year><volume>8</volume><fpage>167</fpage><lpage>173</lpage></element-citation></ref><ref id="B22-sensors-25-05384"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Baddour</surname><given-names>N.</given-names></name></person-group><article-title>Bearing vibration data collected under time-varying rotational speed conditions</article-title><source>Data Brief</source><year>2018</year><volume>21</volume><fpage>1745</fpage><lpage>1749</lpage><pub-id pub-id-type="doi">10.1016/j.dib.2018.11.019</pub-id><pub-id pub-id-type="pmid">30505910</pub-id><pub-id pub-id-type="pmcid">PMC6249544</pub-id></element-citation></ref><ref id="B23-sensors-25-05384"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.-L.</given-names></name></person-group><article-title>A global manifold margin learning method for data feature extraction and classification</article-title><source>Eng. Appl. Artif. Intell.</source><year>2018</year><volume>75</volume><fpage>94</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.engappai.2018.08.004</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05384-f001" orientation="portrait"><label>Figure 1</label><caption><p>Illustrative description of local linear embedding algorithm.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g001.jpg"/></fig><fig position="float" id="sensors-25-05384-f002" orientation="portrait"><label>Figure 2</label><caption><p>Swiss Roll dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g002.jpg"/></fig><fig position="float" id="sensors-25-05384-f003" orientation="portrait"><label>Figure 3</label><caption><p>Sensitivity of LLE algorithm to the neighborhood parameters <inline-formula><mml:math id="mm217" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g003.jpg"/></fig><fig position="float" id="sensors-25-05384-f004" orientation="portrait"><label>Figure 4</label><caption><p>Effect of sample density on neighborhood size.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g004.jpg"/></fig><fig position="float" id="sensors-25-05384-f005" orientation="portrait"><label>Figure 5</label><caption><p>Adaptive neighborhood selection flowchart.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g005.jpg"/></fig><fig position="float" id="sensors-25-05384-f006" orientation="portrait"><label>Figure 6</label><caption><p>The overall flowchart of the ALLRMM algorithm.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g006.jpg"/></fig><fig position="float" id="sensors-25-05384-f007" orientation="portrait"><label>Figure 7</label><caption><p>Mixed manifold structure dataset and rendering effect of dimensional reduction.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g007.jpg"/></fig><fig position="float" id="sensors-25-05384-f008" orientation="portrait"><label>Figure 8</label><caption><p>Sample point neighborhood parameter selection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g008.jpg"/></fig><fig position="float" id="sensors-25-05384-f009" orientation="portrait"><label>Figure 9</label><caption><p>XJTU-SY: (<bold>a</bold>) T-SNE dimension reduction effect of the original data. (<bold>b</bold>) KPCA dimension reduction effect. (<bold>c</bold>) LLE dimension reduction effect. (<bold>d</bold>) LLRMM dimension reduction effect. (<bold>e</bold>) UMAP dimension reduction effect. (<bold>f</bold>) ALLRMM dimension reduction effect.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g009.jpg"/></fig><fig position="float" id="sensors-25-05384-f010" orientation="portrait"><label>Figure 10</label><caption><p>Ottawa-1: (<bold>a</bold>) T-SNE dimension reduction effect of the original data. (<bold>b</bold>) KPCA dimension reduction effect. (<bold>c</bold>) LLE dimension reduction effect. (<bold>d</bold>) LLRMM dimension reduction effect. (<bold>e</bold>) UMAP dimension reduction effect. (<bold>f</bold>) ALLRMM dimension reduction effect.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g010.jpg"/></fig><fig position="float" id="sensors-25-05384-f011" orientation="portrait"><label>Figure 11</label><caption><p>Ottawa-4: (<bold>a</bold>) T-SNE dimension reduction effect of the original data. (<bold>b</bold>) KPCA dimension reduction effect. (<bold>c</bold>) LLE dimension reduction effect. (<bold>d</bold>) LLRMM dimension reduction effect. (<bold>e</bold>) UMAP dimension reduction effect. (<bold>f</bold>) ALLRMM dimension reduction effect.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g011.jpg"/></fig><fig position="float" id="sensors-25-05384-f012" orientation="portrait"><label>Figure 12</label><caption><p>(<bold>a</bold>) The recognition rate of ALLRMM algorithm on different initial neighborhood values. (<bold>b</bold>) The recognition rate of three algorithms on rolling bearing fault datasets with different initial neighborhood values.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g012.jpg"/></fig><fig position="float" id="sensors-25-05384-f013" orientation="portrait"><label>Figure 13</label><caption><p>The highest recognition rate of different algorithms of set Ottawa-1: (<bold>a</bold>) Total features. (<bold>b</bold>) KPCA. (<bold>c</bold>) LLE. (<bold>d</bold>) LLRMM. (<bold>e</bold>) UMAP. (<bold>f</bold>) ALLRMM.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g013.jpg"/></fig><fig position="float" id="sensors-25-05384-f014" orientation="portrait"><label>Figure 14</label><caption><p>The highest recognition rate of different algorithms under four operating conditions of Ottawa-4 set: (<bold>a</bold>) Total features. (<bold>b</bold>) KPCA. (<bold>c</bold>) LLE. (<bold>d</bold>) LLRMM. (<bold>e</bold>) UMAP. (<bold>f</bold>) ALLRMM.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05384-g014.jpg"/></fig><table-wrap position="float" id="sensors-25-05384-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05384-t001_Table 1</object-id><label>Table 1</label><caption><p>Ottawa dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Bearing Health <break/> Conditions</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Speed Varying Conditions</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Increasing Speed</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Decreasing Speed</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Increasing Then <break/> Decreasing Speed</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Decreasing then <break/> Increasing Speed</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" colspan="1">Healthy</td><td align="center" valign="middle" rowspan="1" colspan="1">H-A-1</td><td align="center" valign="middle" rowspan="1" colspan="1">H-B-1</td><td align="center" valign="middle" rowspan="1" colspan="1">H-C-1</td><td align="center" valign="middle" rowspan="1" colspan="1">H-D-1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">H-A-2</td><td align="center" valign="middle" rowspan="1" colspan="1">H-B-2</td><td align="center" valign="middle" rowspan="1" colspan="1">H-C-2</td><td align="center" valign="middle" rowspan="1" colspan="1">H-D-2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">H-A-3</td><td align="center" valign="middle" rowspan="1" colspan="1">H-B-3</td><td align="center" valign="middle" rowspan="1" colspan="1">H-C-3</td><td align="center" valign="middle" rowspan="1" colspan="1">H-D-3</td></tr><tr><td rowspan="3" align="center" valign="middle" colspan="1">Faulty(inner race fault)</td><td align="center" valign="middle" rowspan="1" colspan="1">I-A-1</td><td align="center" valign="middle" rowspan="1" colspan="1">I-B-1</td><td align="center" valign="middle" rowspan="1" colspan="1">I-C-1</td><td align="center" valign="middle" rowspan="1" colspan="1">I-D-1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">I-A-2</td><td align="center" valign="middle" rowspan="1" colspan="1">I-B-2</td><td align="center" valign="middle" rowspan="1" colspan="1">I-C-2</td><td align="center" valign="middle" rowspan="1" colspan="1">I-D-2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">I-A-3</td><td align="center" valign="middle" rowspan="1" colspan="1">I-B-3</td><td align="center" valign="middle" rowspan="1" colspan="1">I-C-3</td><td align="center" valign="middle" rowspan="1" colspan="1">I-D-3</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Faulty(outer race fault)</td><td align="center" valign="middle" rowspan="1" colspan="1">O-A-1</td><td align="center" valign="middle" rowspan="1" colspan="1">O-B-1</td><td align="center" valign="middle" rowspan="1" colspan="1">O-C-1</td><td align="center" valign="middle" rowspan="1" colspan="1">O-D-1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">O-A-2</td><td align="center" valign="middle" rowspan="1" colspan="1">O-B-2</td><td align="center" valign="middle" rowspan="1" colspan="1">O-C-2</td><td align="center" valign="middle" rowspan="1" colspan="1">O-D-2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">O-A-3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">O-B-3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">O-C-3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">O-D-3</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05384-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05384-t002_Table 2</object-id><label>Table 2</label><caption><p>Statistical features in time domain.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mathematical Formula</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mean value </td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm219" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Arithmetic average of signal amplitudes</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maximum amplitude</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm220" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Highest instantaneous value in the signal</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Minimum amplitude</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm221" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Lowest instantaneous value in the signal</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Peak-to-Peak value</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm222" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Dynamic range of signal amplitudes</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Root mean square</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm223" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Quadratic mean representing signal energy</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Variance</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm224" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Measure of signal dispersion</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Raw skewness</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm225" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Unnormalized third moment for asymmetry</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Raw kurtosis</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm226" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mn>4</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Unnormalized fourth moment for tail heaviness</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Smoothness value</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm227" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Sensitivity to small amplitude variations </td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Normalized kurtosis</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm228" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Energy-compensated impulse detection </td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normalized skewness</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm229" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Variance-normalized distribution asymmetry</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Waveform factor</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm230" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Ratio of RMS to mean absolute value</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Crest factor</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm231" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Peak-to-RMS ratio for impulse detection</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Peak-to-Mean ratio</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm232" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Peak amplitude relative to DC component</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Smooth Peak ratio</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm233" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Peak normalized by smoothness value</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Absolute mean</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm234" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfenced close="|" open="|"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:mfenced></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Magnitude of DC component</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05384-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05384-t003_Table 3</object-id><label>Table 3</label><caption><p>Frequency-domain statistical characteristics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mathematical Formula</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spectral centroid</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm235" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mi>&#981;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1"> Energy-weighted mean frequency, <inline-formula><mml:math id="mm236" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#981;</mml:mi><mml:mfenced><mml:mi>i</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes frequency weight of the <italic toggle="yes">i</italic>-th component</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mean square frequency</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm237" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>&#981;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>&#960;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Second moment of spectral distribution</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Root mean square frequency</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm238" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Effective bandwidth measure</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Frequency variance</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm239" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Dispersion around spectral centroid</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral standard deviation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm240" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standard deviation of frequency distribution</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05384-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05384-t004_Table 4</object-id><label>Table 4</label><caption><p>Statistical characteristics of multiple fractal fields.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mathematical Formula</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Minimum singularity exponent</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm241" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Exponent for most singular regions</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maximum singularity exponent</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm242" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Exponent for smoothest regions</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Multifractal spectrum width</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm243" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Range of singularity strengths</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maximum fractal dimension</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm244" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Peak dimension in multifractal spectrum</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spectrum asymmetry</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm245" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Asymmetry measure of singularity spectrum</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Generalized Hurst exponent</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm246" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Scaling exponent function</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mean Hurst exponent</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm247" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Average scaling behavior</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Hurst exponent range</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm248" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">Multifractality strength indicator</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Singularity exponent range</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm249" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Width of singularity support</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05384-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05384-t005_Table 5</object-id><label>Table 5</label><caption><p>Units for magnetic properties.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Initial Neighborhood Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">10</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">15</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">20</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">25</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Rolling bearing fault dataset adaptive <inline-formula><mml:math id="mm250" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> value</td><td align="center" valign="middle" rowspan="1" colspan="1">LLE algorithm</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LLRMM algorithm</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ALLRMM algorithm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05384-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05384-t006_Table 6</object-id><label>Table 6</label><caption><p>Average identification accuracy in Ottawa dataset 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Recognition Rate</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Total features</td><td align="center" valign="middle" rowspan="1" colspan="1">91.67%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LLE</td><td align="center" valign="middle" rowspan="1" colspan="1">90.00%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KPCA</td><td align="center" valign="middle" rowspan="1" colspan="1">92.00%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UMAP</td><td align="center" valign="middle" rowspan="1" colspan="1">93.30%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LLRMM</td><td align="center" valign="middle" rowspan="1" colspan="1">93.33%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ALLRMM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.67%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05384-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05384-t007_Table 7</object-id><label>Table 7</label><caption><p>Average recognition rate of different algorithms in Ottawa dataset 4.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Recognition Rate</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Total features</td><td align="center" valign="middle" rowspan="1" colspan="1">86.67%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KPCA</td><td align="center" valign="middle" rowspan="1" colspan="1">71.33%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UMAP</td><td align="center" valign="middle" rowspan="1" colspan="1">76.65%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LLE</td><td align="center" valign="middle" rowspan="1" colspan="1">81.00%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LLRMM</td><td align="center" valign="middle" rowspan="1" colspan="1">89.33%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ALLRMM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.33%</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>