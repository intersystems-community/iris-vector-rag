<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430933</article-id><article-id pub-id-type="pmcid-ver">PMC12430933.1</article-id><article-id pub-id-type="pmcaid">12430933</article-id><article-id pub-id-type="pmcaiid">12430933</article-id><article-id pub-id-type="doi">10.3390/s25175575</article-id><article-id pub-id-type="publisher-id">sensors-25-05575</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Semantic Path-Guided Remote Sensing Recommendation for Natural Disasters Based on Knowledge Graph</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-3720-3060</contrib-id><name name-style="western"><surname>Zhao</surname><given-names initials="X">Xiangyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05575" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="C">Chunju</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05575" ref-type="aff">1</xref><xref rid="c1-sensors-25-05575" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Luo</surname><given-names initials="C">Chenchen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05575" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="J">Jun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-05575" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-5456-5337</contrib-id><name name-style="western"><surname>Chu</surname><given-names initials="C">Chaoqun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05575" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="C">Chenxi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05575" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Pei</surname><given-names initials="Y">Yifan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-05575" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="Z">Zhaofu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05575" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Pintelas</surname><given-names initials="P">Panagiotis</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05575"><label>1</label>College of Civil Engineering, Hefei University of Technology, Hefei 230009, China; <email>2023110780@mail.hfut.edu.cn</email> (X.Z.); <email>2022110637@mail.hfut.edu.cn</email> (C.L.); <email>2023110779@mail.hfut.edu.cn</email> (C.C.); <email>2024111596@mail.hfut.edu.cn</email> (C.L.); <email>2024110759@mail.hfut.edu.cn</email> (Y.P.); <email>wuzhaofu@hfut.edu.cn</email> (Z.W.)</aff><aff id="af2-sensors-25-05575"><label>2</label>National Geomatics Center of China, Beijing 100830, China; <email>junzhang@ngcc.cn</email></aff><author-notes><corresp id="c1-sensors-25-05575"><label>*</label>Correspondence: <email>zhangspring@hfut.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>06</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5575</elocation-id><history><date date-type="received"><day>16</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>03</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>04</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>06</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05575.pdf"/><abstract><p>To address the challenges of complex task matching, limited semantic representation, and low recommendation efficiency in remote sensing data acquisition for natural disasters, this study proposes a semantic path-guided recommendation method based on a knowledge graph framework. A disaster-oriented remote sensing knowledge graph is constructed by integrating entities such as disaster types, remote sensing tasks, observation requirements, sensors, and satellite platforms. High-order meta-paths with semantic closure are designed to model task&#8211;resource relationships structurally. A Meta-Path2Vec embedding mechanism is employed to learn vector representations of nodes through path-constrained random walks and Skip-Gram training, capturing implicit semantic correlations between tasks and sensors. Cosine similarity and a Top-K ranking strategy are then applied to perform intelligent task-driven sensor recommendation. Experiments on multiple disaster scenarios&#8212;such as floods, landslides, and wildfires&#8212;demonstrate the model&#8217;s high accuracy and robust stability. An interactive recommendation system is also developed, integrating data querying, model inference, and visual feedback, validating the method&#8217;s practicality and effectiveness in real-world applications. This work provides a theoretical foundation and practical solution for intelligent remote sensing data matching in disaster contexts.</p></abstract><kwd-group><kwd>natural disaster</kwd><kwd>remote sensing imagery</kwd><kwd>knowledge graph</kwd><kwd>data recommendation</kwd></kwd-group><funding-group><award-group><funding-source>Fundamental Research Funds for the Central Universities</funding-source><award-id>JZ2024HGTG0288</award-id></award-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>42171453</award-id></award-group><funding-statement>This research was funded by the Fundamental Research Funds for the Central Universities (No.JZ2024HGTG0288), the National Natural Science Foundation of China under Grant 42171453.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05575"><title>1. Introduction</title><p>Remote sensing refers to the acquisition of information about the Earth&#8217;s surface and atmosphere without direct contact, typically via satellite or airborne sensors. As a core technology in disaster monitoring and environmental management, it enables multi-source observation across spatial, spectral, and temporal dimensions. Owing to its broad spatial coverage, high resolution, and frequent revisit cycles, remote sensing data have become indispensable in natural disaster analysis, supporting critical tasks such as disaster detection, dynamic change tracking, and post-event damage assessment [<xref rid="B1-sensors-25-05575" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05575" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05575" ref-type="bibr">3</xref>]. However, the heterogeneity of data sources and the diversity of technical parameters pose significant challenges for users in selecting appropriate data for specific disaster scenarios [<xref rid="B4-sensors-25-05575" ref-type="bibr">4</xref>]. This selection process not only requires the integration of spatial resolution, spectral bands, and acquisition timing [<xref rid="B5-sensors-25-05575" ref-type="bibr">5</xref>] but also demands alignment between task semantics and sensor capabilities, resulting in operational complexity and increased cognitive burden. At present, remote sensing data acquisition still largely relies on expert knowledge [<xref rid="B6-sensors-25-05575" ref-type="bibr">6</xref>] and keyword-based search methods [<xref rid="B7-sensors-25-05575" ref-type="bibr">7</xref>], which lack systematic modeling and semantic representation of the relationships between task objectives and data characteristics [<xref rid="B8-sensors-25-05575" ref-type="bibr">8</xref>]. These limitations hinder the development of intelligent retrieval and automated recommendation mechanisms, particularly in time-critical applications such as emergency response, where accuracy, interpretability, and rapid decision-making are essential [<xref rid="B9-sensors-25-05575" ref-type="bibr">9</xref>].</p><p>In this context, developing a knowledge organization mechanism with semantic understanding and reasoning capabilities has become a crucial step toward improving the intelligence of remote sensing data recommendation systems [<xref rid="B10-sensors-25-05575" ref-type="bibr">10</xref>]. As a knowledge representation framework that integrates ontology modeling, entity-relation structures, and semantic inference [<xref rid="B11-sensors-25-05575" ref-type="bibr">11</xref>], knowledge graphs can systematically organize domain-specific entities, attributes, and relationships within a graph-based structure. This enables the representation of complex semantics and the discovery of inter-entity associations. In recent years, knowledge graphs have shown significant potential in domains such as education [<xref rid="B12-sensors-25-05575" ref-type="bibr">12</xref>], healthcare [<xref rid="B13-sensors-25-05575" ref-type="bibr">13</xref>], and agriculture [<xref rid="B14-sensors-25-05575" ref-type="bibr">14</xref>], where they serve as foundational technologies for intelligent retrieval, personalized recommendation, and semantic question answering.</p><p>In the domains of remote sensing and natural disaster management, several studies have constructed knowledge graphs targeting specific disaster types&#8212;such as earthquakes, floods, and landslides&#8212;resulting in specialized graphs like flood and landslide knowledge graphs. Some researchers have further explored their use in remote sensing image interpretation and disaster recognition. For example, Zhang et al. summarized approaches to remote sensing knowledge graph construction and proposed a semantic network that integrates task types, sensor capabilities, and application scenarios to enhance information organization [<xref rid="B15-sensors-25-05575" ref-type="bibr">15</xref>]. Zhou et al. developed a geoscientific knowledge graph with spatiotemporal reasoning capabilities to support cross-scale semantic fusion of remote sensing data [<xref rid="B16-sensors-25-05575" ref-type="bibr">16</xref>]. Ji et al. provided a comprehensive review of knowledge graph construction and reasoning strategies, underscoring the value of path modeling in semantic abstraction [<xref rid="B17-sensors-25-05575" ref-type="bibr">17</xref>]. Additional studies have explored knowledge graphs for semantic integration and visual reasoning of heterogeneous remote sensing data [<xref rid="B18-sensors-25-05575" ref-type="bibr">18</xref>], fusing remote sensing imagery with social media for disaster estimation [<xref rid="B19-sensors-25-05575" ref-type="bibr">19</xref>], enhancing the synergy of optical and InSAR data for landslide detection [<xref rid="B20-sensors-25-05575" ref-type="bibr">20</xref>], and improving recognition performance by embedding knowledge graphs into deep learning models [<xref rid="B21-sensors-25-05575" ref-type="bibr">21</xref>]. However, most of these efforts have concentrated on semantic modeling and retrieval, with limited attention to recommendation approaches specifically driven by knowledge graphs for remote sensing data selection.</p><p>Current knowledge graph-based recommendation methods are largely developed in general domains such as social media and e-commerce. These systems typically model users, items, and attributes to identify semantic associations between user preferences and item features [<xref rid="B22-sensors-25-05575" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05575" ref-type="bibr">23</xref>]. Introducing knowledge graphs into the remote sensing domain offers a novel semantic paradigm that addresses the shortcomings of traditional rule-based and keyword-driven retrieval methods [<xref rid="B24-sensors-25-05575" ref-type="bibr">24</xref>], facilitating a shift from passive content-based search to structured, task-driven intelligent recommendation [<xref rid="B25-sensors-25-05575" ref-type="bibr">25</xref>]. Given the inherent complexity of matching heterogeneous data sources with task demands and observation parameters&#8212;particularly in time-sensitive disaster response scenarios&#8212;semantic interpretation and path-based reasoning become indispensable. Nevertheless, a unified framework that integrates task semantics, structural modeling, and reasoning mechanisms remains absent, limiting the depth and intelligence of knowledge graph applications in remote sensing data recommendation.</p><p>To address this gap, this study targets remote sensing data recommendation in natural disaster contexts by constructing a disaster-oriented knowledge graph that integrates key elements such as disaster types, task demands, spectral requirements, sensor parameters, and platform configurations. Recognizing the inefficiency of existing approaches in identifying optimal resources for specific disaster scenarios, we propose a semantic path-guided representation learning framework based on the concept of meta-paths [<xref rid="B26-sensors-25-05575" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05575" ref-type="bibr">27</xref>]. These meta-paths are designed around task semantics to model structured semantic relationships among entities. We adopt the Meta-Path2Vec embedding method, combining path-guided random walks with embedding learning, followed by cosine similarity-based ranking to generate prioritized recommendations. This approach supports the computational modeling of semantic meta-paths within the knowledge graph structure, enhancing controllability, interpretability, and generalizability.</p><p>Accordingly, this paper presents a remote sensing data recommendation method that integrates knowledge graph modeling, semantic path-based embedding, and graph-based reasoning. Focusing on representative remote sensing tasks related to natural disasters, we construct a hierarchical semantic knowledge graph, design task-driven semantic paths, and implement reasoning mechanisms under path constraints using embedding-based models. This closed-loop framework&#8212;from task modeling to recommendation output&#8212;offers a novel solution for enhancing semantic organization and advancing the intelligence of remote sensing recommendation systems. By incorporating artificial intelligence into the data selection process, the proposed method reduces reliance on expert knowledge and enables intelligent matching between disaster-specific needs and sensor capabilities. Such approaches may support real-time, AI-driven systems capable of autonomously selecting appropriate data and generating disaster-specific thematic products, laying the groundwork for a new generation of adaptive and automated remote sensing applications.</p></sec><sec id="sec2-sensors-25-05575"><title>2. Construction of a Natural Disaster-Oriented Knowledge Graph for Remote Sensing Data Recommendation</title><sec id="sec2dot1-sensors-25-05575"><title>2.1. Knowledge System for Natural Disaster Remote Sensing Data Recommendation</title><p>The intelligent recommendation of remote sensing data depends fundamentally on a clear understanding of monitoring requirements in natural disaster contexts, as well as the semantic alignment between task objectives and observational parameters. To support this process, the knowledge graph for remote sensing data recommendation in disaster scenarios must be built upon a structurally comprehensive knowledge system that enables logical progression from semantic modeling to path-based reasoning. This system consists of two core components: the first is a task knowledge system centered on natural disaster types and remote sensing application tasks, which defines the monitoring objectives and their associated observation parameters; the second is a remote sensing resource system that encapsulates the capabilities of satellite platforms, sensors, and data products, thereby reflecting the adaptability and responsiveness of available resources. Together, these two components form a semantic chain&#8212;&#8220;disaster type &#8594; remote sensing task demand &#8594; spectral band &#8594; remote sensing platform &#8594; image data&#8221;&#8212;as illustrated in <xref rid="sensors-25-05575-f001" ref-type="fig">Figure 1</xref>, which provides the foundational knowledge framework for reasoning and recommending remote sensing data.</p><p>Within this semantic chain, disaster types, remote sensing tasks, and spectral requirements function as the front-end semantic drivers, while platforms and sensors represent the responsive resource components. Through a path-constrained reasoning mechanism, the system enables intelligent task-to-resource matching, thereby supporting the efficient filtering and recommendation of remote sensing data.</p><sec id="sec2dot1dot1-sensors-25-05575"><title>2.1.1. Knowledge System of Remote Sensing Application Tasks in Natural Disasters</title><p>Remote sensing application tasks constitute the foundational starting point for knowledge graph modeling. Variations in remote sensing response mechanisms across different disaster types result in distinct monitoring timeframes, spatial scales, and sensor configurations. Therefore, it is essential to construct a semantic recommendation chain that links disaster types to specific observational parameters, thereby capturing the mapping between task semantics and observational constraints. According to national standards GB/T 28921-2012 and GB/T 26376-2010 [<xref rid="B28-sensors-25-05575" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05575" ref-type="bibr">29</xref>], natural disasters are classified into four primary categories: meteorological and hydrological disasters, geological and seismic disasters, marine disasters, and biological disasters. Typical events such as floods, landslides, and typhoons are subsumed under these classifications.</p><p>To ensure applicability within the context of remote sensing recommendation, this study selects disaster types that are both responsive to remote sensing and of high research relevance&#8212;namely, landslides, earthquakes, floods, droughts, and wildfires. Based on their characteristic monitoring requirements, task nodes are defined to represent specific objectives such as early warning, real-time monitoring, and post-disaster assessment. These are further decomposed into subtasks, including object recognition, boundary delineation, and deformation monitoring, thereby establishing a structured representation of task requirements and parameters, as illustrated in <xref rid="sensors-25-05575-f002" ref-type="fig">Figure 2</xref>.</p><p>To more systematically present the relationship between task requirements and associated parameters, <xref rid="sensors-25-05575-t001" ref-type="table">Table 1</xref> summarizes the observational demands of remote sensing tasks for three representative disaster types. These demands are categorized along five key dimensions: observed objects, observed attributes, temporal resolution, spatial resolution, and spectral bands. For example, flood-related tasks such as &#8220;water body detection&#8221; typically require optical or near-infrared imagery with a spatial resolution of 10&#8211;100 m and acquisition within 24 h, whereas earthquake-related tasks like &#8220;fault tracing&#8221; demand sub-meter spatial resolution and thermal infrared sensing capabilities. The tabulated information offers a clear overview of the response capabilities required for each task, laying a parameter foundation for subsequent resource modeling and semantic path reasoning.</p><p>Through this structured modeling approach, the knowledge system of remote sensing tasks in natural disaster contexts not only defines the semantic path&#8217;s origin but also establishes a logical and interpretable mapping from task requirements to observational parameters. This provides precise semantic constraints that are essential for intelligent remote sensing data recommendation.</p></sec><sec id="sec2dot1dot2-sensors-25-05575"><title>2.1.2. Remote Sensing Resource Knowledge System</title><p>The remote sensing resource knowledge system serves as the responsive component within the semantic path reasoning chain, addressing the critical question: &#8220;What remote sensing capabilities are available?&#8221; This system is organized into four hierarchical layers&#8212;platform, sensor, parameter, and data product&#8212;as illustrated in <xref rid="sensors-25-05575-f003" ref-type="fig">Figure 3</xref>. Its objective is to construct a knowledge graph that accurately represents the observational capabilities of remote sensing systems, thereby providing the responsive foundation for a closed-loop semantic path connecting task demands with appropriate data resources.</p><p>At the platform level, remote sensing satellites are categorized based on their service objectives and operational contexts into land observation, meteorological, oceanic, and commercial platforms. Each category exhibits distinct capabilities in terms of orbital characteristics, revisit frequencies, and primary application domains. For instance, land observation satellites are typically employed for landslide monitoring and post-earthquake assessments, whereas meteorological satellites are better suited for wide-area flood and typhoon monitoring. As the core observational units of these platforms, sensors are classified into types such as optical, near-infrared, thermal infrared, radar, and microwave. Each sensor type supports specific disaster monitoring tasks depending on its imaging mechanism and spectral characteristics. For example, Synthetic Aperture Radar (SAR) is highly effective for monitoring ground deformation, while thermal infrared sensors are particularly suitable for wildfire detection due to their sensitivity to thermal anomalies.</p><p>In terms of capability representation, platform and sensor parameters play a pivotal role in task-to-resource matching. Platform parameters typically include basic attributes such as satellite name, orbital altitude, and revisit cycle. Sensor parameters, on the other hand, encompass technical specifications such as the number of spectral bands, spatial resolution, and swath width. These attributes serve as weighted indicators within the reasoning process, contributing to candidate filtering and recommendation ranking in the semantic path framework.</p><p>In summary, the remote sensing resource knowledge system in natural disaster scenarios establishes a structured linkage between task semantics and sensor capabilities. This linkage enables a closed-loop semantic representation from task definition to data recommendation. The system not only facilitates the construction of semantically coherent reasoning paths but also underpins the logic required by recommendation algorithms. Moreover, it provides essential knowledge support for the efficient and intelligent allocation of remote sensing resources in disaster-related applications.</p></sec></sec><sec id="sec2dot2-sensors-25-05575"><title>2.2. Schema Layer Construction</title><p>At the core of the natural disaster-oriented remote sensing data recommendation system lies the structured organization and semantic modeling of heterogeneous domain knowledge. As the abstract design layer of the overall knowledge graph, the schema layer plays a pivotal role in ensuring the completeness of data representation and the viability of semantic reasoning. This section details the construction of the schema layer from three essential dimensions&#8212;concepts, attributes, and relationships&#8212;and proposes targeted semantic modeling strategies tailored to remote sensing application scenarios, thereby establishing both the structural and semantic foundation for downstream recommendation mechanisms.</p><p>In the conceptual modeling phase, this study organizes six major categories of semantic entities based on the task demands of remote sensing data recommendation: natural disaster types, remote sensing application tasks, observational parameters, sensors, satellite platforms, and remote sensing imagery. Among these, the entity type Disaster Type serves as the starting point of the graph structure and is further classified into five subcategories: meteorological and hydrological disasters, geological and seismic disasters, marine disasters, biological disasters, and other types. These are then refined into specific disaster events such as landslides, floods, and forest fires, forming a hierarchical taxonomy of disaster categories. The Remote Sensing Task (RS Task) acts as an intermediary semantic unit, bridging disaster types and sensing requirements. Its design fully considers variations across pre-disaster, during-disaster, and post-disaster phases, covering high-frequency modules such as terrain extraction, disaster change detection, and damage assessment, and is formalized into a structured task set.</p><p>To represent spectral requirements, two core semantic entities&#8212;Band Type and Spectral Range&#8212;are introduced. These entities express task dependencies on specific spectral categories (e.g., visible light, near-infrared, microwave) and quantify the required observational wavelength ranges, serving as semantic bridges between task nodes and sensing devices. Additionally, Sensor and Satellite entities are modeled from the perspectives of execution unit and platform support, respectively. Through abstraction and categorization of sensors (e.g., optical, radar) and satellite series (e.g., Gaofen, Sentinel), a clear entity classification system is established to support structured task&#8211;resource matching and scheduling.</p><p><xref rid="sensors-25-05575-f004" ref-type="fig">Figure 4</xref> illustrates the ontological framework of the conceptual system, in which hierarchical nodes capture the dependencies and associations among key semantic concepts, revealing the logical path from disaster recognition to data acquisition. This structure not only standardizes the granularity of entity classification but also defines the starting and terminal points of semantic path modeling. It demonstrates strong semantic closure and scalability, providing a stable knowledge foundation for path-constrained reasoning and embedding-based modeling.</p><p>In attribute modeling, to ensure the knowledge graph accurately reflects the technical characteristics and semantic constraints of each entity, this study defines a set of core attributes. For natural disasters, attributes include causal mechanisms, spatial distribution patterns, and response timeliness. For remote sensing tasks, attributes cover target categories, response stages, and observational dependencies. Sensor-related attributes include spectral type, spatial resolution, and application adaptability, while satellite platform attributes encompass orbital type, revisit cycle, and payload configuration. <xref rid="sensors-25-05575-f005" ref-type="fig">Figure 5</xref> presents a typical attribute structure for satellite platforms, including fields such as country of origin, launch date, and equipped sensors, reflecting the multi-dimensional nature of remote sensing capabilities. Notably, certain attributes&#8212;such as spatial resolution and temporal resolution&#8212;can be abstracted as independent conceptual nodes, allowing them to directly participate in semantic path reasoning. This flexible transformation between attributes and concepts enhances the expressiveness and richness of semantic paths within the graph.</p><p>At the relationship modeling level, to represent the diverse semantic connections among entities, the system incorporates four fundamental relationship types: Hierarchy (HR), Is-A (ISA), Has-A, and Data Property. For example, Landslide as a subclass of Geological Disaster forms an HR relationship; Sentinel-2 as a specific satellite instance represents an ISA relationship; the OLI sensor having multiple bands such as Red and Green is expressed through a Has-A relation; and the spatial resolution of GF-1 WFV being 16 m is modeled via Data Property. In addition, a series of domain-specific functional relationships are defined to support path-based recommendation reasoning. These include ASSOCIATED_WITH (linking disaster types and tasks), REQUIRES_BAND (relating tasks to spectral requirements), and SPATTEMP_RES (associating tasks with spatial and temporal resolution constraints), which are semantically explicit and logically coherent. Other remote sensing-specific relations, such as EQUIPPED_WITH (linking satellites to onboard sensors) and CAPTURES (describing sensor-based data acquisition), further strengthen the logical foundation for matching task demands with sensing capabilities, thereby providing a computational basis for semantic path-guided recommendation reasoning.</p></sec><sec id="sec2dot3-sensors-25-05575"><title>2.3. Data Layer Construction</title><p>The construction of the data layer follows the structural blueprint established by the schema layer and is designed to generate instantiated knowledge content that supports semantic reasoning and recommendation computation. This is achieved through the acquisition, cleaning, extraction, and integration of multi-source data. The data layer not only provides the three fundamental components of a knowledge graph&#8212;entities, attributes, and relationships&#8212;but also demonstrates the feasibility of knowledge extraction techniques, the systematic organization of semantic information, and the practical alignment with real-world remote sensing task scenarios.</p><p>As summarized in <xref rid="sensors-25-05575-t002" ref-type="table">Table 2</xref>, the data sources employed in this study span a variety of formats, including text documents, spreadsheets, and image metadata. These sources are categorized into four main types: (1) natural disaster event data, (2) satellite and sensor information, (3) remote sensing image metadata, and (4) auxiliary background materials. Among them, disaster event data serve as essential inputs for constructing the semantic chain of &#8220;disaster&#8211;task demand,&#8221; providing key parameters such as disaster type, occurrence time, affected area, and observation requirements. These data are primarily obtained from national emergency reports, disaster yearbooks, and official standards. Satellite and sensor information form the structural core of the &#8220;sensor&#8211;platform&#8211;imagery&#8221; linkage and are mainly sourced from technical manuals and databases maintained by organizations such as NASA, ESA, and CNSA. This category includes parameters related to platforms, sensor types, imaging modes, and spectral coverage, which collectively serve as the foundation for task&#8211;resource matching. Furthermore, remote sensing image metadata&#8212;focusing on aspects such as acquisition time, central coordinates, and spectral characteristics&#8212;describe the actual targets of recommendation and are sourced from open-access platforms such as the Geospatial Data Cloud and the China Centre for Resources Satellite Data and Application. Finally, auxiliary background materials serve as contextual supplements, offering information on disaster scenarios, sensor applicability, and classification schemes for remote sensing applications, thereby enriching the semantic depth of the knowledge graph.</p><p>The construction of the data layer not only provides the material basis for instantiating the knowledge graph structure but also functions as a critical support mechanism for implementing the remote sensing data recommendation workflow. In this study, multi-source data are integrated and processed using a BiLSTM-CRF model [<xref rid="B30-sensors-25-05575" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05575" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05575" ref-type="bibr">32</xref>] and semi-manual verification to extract key entities&#8212;such as disaster types, remote sensing tasks, satellite platforms, and sensors&#8212;from unstructured text. To ensure the accuracy and reliability of the constructed knowledge graph, the extracted results were carefully reviewed and corrected where necessary through human validation. Additionally, attribute information such as spatial resolution, spectral range, and imaging mode is extracted to enrich the graph&#8217;s semantic detail. The result is an instance-level remote sensing knowledge graph characterized by rich semantics, clear structure, and robust support for recommendation-related tasks. This comprehensive instance base provides a solid foundation for path-constrained reasoning and model training. <xref rid="sensors-25-05575-f006" ref-type="fig">Figure 6</xref> illustrates the mapping between the schema and data layers: the upper section depicts the abstract conceptual framework, while the lower section shows the populated data instances&#8212;together forming a complete loop from structural definition to semantic realization.</p><p>To provide a clearer overview of the constructed knowledge graph, we summarize in <xref rid="sensors-25-05575-t003" ref-type="table">Table 3</xref> the number of nodes for the main entity types (e.g., disasters, tasks, platforms, sensors, and spectral bands), together with the counts of the core relationship types. This reflects the scale and structural composition of the knowledge graph used in this study.</p></sec><sec id="sec2dot4-sensors-25-05575"><title>2.4. Knowledge Graph Storage and Query</title><p>Graph databases offer superior performance in managing complex and densely connected structures, making them particularly well-suited for implementing knowledge graphs. This advantage is especially prominent in remote sensing applications, where entities often exhibit multi-level and multi-granularity interconnections. To efficiently manage the intricate semantic structures required for remote sensing data recommendation in disaster scenarios, this study adopts the Neo4j graph database for knowledge storage and query execution. Neo4j organizes data through nodes, relationships, and properties, making it a natural choice for modeling multi-relational semantic path structures such as &#8220;disaster type &#8594; remote sensing task demand &#8594; platform &#8594; data&#8221;. Additionally, Neo4j offers excellent scalability and query performance, making it ideal for large-scale semantic reasoning.</p><p>Queries on the knowledge graph are performed using Cypher, a declarative and intuitive graph query language that facilitates a range of operations including node attribute retrieval, path pattern matching, and relationship filtering. Neo4j also provides a robust programmable interface. For Python3.X users, the py2neo library enables seamless integration of the graph database with machine learning-based recommendation models, supporting efficient model training and inference while preserving the integrity of the graph structure.</p><p><xref rid="sensors-25-05575-f007" ref-type="fig">Figure 7</xref> presents a visualized subgraph of the knowledge graph for a flood disaster scenario. The central node, representing &#8220;flood disaster,&#8221; is connected via semantic relationships such as ASSOCIATED_WITH to nodes denoting relevant remote sensing tasks, required spectral band types for water body extraction, and spatial resolution parameters. This forms a semantically complete and logically coherent path network.</p><p><xref rid="sensors-25-05575-f008" ref-type="fig">Figure 8</xref> further illustrates the platform and sensor modules of the graph. Satellite platforms such as Gaofen-1 and Gaofen-6 are connected to onboard sensors like WFV (Wide Field of View) and PMS (Push-broom Multispectral Scanner), along with their associated spectral characteristics. Edges such as HAS_BAND and BELONGS_TO describe key attributes related to sensing capabilities and imaging modes. The interactive interface for the knowledge graph supports not only structural visualization but also dynamic exploration of upstream and downstream nodes and semantic paths. When combined with Cypher queries, this functionality allows for rapid knowledge validation and context-specific information retrieval.</p><p>In summary, the storage and querying capabilities provided by Neo4j ensure efficient, consistent, and scalable data management, while also delivering stable support for semantic modeling, feature acquisition, and reasoning-based remote sensing data recommendation. These functions make it a foundational component of the system architecture developed in this study.</p></sec></sec><sec id="sec3-sensors-25-05575"><title>3. Remote Sensing Data Recommendation Method Guided by Knowledge Graph Semantic Paths</title><p>Remote sensing tasks often involve heterogeneous information encompassing disaster types, observation targets, sensor capabilities, and platform configurations. Traditional recommendation approaches that rely solely on attribute similarity or adjacency-based structures often fail to capture the deep semantic relationships across different types of nodes. As such, developing a recommendation mechanism that integrates structural path reasoning with semantic constraints is essential for improving task-to-resource matching efficiency and enhancing the overall intelligence of remote sensing recommendation systems.</p><p>To enable structured semantic recommendation, this study proposes a knowledge graph semantic path-guided remote sensing data recommendation method. By designing meta-paths that encode high-order semantic constraints and employing the Meta-Path2Vec method [<xref rid="B26-sensors-25-05575" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05575" ref-type="bibr">27</xref>] for node embedding, the model learns task&#8211;resource similarities within the embedding space. A Top-K ranking strategy is then applied to generate prioritized recommendation results. This end-to-end pipeline&#8212;from semantic modeling to embedding-based representation and reasoning-driven inference&#8212;provides both theoretical and practical support for achieving accurate, interpretable, and scalable remote sensing resource recommendation across diverse disaster types and task scenarios.</p><sec id="sec3dot1-sensors-25-05575"><title>3.1. Semantic Path Modeling and Meta-Path Design Strategy</title><p>In remote sensing data recommendation, accurately matching task demands with sensor capabilities is of fundamental importance. To achieve this, we design a core meta-path grounded in the previously constructed natural disaster remote sensing knowledge graph. This meta-path is characterized by both structural connectivity and semantic coherence. Starting from the node representing the remote sensing task demand, the path sequentially connects nodes related to disaster type, spectral response characteristics, sensor capabilities, and satellite platforms. By modeling semantics at the path level, this approach significantly enhances both the accuracy and interpretability of the recommendation reasoning process.</p><p>To effectively capture the heterogeneous spectral requirements of remote sensing tasks, the meta-path structure is constructed with a focus on spectral response capacity as its primary semantic axis. While spatial and temporal resolution parameters remain critical in determining the overall usability of remote sensing data, their relationship with task performance is often more straightforward, as higher resolution generally contributes positively but does not always yield proportionally improved outcomes. In contrast, spectral information plays a more nuanced and task-specific role. For instance, flood detection benefits significantly from near-infrared bands due to the strong absorption characteristics of water, whereas wildfire monitoring relies heavily on mid-wave and thermal infrared bands to identify thermal anomalies. Compared with spatial or temporal resolution parameters, spectral information is inherently more structured and can be explicitly represented as nodes within the knowledge graph, thereby improving the semantic expressiveness of path transitions. For instance, in earthquake scenarios, boundary recognition tasks typically rely on high-resolution visible bands, while fire monitoring tasks prioritize mid-wave and thermal infrared bands to detect temperature anomalies. <xref rid="sensors-25-05575-f009" ref-type="fig">Figure 9</xref> presents a sample meta-path structure for the task &#8220;Analysis of Changes Before and After Floods,&#8221; clearly illustrating the critical role of near-infrared bands in distinguishing water bodies from surrounding vegetation. Although different sensors may vary in their spectral response ranges, many remain suitable candidates within the same spectral category, and are thus included in the recommendation scope.</p><p>In designing the meta-path structure, we emphasize semantic directionality and transition stability, avoiding semantic noise caused by overly long paths or loosely connected relationships. <xref rid="sensors-25-05575-t004" ref-type="table">Table 4</xref> summarizes the node types and relationships involved in the designed meta-path. The path begins at a Disaster_Type node, proceeds through RS_Task, Spectral_Band, Band_Type, and Spectral_Range, and ultimately reaches Sensor nodes that support the required spectral range, as well as their corresponding Satellite nodes. This design not only defines a clear semantic propagation route for task requirements but also provides a standardized template for path-constrained node embedding.</p><p>During path modeling, one-to-many mappings frequently occur&#8212;for example, a single task may correspond to multiple sensors that satisfy its spectral requirements. This reflects the diversity of available remote sensing resources and underscores the need for path constraints and random walk mechanisms, which enhance both the flexibility and generalization capability of the recommendation model.</p><p>In conclusion, the proposed semantic path strategy&#8212;centered on spectral bands&#8212;achieves both structural convergence and semantic consistency. It provides a solid semantic foundation for guiding subsequent embedding learning and reasoning processes, thereby supporting effective and interpretable task-to-resource matching in remote sensing data recommendation.</p></sec><sec id="sec3dot2-sensors-25-05575"><title>3.2. Path-Guided Node Representation Learning Mechanism</title><p>To overcome the limitations of traditional embedding methods in capturing high-order semantic relationships among heterogeneous entity types, this study introduces a path-guided embedding mechanism. Predefined meta-paths are used as semantic structural constraints, systematically guiding the generation of node vector representations. Based on the Meta-Path2Vec model, the mechanism integrates random walk sampling strategies with a Skip-Gram-based Word2Vec training framework [<xref rid="B33-sensors-25-05575" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05575" ref-type="bibr">34</xref>], resulting in a node embedding system that is both structurally constrained and semantically coherent. This ensures that the learned embeddings effectively capture latent semantic relationships between remote sensing tasks and sensor resources.</p><p>The first stage of this embedding mechanism employs a random walk strategy constrained by the predefined semantic paths, with the aim of improving the semantic purity of the training corpus. Traditional random walks often encounter disruptions from irrelevant nodes in the graph, generating noisy sequences that fail to reflect the structural and semantic requirements of the recommendation task. To address this issue, we define a meta-path template&#8212;as shown in Equation (1)&#8212;and impose strict constraints on the types of nodes and edges at each step of the walk. This ensures that generated paths follow the underlying recommendation logic and form semantically closed sequences.<disp-formula id="FD1-sensors-25-05575"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>R</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>B</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>S</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The constrained random walk begins at a Disaster_Type node and sequentially traverses nodes representing Remote Sensing Task (<italic toggle="yes">T</italic>), Spectral Band (<italic toggle="yes">R</italic>), Band Type (<italic toggle="yes">B</italic>), Spectral Range (<italic toggle="yes">W</italic>), Sensor (<italic toggle="yes">S</italic>), and Satellite (<italic toggle="yes">P</italic>), thereby forming path sequences that align with the designed semantic schema. To further enhance sequence quality, we implement a rollback strategy, detailed in <xref rid="sensors-25-05575-t005" ref-type="table">Table 5</xref>: when a target node is missing, a required relationship is undefined, or the path reaches a dead end, the walker backtracks to the previous node and attempts an alternative path. This mechanism helps prevent invalid sequences from entering the training corpus and degrading the model&#8217;s performance.</p><p>After generating high-quality semantic path samples, the process proceeds to the second stage: node embedding training. Each valid node sequence that conforms to the meta-path template is treated as a &#8220;semantic sentence,&#8221; with individual nodes functioning as &#8220;words&#8221; within a structured corpus. To capture the contextual dependencies among nodes along the semantic paths, we apply the Skip-Gram model from the Word2Vec framework. This model learns low-dimensional vector representations by maximizing the co-occurrence probability between a center node and its context nodes within a defined sliding window. As a result, the training not only preserves the high-order semantic structure encoded in the meta-paths but also naturally clusters nodes with similar semantic functions (e.g., sensors sharing comparable spectral capabilities) within the embedding space. The resulting node embeddings provide a robust foundation for subsequent similarity-based matching and remote sensing resource recommendation, facilitating intelligent and semantically informed alignment between task requirements and sensing capabilities.</p><p><xref rid="sensors-25-05575-f010" ref-type="fig">Figure 10</xref> illustrates the complete pipeline for node embedding learning. On the left is the constructed natural disaster knowledge graph; the middle section depicts the path-constrained random walk process; and the right side shows the Skip-Gram-based embedding training structure. In this workflow, the semantic paths and graph structure jointly constrain the learning process, ensuring both structural coherence and semantic expressiveness. Given the total number of nodes |V|, the embedding dimension is set to 128 to balance representational capacity with computational efficiency. The context window size determines the model&#8217;s ability to capture local structure along semantic paths.</p><p>To further clarify the embedding workflow, <xref rid="sensors-25-05575-f011" ref-type="fig">Figure 11</xref> provides a detailed breakdown of the Skip-Gram training process under meta-path guidance. Beginning with predefined meta-paths, semantic-constrained random walks are performed on the knowledge graph to generate node sequences relevant to specific remote sensing tasks. These sequences are then transformed into training samples by splitting each into center&#8211;context pairs using a sliding window. The Skip-Gram model is then used to optimize the conditional probability <italic toggle="yes">P (context</italic>&#8739;<italic toggle="yes">center)</italic>, thereby learning node embeddings that reflect semantic co-occurrence patterns. After training, the resulting node vectors are employed to compute semantic similarity between task and sensor nodes, enabling Top-K recommendation for intelligent, task-driven remote sensing resource matching.</p><p>The proposed path-guided node representation learning mechanism effectively integrates the topological structure of the knowledge graph with the logical reasoning capabilities provided by semantic paths. By imposing meta-path constraints to ensure the quality of training data and applying Word2Vec-based embedding to capture latent semantic relationships, the mechanism produces embeddings that yield coherent and interpretable semantic representations of both remote sensing tasks and data resources. This not only enhances the explainability of recommendation results but also establishes a structured foundation for intelligent remote sensing data retrieval and resource matching.</p></sec><sec id="sec3dot3-sensors-25-05575"><title>3.3. Similarity Reasoning and Recommendation Output Mechanism</title><p>Upon completing the knowledge graph-based node embedding process, the critical next step is to transform the learned embeddings into sensor recommendation results tailored to specific remote sensing tasks. This section outlines the similarity computation and reasoning logic, introducing a Top-K ranking mechanism to produce the final recommendations. Together, these steps form a complete &#8220;embedding&#8211;reasoning&#8211;output&#8221; pipeline, enabling path-driven semantic recommendation. In this framework, the system calculates semantic similarity between task nodes and sensor nodes based on their embedding representations, using cosine similarity as the primary metric. A Top-K selection strategy is then applied to efficiently identify and rank the most relevant sensor candidates.</p><p>For similarity computation, cosine similarity [<xref rid="B35-sensors-25-05575" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05575" ref-type="bibr">36</xref>] is employed as the core metric for measuring node proximity. Unlike Euclidean distance, cosine similarity is better suited for assessing the directional alignment of vectors in high-dimensional semantic space, making it particularly effective for capturing semantic closeness between vectors of different magnitudes. Specifically, each remote sensing task is represented by a composite embedding vector <italic toggle="yes">u</italic>, derived from both the disaster type and the associated remote sensing task node, while each sensor node is represented by a vector <italic toggle="yes">v</italic>. The semantic similarity between them is calculated using the cosine similarity function, as defined in Equation (2):<disp-formula id="FD2-sensors-25-05575"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Sim</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#8901;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>v</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">u</italic>&#183;<italic toggle="yes">v</italic> denotes the dot product of the vectors, ||<italic toggle="yes">u</italic>|| and ||<italic toggle="yes">v</italic>|| represent their Euclidean norms. A similarity value closer to 1 indicates a higher degree of semantic consistency, and thus a stronger justification for recommendation. This semantic similarity-based mechanism allows the system to move beyond conventional rule-based approaches, adopting a more flexible and interpretable structure-aware recommendation paradigm.</p><p>Once similarity scores are computed for all candidate sensors, the system ranks them in descending order of similarity with respect to the target task node. The top <italic toggle="yes">K</italic> sensors are then selected as the final recommendation list. In this study, <italic toggle="yes">K</italic> is set to 4, striking a balance between comprehensive coverage of commonly used sensor resources and practical relevance in real-world applications.</p><p>It is worth emphasizing that while the meta-paths used in this study are primarily constructed based on spectral band requirements&#8212;to ensure alignment between recommended sensors and task-specific spectral needs&#8212;the underlying mechanism is inherently extensible. Additional constraints, such as temporal resolution and spatial resolution, can be readily incorporated in future iterations to further refine the precision of the recommendations under different task contexts. Ultimately, through the integration of semantic path modeling, embedding-based representation, and similarity reasoning, the proposed system achieves intelligent, interpretable, and task-driven sensor recommendation for remote sensing applications in disaster scenarios.</p></sec><sec id="sec3dot4-sensors-25-05575"><title>3.4. Experimental Data and Parameter Settings</title><p>The experimental dataset used in this study is derived from the previously constructed knowledge graph for natural disaster-oriented remote sensing data recommendation. This dataset encompasses multiple heterogeneous entity types&#8212;including disaster types, remote sensing tasks, spectral requirements, sensors, and satellite platforms&#8212;along with the semantic relationships linking them. It is specifically designed to support sensor recommendation tasks based on the Meta-Path2Vec embedding model, facilitating the learning of high-order semantic paths and the discovery of latent associations between task requirements and sensing resources. To ensure both robust model training and reliable inference results, the knowledge graph underwent systematic cleaning and processing, and all experiments were conducted in a high-performance computing environment.</p><p>During the data preprocessing phase, isolated nodes not connected to the target meta-paths were removed to ensure the structural integrity and semantic consistency of the embedding corpus. Subsequently, structured random walks were carried out based on predefined meta-path templates, generating node pair sequences used to train the Word2Vec model and learn vector-space representations. This process emphasized the modeling of co-occurrence relationships along semantic paths, resulting in node embeddings that simultaneously capture semantic coherence and discriminative power, thereby laying a solid foundation for the downstream recommendation process. <xref rid="sensors-25-05575-t006" ref-type="table">Table 6</xref> summarizes the key statistics of the experimental knowledge graph, which includes 872 nodes and 1208 relationships. A total of 500 structured walk sequences were generated under the guidance of the meta-path template to form the embedding corpus.</p><p>For node embedding implementation, the classical Word2Vec algorithm with a Skip-Gram architecture was employed to learn vector representations from the walk sequences. To enhance both model expressiveness and training efficiency, key hyperparameters were carefully configured, as summarized in <xref rid="sensors-25-05575-t007" ref-type="table">Table 7</xref>. These parameter settings were not chosen arbitrarily; rather, they represent a combination of empirical experience from related studies and practical tuning on our dataset, where the current configuration provided a balanced trade-off between model performance and computational efficiency. The embedding dimension was set to 128, balancing representation capacity and computational cost. A context window size of 5 was chosen to capture the local neighborhood structure, while the number of negative samples was set to 10 to improve contrastive learning. The model was trained for 30 epochs, using an initial learning rate of 0.01 with exponential decay to ensure stable convergence. Additionally, to guarantee sufficient diversity and coverage of training samples, each node was sampled 50 times with a walk length of 7, producing high-quality semantic sequences for embedding learning.</p><p>Through the above data preparation pipeline and parameter configuration, this study successfully developed an embedding model characterized by strong semantic alignment and structural stability. The entire process&#8212;from semantic path design and structured sampling to model training and vector output&#8212;demonstrates methodological rigor and systematic implementation. This embedding system provides a reliable and interpretable foundation for the subsequent sensor recommendation algorithm and its comprehensive evaluation in disaster-related remote sensing applications.</p></sec></sec><sec id="sec4-sensors-25-05575"><title>4. Remote Sensing Data Recommendation: Validation and Analysis</title><sec id="sec4dot1-sensors-25-05575"><title>4.1. Evaluation of Recommendation Results Based on Knowledge Graph Semantic Path Guidance</title><p>To evaluate the adaptability and effectiveness of the proposed semantic path embedding-based remote sensing data recommendation model across diverse disaster-related tasks, a task-driven evaluation framework was established. Drawing from remote sensing case studies on floods, landslides, and wildfires&#8212;collected from the China National Knowledge Infrastructure (CNKI)&#8212;a validation dataset comprising 14 distinct task types and a total of 240 instances was systematically curated. The dataset spans a broad spectrum of spectral combinations, ranging from visible to microwave bands, while also incorporating variations in spatial and temporal resolution requirements, thereby providing a comprehensive testbed for model validation.</p><p>A three-tier validation mechanism was adopted to assess the recommendation results, integrating (1) a literature-based reference library, (2) expert cross-validation, and (3) automated parameter verification. First, a standardized knowledge base was constructed to map remote sensing task demands to corresponding sensors, serving as a benchmark for evaluation. Second, domain experts in remote sensing were invited to manually assess the relevance and suitability of recommended sensors with respect to real-world task objectives. This expert review was supplemented by semantic retrieval and parameter-based verification techniques, ensuring the objectivity, thoroughness, and domain validity of the assessment process. The recommendation accuracy was calculated using Equation (3), defined as the proportion of valid recommendations within the model&#8217;s Top-K outputs.<disp-formula id="FD3-sensors-25-05575"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Overall</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Accuracy</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mi>Number</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Valid</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Recommendations</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>Total</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Number</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>of</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Top</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">K</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>Recommendations</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this experimental evaluation, the model produced 56 sensor recommendations for 14 tasks spanning three disaster types. Among them, 31 recommendations were identified as valid, meaning they met the technical and semantic requirements of the corresponding tasks. This yields a task-level accuracy that reflects the model&#8217;s strong task-matching capability and cross-scenario adaptability. As shown in <xref rid="sensors-25-05575-t008" ref-type="table">Table 8</xref>, the model performed particularly well for thermal infrared-related tasks, such as &#8220;fire detection and monitoring&#8221; and &#8220;fire intensity and behavior analysis&#8221;, both achieving 100% accuracy. In contrast, for tasks requiring high spatial resolution, such as &#8220;landslide boundary identification&#8221;, the model exhibited some deviation, resulting in lower accuracy. These results suggest that the proposed approach is particularly effective for thermal anomaly detection but requires further refinement for complex terrain interpretation.</p><p>Overall, the model&#8217;s performance remains stable and reliable when task definitions include explicit semantic paths and clearly defined spectral requirements. However, for tasks with incomplete metadata or those affected by multi-source interference, the recommendation accuracy tends to decrease. To address these limitations, future work may focus on enriching the semantic structure of the knowledge graph and incorporating dynamic constraints&#8212;such as real-time resolution thresholds or environmental conditions&#8212;to further enhance the precision, flexibility, and robustness of the recommendation system.</p></sec><sec id="sec4dot2-sensors-25-05575"><title>4.2. Stability Analysis of Remote Sensing Data Recommendation Based on Knowledge Graph Semantic Path Guidance</title><p>In practical applications, a recommendation system must exhibit robust stability, meaning it should consistently generate similar outputs under varying initialization conditions. To evaluate this aspect, we conducted a series of random seed experiments, using five regular seeds (10, 20, 30, 40, 50) and five irregular seeds (186, 360, 418, 718, 850), to comprehensively assess the model&#8217;s variability across different random initialization scenarios. For each seed, a unified semantic path embedding and recommendation process was applied, and results were evaluated accordingly. Among the tested seeds, Seed 850 demonstrated the best overall performance and was selected as the baseline for subsequent stability analysis.</p><p>To quantify the consistency of recommendation outputs across different seeds, we employed the Jaccard similarity metric [<xref rid="B37-sensors-25-05575" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05575" ref-type="bibr">38</xref>] as the primary evaluation indicator. The corresponding formula is shown in Equation (4):<disp-formula id="FD4-sensors-25-05575"><label>(4)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Jaccard</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#8745;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#8746;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where sets <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic> represent the Top-K sensor recommendations for the same task under two different seed initializations. Jaccard similarity measures the ratio of intersection to union between two sets, making it especially suitable for evaluating the binary set overlap characteristic of recommendation outputs. To comprehensively assess model stability, we further calculated the mean Jaccard similarity (Jaccard Mean) and the variance of Jaccard similarity (Jaccard Variance) across all tasks and seed combinations. A higher Jaccard Mean indicates greater overall consistency, while a lower Jaccard Variance reflects reduced fluctuation between runs. Ideally, the Jaccard Mean should approach 1, and the Jaccard Variance should approach 0, indicating high stability.</p><p><xref rid="sensors-25-05575-f012" ref-type="fig">Figure 12</xref> visualizes the stability results for different remote sensing tasks under Seed 850. The horizontal axis denotes Jaccard Mean, while the vertical axis represents Jaccard Variance. Tasks such as &#8220;fire detection and monitoring&#8221; and &#8220;terrain and geomorphological feature extraction in landslide areas&#8221; appear in the lower-right quadrant, indicating high consistency and low variance in the recommendations&#8212;demonstrating excellent model stability in these scenarios. In contrast, tasks like &#8220;assessment of secondary disasters from landslides&#8221; and &#8220;pre- and post-flood change analysis&#8221; are positioned in the upper-left quadrant, reflecting lower consistency and higher variability, possibly due to the semantic complexity or broader meta-path variability associated with these tasks.</p><p>To further evaluate the effectiveness of the proposed semantic path-based recommendation, we conducted a comparative experiment with a representative graph neural network (GNN)-based approach. <xref rid="sensors-25-05575-t009" ref-type="table">Table 9</xref> summarizes the matched counts across several disaster monitoring tasks. As shown, our method achieves more consistent alignment with task-specific requirements, especially in complex scenarios such as secondary disaster assessment and fire monitoring, where semantic interpretability plays a crucial role. In contrast, the GNN approach, while effective in some straightforward tasks, tends to underperform in cases requiring fine-grained semantic reasoning.</p><p>In addition to stability, computational efficiency is also a critical factor in evaluating the practical viability of recommendation models. To this end, we conducted a runtime comparison between the proposed semantic path-guided embedding model and a standard four-layer Graph Neural Network (GNN) model under identical computational environments. The experimental results indicate that the proposed model requires only 7.75 &#215; 10<sup>&#8722;3</sup> s per epoch on average, while the four-layer GNN model takes 8.89 &#215; 10<sup>&#8722;3</sup> s per epoch. Beyond runtime efficiency, the GNN baseline often produced fewer correct matches in complex disaster scenarios (see <xref rid="sensors-25-05575-t009" ref-type="table">Table 9</xref>), particularly when task-specific spectral requirements had to be explicitly incorporated. In contrast, our semantic path-guided approach not only maintains computational efficiency but also achieves more reliable and interpretable recommendations. This balance of low training overhead and semantic accuracy makes the proposed method especially well-suited for real-time or near-real-time remote sensing data recommendation in time-sensitive disaster scenarios.</p><p>Overall, the model demonstrates strong stability for terrain-related and thermal anomaly related tasks, suggesting that the semantic path embedding strategy is particularly effective in these contexts. However, for more complex or ambiguous tasks, noticeable variations remain, pointing to a need for further optimization in meta-path design and feature constraint mechanisms. Enhancing the stability and generalizability of the system across diverse task scenarios will be a key focus for future improvements.</p></sec><sec id="sec4dot3-sensors-25-05575"><title>4.3. Remote Sensing Data Recommendation System</title><p>To facilitate efficient matching and intelligent acquisition of remote sensing data in natural disaster scenarios, this study develops a recommendation system grounded in a knowledge graph framework. The system adopts a browser&#8211;server (B/S) architecture with front-end/back-end separation, integrating knowledge graph querying, model inference, and visualization functionalities. This design ensures strong scalability, modularity, and interactivity. The overall system architecture is organized into three core functional modules: data processing, recommendation computation, and visual presentation. The data processing module handles user input, queries the Neo4j-based knowledge graph, extracts task-relevant subgraphs, and performs attribute-level preprocessing to ensure the semantic completeness and structural integrity of meta-paths required for recommendation. The visualization module dynamically displays the recommendation paths, outputs the Top-K sensor list, and presents associated metadata including similarity scores, image identifiers, and external data links, thereby supporting interactive query, semantic traceability, and result interpretability.</p><p>To validate the system&#8217;s functionality, a real-world case study is employed: the Henan Zhengzhou flood event on 20 July 2021, with the corresponding remote sensing task defined as &#8220;Flood water body detection and mapping.&#8221; As illustrated in <xref rid="sensors-25-05575-f013" ref-type="fig">Figure 13</xref>, the system automatically generates the task-specific subgraph, invokes the recommendation model, and returns a Top-K list of sensor candidates with detailed attributes. The visualization interface displays a semantic path centered on the task node, linking it to the disaster type, spectral requirements, platform configurations, and resolution constraints, thereby forming a structured and semantically coherent representation that enhances user comprehension and semantic traceability.</p><p>As shown in the figure, the recommended sensors include Gaofen-1 WFV, Gaofen-1 PMS, and Sentinel-1 SAR, covering both optical and microwave modalities. Each sensor is annotated with its semantic similarity score, corresponding remote sensing image ID, acquisition time, and a direct download link, thus completing the loop from task input to data acquisition. The system&#8217;s visualization mechanism significantly improves the transparency, usability, and operational efficiency of the recommendation process, demonstrating the practical viability and application stability of the proposed framework in real-world disaster response scenarios.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05575"><title>5. Conclusions</title><p>This study addresses the challenge of intelligent remote sensing data recommendation in natural disaster scenarios by proposing a comprehensive methodological framework that integrates knowledge graph modeling, semantic path embedding, and recommendation reasoning mechanisms. A hierarchical disaster-oriented remote sensing knowledge graph was constructed to formally define the semantic chain of &#8220;natural disaster &#8594; task demand &#8594; sensor &#8594; remote sensing platform.&#8221; High-order meta-paths, designed based on task semantics, served as structural constraints in the Meta-Path2Vec-based node embedding process, enabling fine-grained semantic mapping between disaster monitoring tasks and observational resources. Through semantic similarity computation and Top-K ranking, the proposed framework achieves a closed-loop pipeline from semantic modeling to intelligent recommendation inference. Experimental evaluations demonstrate that the method achieves promising recommendation accuracy and stability across representative disaster scenarios, including flood monitoring, landslide detection, and wildfire assessment. Furthermore, system implementation confirms the feasibility, scalability, and practical value of the approach in real-world remote sensing applications, providing both a theoretical foundation and an engineering pathway for task-driven, intelligent matching of remote sensing resources.</p><p>Despite the effective integration of knowledge graph structures and path-based semantic embedding for remote sensing recommendation, the current system remains centered at the &#8220;data recommendation&#8221; level, primarily outputting raw imagery datasets. It has yet to incorporate higher-level thematic products, analysis-ready data, or automated processing workflows that are critical for end-to-end disaster response and decision-making. As a result, the system still falls short of supporting the full operational cycle from disaster onset to actionable information delivery. In addition, the recommendation accuracy is currently constrained by the static structure and limited semantic expressiveness of the knowledge graph, which hinders adaptability in handling diverse task types and uncertain or incomplete information. Future work should explore the integration of multi-dimensional constraints&#8212;such as temporal dynamics, spatial resolution requirements, and product-level metadata&#8212;alongside real-time graph updating mechanisms, to evolve the current architecture into a full-process intelligent service system spanning the chain of &#8220;disaster occurrence &#8594; data acquisition &#8594; product generation &#8594; knowledge application.&#8221; This shift&#8212;from data recommendation to content recommendation and workflow orchestration&#8212;would significantly elevate the intelligence, robustness, and applicability of the system, laying the groundwork for next-generation remote sensing recommendation services in complex disaster management scenarios.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to thank the <italic toggle="yes">Sensors</italic> editors and anonymous reviewers for their detailed and constructive comments and suggestions, which greatly helped in improving the quality of this paper.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, X.Z. and C.L. (Chenchen Luo); methodology, X.Z. and C.L. (Chenchen Luo); software, X.Z. and C.L. (Chenchen Luo); validation, X.Z., C.L. (Chenchen Luo), Y.P. and C.L. (Chenxi Li); formal analysis, X.Z. and C.L. (Chenchen Luo); investigation, X.Z. and C.C.; resources, J.Z.; data curation, C.L. (Chenxi Li), Y.P. and C.C.; writing&#8212;original draft preparation, X.Z.; writing&#8212;review and editing, X.Z., C.Z. and Z.W.; visualization, X.Z. and C.L. (Chenchen Luo); supervision, C.Z. and J.Z.; project administration, C.Z. and X.Z.; funding acquisition, C.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data could be shared upon request to the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05575"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Natijne</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Lindenbergh</surname><given-names>R.C.</given-names></name><name name-style="western"><surname>Bogaard</surname><given-names>T.A.</given-names></name></person-group><article-title>Machine learning: New potential for local and regional deep-seated landslide nowcasting</article-title><source>Sensors</source><year>2020</year><volume>5</volume><elocation-id>1425</elocation-id><pub-id pub-id-type="doi">10.3390/s20051425</pub-id><pub-id pub-id-type="pmcid">PMC7085549</pub-id><pub-id pub-id-type="pmid">32151069</pub-id></element-citation></ref><ref id="B2-sensors-25-05575"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Munawar</surname><given-names>H.S.</given-names></name><name name-style="western"><surname>Hammad</surname><given-names>A.W.</given-names></name><name name-style="western"><surname>Waller</surname><given-names>S.T.</given-names></name></person-group><article-title>Remote sensing methods for flood prediction: A review</article-title><source>Sensors</source><year>2022</year><volume>3</volume><elocation-id>960</elocation-id><pub-id pub-id-type="doi">10.3390/s22030960</pub-id><pub-id pub-id-type="pmcid">PMC8838435</pub-id><pub-id pub-id-type="pmid">35161706</pub-id></element-citation></ref><ref id="B3-sensors-25-05575"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>L.</given-names></name></person-group><article-title>An image registration method for multisource high-resolution remote sensing images for earthquake disaster assessment</article-title><source>Sensors</source><year>2020</year><volume>8</volume><elocation-id>2286</elocation-id><pub-id pub-id-type="doi">10.3390/s20082286</pub-id><pub-id pub-id-type="pmcid">PMC7219069</pub-id><pub-id pub-id-type="pmid">32316439</pub-id></element-citation></ref><ref id="B4-sensors-25-05575"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>N.</given-names></name><name name-style="western"><surname>Si</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name></person-group><article-title>A dynamic barycenter bridging network for federated transfer fault diagnosis in machine groups</article-title><source>Mech. Syst. Signal Process.</source><year>2025</year><volume>230</volume><fpage>112605</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2025.112605</pub-id></element-citation></ref><ref id="B5-sensors-25-05575"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>A review for sample datasets of remote sensing imagery</article-title><source>Natl. Remote Sens. Bull.</source><year>2022</year><volume>26</volume><fpage>589</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.11834/jrs.20221162</pub-id></element-citation></ref><ref id="B6-sensors-25-05575"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cong</surname><given-names>R.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name></person-group><article-title>A parallel down-up fusion network for salient object detection in optical remote sensing images</article-title><source>Neurocomputing</source><year>2020</year><volume>415</volume><fpage>411</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2020.05.108</pub-id></element-citation></ref><ref id="B7-sensors-25-05575"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tuia</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Fraundorfer</surname><given-names>F.</given-names></name></person-group><article-title>Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources</article-title><source>IEEE Geosci. Remote Sens. Mag.</source><year>2017</year><volume>5</volume><fpage>8</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1109/MGRS.2017.2762307</pub-id></element-citation></ref><ref id="B8-sensors-25-05575"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>Y.</given-names></name></person-group><article-title>Deep Learning-Based Classification of Hyperspectral Data</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2014</year><volume>7</volume><fpage>2094</fpage><lpage>2107</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2014.2329330</pub-id></element-citation></ref><ref id="B9-sensors-25-05575"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name></person-group><article-title>Study on Urgent Monitoring and Assessment in Wenchuan Earthquake</article-title><source>J. Remote Sens.</source><year>2008</year><volume>6</volume><fpage>858</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.11834/jrs.200806115</pub-id></element-citation></ref><ref id="B10-sensors-25-05575"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ge</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>L.</given-names></name></person-group><article-title>Disaster Prediction Knowledge Graph Based on Multi-Source Spatio-Temporal Information</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>1214</elocation-id><pub-id pub-id-type="doi">10.3390/rs14051214</pub-id></element-citation></ref><ref id="B11-sensors-25-05575"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.</given-names></name></person-group><article-title>Survey of Knowledge Graph Construction Techniques</article-title><source>Comput. Eng.</source><year>2022</year><volume>48</volume><fpage>23</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.19678/j.issn.1000-3428.0061803</pub-id></element-citation></ref><ref id="B12-sensors-25-05575"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>F.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name></person-group><article-title>Survey of Research on Curriculum Knowledge Graph Construction Techniques</article-title><source>Comput. Eng.</source><year>2024</year><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.19678/j.issn.1000-3428.0069543</pub-id></element-citation></ref><ref id="B13-sensors-25-05575"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>T.-H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Real-world data medical knowledge graph: Construction and applications</article-title><source>Artif. Intell. Med.</source><year>2020</year><volume>103</volume><fpage>101817</fpage><pub-id pub-id-type="doi">10.1016/j.artmed.2020.101817</pub-id><pub-id pub-id-type="pmid">32143785</pub-id></element-citation></ref><ref id="B14-sensors-25-05575"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name></person-group><article-title>Relation Extraction for Knowledge Graph Generation in the Agriculture Domain: A Case Study on Soybean Pests and Disease</article-title><source>Appl. Eng. Agric.</source><year>2023</year><volume>39</volume><fpage>215</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.13031/aea.15124</pub-id></element-citation></ref><ref id="B15-sensors-25-05575"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name></person-group><article-title>Remote sensing knowledge graph construction and its application in typical scenarios</article-title><source>Natl. Remote Sens. Bull.</source><year>2023</year><volume>27</volume><fpage>249</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.11834/jrs.20210469</pub-id></element-citation></ref><ref id="B16-sensors-25-05575"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>H.</given-names></name><etal/></person-group><article-title>Geoscience knowledge graph in the big data era</article-title><source>Sci. China Earth Sci.</source><year>2021</year><volume>64</volume><fpage>1105</fpage><lpage>1114</lpage><pub-id pub-id-type="doi">10.1007/s11430-020-9750-4</pub-id></element-citation></ref><ref id="B17-sensors-25-05575"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ji</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cambria</surname><given-names>E.</given-names></name><name name-style="western"><surname>Marttinen</surname><given-names>P.</given-names></name><name name-style="western"><surname>Philip</surname><given-names>S.Y.</given-names></name></person-group><article-title>A survey on knowledge graphs: Representation, acquisition, and applications</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2021</year><volume>33</volume><fpage>494</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2021.3070843</pub-id><pub-id pub-id-type="pmid">33900922</pub-id></element-citation></ref><ref id="B18-sensors-25-05575"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name></person-group><article-title>Construction and Application of a Knowledge Graph</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>2511</elocation-id><pub-id pub-id-type="doi">10.3390/rs13132511</pub-id></element-citation></ref><ref id="B19-sensors-25-05575"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>A.</given-names></name></person-group><article-title>Construction and Application of Flood Disaster Knowledge Graph Based on Multi-modal Data</article-title><source>Geomat. Inf. Sci. Wuhan Univ.</source><year>2023</year><volume>48</volume><fpage>2009</fpage><lpage>2018</lpage><pub-id pub-id-type="doi">10.13203/j.whugis20220509</pub-id></element-citation></ref><ref id="B20-sensors-25-05575"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>C.</given-names></name></person-group><article-title>Multi-source remote sensing landslide hazard identification method driven by knowledge graph</article-title><source>Bull. Surv. Mapp.</source><year>2024</year><volume>0</volume><fpage>12</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.13474/j.cnki.11-2246.2024.0103</pub-id></element-citation></ref><ref id="B21-sensors-25-05575"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Su</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>W.</given-names></name></person-group><article-title>Landslide Identification Method Based on the FKGRNet Model for Remote Sensing Images</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>3407</elocation-id><pub-id pub-id-type="doi">10.3390/rs15133407</pub-id></element-citation></ref><ref id="B22-sensors-25-05575"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>F.</given-names></name></person-group><article-title>Survey of Recommender Systems Based on Graph Learning</article-title><source>Comput. Sci.</source><year>2022</year><volume>49</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.11896/jsjkx.210900072</pub-id></element-citation></ref><ref id="B23-sensors-25-05575"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hailong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Haiyan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pei</surname><given-names>D.</given-names></name></person-group><article-title>Survey of Knowledge Graph Recommendation System Research</article-title><source>J. Front. Comput. Sci. Technol.</source><year>2023</year><volume>17</volume><fpage>771</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.3778/j.issn.1673-9418.2205052</pub-id></element-citation></ref><ref id="B24-sensors-25-05575"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Niu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>Review and perspective of Earth Science Knowledge Graph in Big Data Era</article-title><source>Acta Seismol. Sin.</source><year>2024</year><volume>46</volume><fpage>353</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.11939/jass.20230157</pub-id></element-citation></ref><ref id="B25-sensors-25-05575"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xun</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name></person-group><article-title>Geographical Scenario Knowledge-Informed Graph Structure Attention for Image Segmentation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>63</volume><fpage>4401016</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3521238</pub-id></element-citation></ref><ref id="B26-sensors-25-05575"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name></person-group><article-title>Inductive Meta-Path Learning for Schema-Complex Heterogeneous Information Networks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>10196</fpage><lpage>10209</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2024.3435055</pub-id><pub-id pub-id-type="pmid">39074011</pub-id></element-citation></ref><ref id="B27-sensors-25-05575"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>W.X.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>P.S.</given-names></name></person-group><article-title>Heterogeneous Information Network Embedding for Recommendation</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2019</year><volume>31</volume><fpage>357</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2018.2833443</pub-id></element-citation></ref><ref id="B28-sensors-25-05575"><label>28.</label><element-citation publication-type="book"><std>GB/T 28921-2012</std><source>Classification and Codes for Natural Disasters</source><publisher-name>AQSIQ</publisher-name><publisher-loc>Beijing, China</publisher-loc><publisher-name>Standardization Administration of China</publisher-name><publisher-loc>Beijing, China</publisher-loc><year>2012</year></element-citation></ref><ref id="B29-sensors-25-05575"><label>29.</label><element-citation publication-type="book"><std>GB/T 26376-2010</std><source>Basic Terms on Natural Disaster Management</source><publisher-name>AQSIQ</publisher-name><publisher-loc>Beijing, China</publisher-loc><publisher-name>Standardization Administration of China</publisher-name><publisher-loc>Beijing, China</publisher-loc><year>2011</year></element-citation></ref><ref id="B30-sensors-25-05575"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Research on the Method of Air Traffic Control Instruction Keyword Extraction Based on the Roberta-Attention-BiLSTM-CRF Model</article-title><source>Aerospace</source><year>2025</year><volume>12</volume><elocation-id>376</elocation-id><pub-id pub-id-type="doi">10.3390/aerospace12050376</pub-id></element-citation></ref><ref id="B31-sensors-25-05575"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN</article-title><source>Expert Syst. Appl.</source><year>2017</year><volume>72</volume><fpage>221</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2016.10.065</pub-id></element-citation></ref><ref id="B32-sensors-25-05575"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qiao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ran</surname><given-names>R.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>W.</given-names></name></person-group><article-title>Multi energy carbon emission named entity recognition based on BiLSTM-CRF</article-title><source>Proceedings of the 2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)</source><conf-loc>Zhuhai, China</conf-loc><conf-date>28&#8211;30 June 2024</conf-date><fpage>110</fpage><lpage>113</lpage></element-citation></ref><ref id="B33-sensors-25-05575"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rozemberczki</surname><given-names>B.</given-names></name><name name-style="western"><surname>Allen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sarkar</surname><given-names>R.</given-names></name></person-group><article-title>Multi-Scale attributed node embedding</article-title><source>J. Complex Netw.</source><year>2021</year><volume>9</volume><fpage>cnab014</fpage><pub-id pub-id-type="doi">10.1093/comnet/cnab014</pub-id></element-citation></ref><ref id="B34-sensors-25-05575"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name></person-group><article-title>Improved Skip-Gram Based on Graph Structure Information</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>6527</elocation-id><pub-id pub-id-type="doi">10.3390/s23146527</pub-id><pub-id pub-id-type="pmid">37514822</pub-id><pub-id pub-id-type="pmcid">PMC10383593</pub-id></element-citation></ref><ref id="B35-sensors-25-05575"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>L.</given-names></name></person-group><article-title>Research on the Application of Improved Collaborative Filtering Algorithm in Course Recommendation</article-title><source>Proceedings of the 2023 International Conference on New Trends in Computational Intelligence (NTCI)</source><conf-loc>Qingdao, China</conf-loc><conf-date>3&#8211;5 November 2023</conf-date><fpage>90</fpage><lpage>93</lpage></element-citation></ref><ref id="B36-sensors-25-05575"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qiu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Olofsson</surname><given-names>P.</given-names></name><name name-style="western"><surname>Woodcock</surname><given-names>C.E.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>S.</given-names></name></person-group><article-title>Evaluation of Landsat image compositing algorithms</article-title><source>Remote Sens. Environ.</source><year>2023</year><volume>285</volume><fpage>113375</fpage><pub-id pub-id-type="doi">10.1016/j.rse.2022.113375</pub-id></element-citation></ref><ref id="B37-sensors-25-05575"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Michael</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kozokaro</surname><given-names>G.</given-names></name><name name-style="western"><surname>Brenner</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lensky</surname><given-names>I.M.</given-names></name></person-group><article-title>Improving WRF-Fire Wildfire Simulation Accuracy Using SAR and Time Series of Satellite-Based Vegetation Indices</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>2941</elocation-id><pub-id pub-id-type="doi">10.3390/rs14122941</pub-id></element-citation></ref><ref id="B38-sensors-25-05575"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Murray</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gullick</surname><given-names>D.</given-names></name><name name-style="western"><surname>Blackburn</surname><given-names>G.A.</given-names></name><name name-style="western"><surname>Whyatt</surname><given-names>J.D.</given-names></name><name name-style="western"><surname>Edwards</surname><given-names>C.</given-names></name></person-group><article-title>ARBOR: A new framework for assessing the accuracy of individual tree crown delineation from remotely-sensed data</article-title><source>Remote Sens. Environ.</source><year>2019</year><volume>231</volume><fpage>111256</fpage><pub-id pub-id-type="doi">10.1016/j.rse.2019.111256</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05575-f001" orientation="portrait"><label>Figure 1</label><caption><p>Semantic framework of the disaster knowledge graph for remote sensing data recommendation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g001.jpg"/></fig><fig position="float" id="sensors-25-05575-f002" orientation="portrait"><label>Figure 2</label><caption><p>Example of remote sensing tasks and observation parameters for landslide disasters.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g002.jpg"/></fig><fig position="float" id="sensors-25-05575-f003" orientation="portrait"><label>Figure 3</label><caption><p>Framework of the Remote Sensing Satellite Knowledge System.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g003.jpg"/></fig><fig position="float" id="sensors-25-05575-f004" orientation="portrait"><label>Figure 4</label><caption><p>Ontological Structure of the Concept Layer.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g004.jpg"/></fig><fig position="float" id="sensors-25-05575-f005" orientation="portrait"><label>Figure 5</label><caption><p>Illustration of Satellite Entity and Attributes.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g005.jpg"/></fig><fig position="float" id="sensors-25-05575-f006" orientation="portrait"><label>Figure 6</label><caption><p>Mapping Between Schema Layer and Data Layer.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g006.jpg"/></fig><fig position="float" id="sensors-25-05575-f007" orientation="portrait"><label>Figure 7</label><caption><p>Knowledge Graph Visualization: Disaster Example.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g007.jpg"/></fig><fig position="float" id="sensors-25-05575-f008" orientation="portrait"><label>Figure 8</label><caption><p>Knowledge Graph Visualization: Satellite and Sensor Example.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g008.jpg"/></fig><fig position="float" id="sensors-25-05575-f009" orientation="portrait"><label>Figure 9</label><caption><p>Example of a Meta-Path Structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g009.jpg"/></fig><fig position="float" id="sensors-25-05575-f010" orientation="portrait"><label>Figure 10</label><caption><p>Meta-Path2Vec Training Based on Knowledge Graph.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g010.jpg"/></fig><fig position="float" id="sensors-25-05575-f011" orientation="portrait"><label>Figure 11</label><caption><p>Skip-Gram Embedding Workflow Guided by Semantic Paths.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g011.jpg"/></fig><fig position="float" id="sensors-25-05575-f012" orientation="portrait"><label>Figure 12</label><caption><p>Stability Analysis of Remote Sensing Recommendation Tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g012.jpg"/></fig><fig position="float" id="sensors-25-05575-f013" orientation="portrait"><label>Figure 13</label><caption><p>System Output Example.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05575-g013.jpg"/></fig><table-wrap position="float" id="sensors-25-05575-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t001_Table 1</object-id><label>Table 1</label><caption><p>Observation parameter examples for remote sensing tasks in typical natural disaster scenarios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disaster<break/>Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Remote Sensing<break/>Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Observed<break/>Object</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Observed<break/>Attribute</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Temporal<break/>Resolution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spatial<break/>Resolution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spectral<break/>Bands</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Flood Disaster</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flood water detection and mapping</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flood extent</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Water body boundary, expansion trend</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;24 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10&#8211;100 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visible, Near-infrared</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Estimation of flood depth and extent</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flood depth and boundaries</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Water body thickness, height variation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;24 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10&#8211;100 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thermal infrared, Radar</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pre- and post-flood change analysis</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Water body distribution</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Water coverage changes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#8211;3 days</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30&#8211;100 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visible, Near-infrared</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flood impact assessment</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Affected area</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Disaster extent and distribution</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3&#8211;7 days</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30&#8211;100 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrared, Radar</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Earthquake</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Surface deformation monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Crustal displacement</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D deformation, displacement direction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;12 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#8211;10 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">InSAR, LiDAR</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Crack and fault line tracing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Surface rupture zones</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Crack direction, width, length</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;24 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3&#8211;5 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visible, Thermal infrared</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Damage assessment</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Buildings and infrastructure</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Collapse rate, degree of damage</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#8211;3 days</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5&#8211;3 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visible, Near-infrared, Radar</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Secondary disaster detection</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Landslides/lakes/liquefaction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scope and dynamics of secondary disasters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;24 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10&#8211;30 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visible, Thermal infrared, Radar</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Post-disaster recovery and reconstruction</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rebuilding area</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Land use change, engineering progress</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weeks&#8211;Months</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#8211;30 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visible, Multi-temporal imagery</td></tr><tr><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Forest/Grassland Fire</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fire point detection and monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hotspots and ignition areas</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thermal anomalies</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;12 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30&#8211;100 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thermal infrared</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burned area and fireline mapping</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burned area boundaries</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High-temperature regions</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;24 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10&#8211;30 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visible, Thermal infrared</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fire intensity and behavior analysis</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fire dynamics</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fireline movement trajectories</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;24 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30&#8211;100 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visible, Thermal infrared</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smoke and pollutant dispersion monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smoke and gas diffusion</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smoke concentration and dispersion trend</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;24 h</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30&#8211;500 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thermal infrared, Visible</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Post-fire assessment and damage evaluation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burned zones</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burned area size</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3&#8211;7 days</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10&#8211;50 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrared</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05575-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t002_Table 2</object-id><label>Table 2</label><caption><p>Data Sources.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Format(s)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Content</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Source</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Natural<break/>Disaster Event<break/>Data</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">.txt;<break/>.xls;<break/>.pdf</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Disaster type;<break/>occurrence time;<break/>affected area;<break/>impact status;<break/>observation parameters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Disaster monitoring reports;<break/>academic literature;<break/>national standards;<break/>historical disaster records</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Satellite<break/>And<break/>Sensor<break/>Information</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">.csv;<break/>.json;<break/>.pdf;<break/>.xls</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Satellite name;<break/>orbital parameters;<break/>observation frequency;<break/>onboard sensors;<break/>sensor specifications<break/>(spectral range, resolution, et al.)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Databases from space agencies such as NASA, ESA, CNSA;<break/>satellite manuals;<break/>technical documents;<break/>official websites</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Remote<break/>Sensing<break/>Image<break/>Metadata</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">.csv;<break/>.mtl;<break/>.txt</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metadata of multispectral, thermal infrared, and SAR images;<break/>spatial and temporal resolution;<break/>spectral information;<break/>acquisition time</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Geospatial Data Cloud;<break/>China Resources Satellite Data Center;<break/>NOAA;<break/>et al.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Auxiliary<break/>Background<break/>Materials</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">.txt;<break/>.xls;<break/>.pdf</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Typical disaster case;<break/>disaster monitoring models;<break/>satellite applicability analysis;<break/>remote sensing classification standards</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Remote sensing research papers;<break/>government environmental monitoring reports;<break/>international disaster databases</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05575-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t003_Table 3</object-id><label>Table 3</label><caption><p>Knowledge Graph Scale Statistics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Category</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Count</th></tr></thead><tbody><tr><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Nodes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Disaster_Categories</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Research_Objective</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral_Band_Requirement</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Band Type</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral Range</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">195</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Satellite</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Relations</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASSOCIATED_WITH</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">REQUIRES_BAND</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MEET</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BELONGS_TO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">191</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HAVE_BANDS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">232</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EQUIPPED_WITH</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05575-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t004_Table 4</object-id><label>Table 4</label><caption><p>Meta-Path Node Types and Relationships.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Source Node Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Relationship</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Target Node Type</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Disaster_Type (D)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASSOCIATED_WITH</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RS_Task (T)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RS_Task (T)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">REQUIRES_BAND</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral_Band (R)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Band_Type (B)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MEET</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral_Band (R)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral_Range (W)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BELONGS_TO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Band_Type (B)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensor (S)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HAVE_BANDS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spectral_Range (W)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Satellite (P)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EQUIPPED_WITH</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensor (S)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05575-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t005_Table 5</object-id><label>Table 5</label><caption><p>Random Walk Rollback Strategy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Encountered Issue</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Action Taken</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Target node does not exist</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Backtrack to the previous node and select an alternative path</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Relationship does not exist</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Backtrack to the previous node and try a different relation</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No available path</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mark as dead-end and discard the current path</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05575-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t006_Table 6</object-id><label>Table 6</label><caption><p>Statistics of Knowledge Graph Training Data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Type</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Quantity</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of nodes acquired</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">872</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of relationships acquired</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1208</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of walk paths generated</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05575-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t007_Table 7</object-id><label>Table 7</label><caption><p>Word2Vec Model Parameter Settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vector Dimension</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dimension of node embedding vectors; higher dimensions improve representation but increase computational cost</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Window Size</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Size of the Skip-Gram training window, determining the sampling range of neighboring nodes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Negative Samples</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of negative samples, used to enhance training efficiency and embedding quality</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Training Epochs</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of training iterations to ensure sufficient model learning</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning Rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Initial learning rate with exponential decay to ensure stable convergence</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walk Length</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of steps in each random walk, defining the length of the path</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walks per Node</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum number of random walks per starting node, ensuring adequate sample generation</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05575-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t008_Table 8</object-id><label>Table 8</label><caption><p>Validation Analysis of Recommendation Results (Examples).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disaster Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Matched Count</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Task Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Evaluation Notes</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Terrain and geomorphological feature extraction in landslide areas</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C-band multi-polarized SAR, LiDAR, and PMS (2 m panchromatic/8 m multispectral) are effective; MWR is less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Monitoring of landslide surface deformation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LiDAR, C-band multi-polarized SAR, and C-band SAR are effective; SRAL is less effective.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Assessment of secondary disasters triggered by landslides</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 m-resolution WFV and HRV-1/2 are effective; OLCI and stare cameras are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Evaluation of vegetation and ecological impacts of landslides</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HRVIR-1/2 and PMS are effective; GMI and TROPOMI are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precise identification of landslide boundaries</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 m-resolution WFV is effective; HRV-1/2, EMI, and NAOMI are less effective.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fire intensity and behavior analysis</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TIRS, TIRS-2, AHSI, and C-band SAR are all core data sources.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Post-fire damage assessment and loss evaluation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AVHRR/3 and MSI are effective; GMI and TROPOMI are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smoke and pollutant dispersion monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EMI and WFV are effective; stare cameras and NAOMI are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burned area and fireline mapping</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WFV and HRV-1/2 are effective; EMI and NAOMI are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fire point detection and monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TIRS-2, AHSI, TIRS, and ABI are all primary data sources.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05575-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05575-t009_Table 9</object-id><label>Table 9</label><caption><p>Comparative Results of Disaster Task Recommendations between the Proposed Semantic Path-Guided Model and the GNN Baseline.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disaster Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Matched Count (Proposed Model)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Matched Count (GNN)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Evaluation Notes</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Terrain and geomorphological feature extraction in landslide areas</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PMS, MSI, and OLI are effective; <break/>TROPOMI is less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Monitoring of landslide surface deformation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSI, HRVIR-1/2, and OLI-2 are effective;<break/>AVHRR is less effective.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Assessment of secondary disasters triggered by landslides</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OLI is effective;<break/>OLCI, GMI and DPC are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fire intensity and behavior analysis</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C-band SAR (Sentinel-1 and GF-3) is effective;<break/>TIRS-2 and Laser altimeter are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Post-fire damage assessment and loss evaluation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OLI is effective;<break/>OLCI, DPC and VIMS are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smoke and pollutant dispersion monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SLSTR and OLI-2 are effective;<break/>VIIRS and ABI are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Burned area and fireline mapping</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SLSTR and OLI-2 are effective;<break/>VIIRS and ABI are less relevant.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fire point detection and monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2/4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TIRS-2, and ABI are effective;<break/>VIMS and AVHRR/3 are less relevant.</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>