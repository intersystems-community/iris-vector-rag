<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431184</article-id><article-id pub-id-type="pmcid-ver">PMC12431184.1</article-id><article-id pub-id-type="pmcaid">12431184</article-id><article-id pub-id-type="pmcaiid">12431184</article-id><article-id pub-id-type="doi">10.3390/s25175426</article-id><article-id pub-id-type="publisher-id">sensors-25-05426</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>YOLO-AR: An Improved Artificial Reef Segmentation Algorithm Based on YOLOv11</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="Y">Yuxiang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05426" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jiang</surname><given-names initials="T">Tingchen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05426" ref-type="aff">1</xref><xref rid="c1-sensors-25-05426" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Xi</surname><given-names initials="Z">Zhi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05426" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yin</surname><given-names initials="F">Fei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-05426" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="X">Xiuping</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05426" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Komatsu</surname><given-names initials="T">Teruhisa</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05426"><label>1</label>College of Marine Technology and Surveying, Jiangsu Ocean University, Lianyungang 222005, China; <email>2023210202@jou.edu.cn</email> (Y.W.); <email>wxp313@sina.com</email> (X.W.)</aff><aff id="af2-sensors-25-05426"><label>2</label>Lianyungang Water Resources Bureau, Lianyungang 222061, China; <email>xz59138333@163.com</email> (Z.X.); <email>lygshzzbgs@163.com</email> (F.Y.)</aff><author-notes><corresp id="c1-sensors-25-05426"><label>*</label>Correspondence: <email>jiangtc@jou.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5426</elocation-id><history><date date-type="received"><day>02</day><month>8</month><year>2025</year></date><date date-type="rev-recd"><day>19</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>27</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05426.pdf"/><abstract><p>Artificial reefs serve as a crucial measure for preventing habitat degradation, enhancing primary productivity in marine areas, and restoring and increasing fishery resources, making them an essential component of marine ranching development. Accurate identification and detection of artificial reefs are vital for ecological conservation and fishery resource management. To achieve precise segmentation of artificial reefs in multibeam sonar images, this study proposes an improved YOLOv11-based model, YOLO-AR. Specifically, the DCCA (Dynamic Convolution Coordinate Attention) module is introduced into the backbone network to reduce the model&#8217;s sensitivity to complex seafloor environments. Additionally, a small-object detection layer is added to the neck network, along with the ultra-lightweight dynamic upsampling operator DySample (Dynamic Sampling), which enhances the model&#8217;s ability to segment small artificial reefs. Furthermore, some standard convolution layers in the backbone are replaced with ADown (Advanced Downsampling) to reduce the model&#8217;s complexity. Experimental results demonstrate that YOLO-AR achieves an mAP@0.5 of 0.912, an intersection-over-union (IOU) of 0.832, and an F1 score of 0.908. Meanwhile, the parameters and model size of YOLO-AR are 2.67 million and 5.58 MB. Compared to other advanced segmentation models, YOLO-AR maintains a more lightweight structure while delivering a superior segmentation performance. In real-world multibeam sonar images, YOLO-AR can accurately segment artificial reefs, making it highly effective for practical applications.</p></abstract><kwd-group><kwd>artificial reef detection</kwd><kwd>deep learning</kwd><kwd>multibeam sonar images</kwd><kwd>YOLOv11</kwd></kwd-group><funding-group><award-group><funding-source>Jiangsu Province&#8217;s Marine Science and Technology Innovation Project</funding-source><award-id>JSZRHYKJ202201</award-id></award-group><award-group><funding-source>Jiangsu Province&#8217;s Water Conservancy Science and Technology Project</funding-source><award-id>2020058</award-id></award-group><funding-statement>This work was supported by Jiangsu Province&#8217;s Marine Science and Technology Innovation Project [No. JSZRHYKJ202201] and Jiangsu Province&#8217;s Water Conservancy Science and Technology Project [No. 2020058].</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05426"><title>1. Introduction</title><p>Marine ecological security is an important foundation for the development of marine business [<xref rid="B1-sensors-25-05426" ref-type="bibr">1</xref>]. In recent years, the development and use of global marine resources have intensified. This has caused more and more threats to the health of marine ecosystems. To deal with this, people have taken a lot of measures. These measures aim to reduce the bad effects of human activities and restore the damaged marine habitats [<xref rid="B2-sensors-25-05426" ref-type="bibr">2</xref>]. Among them, marine ranches play a significant role in improving the marine ecological environment and promoting sustainable ecological development [<xref rid="B3-sensors-25-05426" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05426" ref-type="bibr">4</xref>]. Artificial reefs are an artificially constructed underwater ecosystem. They have been widely used in many countries and regions. Building and setting up artificial reefs are important technological methods in marine ranching [<xref rid="B5-sensors-25-05426" ref-type="bibr">5</xref>]. The surface and internal environment of artificial reefs provide suitable habitats and hiding places for marine organisms [<xref rid="B6-sensors-25-05426" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05426" ref-type="bibr">7</xref>]. So, they play an important part in increasing biodiversity and helping fishery development [<xref rid="B8-sensors-25-05426" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05426" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05426" ref-type="bibr">10</xref>]. Their three-dimensional structure creates more hiding places and feeding opportunities. It also influences species interactions, such as settlement, competition, and hunting behavior. This helps to improve the underwater environment and the variety of aquatic life. It also helps with the long-term sustainable use of fishery resources [<xref rid="B11-sensors-25-05426" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05426" ref-type="bibr">12</xref>]. Numerous studies have shown that artificial reefs generally help improve fishery resources in nearby areas. This improvement is linked to increased productivity and the attraction and aggregation of fish in the region [<xref rid="B13-sensors-25-05426" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05426" ref-type="bibr">14</xref>]. However, artificial reefs are susceptible to burial by sediment due to water flow erosion, necessitating regular monitoring. Accurately detecting and assessing artificial reefs is of great significance for evaluating marine ranching biological resources [<xref rid="B15-sensors-25-05426" ref-type="bibr">15</xref>]. Traditional methods for monitoring artificial reefs, such as diver-based surveys and underwater cameras, are inefficient, costly, and unsuitable for large-scale reef monitoring [<xref rid="B16-sensors-25-05426" ref-type="bibr">16</xref>]. Sonar uses acoustic wave technology for target detection and can penetrate dark and murky waters, making it a key detection method in underwater environments [<xref rid="B17-sensors-25-05426" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05426" ref-type="bibr">18</xref>]. A multi-beam echo sounder can perform high-precision depth measurement and provide backscatter images of the seabed [<xref rid="B19-sensors-25-05426" ref-type="bibr">19</xref>]. It is currently widely used in seabed measurement [<xref rid="B20-sensors-25-05426" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05426" ref-type="bibr">21</xref>]. While multibeam sonar can efficiently detect artificial reefs on the seafloor, the detection of artificial reefs in sonar images still largely relies on manual identification. Manual detection accuracy depends on individual expertise and image quality, leading to low precision and efficiency. To tackle these challenges, deep learning-based object detection has increasingly attracted research attention. One of the central concerns in current studies is how to refine existing models and develop deep learning frameworks that are better tailored to the specific demands of artificial reef detection.</p></sec><sec id="sec2-sensors-25-05426"><title>2. Related Work</title><p>As marine resources keep developing, underwater target detection has become a key point in both marine engineering and ecological monitoring. Traditional methods for underwater image processing usually depend on manual feature extraction. When this is combined with the complexity of underwater environments, it leads to limited detection accuracy and a poor generalization performance. On the other hand, the appearance of deep learning has brought a hopeful alternative. It has strong abilities in automatic feature learning, and these abilities can help solve those technical problems [<xref rid="B22-sensors-25-05426" ref-type="bibr">22</xref>]. In the past few years, object detection based on deep learning has been used in many different fields. This has led to a lot of studies that aim to improve underwater target detection. Zhang et al. (2025) [<xref rid="B23-sensors-25-05426" ref-type="bibr">23</xref>] proposed a novel detection framework, UUVDNet, which incorporates an enhanced training strategy and attention mechanisms for sonar image detection tasks, improving both detection accuracy and speed. &#352;iaulys et al. (2024) [<xref rid="B24-sensors-25-05426" ref-type="bibr">24</xref>] applied deep learning models to perform segmentation on underwater images of reefs in the southeastern Baltic Sea, estimating the coverage of benthic habitats. Their findings confirm that deep learning models can achieve the required accuracy for underwater sonar image detection while significantly outperforming manual annotation in efficiency. Marre et al. (2020) [<xref rid="B25-sensors-25-05426" ref-type="bibr">25</xref>] designed a custom network that classifies coral reefs with high accuracy. Using this model, they assessed reef biodiversity and ecological conditions through the Coral Assemblage Index (CAI) and Shannon Index, providing a cost-effective and efficient tool for coral reef analysis. Li and Zhang (2024) [<xref rid="B26-sensors-25-05426" ref-type="bibr">26</xref>] proposed a lightweight underwater garbage segmentation network, effectively enabling fine-grained localization and recognition of debris in complex underwater environments, thus contributing to improved marine waste recycling rates. Li et al. (2023) [<xref rid="B27-sensors-25-05426" ref-type="bibr">27</xref>] developed the MA-YOLOv7 network to detect objects in sidescan sonar images. This algorithm achieved a state-of-the-art performance and is suitable for real-world underwater applications. Shi et al. (2024) [<xref rid="B28-sensors-25-05426" ref-type="bibr">28</xref>] proposed an advanced detection framework using EfficientNet as the backbone network, combining efficient feature extraction with multi-scale feature fusion. Their framework demonstrated high accuracy in sonar image target detection and effectively improved the performance under noisy conditions. Qin et al. (2024) [<xref rid="B29-sensors-25-05426" ref-type="bibr">29</xref>] introduced a model called YOLOv7C, which achieved a 1.9% increase in average precision.</p><p>The integration of object detection and segmentation has become a major trend in computer vision. As a representative of single-stage detection frameworks, the YOLO series has demonstrated unique value in underwater scenarios requiring real-time processing, thanks to its efficient detection speed and end-to-end architecture. From YOLOv5 onward, segmentation heads were introduced, marking the framework&#8217;s formal entry into the field of instance segmentation. YOLO has long been a research hotspot in object detection and has now shown an excellent performance in segmentation as well. Yang et al. (2025) [<xref rid="B30-sensors-25-05426" ref-type="bibr">30</xref>] utilized emerging Large Vision Models (LVMs) along with the YOLOv5 model to propose a simple yet powerful teacher&#8211;student framework (TeSF). This framework achieved a good balance between segmentation accuracy and computational efficiency on the Shanghai Metro tunnel surface defect dataset. Lin et al. (2025) [<xref rid="B31-sensors-25-05426" ref-type="bibr">31</xref>] proposed a novel deep learning framework, Multi-Scale Task-Aligned YOLO (MSTA-YOLO), which effectively segments retinal ganglion cells labeled with different markers. Silpalatha et al. (2025) [<xref rid="B32-sensors-25-05426" ref-type="bibr">32</xref>] leveraged the latest innovations in the YOLO architecture to optimize the speed and accuracy of remote sensing image segmentation tasks. Their model accurately handled geometric complexities in image data, achieving improvements in accuracy, precision, recall, and IOU while significantly reducing the processing time compared to traditional segmentation methods. Shams et al. (2025) [<xref rid="B33-sensors-25-05426" ref-type="bibr">33</xref>] used YOLOv8 for broiler segmentation, significantly improving the weight estimation accuracy without requiring size measurements, making the process more efficient and convenient. Shen et al. (2025) [<xref rid="B34-sensors-25-05426" ref-type="bibr">34</xref>] proposed an efficient instance segmentation model, MSA-YOLO, based on YOLOv8, which greatly enhanced the accuracy and robustness of grape peduncle instance segmentation. Su et al. (2022) [<xref rid="B35-sensors-25-05426" ref-type="bibr">35</xref>] introduced a dual-lens model that simultaneously utilizes YOLO and LOGO architectures for quality inspection and segmentation. Their model demonstrated higher efficiency and a superior performance compared to previous approaches, reducing computational demands and improving the generalization and accuracy of computer-aided breast cancer diagnosis. Xu et al. (2025) [<xref rid="B36-sensors-25-05426" ref-type="bibr">36</xref>] enhanced the lightweight segmentation network BiSeNet and integrated it into the YOLOv5 network. Their approach achieved a better balance between accuracy and efficiency in real-world coal mining applications.</p><p>Currently, deep learning-based object detection research has been applied across various domains, and its use in underwater target detection and recognition is attracting increasing attention. However, research on artificial reef group detection remains limited, particularly in the field of artificial reef segmentation models. At present, artificial reef segmentation still faces several challenges. In sonar images, artificial reefs often appear as small targets and may be partially buried by sediment, leading to severe missed detections of small objects. Moreover, sonar images are characterized by strong noise and low contrast, with minimal grayscale differences between artificial reefs and their surroundings, which can easily result in false detections.</p><p>To address these issues and achieve accurate, efficient detection of artificial reefs in sonar images&#8212;facilitating effective reef monitoring&#8212;this study proposes a YOLOv11-based detection framework capable of high-precision segmentation of artificial reefs in multibeam sonar imagery. First, based on the Coordinate Attention (CA) mechanism, a spatial attention module tailored for artificial reef semantic segmentation, termed DCCA, is introduced. The DCCA module is integrated into the backbone network to enhance feature extraction capability. Subsequently, an additional small-object detection layer and an extra detection head are added in the neck. Furthermore, an ultra-lightweight dynamic upsampling operator, DySample, is incorporated into the neck to refine the upsampling process, improving model performance while minimizing computational resource consumption. To further achieve network lightweighting, conventional convolution layers in the backbone are replaced with the ADown downsampling module, reducing model complexity.</p></sec><sec id="sec3-sensors-25-05426"><title>3. Materials and Methods</title><sec id="sec3dot1-sensors-25-05426"><title>3.1. Image Annotation and Dataset Construction</title><p>Deep learning-based segmentation requires a large amount of data for training to achieve a high recognition accuracy. In this study, we utilize the FIO-AR dataset, a multibeam sonar image artificial reef detection dataset created by Dong et al. (2022) [<xref rid="B5-sensors-25-05426" ref-type="bibr">5</xref>]. From this dataset, we select 385 raw multibeam sonar images without augmentation and annotate them using LabelMe. The resolution of these images is 512 &#215; 512. Some of the images and their corresponding annotations are shown in <xref rid="sensors-25-05426-f001" ref-type="fig">Figure 1</xref>. To enhance data complexity and improve model accuracy, we apply data augmentation after image partitioning. The augmentation operations include horizontal and vertical flipping, scaling, Gaussian noise addition, and random brightness adjustment. Among them, the scaling ratio range is 50&#8211;150%, the mean of Gaussian noise is 0, the standard deviation is 0.2, and the brightness adjustment factor range is 0.5&#8211;1.5. As a result, we obtain a total of 3080 images, which are then divided into a training set (2156 images), a validation set (616 images), and a test set (308 images).</p></sec><sec id="sec3dot2-sensors-25-05426"><title>3.2. YOLO-AR Model</title><p>YOLOv11 is the latest model released by Ultralytics. Compared to its predecessor, YOLOv8, its main innovations lie in the introduction of the C3k2 mechanism and the C2PSA module. The C3k2 module is an extension of C2f, functioning as a feature fusion module. C3k2 builds upon the C2f module by introducing a configurable parameter, C3k, which defines the module&#8217;s operational mode. When C3k is set to False, the module behaves like the original C2f with a conventional bottleneck design. When C3k is set to True, the bottleneck is replaced by the C3 module. This configurable design enhances the model&#8217;s adaptability, allowing it to better meet the varying demands of different detection tasks and application scenarios.</p><p>YOLOv11 introduces the C2PSA module&#8212;an enhanced version of the C2f module that integrates the Pointwise Spatial Attention (PSA) mechanism. This integration significantly boosts the model&#8217;s capacity to extract and emphasize critical features. The PSA mechanism uses a multi-head attention structure with a feedforward neural network, enabling the model to focus on important regions and filter out irrelevant information. The feedforward component further refines these features, enabling the network to capture more complex, nonlinear relationships. This type of attention mechanism proves especially effective in processing complex image data, as it helps the model concentrate on essential object characteristics, ultimately enhancing detection performance.</p><p>In this study, YOLOv11 is chosen as the baseline model due to its high accuracy and low parameter count as the latest iteration of the YOLO series. However, when directly applied to artificial reef detection in complex seafloor environments, its performance is suboptimal. This is because multibeam sonar images differ significantly from natural images, exhibiting lower resolution, higher noise levels, fewer distinct target features, and varying artificial reef sizes&#8212;requiring both large and small object detection capabilities. To address these limitations and ensure precise detection, we modify the original YOLOv11 model specifically for artificial reef detection, naming the improved version YOLO-AR. As illustrated in <xref rid="sensors-25-05426-f002" ref-type="fig">Figure 2</xref>, the modified components are highlighted with dashed lines. To enhance the network&#8217;s focus on artificial reef features, we propose a spatial information attention mechanism module called DCCA and integrate it into the backbone network. Detailed information about the DCCA module can be found in <xref rid="sec3dot2dot1-sensors-25-05426" ref-type="sec">Section 3.2.1</xref>. Additionally, to improve the model&#8217;s performance in detecting small artificial reefs in sonar images, we refine the neck structure and modify the upsampling module, with further details available in <xref rid="sec3dot2dot2-sensors-25-05426" ref-type="sec">Section 3.2.2</xref>. Furthermore, to maintain model accuracy while reducing computational complexity, we replace the last two Conv layers in the backbone network with an innovative downsampling module (ADown). This modification enhances network precision while reducing the number of parameters. The specifics of the ADown module are detailed in <xref rid="sec3dot2dot3-sensors-25-05426" ref-type="sec">Section 3.2.3</xref>.</p><sec id="sec3dot2dot1-sensors-25-05426"><title>3.2.1. Improved Backbone Network with DCCA Module</title><p>Artificial reefs usually have specific geometric shapes. The Coordinate Attention mechanism can more accurately capture the edge and spatial distribution characteristics of the target through coordinate information. Coordinate Attention (CA) is an efficient attention module particularly suitable for lightweight network models. Based on traditional channel attention mechanisms (such as the SE module), the CA mechanism introduces spatial position information to generate direction-sensitive attention maps, thereby significantly improving the network&#8217;s capability in precise localization and feature representation. Unlike traditional attention mechanisms that compress features into a single vector via global pooling, CA employs two one-dimensional global pooling operations to encode features along the horizontal and vertical directions separately. This method not only captures long-range dependencies along one direction but also retains positional information in the other direction, which helps accurately characterize the spatial structure of target objects. The Coordinate Attention module encodes direction-aware information through two complementary attention maps&#8212;one emphasizing horizontal spatial relationships and the other focusing on vertical ones. These maps are applied to the input feature map via pixel-wise multiplication, allowing the network to better concentrate on regions of interest. Designed with efficiency in mind, the CA module is both lightweight and flexible, incurring minimal computational cost, which makes it well-suited for dense prediction tasks [<xref rid="B37-sensors-25-05426" ref-type="bibr">37</xref>].</p><p>However, in complex underwater environments&#8212;particularly when dealing with the irregular shapes of artificial reefs and silt accumulation caused by dynamic water flows&#8212;Coordinate Attention (CA) can enhance the weighting of feature channels, but standard convolution remains limited in feature representation when sonar images exhibit high noise and weak textures. In contrast, dynamic convolution can adaptively adjust convolution kernels according to the input features, enabling better adaptation to diverse reef morphologies and noise patterns, thereby improving the model&#8217;s responsiveness to artificial reefs. Moreover, compared with deformable convolution (DCN), dynamic convolution incurs a lower computational overhead and does not require additional learning of sampling locations, making it more suitable for deployment in sonar-based tasks. To address this issue and enhance the segmentation performance, we propose an improved attention mechanism named DCCA (Dynamic Convolution Coordinate Attention). In DCCA, the standard convolution layers in CA are replaced with dynamic convolution operations [<xref rid="B38-sensors-25-05426" ref-type="bibr">38</xref>]. Dynamic convolution is an emerging technique that adaptively generates convolution kernels based on individual input samples. This approach increases the network&#8217;s representational capacity without a proportional rise in computational cost. Unlike traditional static convolution, dynamic convolution utilizes a lightweight parameter generator to produce multiple weight coefficients from input features. These coefficients are subsequently used to dynamically combine a set of predefined convolution kernels. More specifically, each dynamic convolution operation begins by extracting global contextual information through average pooling, followed by a two-layer MLP that outputs kernel weight coefficients. These weights are normalized using a softmax function and utilized to construct the final dynamic convolution kernel. This mechanism captures richer feature relationships, enhances the model&#8217;s representation ability, and maintains a low computational cost. Moreover, dynamic convolution significantly improves model performance in small-sample scenarios, making it particularly effective for segmentation tasks involving complex seabed substrates of artificial reefs.</p><p>The structural diagram of the DCCA module is shown in <xref rid="sensors-25-05426-f003" ref-type="fig">Figure 3</xref>. The specific operation of the DCCA module involves performing two independent one-dimensional global pooling operations on the input feature map in the horizontal and vertical directions, aggregating them into two feature maps with specific directional and positional information, as shown in Equations (1) and (2):<disp-formula id="FD1-sensors-25-05426"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05426"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The two feature maps are concatenated and passed through a convolution operation. The newly generated feature map is then batch normalized and activated non-linearly, producing a feature map <italic toggle="yes">f</italic> &#8712; <italic toggle="yes">R<sup>C</sup></italic><sup>/<italic toggle="yes">r</italic>*1*(<italic toggle="yes">W</italic>+<italic toggle="yes">H</italic>)</sup>. Subsequently, <italic toggle="yes">f</italic> is split along the spatial dimension and processed using dynamic convolution. The outputs are then activated using the sigmoid function to obtain <italic toggle="yes">g<sup>h</sup></italic> and <italic toggle="yes">g<sup>w</sup></italic>, as shown in Equations (3) and (4):<disp-formula id="FD3-sensors-25-05426"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05426"><label>(4)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where the <italic toggle="yes">F<sub>h</sub></italic> and <italic toggle="yes">F<sub>w</sub></italic> are the convolution operations and the <italic toggle="yes">&#963;</italic> is the sigmoid activation layer.</p><p>Finally, the obtained weights are element-wise multiplied with the original input image to generate the weighted attention feature map.</p><p>The DCCA model not only captures directional and positional information but also enhances sensitivity to diverse features through the high parameter capacity of dynamic convolution. This improvement translates into better segmentation accuracy and more precise target region identification in experiments, particularly in marine environments with complex textures and sparse targets.</p></sec><sec id="sec3dot2dot2-sensors-25-05426"><title>3.2.2. Small-Target Detection Neck with DySample Module</title><p>Due to the varied shapes of artificial reefs, some are relatively small in size. Additionally, as a result of sediment deposition caused by underwater currents, portions of artificial reefs may be buried by silt. The network faces challenges in effectively detecting these small artificial reefs. To enhance the detection of small targets in artificial reef segmentation, we improved the neck structure of the network. Traditional YOLO models perform predictions at different feature map scales; however, the large downsampling ratios limit the resolution for small targets, reducing detection accuracy. To address this issue, we designed a new branch in the network&#8217;s neck structure with a lower downsampling rate to preserve more detail information. Additionally, we introduced an extra detection head in the neck to better capture small target features.</p><p>Specifically, we incorporated an additional shallow feature map into the original network as input. After applying convolution operations for channel compression and feature fusion, a dedicated feature map for small-target detection was generated. This feature map integrates deep information with shallow spatial information, improving the model&#8217;s perception of small targets while maintaining computational efficiency. Finally, this branch, along with other scale-specific output layers, contributes to object detection, enhancing the model&#8217;s overall performance across multiple scales.</p><p>Furthermore, we replaced the upsampling module in the neck structure with DySample [<xref rid="B39-sensors-25-05426" ref-type="bibr">39</xref>], an efficient and lightweight dynamic upsampling module designed to address the high computational complexity and implementation challenges of existing dynamic upsampling methods. Its structure is illustrated in <xref rid="sensors-25-05426-f004" ref-type="fig">Figure 4</xref>, where <xref rid="sensors-25-05426-f004" ref-type="fig">Figure 4</xref>a represents the structure of DySample, based on dynamic sampling, where the sampling set is generated by a sampling point generator, and the input features are resampled by a grid sampling function. <xref rid="sensors-25-05426-f004" ref-type="fig">Figure 4</xref>b illustrates two types of sampling point generators: the static range factor version, in which offsets are generated by a linear layer, and the dynamic range factor version, in which a range factor is first generated and then used to modulate the offsets. Unlike FADE and SAPA, which rely on dynamic convolution kernels, DySample focuses on point sampling and resamples the feature map by generating content-aware offsets. This design avoids the high computational cost of dynamic convolution, eliminates the need for high-resolution guidance features, and is implemented using PyTorch&#8217;s built-in functions without requiring additional CUDA optimizations. As a result, DySample significantly reduces the model&#8217;s parameter count, computation cost, and memory usage. DySample achieves a computational efficiency comparable to traditional bilinear interpolation while delivering superior performance improvements in various dense prediction tasks. In artificial reef segmentation, conventional upsampling modules may introduce blurring or pseudo-textures in sonar noise scenarios, whereas DySample dynamically generates sampling weights based on local features and adaptively adjusts sampling points. This allows it to effectively capture the complex boundaries and textural details of reefs in challenging underwater environments. Its dynamic sampling mechanism ensures that the upsampled feature maps are more accurately aligned with the true boundaries, thereby improving segmentation accuracy and detail preservation. Moreover, unlike multi-scale feature fusion refinements, DySample does not introduce additional feature branches but directly enhances the quality of upsampling, reducing redundant computation. Its lightweight design is particularly well-suited for processing large-scale, high-resolution underwater data, in improving computational efficiency while lowering hardware resource requirements.</p></sec><sec id="sec3dot2dot3-sensors-25-05426"><title>3.2.3. Reduce Network Parameters with ADown Module</title><p>Sonar images have a high resolution. Directly using deep trunks to extract features would introduce a large amount of redundant computation. In YOLOv9, there is an ADown (Advanced Downsampling) module [<xref rid="B40-sensors-25-05426" ref-type="bibr">40</xref>]. In a novel development, it combines average pooling and max pooling in a smart way. This helps to extract and keep valuable feature information more effectively. We achieve network lightweighting by replacing the downsampling module in the network backbone with ADown, and thereby reducing model complexity while retaining key information.</p><p>As shown in <xref rid="sensors-25-05426-f005" ref-type="fig">Figure 5</xref>, the ADown module starts by using average pooling on the input feature map. After that, the feature map is split equally along the channel dimension. One half goes through a 3 &#215; 3 convolution for downsampling. The other half is first processed by max pooling and then goes through a 1 &#215; 1 convolution. This dual-branch design allows the model to use different types of feature cues at the same time. Average pooling can produce a smoothed and global representation. On the other hand, max pooling highlights sharp and prominent features. The outputs from these two branches are finally combined. This results in a feature map that has a lot of details and structural diversity.</p><p>Compared to traditional downsampling methods, ADown has several important advantages. Its design can not only make it more flexible to keep the spatial and channel-level information but also maintain computational efficiency. The parallel-branch structure of the module allows it to reduce the resolution. At the same time, it can keep the important characteristics of features. This makes it very suitable for applications like object detection and image segmentation. In these applications, it is crucial to preserve detailed features. In actual application, ADown has shown really great effectiveness. Standard downsampling operations often lead to information loss. It addresses this common problem. By doing so, it can provide more representative feature maps to the downstream network layers. As a result, it can improve the model&#8217;s performance in complex visual tasks.</p></sec></sec></sec><sec sec-type="results" id="sec4-sensors-25-05426"><title>4. Results</title><sec id="sec4dot1-sensors-25-05426"><title>4.1. Experimental Hardware Configuration and Parameter Setting</title><p>The hardware and software configurations, along with experimental parameters, are detailed in <xref rid="sensors-25-05426-t001" ref-type="table">Table 1</xref>. The experiments were conducted on a system equipped with an Intel Core i7-9750H manufactured by Intel Corporation (Santa Clara, CA, USA) and an NVIDIA RTX 4090D GPU (24 GB memory) manufactured by NVIDIA Corporation (Santa Clara, CA, USA). The software environment included Python 3.8, PyTorch 2.0.0, and CUDA 11.8. For training, the following parameter settings were adopted: an initial learning rate of 0.01, a batch size of 16, 200 training epochs, a momentum value of 0.937, and a weight decay of 0.0005. To ensure the consistency and reliability of the results, all experiments were conducted under the same hardware and parameter settings throughout the research process, and all models in this study were trained from scratch.</p></sec><sec id="sec4dot2-sensors-25-05426"><title>4.2. Precision Evaluation Index</title><p>In deep learning-based segmentation tasks, commonly used accuracy evaluation metrics include precision (P), recall (R), mean average precision (mAP), F1 score, and intersection over union (IOU).</p><p>Precision refers to the proportion of instances predicted as positive samples that are actually positive. A high precision score indicates that the model is more accurate in predicting positive samples. The calculation formula is as follows:<disp-formula id="FD5-sensors-25-05426"><label>(5)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Recall refers to the proportion of actual positive samples that the model correctly identifies as positive. A high recall score indicates that the model can detect most positive samples with a low false negative rate. The calculation formula is as follows:<disp-formula id="FD6-sensors-25-05426"><label>(6)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where TP (True Positive) represents the number of correctly predicted positive samples, FP (False Positive) represents the number of incorrectly predicted positive samples, and FN (False Negative) represents the number of actual positive samples that were incorrectly predicted as negative.</p><p>The mean average precision (mAP) is used to measure the model&#8217;s detection accuracy across different thresholds. mAP is the average of the average precision (AP) values across all categories. Since this study involves a single class of reef targets, mAP is equal to AP. AP is computed as the area under the precision&#8211;recall (P-R) curve using integration, and its formula is as follows:<disp-formula id="FD7-sensors-25-05426"><label>(7)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8747;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The F1 score combines precision and recall and is their harmonic mean. The F1 score is calculated as follows:<disp-formula id="FD8-sensors-25-05426"><label>(8)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Intersection over union (IOU) is used to measure the overlap between the model&#8217;s predicted region and the ground-truth region. The IOU calculation formula is as follows:<disp-formula id="FD9-sensors-25-05426"><label>(9)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec4dot3-sensors-25-05426"><title>4.3. Analysis of Artificial Reef Segmentation Results</title><p>Accurately detecting artificial reefs in multibeam sonar images is crucial for reef deployment and management. To evaluate the detection performance of YOLO-AR, artificial reefs were detected in the test set of multibeam sonar images. <xref rid="sensors-25-05426-f006" ref-type="fig">Figure 6</xref> presents the detection results of artificial reefs using YOLOv11 and YOLO-AR. As shown in <xref rid="sensors-25-05426-f006" ref-type="fig">Figure 6</xref>, the yellow boxes indicate missed detections by YOLOv11, while YOLO-AR accurately detects these reefs. YOLO-AR effectively segments artificial reefs in multibeam sonar images, demonstrating a superior performance over YOLOv11. Furthermore, YOLO-AR can be applied to detect most continuous artificial reef groups.</p><p>To further quantitatively evaluate the artificial reef detection capability of YOLO-AR, <xref rid="sensors-25-05426-t002" ref-type="table">Table 2</xref> presents the detection results of YOLOv11 and YOLO-AR. As shown in <xref rid="sensors-25-05426-t002" ref-type="table">Table 2</xref>, the precision, recall, mAP@0.5, mAP@0.5-0.95, IOU, and F1 score of YOLO-AR reached 0.939, 0.879, 0.912, 0.601, 0.832, and 0.908, respectively, which are 0.046, 0.085, 0.059, 0.048, 0.107, and 0.07 higher than those of YOLOv11. Additionally, the parameter count was reduced by 162,259. These results indicate that, compared to YOLOv11, the YOLO-AR model not only achieves a better artificial reef detection performance with higher accuracy but also requires fewer parameters.</p></sec><sec id="sec4dot4-sensors-25-05426"><title>4.4. Visual Evaluation of YOLO-AR by Grad-CAM</title><p>To fully evaluate the performance of YOLO-AR in detecting artificial reefs in multibeam sonar images, we visualized the heatmaps generated by Gradient-weighted Class Activation Mapping (Grad-CAM) for YOLO-AR and YOLOv11, as shown in <xref rid="sensors-25-05426-f007" ref-type="fig">Figure 7</xref>. These heatmaps provide a visualization of artificial reef detection results. All sonar images in <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref> are sourced from the test set of the artificial reef detection dataset. Warm colors in the heatmaps highlight regions with a greater contribution to artificial reef detection.</p><p>As shown in <xref rid="sensors-25-05426-f007" ref-type="fig">Figure 7</xref>a,b, when using YOLOv11 to segment artificial reefs, the model is highly sensitive to background interference, leading to reduced attention to the reef structures. This is a major reason why small artificial reefs are often missed or misclassified when using YOLOv11 for detection. However, after optimization, the heatmaps of YOLO-AR reveal a significant enhancement in attention to artificial reef regions while suppressing irrelevant background information (see <xref rid="sensors-25-05426-f007" ref-type="fig">Figure 7</xref>c,d). This observation confirms the effectiveness of the optimizations, demonstrating that YOLO-AR can strengthen its focus on artificial reefs while mitigating background interference. These optimizations prove that the model can effectively utilize contextual information, enhancing attention to artificial reefs while reducing distractions from irrelevant background regions. The visualization results from Grad-CAM confirm that these improvements contribute to a better artificial reef detection performance in multibeam sonar images.</p></sec><sec id="sec4dot5-sensors-25-05426"><title>4.5. Model Parameter Evaluation</title><p>To evaluate the model size, <xref rid="sensors-25-05426-t003" ref-type="table">Table 3</xref> presents the parameter count, model size, and FLOPs of six mainstream segmentation models (YOLOv8, YOLOv9, U-Net, SegNet, FCN, and YOLO-AR). FLOPs (floating point operations) represent the number of computations required by an algorithm and serve as a measure of its complexity. The parameter counts of the six segmentation models are 3.26 million, 27.84 million, 24.59 million, 29.46 million, 18.64 million, and 2.67 million, with YOLO-AR having the fewest parameters among them. The model sizes of the six segmentation models are 6.46 MB, 106.91 MB, 94.97 MB, 337.45 MB, 269.74 MB, and 5.58 MB, with YOLO-AR being the smallest. The FLOPs for the six models are 12.1 G, 159.1 G, 361.85 G, 327.13 G, 203.99 G, and 23.2 G, where YOLO-AR has slightly higher FLOPs than YOLOv8 but significantly lower than the other four models. The results demonstrate that YOLO-AR has the fewest parameters and the smallest model size. Although its FLOPs are slightly higher than YOLOv8, it remains the most lightweight among the six segmentation models overall.</p></sec><sec id="sec4dot6-sensors-25-05426"><title>4.6. Performance Comparison Experiment of the Mainstream Segmentation Model</title><p>To further evaluate the performance of YOLO-AR, we compared it with mainstream segmentation models, including FCN, U-Net, YOLOv12, SegNet, and YOLOv8. <xref rid="sensors-25-05426-t004" ref-type="table">Table 4</xref> presents the evaluation metric comparisons of these six segmentation models. The mAP@0.5 values for the six segmentation models are 0.842, 0.851, 0.820, 0.822, 0.718, and 0.912, respectively. YOLO-AR achieves the highest mAP among all the models, indicating its superior ability to accurately segment the target. The IOU values for the six segmentation models are 0.714, 0.733, 0.747, 0.798, 0.683, and 0.832, respectively. YOLO-AR has the highest IOU value among the six models, demonstrating the highest spatial overlap between predictions and ground truth, with the most precise boundary alignment. The F1 scores for the six segmentation models are 0.833, 0.846, 0.855, 0.888, 0.812, and 0.908, respectively. YOLO-AR achieves the highest F1 score, indicating the best balance between segmentation confidence and minimizing missed detections. The experimental results show that among the six segmentation models, YOLO-AR has the best performance, making it the most suitable for artificial reef segmentation in multi-beam sonar images.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05426"><title>5. Discussion</title><sec id="sec5dot1-sensors-25-05426"><title>5.1. Ablation Experiment</title><p>The ablation experiment was conducted by selectively disabling the DCCA module, ADown module, and the improved neck structure (denoted as DNeck) in YOLO-AR to observe their impact on performance. This approach aimed to validate the necessity of these feature enhancements. Additionally, to ensure the accuracy of the ablation experiments, all tests were performed under the same environment and with identical hyperparameters. The results of the ablation study for the YOLO-AR model are presented in <xref rid="sensors-25-05426-t005" ref-type="table">Table 5</xref>. Notably, a &#10003; symbol indicates an enabled module, while a &#215; symbol represents a disabled module.</p><p>From the results in <xref rid="sensors-25-05426-t005" ref-type="table">Table 5</xref>, it is evident that the DCCA module, ADown module, and the improved neck structure all provided positive contributions to artificial reef detection. Specifically, after incorporating the DCCA module into the model, the recall, mAP@0.5, mAP@[0.5:0.95], and IOU increased by 0.012, 0.008, 0.012, and 0.009, respectively. When improving the neck structure of YOLOv11, the precision, recall, mAP@0.5, mAP@0.5-0.95, IOU, and F1 scores increased by 0.032, 0.069, 0.046, 0.011, 0.076, and 0.052, respectively. However, the introduction of DCCA and the improvement in neck structure have led to an increase of 0.1 million and 0.08 million parameters, respectively.</p><p>To address this, the ADown module was introduced into the network. Although this slightly reduced some evaluation metrics, it significantly reduced the model&#8217;s parameter count by 0.34 million. After integrating ADown into the network and improving the neck structure, the precision, recall, mAP@0.5, mAP@[0.5:0.95], IOU, and F1 scores reached 0.930, 0.866, 0.902, 0.581, 0.813, and 0.897, respectively, outperforming the configuration with only the improved neck structure. This improvement is likely because the ADown module, by reducing feature map dimensions and working with the small-object detection layer, was able to extract more discriminative features. Meanwhile, DySample facilitated a more effective recovery of high-resolution details during the upsampling process. This combination likely helped strike a better balance between high-level features and low-level details, thereby improving the overall model accuracy. Without ADown, feature complexity and redundancy could increase, leading to performance degradation. However, incorporating ADown helped compress the network and extract more useful features, synergizing well with the optimized neck structure to achieve a superior performance.</p><p>For the final YOLO-AR model, which integrates the DCCA module, ADown module, and the improved neck structure, the precision, recall, mAP@0.5, mAP@[0.5:0.95], IOU, and F1 scores reached 0.939, 0.879, 0.912, 0.601, 0.832, and 0.908, respectively, achieving the best performance among all tested models. Additionally, this configuration maintained a relatively small parameter count of 2.67 million while achieving the highest accuracy. The ablation study results confirm the effectiveness of the DCCA module, ADown module, and DNeck structure in the YOLO-AR model.</p></sec><sec id="sec5dot2-sensors-25-05426"><title>5.2. Visual Evaluation of Artificial Reef Segmentation by Different Models</title><p>To evaluate the artificial reef segmentation performance of YOLO-AR on real sonar images, a visual comparison was conducted between YOLO-AR and five other segmentation models using test set images. As shown in <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref>, we plotted a binary segmentation graph based on the segmentation results of the six models to better observe the segmentation effects of each model. In <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref>, the first column shows the sonar images in the test set, and the second column presents the true labels of the artificial reefs. The following six columns display the artificial reef segmentation results of YOLO-AR, YOLOv9, YOLOv8, U-Net, SegNet, and FCN. The red rectangles in the figure indicate regions where the models exhibit a poor segmentation performance for artificial reefs.</p><p>In <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref>a, YOLOv8 fails to detect artificial reefs located at the edges of the image. SegNet produces incomplete detections for continuous reefs in the image. YOLOv9, U-Net, and FCN struggle with detecting smaller reef targets. Although YOLO-AR also exhibits some deficiencies in reef segmentation within the red rectangles, it performs the best among the six segmentation models. In <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref>b, YOLOv8, YOLOv9, and FCN incorrectly classify background areas as artificial reefs. U-Net and SegNet fail to detect large portions of small reef targets. YOLO-AR accurately detects large clusters of small reef targets in the image. In <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref>c, YOLOv8, YOLOv9, and U-Net misclassify the background within the red rectangle as reefs. U-Net struggles with segmenting closely spaced reefs, resulting in merging errors. SegNet and FCN suffer from missed detections. YOLO-AR, on the other hand, provides segmentation results that closely match the actual reef boundaries. In <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref>d, YOLOv8 produces incorrect detections, while YOLOv9 and U-Net fail to effectively segment closely spaced reefs. SegNet misses large portions of small reef targets. YOLO-AR successfully segments both small reefs and closely spaced reefs with high accuracy. In <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref>e, YOLOv8 and U-Net mistakenly classify background areas as reefs, while YOLOv9, SegNet, and FCN suffer from missed detections. YOLO-AR&#8217;s segmentation results closely align with the ground truth labels. In <xref rid="sensors-25-05426-f008" ref-type="fig">Figure 8</xref>f, YOLOv8, U-Net, and FCN struggle to segment closely spaced reefs, with U-Net also misclassifying parts of the background as artificial reefs. YOLOv9 and SegNet exhibit missed detections. YOLO-AR does not suffer from these issues and delivers an excellent segmentation performance.</p><p>The visual evaluation results demonstrate that YOLO-AR outperforms the other five segmentation models in segmentation of artificial reefs in multibeam sonar images. YOLO-AR accurately detects both large and small reef targets and effectively segments the boundaries of closely spaced reefs.</p></sec><sec id="sec5dot3-sensors-25-05426"><title>5.3. Research Limitations and Future Prospects</title><p>The proposed model achieves efficient and accurate segmentation of artificial reefs in multibeam sonar images. However, this study still has certain limitations. First, artificial reefs exist in various shapes, such as cubes and triangular pyramids, but the dataset used in this work contains only a single reef type, meaning it is currently unable to differentiate multiple categories of reefs. Second, due to hydrodynamic erosion, artificial reefs may gradually become buried over time, leading to variations in reef sizes within images, which poses a challenge for accurate detection. Although a small-object detection layer was introduced to address this issue, its specific effectiveness remains unclear. In the future, we plan to conduct in situ surveys in marine ranches to collect more diverse artificial reef data, enabling the model to distinguish reefs of different shapes and conditions. Additionally, during data annotation, reefs of different sizes will be labeled separately to better investigate the impact of various improvement strategies. Furthermore, by incorporating quantitative indicators such as the number or area of detected reefs, the model could be extended to estimate the utilization rate of artificial reef areas, thereby contributing to more effective planning and maintenance of artificial reef structures.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05426"><title>6. Conclusions</title><p>Accurate detection of artificial reefs in sonar images using deep learning models is of great significance for artificial reef monitoring and maintenance. In this study, we propose a deep learning model, YOLO-AR, capable of precise and efficient edge detection of artificial reefs. Built upon YOLOv11, YOLO-AR incorporates several key improvements. First, we introduce the attention module DCCA to enhance the model&#8217;s focus on artificial reef regions. Second, we modify the network&#8217;s neck structure by adding a small-object detection layer and integrating the dynamic upsampling module DySample, improving the model&#8217;s ability to detect smaller reefs. Finally, to reduce the parameter count, we replace some convolutional layers in the backbone with the lightweight ADown downsampling module. Experimental results demonstrate that YOLO-AR achieves a precision of 0.939, a recall of 0.879, an mAP@0.5 of 0.912, an mAP@[0.5:0.95] of 0.601, an IOU of 0.832, and an F1 score of 0.908, with a parameter count of only 2.67 million. Comparisons with mainstream deep learning models show that YOLO-AR achieves a high level of accuracy while maintaining the smallest model size and the lowest parameter count, effectively balancing detection accuracy and model complexity. It accurately detects artificial reefs of different sizes, whether they are continuous or isolated. In future work, we plan to expand the dataset and further optimize YOLO-AR to adapt the model for detecting and distinguishing various types of artificial reefs.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors express their gratitude to Hao Liu from Lianyungang Water Resources Bureau for his assistance in visualization and to Yang Fan from Lianyungang Port Holding Group Gan Yu Co., Ltd. for his help in resource provision.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Y.W. and T.J.; methodology, Y.W.; validation, T.J.; formal analysis, F.Y.; investigation, Z.X.; writing&#8212;original draft preparation, Y.W.; writing&#8212;review and editing, T.J. and Z.X.; supervision, F.Y. and X.W.; project administration, T.J.; funding acquisition, T.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The dataset of this article and the FIO-AR dataset can be downloaded at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://pan.baidu.com/s/1nCqWAKxWE6kC4pAPzfPtFw">https://pan.baidu.com/s/1nCqWAKxWE6kC4pAPzfPtFw</uri> (password: abcd) (accessed on 30 July 2025). The source code of YOLO-AR can be obtained from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05426"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ni</surname><given-names>J.</given-names></name></person-group><article-title>Evolution of Marine Ecology-Industry Symbiosis Patterns and Ecological Security Assessment: New Evidence from Coastal Areas of China</article-title><source>Ocean. Coast. Manag.</source><year>2024</year><volume>247</volume><fpage>106939</fpage><pub-id pub-id-type="doi">10.1016/j.ocecoaman.2023.106939</pub-id></element-citation></ref><ref id="B2-sensors-25-05426"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Komyakova</surname><given-names>V.</given-names></name><name name-style="western"><surname>Chamberlain</surname><given-names>D.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>G.P.</given-names></name><name name-style="western"><surname>Swearer</surname><given-names>S.E.</given-names></name></person-group><article-title>Assessing the Performance of Artificial Reefs as Substitute Habitat for Temperate Reef Fishes: Implications for Reef Design and Placement</article-title><source>Sci. Total Environ.</source><year>2019</year><volume>668</volume><fpage>139</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.scitotenv.2019.02.357</pub-id><pub-id pub-id-type="pmid">30852192</pub-id></element-citation></ref><ref id="B3-sensors-25-05426"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>S.</given-names></name></person-group><article-title>Regional Patterns and Factors Analysis of the Sustainable Development of Benefits in China&#8217;s National-Level Marine Ranching: Based on Shellfish and Algae</article-title><source>J. Clean. Prod.</source><year>2024</year><volume>467</volume><fpage>142994</fpage><pub-id pub-id-type="doi">10.1016/j.jclepro.2024.142994</pub-id></element-citation></ref><ref id="B4-sensors-25-05426"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Evolution of Marine Ranching Policies in China: Review, Performance and Prospects</article-title><source>Sci. Total Environ.</source><year>2020</year><volume>737</volume><fpage>139782</fpage><pub-id pub-id-type="doi">10.1016/j.scitotenv.2020.139782</pub-id><pub-id pub-id-type="pmid">32521364</pub-id></element-citation></ref><ref id="B5-sensors-25-05426"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>F.</given-names></name></person-group><article-title>Artificial Reef Detection Method for Multibeam Sonar Imagery Based on Convolutional Neural Networks</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>4610</elocation-id><pub-id pub-id-type="doi">10.3390/rs14184610</pub-id></element-citation></ref><ref id="B6-sensors-25-05426"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Francescangeli</surname><given-names>M.</given-names></name><name name-style="western"><surname>Toma</surname><given-names>D.M.</given-names></name><name name-style="western"><surname>Mendizabal</surname><given-names>V.</given-names></name><name name-style="western"><surname>Carandell</surname><given-names>M.</given-names></name><name name-style="western"><surname>Martinez</surname><given-names>E.</given-names></name><name name-style="western"><surname>Martin</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mura</surname><given-names>M.P.</given-names></name><name name-style="western"><surname>Aguzzi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gil Espert</surname><given-names>L.</given-names></name><name name-style="western"><surname>Del Rio</surname><given-names>J.</given-names></name></person-group><article-title>Artificial Reef Based Ecosystem Design and Monitoring</article-title><source>Ecol. Eng.</source><year>2025</year><volume>221</volume><fpage>107752</fpage><pub-id pub-id-type="doi">10.1016/j.ecoleng.2025.107752</pub-id></element-citation></ref><ref id="B7-sensors-25-05426"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Song</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name></person-group><article-title>Improving Costal Marine Habitats in the Northern Yellow Sea: The Role of Artificial Reefs on Macrobenthic Communities and Eco-Exergy</article-title><source>Sci. Total Environ.</source><year>2025</year><volume>971</volume><fpage>179027</fpage><pub-id pub-id-type="doi">10.1016/j.scitotenv.2025.179027</pub-id><pub-id pub-id-type="pmid">40058015</pub-id></element-citation></ref><ref id="B8-sensors-25-05426"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Falc&#227;o</surname><given-names>M.</given-names></name><name name-style="western"><surname>Santos</surname><given-names>M.N.</given-names></name><name name-style="western"><surname>Drago</surname><given-names>T.</given-names></name><name name-style="western"><surname>Serpa</surname><given-names>D.</given-names></name><name name-style="western"><surname>Monteiro</surname><given-names>C.</given-names></name></person-group><article-title>Effect of Artificial Reefs (Southern Portugal) on Sediment&#8211;Water Transport of Nutrients: Importance of the Hydrodynamic Regime</article-title><source>Estuar. Coast. Shelf Sci.</source><year>2009</year><volume>83</volume><fpage>451</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1016/j.ecss.2009.04.028</pub-id></element-citation></ref><ref id="B9-sensors-25-05426"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>D.</given-names></name><name name-style="western"><surname>Woo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yoon</surname><given-names>H.-S.</given-names></name><name name-style="western"><surname>Na</surname><given-names>W.-B.</given-names></name></person-group><article-title>Efficiency, Tranquillity and Stability Indices to Evaluate Performance in the Artificial Reef Wake Region</article-title><source>Ocean Eng.</source><year>2016</year><volume>122</volume><fpage>253</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1016/j.oceaneng.2016.06.030</pub-id></element-citation></ref><ref id="B10-sensors-25-05426"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name></person-group><article-title>Numerical Study of Efficiency Indices to Evaluate the Effect of Layout Mode of Artificial Reef Unit on Flow Field</article-title><source>J. Mar. Sci. Eng.</source><year>2021</year><volume>9</volume><elocation-id>770</elocation-id><pub-id pub-id-type="doi">10.3390/jmse9070770</pub-id></element-citation></ref><ref id="B11-sensors-25-05426"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hackradt</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>F&#233;lix-Hackradt</surname><given-names>F.C.</given-names></name><name name-style="western"><surname>Garc&#237;a-Charton</surname><given-names>J.A.</given-names></name></person-group><article-title>Influence of Habitat Structure on Fish Assemblage of an Artificial Reef in Southern Brazil</article-title><source>Mar. Environ. Res.</source><year>2011</year><volume>72</volume><fpage>235</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.marenvres.2011.09.006</pub-id><pub-id pub-id-type="pmid">22014376</pub-id></element-citation></ref><ref id="B12-sensors-25-05426"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kerry</surname><given-names>J.T.</given-names></name><name name-style="western"><surname>Bellwood</surname><given-names>D.R.</given-names></name></person-group><article-title>The Effect of Coral Morphology on Shelter Selection by Coral Reef Fishes</article-title><source>Coral Reefs</source><year>2012</year><volume>31</volume><fpage>415</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1007/s00338-011-0859-7</pub-id></element-citation></ref><ref id="B13-sensors-25-05426"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Seaman</surname><given-names>W.</given-names></name></person-group><article-title>Unifying Trends and Opportunities in Global Artificial Reef Research, Including Evaluation</article-title><source>ICES J. Mar. Sci.</source><year>2002</year><volume>59</volume><fpage>S14</fpage><lpage>S16</lpage><pub-id pub-id-type="doi">10.1006/jmsc.2002.1277</pub-id></element-citation></ref><ref id="B14-sensors-25-05426"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tweedley</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Loneragan</surname><given-names>N.R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Artificial Reefs Can Mimic Natural Habitats for Fish and Macroinvertebrates in Temperate Coastal Waters of the Yellow Sea</article-title><source>Ecol. Eng.</source><year>2019</year><volume>139</volume><fpage>105579</fpage><pub-id pub-id-type="doi">10.1016/j.ecoleng.2019.08.009</pub-id></element-citation></ref><ref id="B15-sensors-25-05426"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Becker</surname><given-names>A.</given-names></name><name name-style="western"><surname>Taylor</surname><given-names>M.D.</given-names></name><name name-style="western"><surname>Lowry</surname><given-names>M.B.</given-names></name></person-group><article-title>Monitoring of Reef Associated and Pelagic Fish Communities on Australia&#8217;s First Purpose Built Offshore Artificial Reef</article-title><source>ICES J. Mar. Sci.</source><year>2017</year><volume>74</volume><fpage>277</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1093/icesjms/fsw133</pub-id></element-citation></ref><ref id="B16-sensors-25-05426"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lowry</surname><given-names>M.</given-names></name><name name-style="western"><surname>Folpp</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gregson</surname><given-names>M.</given-names></name><name name-style="western"><surname>Suthers</surname><given-names>I.</given-names></name></person-group><article-title>Comparison of Baited Remote Underwater Video (BRUV) and Underwater Visual Census (UVC) for Assessment of Artificial Reefs in Estuaries</article-title><source>J. Exp. Mar. Biol. Ecol.</source><year>2012</year><volume>416&#8211;417</volume><fpage>243</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1016/j.jembe.2012.01.013</pub-id></element-citation></ref><ref id="B17-sensors-25-05426"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kondyukov</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>G.</given-names></name><name name-style="western"><surname>Pander</surname><given-names>J.</given-names></name><name name-style="western"><surname>Knott</surname><given-names>J.</given-names></name><name name-style="western"><surname>Geist</surname><given-names>J.</given-names></name><name name-style="western"><surname>Melesse</surname><given-names>M.B.</given-names></name><name name-style="western"><surname>Jacobson</surname><given-names>P.</given-names></name></person-group><article-title>Towards Automated and Real-Time Multi-Object Detection of Anguilliform Fish from Sonar Data Using YOLOv8 Deep Learning Algorithm</article-title><source>Ecol. Inform.</source><year>2025</year><volume>91</volume><fpage>103381</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2025.103381</pub-id></element-citation></ref><ref id="B18-sensors-25-05426"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>M.</given-names></name></person-group><article-title>A Lightweight Detector for Small Targets Using Forward-Looking Sonar in Underwater Search Scenarios</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>290</volume><fpage>128373</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2025.128373</pub-id></element-citation></ref><ref id="B19-sensors-25-05426"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abdullah</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Chuah</surname><given-names>L.F.</given-names></name><name name-style="western"><surname>Zakariya</surname><given-names>R.</given-names></name><name name-style="western"><surname>Syed</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hasan</surname><given-names>R.C.</given-names></name><name name-style="western"><surname>Mahmud</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Elgorban</surname><given-names>A.M.</given-names></name><name name-style="western"><surname>Bokhari</surname><given-names>A.</given-names></name><name name-style="western"><surname>Akhtar</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>AL-Shwaiman</surname><given-names>H.A.</given-names></name></person-group><article-title>Evaluating Climate Change Impacts on Reef Environments via Multibeam Echosounder and Acoustic Doppler Current Profiler Technology</article-title><source>Environ. Res.</source><year>2024</year><volume>252</volume><fpage>118858</fpage><pub-id pub-id-type="doi">10.1016/j.envres.2024.118858</pub-id><pub-id pub-id-type="pmid">38609066</pub-id></element-citation></ref><ref id="B20-sensors-25-05426"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Walree</surname><given-names>P.A.</given-names></name><name name-style="western"><surname>T&#281;gowski</surname><given-names>J.</given-names></name><name name-style="western"><surname>Laban</surname><given-names>C.</given-names></name><name name-style="western"><surname>Simons</surname><given-names>D.G.</given-names></name></person-group><article-title>Acoustic Seafloor Discrimination with Echo Shape Parameters: A Comparison with the Ground Truth</article-title><source>Cont. Shelf Res.</source><year>2005</year><volume>25</volume><fpage>2273</fpage><lpage>2293</lpage><pub-id pub-id-type="doi">10.1016/j.csr.2005.09.002</pub-id></element-citation></ref><ref id="B21-sensors-25-05426"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McGonigle</surname><given-names>C.</given-names></name><name name-style="western"><surname>Collier</surname><given-names>J.S.</given-names></name></person-group><article-title>Interlinking Backscatter, Grain Size and Benthic Community Structure</article-title><source>Estuar. Coast. Shelf Sci.</source><year>2014</year><volume>147</volume><fpage>123</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/j.ecss.2014.05.025</pub-id></element-citation></ref><ref id="B22-sensors-25-05426"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Underwater Moving Target Detection and Tracking Based on Enhanced You Only Look Once and Deep Simple Online and Realtime Tracking Strategy</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>143</volume><fpage>109982</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.109982</pub-id></element-citation></ref><ref id="B23-sensors-25-05426"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ling</surname><given-names>K.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>P.</given-names></name><name name-style="western"><surname>Song</surname><given-names>B.</given-names></name></person-group><article-title>UUVDNet: An Efficient Unmanned Underwater Vehicle Target Detection Network for Multibeam Forward-Looking Sonar</article-title><source>Ocean. Eng.</source><year>2025</year><volume>315</volume><fpage>119820</fpage><pub-id pub-id-type="doi">10.1016/j.oceaneng.2024.119820</pub-id></element-citation></ref><ref id="B24-sensors-25-05426"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>&#352;iaulys</surname><given-names>A.</given-names></name><name name-style="western"><surname>Vai&#269;iukynas</surname><given-names>E.</given-names></name><name name-style="western"><surname>Medelyt&#279;</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bu&#353;kus</surname><given-names>K.</given-names></name></person-group><article-title>Coverage Estimation of Benthic Habitat Features by Semantic Segmentation of Underwater Imagery from South-Eastern Baltic Reefs Using Deep Learning Models</article-title><source>Oceanologia</source><year>2024</year><volume>66</volume><fpage>286</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1016/j.oceano.2023.12.004</pub-id></element-citation></ref><ref id="B25-sensors-25-05426"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marre</surname><given-names>G.</given-names></name><name name-style="western"><surname>De Almeida Braga</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ienco</surname><given-names>D.</given-names></name><name name-style="western"><surname>Luque</surname><given-names>S.</given-names></name><name name-style="western"><surname>Holon</surname><given-names>F.</given-names></name><name name-style="western"><surname>Deter</surname><given-names>J.</given-names></name></person-group><article-title>Deep Convolutional Neural Networks to Monitor Coralligenous Reefs: Operationalizing Biodiversity and Ecological Assessment</article-title><source>Ecol. Inform.</source><year>2020</year><volume>59</volume><fpage>101110</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2020.101110</pub-id></element-citation></ref><ref id="B26-sensors-25-05426"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Lightweight Deep Learning Model for Underwater Waste Segmentation Based on Sonar Images</article-title><source>Waste Manag.</source><year>2024</year><volume>190</volume><fpage>63</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.wasman.2024.09.008</pub-id><pub-id pub-id-type="pmid">39277917</pub-id></element-citation></ref><ref id="B27-sensors-25-05426"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yue</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>X.</given-names></name></person-group><article-title>Real-Time Underwater Target Detection for AUV Using Side Scan Sonar Images Based on Deep Learning</article-title><source>Appl. Ocean. Res.</source><year>2023</year><volume>138</volume><fpage>103630</fpage><pub-id pub-id-type="doi">10.1016/j.apor.2023.103630</pub-id></element-citation></ref><ref id="B28-sensors-25-05426"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>P.</given-names></name><name name-style="western"><surname>He</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xin</surname><given-names>Y.</given-names></name></person-group><article-title>Multi-Scale Fusion and Efficient Feature Extraction for Enhanced Sonar Image Object Detection</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>256</volume><fpage>124958</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.124958</pub-id></element-citation></ref><ref id="B29-sensors-25-05426"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>K.S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><article-title>Improved YOLOv7 Model for Underwater Sonar Image Object Detection</article-title><source>J. Vis. Commun. Image Represent.</source><year>2024</year><volume>100</volume><fpage>104124</fpage><pub-id pub-id-type="doi">10.1016/j.jvcir.2024.104124</pub-id></element-citation></ref><ref id="B30-sensors-25-05426"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.-J.</given-names></name></person-group><article-title>A Teacher-Student Framework Leveraging Large Vision Model for Data Pre-Annotation and YOLO for Tunnel Lining Multiple Defects Instance Segmentation</article-title><source>J. Ind. Inf. Integr.</source><year>2025</year><volume>44</volume><fpage>100790</fpage><pub-id pub-id-type="doi">10.1016/j.jii.2025.100790</pub-id></element-citation></ref><ref id="B31-sensors-25-05426"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Y.</given-names></name></person-group><article-title>MSTA-YOLO: A Novel Retinal Ganglion Cell Instance Segmentation Method Using a Task-Aligned Coupled Head and Efficient Multi-Scale Attention for Glaucoma Analysis</article-title><source>Biomed. Signal Process. Control.</source><year>2025</year><volume>106</volume><elocation-id>107695</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2025.107695</pub-id></element-citation></ref><ref id="B32-sensors-25-05426"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Silpalatha</surname><given-names>G.</given-names></name><name name-style="western"><surname>Jayadeva</surname><given-names>T.S.</given-names></name></person-group><article-title>Accelerating Fast and Accurate Instantaneous Segmentation with YOLO-v8 for Remote Sensing Image Analysis</article-title><source>Remote Sens. Appl. Soc. Environ.</source><year>2025</year><volume>37</volume><fpage>101502</fpage><pub-id pub-id-type="doi">10.1016/j.rsase.2025.101502</pub-id></element-citation></ref><ref id="B33-sensors-25-05426"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shams</surname><given-names>M.Y.</given-names></name><name name-style="western"><surname>Elmessery</surname><given-names>W.M.</given-names></name><name name-style="western"><surname>Oraiath</surname><given-names>A.A.T.</given-names></name><name name-style="western"><surname>Elbeltagi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Salem</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>P.</given-names></name><name name-style="western"><surname>El-Messery</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>El-Hafeez</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Abdelshafie</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Abd El-Wahhab</surname><given-names>G.G.</given-names></name><etal/></person-group><article-title>Automated On-Site Broiler Live Weight Estimation through YOLO-Based Segmentation</article-title><source>Smart Agric. Technol.</source><year>2025</year><volume>10</volume><fpage>100828</fpage><pub-id pub-id-type="doi">10.1016/j.atech.2025.100828</pub-id></element-citation></ref><ref id="B34-sensors-25-05426"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>D.</given-names></name></person-group><article-title>Multi-Scale Adaptive YOLO for Instance Segmentation of Grape Pedicels</article-title><source>Comput. Electron. Agric.</source><year>2025</year><volume>229</volume><fpage>109712</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.109712</pub-id></element-citation></ref><ref id="B35-sensors-25-05426"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>P.</given-names></name></person-group><article-title>YOLO-LOGO: A Transformer-Based YOLO Segmentation Model for Breast Mass Detection and Segmentation in Digital Mammograms</article-title><source>Comput. Methods Programs Biomed.</source><year>2022</year><volume>221</volume><elocation-id>106903</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2022.106903</pub-id><pub-id pub-id-type="pmid">35636358</pub-id></element-citation></ref><ref id="B36-sensors-25-05426"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bo</surname><given-names>Y.</given-names></name></person-group><article-title>Coal-Rock Interface Real-Time Recognition Based on the Improved YOLO Detection and Bilateral Segmentation Network</article-title><source>Undergr. Space</source><year>2025</year><volume>21</volume><fpage>22</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.undsp.2024.07.003</pub-id></element-citation></ref><ref id="B37-sensors-25-05426"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name></person-group><article-title>Coordinate Attention for Efficient Mobile Network Design</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>13713</fpage><lpage>13722</lpage></element-citation></ref><ref id="B38-sensors-25-05426"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>E.</given-names></name></person-group><article-title>ParameterNet: Parameters Are All You Need for Large-Scale Visual Pretraining of Mobile Networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>15751</fpage><lpage>15761</lpage></element-citation></ref><ref id="B39-sensors-25-05426"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name></person-group><article-title>Learning to Upsample by Learning to Sample</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Paris, France</conf-loc><conf-date>1&#8211;6 October 2023</conf-date><fpage>6027</fpage><lpage>6037</lpage></element-citation></ref><ref id="B40-sensors-25-05426"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>I.-H.</given-names></name><name name-style="western"><surname>Mark Liao</surname><given-names>H.-Y.</given-names></name></person-group><article-title>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</article-title><source>Proceedings of the 18th European Conference on Computer Vision&#8212;ECCV 2024</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Leonardis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ricci</surname><given-names>E.</given-names></name><name name-style="western"><surname>Roth</surname><given-names>S.</given-names></name><name name-style="western"><surname>Russakovsky</surname><given-names>O.</given-names></name><name name-style="western"><surname>Sattler</surname><given-names>T.</given-names></name><name name-style="western"><surname>Varol</surname><given-names>G.</given-names></name></person-group><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2025</year><fpage>1</fpage><lpage>21</lpage></element-citation></ref><ref id="B41-sensors-25-05426"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>Proceedings of the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention&#8212;MICCAI 2015</source><conf-loc>Munich, Germany</conf-loc><conf-date>5&#8211;9 October 2015</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Navab</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hornegger</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wells</surname><given-names>W.M.</given-names></name><name name-style="western"><surname>Frangi</surname><given-names>A.F.</given-names></name></person-group><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="B42-sensors-25-05426"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kendall</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cipolla</surname><given-names>R.</given-names></name></person-group><article-title>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>39</volume><fpage>2481</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type="pmid">28060704</pub-id></element-citation></ref><ref id="B43-sensors-25-05426"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shelhamer</surname><given-names>E.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully Convolutional Networks for Semantic Segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05426-f001" orientation="portrait"><label>Figure 1</label><caption><p>Selected artificial reef images and annotations.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05426-g001.jpg"/></fig><fig position="float" id="sensors-25-05426-f002" orientation="portrait"><label>Figure 2</label><caption><p>Structure diagram of YOLO-AR module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05426-g002.jpg"/></fig><fig position="float" id="sensors-25-05426-f003" orientation="portrait"><label>Figure 3</label><caption><p>Structure diagram of DCCA module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05426-g003.jpg"/></fig><fig position="float" id="sensors-25-05426-f004" orientation="portrait"><label>Figure 4</label><caption><p>Sampling-based dynamic upsampling and sampling point generator designs in DySample.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05426-g004.jpg"/></fig><fig position="float" id="sensors-25-05426-f005" orientation="portrait"><label>Figure 5</label><caption><p>Structure diagram of ADown module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05426-g005.jpg"/></fig><fig position="float" id="sensors-25-05426-f006" orientation="portrait"><label>Figure 6</label><caption><p>The segmentation results of an artificial reef using YOLOv11 (<bold>a</bold>&#8211;<bold>d</bold>) and YOLO-AR (<bold>e</bold>&#8211;<bold>h</bold>). The red area represents the artificial reefs segmented by the model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05426-g006.jpg"/></fig><fig position="float" id="sensors-25-05426-f007" orientation="portrait"><label>Figure 7</label><caption><p>Visualization of artificial reef segmentation heatmaps of YOLOv11 and YOLO-AR. (<bold>a</bold>) Segmentation results of YOLOv11, (<bold>b</bold>) segmentation heatmap of YOLOv11, (<bold>c</bold>) segmentation results of YOLO-AR, and (<bold>d</bold>) segmentation heatmap of YOLO-AR.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05426-g007.jpg"/></fig><fig position="float" id="sensors-25-05426-f008" orientation="portrait"><label>Figure 8</label><caption><p>Comparison of multibeam sonar image artificial reef segmentation results among six segmentation models. (<bold>a</bold>&#8211;<bold>f</bold>) are six images in the test set. The first two columns are sonar images and real labels respectively. The following six columns show the artificial reef segmentation results of YOLOv8, YOLOv9, U-Net, SegNet, FCN and YOLO-AR. The red box represents the key comparison areas of the segmentation results.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05426-g008.jpg"/></fig><table-wrap position="float" id="sensors-25-05426-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05426-t001_Table 1</object-id><label>Table 1</label><caption><p>Experimental software and hardware configuration and training parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hardware/Software</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Configuration</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Training Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Configuration</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel Core i7-9750H</td><td align="center" valign="middle" rowspan="1" colspan="1">Initial learning rate</td><td align="center" valign="top" rowspan="1" colspan="1">0.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA RTX4090D</td><td align="center" valign="middle" rowspan="1" colspan="1">Momentum</td><td align="center" valign="top" rowspan="1" colspan="1">0.937</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Python</td><td align="center" valign="middle" rowspan="1" colspan="1">3.8.10</td><td align="center" valign="middle" rowspan="1" colspan="1">Weight decay</td><td align="center" valign="top" rowspan="1" colspan="1">0.0005</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pytorch</td><td align="center" valign="middle" rowspan="1" colspan="1">2.0.0</td><td align="center" valign="middle" rowspan="1" colspan="1">Bach size</td><td align="center" valign="top" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cuda</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning epoch</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05426-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05426-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance comparison of artificial reef segmentation between YOLOv11 and YOLO-AR.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">R</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@[0.5:0.95]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">IOU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.893</td><td align="center" valign="middle" rowspan="1" colspan="1">0.794</td><td align="center" valign="middle" rowspan="1" colspan="1">0.853</td><td align="center" valign="top" rowspan="1" colspan="1">0.553</td><td align="center" valign="middle" rowspan="1" colspan="1">0.725</td><td align="center" valign="middle" rowspan="1" colspan="1">0.841</td><td align="center" valign="middle" rowspan="1" colspan="1">2834763</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO-AR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.939</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.879</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.912</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.601</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.832</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.908</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2672504</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05426-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05426-t003_Table 3</object-id><label>Table 3</label><caption><p>The parameter, size, and FLOPs of six segmentation models. The bold numbers represent the minimum value in each column.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters (Million)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Size (MB)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8</td><td align="center" valign="middle" rowspan="1" colspan="1">3.26</td><td align="center" valign="middle" rowspan="1" colspan="1">6.46</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>12.1</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv9 [<xref rid="B40-sensors-25-05426" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">27.84</td><td align="center" valign="middle" rowspan="1" colspan="1">106.91</td><td align="center" valign="top" rowspan="1" colspan="1">159.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Net [<xref rid="B41-sensors-25-05426" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.89</td><td align="center" valign="middle" rowspan="1" colspan="1">94.97</td><td align="center" valign="top" rowspan="1" colspan="1">361.85</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegNet [<xref rid="B42-sensors-25-05426" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">29.46</td><td align="center" valign="middle" rowspan="1" colspan="1">337.45</td><td align="center" valign="top" rowspan="1" colspan="1">327.13</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCN [<xref rid="B43-sensors-25-05426" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">18.64</td><td align="center" valign="middle" rowspan="1" colspan="1">269.74</td><td align="center" valign="top" rowspan="1" colspan="1">203.99</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO-AR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.67</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.58</bold>
</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">23.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05426-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05426-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance comparisons of the six segmentation models. The bold numbers represent the maximum value in each column.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">R</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">IOU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.887</td><td align="center" valign="middle" rowspan="1" colspan="1">0.786</td><td align="center" valign="middle" rowspan="1" colspan="1">0.842</td><td align="center" valign="middle" rowspan="1" colspan="1">0.714</td><td align="center" valign="middle" rowspan="1" colspan="1">0.833</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv9 [<xref rid="B40-sensors-25-05426" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.893</td><td align="center" valign="middle" rowspan="1" colspan="1">0.804</td><td align="center" valign="middle" rowspan="1" colspan="1">0.851</td><td align="center" valign="middle" rowspan="1" colspan="1">0.733</td><td align="center" valign="middle" rowspan="1" colspan="1">0.846</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">U-Net [<xref rid="B41-sensors-25-05426" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.873</td><td align="center" valign="middle" rowspan="1" colspan="1">0.838</td><td align="center" valign="middle" rowspan="1" colspan="1">0.820</td><td align="center" valign="middle" rowspan="1" colspan="1">0.747</td><td align="center" valign="middle" rowspan="1" colspan="1">0.855</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCN [<xref rid="B42-sensors-25-05426" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.966</td><td align="center" valign="middle" rowspan="1" colspan="1">0.821</td><td align="center" valign="middle" rowspan="1" colspan="1">0.822</td><td align="center" valign="middle" rowspan="1" colspan="1">0.798</td><td align="center" valign="middle" rowspan="1" colspan="1">0.888</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SegNet [<xref rid="B43-sensors-25-05426" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.941</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.714</td><td align="center" valign="middle" rowspan="1" colspan="1">0.718</td><td align="center" valign="middle" rowspan="1" colspan="1">0.683</td><td align="center" valign="middle" rowspan="1" colspan="1">0.812</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLO-AR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.939</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.879</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.912</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.832</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9</bold>
<bold>08</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05426-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05426-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation experiment results of different modules.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">YOLOv11</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">DCCA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ADown</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">DNeck</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">R</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@[0.5:0.95]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">IOU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters (Million)</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">0.893</td><td align="center" valign="top" rowspan="1" colspan="1">0.794</td><td align="center" valign="top" rowspan="1" colspan="1">0.853</td><td align="center" valign="middle" rowspan="1" colspan="1">0.553</td><td align="center" valign="top" rowspan="1" colspan="1">0.725</td><td align="center" valign="top" rowspan="1" colspan="1">0.841</td><td align="center" valign="top" rowspan="1" colspan="1">2.83</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">0.892</td><td align="center" valign="top" rowspan="1" colspan="1">0.806</td><td align="center" valign="top" rowspan="1" colspan="1">0.861</td><td align="center" valign="middle" rowspan="1" colspan="1">0.565</td><td align="center" valign="top" rowspan="1" colspan="1">0.734</td><td align="center" valign="top" rowspan="1" colspan="1">0.847</td><td align="center" valign="top" rowspan="1" colspan="1">2.93</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">0.891</td><td align="center" valign="top" rowspan="1" colspan="1">0.788</td><td align="center" valign="top" rowspan="1" colspan="1">0.848</td><td align="center" valign="middle" rowspan="1" colspan="1">0.539</td><td align="center" valign="top" rowspan="1" colspan="1">0.719</td><td align="center" valign="top" rowspan="1" colspan="1">0.836</td><td align="center" valign="top" rowspan="1" colspan="1">2.49</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">0.925</td><td align="center" valign="top" rowspan="1" colspan="1">0.863</td><td align="center" valign="top" rowspan="1" colspan="1">0.899</td><td align="center" valign="middle" rowspan="1" colspan="1">0.564</td><td align="center" valign="top" rowspan="1" colspan="1">0.801</td><td align="center" valign="top" rowspan="1" colspan="1">0.893</td><td align="center" valign="top" rowspan="1" colspan="1">2.91</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">0.891</td><td align="center" valign="top" rowspan="1" colspan="1">0.801</td><td align="center" valign="top" rowspan="1" colspan="1">0.856</td><td align="center" valign="middle" rowspan="1" colspan="1">0.564</td><td align="center" valign="top" rowspan="1" colspan="1">0.730</td><td align="center" valign="top" rowspan="1" colspan="1">0.844</td><td align="center" valign="top" rowspan="1" colspan="1">2.57</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">0.923</td><td align="center" valign="top" rowspan="1" colspan="1">0.866</td><td align="center" valign="top" rowspan="1" colspan="1">0.902</td><td align="center" valign="middle" rowspan="1" colspan="1">0.574</td><td align="center" valign="top" rowspan="1" colspan="1">0.808</td><td align="center" valign="top" rowspan="1" colspan="1">0.894</td><td align="center" valign="top" rowspan="1" colspan="1">2.99</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#215;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" rowspan="1" colspan="1">0.930</td><td align="center" valign="top" rowspan="1" colspan="1">0.866</td><td align="center" valign="top" rowspan="1" colspan="1">0.902</td><td align="center" valign="middle" rowspan="1" colspan="1">0.581</td><td align="center" valign="top" rowspan="1" colspan="1">0.813</td><td align="center" valign="top" rowspan="1" colspan="1">0.897</td><td align="center" valign="top" rowspan="1" colspan="1">2.57</td></tr><tr><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.939</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.879</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.912</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.601</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.832</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">0.908</td><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">2.67</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>