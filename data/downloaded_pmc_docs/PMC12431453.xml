<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431453</article-id><article-id pub-id-type="pmcid-ver">PMC12431453.1</article-id><article-id pub-id-type="pmcaid">12431453</article-id><article-id pub-id-type="pmcaiid">12431453</article-id><article-id pub-id-type="doi">10.3390/s25175325</article-id><article-id pub-id-type="publisher-id">sensors-25-05325</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An Improved Multi-Object Tracking Algorithm Designed for Complex Environments</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-9032-6710</contrib-id><name name-style="western"><surname>Liu</surname><given-names initials="W">Wuyuhan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05325" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yao</surname><given-names initials="J">Jian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af2-sensors-25-05325" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jiang</surname><given-names initials="F">Feng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05325" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="M">Meng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05325" ref-type="aff">1</xref><xref rid="c1-sensors-25-05325" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Smolka</surname><given-names initials="B">Bogdan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05325"><label>1</label>School of Electronic Information and Physics, Central South University of Forestry and Technology, Changsha 410004, China; <email>sixfive6655@163.com</email> (W.L.); <email>t20040535@csuft.edu.cn</email> (F.J.)</aff><aff id="af2-sensors-25-05325"><label>2</label>School of Computer and Information Engineering, Central South University of Forestry and Technology, Changsha 410004, China; <email>t20110565@csuft.edu.cn</email></aff><author-notes><corresp id="c1-sensors-25-05325"><label>*</label>Correspondence: <email>sixfive6655@126.com</email></corresp></author-notes><pub-date pub-type="epub"><day>27</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5325</elocation-id><history><date date-type="received"><day>01</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>18</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>27</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 14:25:13.570"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05325.pdf"/><abstract><p>Multi-object tracking (MOT) algorithms are a key research direction in the field of computer vision. Among them, the joint detection and embedding (JDE) method, with its excellent speed and accuracy performance, has become the current mainstream solution. However, in complex scenes with dense targets or occlusions, the tracking performance of existing algorithms is often limited, especially in terms of unstable identity assignment and insufficient tracking accuracy. To address these challenges, this paper proposes a new multi-object tracking model&#8212;the Reparameterized and Global Context Track (RGTrack). This model is based on the Correlation-Sensitive Track (CSTrack) framework and innovatively introduces multi-branch training and attention mechanisms, combined with reparameterized convolutional networks and global attention modules, significantly enhancing the network&#8217;s feature extraction ability in complex scenes, especially in ignoring irrelevant information and focusing on key areas. It adopted a multiple association strategy to better establish the association relationship between targets in consecutive frames. Through this improvement, the Reparameterized and Global Context Track can better handle scenes with dense targets and severe occlusions, providing more accurate target identity matching and continuous tracking. Experimental results show that compared with the Correlation-Sensitive Track, the Reparameterized and Global Context Track has significant improvements in multiple key indicators: multi-object tracking accuracy (MOTA) increased by 1.15%, Identity F1 Score (IDF1) increased by 1.73%, and Mostly Tracked (MT) increased by 6.86%, while ID-switched (ID Sw) decreased by 47.49%. These results indicate that the Reparameterized and Global Context Track not only can stably track targets in more complex scenes but also significantly improves the continuity of target identities. Moreover, the Reparameterized and Global Context Track increased the frames per second (FPS) by 51.48% and reduced the model size by 3.08%, demonstrating its significant advantages in real-time performance and computational efficiency. Therefore, the Reparameterized and Global Context Track model maintains high accuracy while having stronger real-time processing capabilities, making it especially suitable for embedded devices and resource-constrained application environments.</p></abstract><kwd-group><kwd>MOT</kwd><kwd>JDE</kwd><kwd>reparameterization</kwd><kwd>attention mechanism</kwd><kwd>RGTrack</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05325"><title>1. Introduction</title><p>Multi-object tracking is one of the classical problems in computer vision engineering. It mainly involves a set of methods for accurately tracking the positions of multiple targets in a specific category in a video and giving each target different identity information [<xref rid="B1-sensors-25-05325" ref-type="bibr">1</xref>]. Object detection usually inputs the image to be detected into the detection model. Finally, it uses rectangular boxes to display the position of interesting objects in the original image and determine their category. Unlike object detection, the MOT algorithm needs not only to output the location of the target of interest but also to associate the target&#8217;s identity with the location information output by the detection task and maintain the state of association between the identity information and the bounding box [<xref rid="B2-sensors-25-05325" ref-type="bibr">2</xref>]. Most of today&#8217;s excellent multi-object tracking solutions use methods based on deep learning, among which the principal method is multiple-object Tracking By Detection (TBD) [<xref rid="B3-sensors-25-05325" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05325" ref-type="bibr">4</xref>]. Specifically, this method identifies and detects targets in the video, and then the data association strategy is used to generate the target trajectory. Such methods finely decompose MOT into the following steps: object detection, feature extraction, motion prediction, similarity computation, and data association. The entire flow is illustrated in <xref rid="sensors-25-05325-f001" ref-type="fig">Figure 1</xref>. Firstly, the image is input into the detection model to obtain the position information of multiple objects in a single frame. Secondly, the tracking target is cut out to obtain the appearance information of the target. The target&#8217;s position in the next frame is obtained by motion prediction. Finally, the affinity matrix between the appearance information of the target in the previous frame and the position information of the target in the current frame is calculated in two adjacent frames. Each target&#8217;s unique assignment of identity information is obtained by maximum matching, and the current target is finally associated with the existing trajectory. The above process is recursive to complete the tracking of multiple targets.</p><p>According to the above analysis of TBD, we can find that this kind of method requires two computationally intensive components, the detection model and the embedding model, in which the embedding model often uses the person reidentification (ReID) model [<xref rid="B5-sensors-25-05325" ref-type="bibr">5</xref>]. In recent years, many excellent detection methods and ReID models have emerged with the iterative updating of deep learning technology. Benefiting from the continuous updating and development of the two components, the accuracy of TBD methods is getting higher and higher. Although the TBD method performs well in tracking performance, it is challenging for it to meet the actual needs in terms of tracking speed. One of the possible reasons for its unsatisfactory speed is that most models need to process features through two computation-intensive components respectively. This leads to a long inference time of the whole system and finally leads to the problem of slow tracking speed with low real-time performance.</p><p>With the deepening of the research on MOT algorithms based on deep learning, a class of methods that integrate detection components and embedding models into a single model to complete the detection and appearance feature extraction tasks simultaneously has attracted much attention. The main reason is that this alleviates the speed bottleneck problem of multi-step multi-object tracking schemes. Inspired by You Only Look Once (YOLO) [<xref rid="B6-sensors-25-05325" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05325" ref-type="bibr">7</xref>], a one-stage object detection framework in the field of object detection, which simultaneously completes the tasks of object classification and location, Wang et al. creatively proposed the JDE [<xref rid="B8-sensors-25-05325" ref-type="bibr">8</xref>] paradigm based on YOLOv3 [<xref rid="B9-sensors-25-05325" ref-type="bibr">9</xref>]. JDE converts multi-object tracking into multi-task learning. Three tasks of classification, regression, and appearance extraction are completed while a single network is performing forward propagation. By carefully designing the structure of the target detection network, the detection model and embedding model can share the same set of features, which avoids the repeated calculation of the detection and embedding steps, saves the network inference time, and improves the processing speed of the model. Finally, the experiment proves that the proposed method can run in near real-time speed, while the accuracy is comparable to that of the two-stage model.</p><p>Many researchers have recognized the multi-object tracking of the JDE paradigm since it was proposed. However, its application in actual scenes is still very challenging. In areas with high crowd density and heavy shading, the detection model has difficulty locating the object of interest, resulting in the detection box not being apparent, which, due to unreliable data correlation IDs, leads to the tracking effect not being ideal. The specific details of the JDE paradigm still need to be improved. CSTrack [<xref rid="B10-sensors-25-05325" ref-type="bibr">10</xref>] is a representative method of improving the JDE model, which adds a cross-correlation network (CCN) and scale-aware attention network (SAAN) based on JDE, and alleviates the excessive competition problem of detection tasks and embedding tasks in JDE.</p><p>In order to improve the accuracy of multi-object tracking algorithms in occlusion scenes, this paper adds multi-branch training to the CSTrack framework, introduces the attention mechanism to deeply aggregate high-level semantics, and establishes an RGTrack model with enhanced detection ability.</p><p>The work in this paper can be summarized as follows:<list list-type="order"><list-item><p>We introduce global context (GC) attention to enhance the detection ability of the network.</p></list-item><list-item><p>We use the Reparameterized Visual Geometry Group (RepVGG) convolution module to add multi-branch training to improve the feature fusion ability of the network.</p></list-item><list-item><p>We introduce the multiple association strategy to better establish the association relationship of the target between consecutive frames.</p></list-item></list></p><p>The remainder of this article is organized as follows. <xref rid="sec2-sensors-25-05325" ref-type="sec">Section 2</xref> reviews the related work on multi-object tracking. <xref rid="sec3-sensors-25-05325" ref-type="sec">Section 3</xref> provides a detailed description of the proposed RGTrack model. <xref rid="sec4-sensors-25-05325" ref-type="sec">Section 4</xref> presents the experimental results, and <xref rid="sec5-sensors-25-05325" ref-type="sec">Section 5</xref> discusses the findings and limitations. Finally, <xref rid="sec6-sensors-25-05325" ref-type="sec">Section 6</xref> concludes the paper and outlines directions for future research.</p></sec><sec id="sec2-sensors-25-05325"><title>2. Related Work</title><p>According to whether the MOT model uses a single model for object detection and appearance extraction, we can divide the detection-based multi-object tracking research work into two categories: single models that perform object detection and appearance extraction separately, and single models that perform object detection and appearance extraction simultaneously.</p><sec id="sec2dot1-sensors-25-05325"><title>2.1. SDE Model</title><p>The separate model transforms the video into image sequences to complete the specific tracking task through the detection model and the embedding model. Bewley et al. [<xref rid="B11-sensors-25-05325" ref-type="bibr">11</xref>] used a simple combination of the Kalman filter and Hungarian algorithm in the tracking component and called it the SORT algorithm, using it to solve common problems in data association. They proposed a simple online tracking framework based on inter-frame prediction and association and used classical tracking methods to achieve advanced tracking quality. It achieved good performance in terms of speed and accuracy. Wojke et al. [<xref rid="B12-sensors-25-05325" ref-type="bibr">12</xref>] analyzed the advantages and disadvantages of the SORT algorithm. In order to track long-term occluded targets and effectively reduce the number of identity switches, they proposed DeepSORT by combining appearance information with pre-trained association metrics. Compared with SORT, DeepSORT reduces the number of identity exchanges after occlusion disappearance by 45% and can also achieve good results at high resolution. Feichtenhofer et al. [<xref rid="B13-sensors-25-05325" ref-type="bibr">13</xref>] proposed the D&amp;T framework by summarizing the previous tracking methods. They established the ConvNet architecture of simultaneous detection and tracking, introduced relevant features to represent the co-occurrence of objects in time, and connected the cross-frame trajectory and frame-level detection, resulting in video-level high-precision detection. Experiments show that increasing the time stride can significantly improve the tracker&#8217;s speed. In order to associate unreliable detection results with existing trajectories, Chen et al. [<xref rid="B14-sensors-25-05325" ref-type="bibr">14</xref>] adopted the detection strategy of collecting candidate detections from detection and tracking outputs to deal with unreliable detection. They proposed MOTDT, an online multi-person tracking framework based on the loss function of a fully convolutional neural network, then introduced the appearance representation of deep learning. Finally, training on a large-scale human recognition dataset improves the recognition ability when dealing with intra-class occlusion. In order to alleviate the problems of ReID, motion prediction, and occlusion processing, Bergmann et al. [<xref rid="B15-sensors-25-05325" ref-type="bibr">15</xref>] proposed Tractor++. This universal tracker does not train and optimize the tracking data. They used the detector&#8217;s border regression to predict the target&#8217;s position in the next frame, convert the detector into a tracker, and deal with the simplest tracking scenarios.</p></sec><sec id="sec2dot2-sensors-25-05325"><title>2.2. JDE Model</title><p>Due to the decoupling of the object detection model and ReID model, the SDE method has tremendous advantages in multi-object tracking accuracy. However, its running speed is not sufficient to meet the actual demand. Integrating detection and appearance feature extraction with a single model is a timely emerging field. More and more scholars are committed to researching MOT algorithms for multiple tasks in a single model. Wang et al. [<xref rid="B8-sensors-25-05325" ref-type="bibr">8</xref>] discussed the detection-based tracking paradigm, including the detection model of target positioning and the appearance embedding model of data association. They found that executing the two models separately would lead to efficiency problems because the running time was the sum of the two steps, and the potential sharing structure between them was not studied. Most of the existing MOT studies focus on the data association steps. In essence, it is a real-time association method rather than a real-time MOT system. Based on the above discussion and analysis, the JDE framework is proposed to realize the simultaneous learning of object detection and appearance embedding in a shared model. It dramatically reduces the running time of the whole MOT system and enables it to run at near real-time speed. Wang et al. [<xref rid="B16-sensors-25-05325" ref-type="bibr">16</xref>] considered that the single-object detection in FairMOT only uses the attributes of the current frame but does not use the target information of the preceding and posterior frames. Hence, they proposed integrating the Graph Neural Network (GNN) into the object detection framework to form a joint framework. They used the GNN to extract relationships between objects and learn better features to improve detection and data association. Liang et al. [<xref rid="B10-sensors-25-05325" ref-type="bibr">10</xref>] analyzed the joint process of object detection and ReID tasks in JDE. They found that the differences between the two tasks were ignored in the joint training process, which eventually caused performance problems. In order to alleviate the excessive competition between detection subtasks and reidentification subtasks in the MOT system, they proposed a mutual correlation network and scale-aware attention network and demonstrated the effectiveness of this method by ablation experiments.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05325"><title>3. Methods</title><p>With the deepening of the research on the one-stage detection algorithm YOLO, its performance in object detection has also steadily improved. Consistent with the CSTrack framework, this paper adopts the most widely used YOLOv5 [<xref rid="B17-sensors-25-05325" ref-type="bibr">17</xref>] algorithm as the basic framework for research. Based on version 6.0, this paper studies the MOT algorithm for joint detection and embedding tasks in a single model. The overall structure of the network is shown in <xref rid="sensors-25-05325-f002" ref-type="fig">Figure 2</xref>, consisting of the feature extraction network backbone, feature fusion component neck, and network prediction component head. Within them, &#8220;Conv&#8221; is the abbreviation of &#8220;Convolutional Layer&#8221;, indicating a convolutional layer, which is one of the core modules used in deep learning for image processing. &#8220;C3&#8221; is the abbreviation of &#8220;Cross Stage Partial 3&#8221;, and it is a module design in the CSPNet (Cross Stage Partial Network) architecture. &#8220;GC&#8221; is the abbreviation of &#8220;Global Context Block&#8221;. &#8220;UpSample&#8221; is the abbreviation of &#8220;Upsampling Layer&#8221;, representing an upsampling layer. &#8220;Concat&#8221; is the abbreviation of &#8220;Concatenation Layer&#8221;, indicating a concatenation layer. &#8220;RepConv&#8221; is the abbreviation of &#8220;Reparameterized Convolution&#8221;. &#8220;CCN&#8221; is the abbreviation of &#8220;cross-correlation network&#8221;. &#8220;SAAN&#8221; is the abbreviation of &#8220;scale-aware attention network&#8221;.</p><sec id="sec3dot1-sensors-25-05325"><title>3.1. Backbone</title><p>Feature extraction is performed to reduce the dimension of the original input data or recombine the original features for subsequent use, that is, to reduce the dimension of the data and to sort out the existing data features [<xref rid="B18-sensors-25-05325" ref-type="bibr">18</xref>]. The feature extraction network of Yolo series algorithms is highly researchable, and much work has been performed to improve it [<xref rid="B19-sensors-25-05325" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05325" ref-type="bibr">20</xref>]. In order to improve the detection ability of the MOT model, the feature reconstruction part of the feature extraction network is improved. Since multi-object tracking based on deep learning mainly adopts high-resolution images for training, the default input size of the network (640,640,3) is changed to (1088,608,3). Multiple convolutions are used to increase channel information and reduce the information loss caused by downsampling. Next, multiple convolutions with a convolution kernel of 3 &#215; 3 and step of 2 are used to downsample the image while deepening the number of feature map channels. Then, high-level features are extracted through the global context module. Finally, the feature map relative to the original image at 1/4, 1/8, 1/16, and 1/32 is obtained. Due to the large resolution of the input feature map and considering the overall computational effort, the last three layers of the feature map are retained for subsequent operations such as feature fusion. Finally, the feature map is converted to a fixed-size feature vector using the SPPF layer.</p><sec><title>Global Context Block</title><p>As convolutional networks suffer from low learning efficiency and cumulative over-deepness of the network, Wang et al. [<xref rid="B21-sensors-25-05325" ref-type="bibr">21</xref>] proposed a generalized, simple, and non-local operation named Non-local. It can be embedded into the current network based on the non-local mean filtering operation in the field of picture filtering. They proposed self-attention [<xref rid="B22-sensors-25-05325" ref-type="bibr">22</xref>], an attention mechanism for non-local information statistics based on capturing dependencies between long-range features. The general formula for Non-local is shown in Equation (1). Here, <italic toggle="yes">x</italic> represents the input signal, which is generally used in computer vision as a feature map representation, i represents the output location, f is a function that calculates the similarity of i to j, g is a representation of the feature map at location j, and C(x) represents the response factor.<disp-formula id="FD1-sensors-25-05325"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mo>&#8704;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Hu et al. [<xref rid="B23-sensors-25-05325" ref-type="bibr">23</xref>] suggested correcting the features of channels by modeling the relationship between them and proposed implementing Senet Block by three operations, Squeeze, Excitation, and Reweight, to enhance the characterization ability of neural networks. Cao et al. [<xref rid="B24-sensors-25-05325" ref-type="bibr">24</xref>] discussed the computational waste problem of non-local operators and the problem that SE Block cannot fully use global context information. Then they combined both advantages and proposed GC Block, which has the global context modeling capability of Non-local Block and can save computation like SE Block. <xref rid="sensors-25-05325-f003" ref-type="fig">Figure 3</xref> shows that this module contains three operations: context modeling, channel transform, and feature fusion.</p></sec></sec><sec id="sec3dot2-sensors-25-05325"><title>3.2. Neck</title><p>The feature extraction network [<xref rid="B25-sensors-25-05325" ref-type="bibr">25</xref>] obtains visual, attribute, and semantic features in order and does not adequately understand the three hierarchical features. In this paper, we introduce Reparameterized Convolution (RepConv) in the neck fusion component to further process the backbone-extracted features and hierarchically fuse the three features to improve the detection model performance.</p><sec><title>RepVGG Block</title><p>In classification networks, the backbone network is the benchmark network for many advanced tasks like object detection, and its performance largely determines part of the upper limit of that network, among which representative networks are AlexNet [<xref rid="B26-sensors-25-05325" ref-type="bibr">26</xref>], ResNet [<xref rid="B27-sensors-25-05325" ref-type="bibr">27</xref>], and VGGNet [<xref rid="B28-sensors-25-05325" ref-type="bibr">28</xref>]. The key idea of the RepVGG [<xref rid="B29-sensors-25-05325" ref-type="bibr">29</xref>] classification network is to use the multi-branching approach of ResNet to avoid gradient disappearance during training and to use the single-way mode of VGG during inference by operator fusion to ensure constant inference. Since RepVGG has the advantage of multi-branch training with constant inference speed, this paper replaces the convolutional layer with recombining parameter convolution. In the following, the advantages of the RepVGG module are described in detail in the training and inference phases. In the training stage, as shown in <xref rid="sensors-25-05325-f004" ref-type="fig">Figure 4</xref>, the original structure uses a 3 &#215; 3 convolution kernel with a stride of 1 and a padding of 1 to downsample the feature map. RepConv has two more branches compared to the original convolution structure, one of which uses a 1 &#215; 1 convolution kernel with a stride of 2 and a padding of 0 for standard convolution, and the other branch performs a batch normalization (BN) operation on the input map.</p><p>In the inference stage, as shown in <xref rid="sensors-25-05325-f005" ref-type="fig">Figure 5</xref>, the original 3 &#215; 3 convolution with BN is kept unchanged first, after which the 1 &#215; 1 convolution with BN and BN in the training branch are all converted to 3 &#215; 3 convolutions with BN. Finally, the three 3 &#215; 3 convolution kernels with BN are added separately to form one 3 &#215; 3 convolution with BN operation.</p><p>The first problem in the inference phase is fusing the 3 &#215; 3 convolution, 1 &#215; 1 convolution, and no-convolution operations in the three branches into a single 3 &#215; 3 convolution. Due to the linear characteristic of convolution, the original 3 &#215; 3 convolution is kept unchanged, the 1 &#215; 1 convolution is expanded to 3 &#215; 3 convolution, and BN is expanded to a constant mapping so that the input and output of the constructed convolution layer are equal. As shown in <xref rid="sensors-25-05325-f006" ref-type="fig">Figure 6</xref>, the 1 &#215; 1 convolution expansion specifically involves padding eight zeros around the 1 &#215; 1 convolution kernel, and the padding is 1. As shown in <xref rid="sensors-25-05325-f007" ref-type="fig">Figure 7</xref>, the input channels are assumed to be 2, and the BN layer is expanded to have the center of the convolutional kernel corresponding to each channel of 1, and the rest are 0.</p><p>The second problem of the inference phase is how to fuse Conv and BN layers for inference acceleration. Considering that Conv and BN are linear operations, the Conv and BN modules can be set into a single linear operation for inference acceleration. For the convolutional layer, the number of channels in each convolutional kernel is the same as the number of channels in the input feature map. The number of convolutional kernels determines the number of channels in the output feature map. In contrast, the inference pattern BN layer (&#947; and &#946; are known) contains four main parameters: mean &#956;, variance &#963;, and learnable parameters &#947; and &#946;. Here &#956; and &#963; are the statistics of the training process, and &#947; and &#946; are learned through the network. For the ith channel of the feature map, BN is calculated as in Equation (2). Therefore, the weight calculation formula of the ith convolution kernel of the new convolutional layer after transformation is as shown in Equations (3) and (4).<disp-formula id="FD2-sensors-25-05325"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>&#956;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#947;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#946;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#215;</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#947;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05325"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#947;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05325"><label>(4)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>&#947;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After the above operation, introducing the RepConv module for multi-branch learning during training enables the network to focus on more helpful information. At the same time, the fusion of multiple branches into a single branch during inference avoids the problem of increasing the inference speed.</p></sec></sec><sec id="sec3dot3-sensors-25-05325"><title>3.3. Head</title><p>In order to ensure that the detection task and the ReID task operate on the feature maps independently of each other, the three feature maps obtained by the feature fusion component are subjected to the CCN operation. Then the three different-sized process feature maps are input to the detection module and the SAAN module with scale awareness, respectively. The Detect module is used to perform the object detection task on the feature map, predict the anchor boxes by the grid, and perform the classification loss and regression loss calculation based on the class and location of the ground truth. The SAAN module fuses three different scales of different channel feature maps to unify the ReID feature scale and then captures the relevant information by convolution according to the total number of IDs to output a 1D vector representation of the corresponding feature map.</p><sec id="sec3dot3dot1-sensors-25-05325"><title>3.3.1. CCN Operation</title><p>The core objective of the CCN is to enhance feature representation capability, aiming to extract general and specific features that are more suitable for detection and ReID tasks. It achieves this by exploring the autocorrelation between feature channels and the cross-correlation between tasks, thereby improving the model&#8217;s ability to capture key features and obtaining more robust and discriminative feature representations in different tasks.</p><p>The input is the feature map <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> from the neck part. Here, <italic toggle="yes">C</italic> represents the number of channels, and <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> denote the height and width of the feature map. To reduce computational complexity while retaining key statistical information, an average pooling operation is first performed on the feature map to obtain the reduced-dimensional feature map <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#8242;</mml:mo><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <italic toggle="yes">H&#8242;</italic> &lt; <italic toggle="yes">H</italic> and <italic toggle="yes">W&#8242;</italic> &lt; <italic toggle="yes">W</italic>. Next, two different convolutional layers are used to process the reduced-dimensional feature map, generating two intermediate feature maps <bold><italic toggle="yes">T</italic><sub>1</sub></bold> and <bold><italic toggle="yes">T</italic><sub>2</sub></bold>. These two feature maps correspond to the feature representation branches for the detection task and the ReID task, respectively. Subsequently, <bold><italic toggle="yes">T</italic><sub>1</sub></bold> and <bold><italic toggle="yes">T</italic><sub>2</sub></bold> are reshaped into matrix forms <bold><italic toggle="yes">M</italic><sub>1</sub></bold> and <bold><italic toggle="yes">M</italic><sub>2</sub></bold>, with dimensions <italic toggle="yes">C</italic> &#215; <italic toggle="yes">N</italic>&#8242;, where <italic toggle="yes">N</italic>&#8242; = <italic toggle="yes">H</italic>&#8242; &#215; <italic toggle="yes">W</italic>&#8242;.</p><p>To capture the channel dependencies within each task, the CCN computes the self-attention graph. Specifically, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="bold-italic">T</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the self-attention of the detection branch, reflecting the correlation between the relevant channels for detection; similarly, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="bold-italic">T</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the self-attention of the ReID branch, emphasizing the relationship between the identity discrimination channels. To facilitate knowledge sharing and complementarity between tasks, the CCN also computes the cross-attention graph. <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="bold-italic">T</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the cross-task attention from the ReID branch to the detection branch, while <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="bold-italic">T</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the attention from detection to ReID. These cross-attention graphs help the model learn the shared semantic information between the two tasks.</p><p>Subsequently, the self-attention and cross-attention are fused. For the detection branch, the fused attention map is <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. For the ReID branch, it is <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. This fusion mechanism retains the task-specific feature responses while introducing cross-task context information. Finally, the weighted and reconstructed original feature maps are obtained using the fused attention map. Specifically, the output feature maps <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#183;</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are achieved, achieving feature enhancement. Through the residual connection structure, the original information is not damaged, and the response of the key channels is enhanced.</p><p>In summary, the CCN achieves effective enhancement of the features for detection and ReID tasks through steps such as dimensionality reduction, branch processing, self-attention calculation, cross-attention calculation, attention fusion, and feature reconstruction. This structure not only improves the performance of a single task but also promotes collaborative learning among multiple tasks.</p></sec><sec id="sec3dot3dot2-sensors-25-05325"><title>3.3.2. Detection Loss Calculation</title><p>The Detect module splits the input feature map into grids. It then maps the grid coordinates to the original map, with each grid predicting three metrics: rectangular box, confidence, and classification probability [<xref rid="B30-sensors-25-05325" ref-type="bibr">30</xref>]. The rectangular box characterizes the size and position of the object. The confidence characterizes the confidence level of the predicted rectangular box and ranges from 0 to 1. The classification probability characterizes the class of the target. The loss function measures the distance between predicted and expected information. The closer the predicted information is to the expected information, the smaller the value of the loss function. Training contains three main aspects of loss: rectangular box loss, confidence loss, and classification loss. Thus, the loss function of detection is defined as in Equation (5).<disp-formula id="FD5-sensors-25-05325"><label>(5)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#947;</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The overall loss is the weighted sum of three losses. Usually <italic toggle="yes">&#946;</italic> is 0.4, and <italic toggle="yes">&#945;</italic> and <italic toggle="yes">&#947;</italic> are both equal to 0.3. In this paper, we use the siou loss [<xref rid="B31-sensors-25-05325" ref-type="bibr">31</xref>] to calculate the rectangular box loss, and both confidence and classification losses are calculated by cross-entropy loss.</p></sec><sec id="sec3dot3dot3-sensors-25-05325"><title>3.3.3. Appearance Embedding Loss Calculation</title><p>Appearance embedding requires that the distance metric of different targets should be large enough, so this network treats the appearance embedding problem as a classification problem. The embedding dimension is mapped into categories of the total number of IDs by a linear classification layer. Finally, the loss is calculated according to the cross-entropy loss of the output and the label. Suppose an anchor box instance in a batch of samples is <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, a positive sample is <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and a negative sample is <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>&#8722;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. In the loss calculation, we focus on all negative sample classifications. Here, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>f</mml:mi><mml:mo>&#8722;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability that the anchor instance is considered a positive sample and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>f</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability that the anchor is considered the <italic toggle="yes">j</italic>th category, where the subscript <italic toggle="yes">i</italic> denotes the <italic toggle="yes">i</italic>th sample. As in Equation (6), the appearance embedding loss is calculated using a form similar to the cross-entropy loss.<disp-formula id="FD6-sensors-25-05325"><label>(6)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>log</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#8721;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>&#8722;</mml:mo></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As in Equation (7), the total loss is the weighted sum of the two losses. Here &#945; and &#946; are task-independent uncertainties, which are parameters learned based on the data distribution of the dataset.<disp-formula id="FD7-sensors-25-05325"><label>(7)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec3dot4-sensors-25-05325"><title>3.4. Multiple Association Strategies</title><p>Based on YOLOv5, a data association module was integrated to establish the association relationship of targets between consecutive frames. Specifically, we used Kalman filtering to track the detected targets, thereby achieving multi-target tracking. The description of the entire association process is shown in the following <xref rid="sensors-25-05325-f008" ref-type="fig">Figure 8</xref>.</p><p>For the first frame of the tracking sequence, the status of all detection boxes is recorded and they are added to the historical trajectory set, marking them as in the &#8220;pending&#8221; state. If there are detection boxes that match the trajectory in the next three consecutive frames, then the &#8220;pending&#8221; state of the trajectory is converted to the &#8220;tracked&#8221; state.</p><p>When conducting subsequent tracking of the sequence, there are four matching processes: first association, where the high-score detections that failed to match are re-matched with the joint trajectories; second association, where the low-score detections and the trajectories in the tracking state are matched; third association, where the high-score detections are matched with the undetermined trajectories; and fourth association, where all the trajectories are matched. Before the matching, based on the confidence score of the detection box, the detections are classified as high-score detections and low-score detections. At the same time, the Kalman filter is used to predict the positions of the trajectories in the tracking state and the undetermined state. Then, the distance measurement matrix <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated, and its calculation method can be obtained from Equation (8).<disp-formula id="FD8-sensors-25-05325"><label>(8)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mi>i</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mi>j</mml:mi></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi><mml:mi>I</mml:mi><mml:mi>O</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mi>j</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equation (8), <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> represents the appearance information, <italic toggle="yes">Embedding</italic> is a function that measures the cosine distance of the appearance information, <italic toggle="yes">KF</italic> is the Kalman filtering operation, and <italic toggle="yes">GIOU</italic> is a function that measures the intersection-over-union ratio between the detection box and the motion prediction box. Compared with the original IOU distance measurement, the value range of <italic toggle="yes">GIOU</italic> measurement has been doubled. Therefore, a scaling factor is added in front of it to reduce the contribution of <italic toggle="yes">GIOU</italic> measurement to the total distance.</p><p>The first matching process is to match the high-score detections with the associated trajectories, resulting in unmatched trajectories, unmatched high-score detections, and matched detections and trajectories. For the matched high-score detections, they are added to the corresponding trajectories and the trajectories are updated; for the unmatched joint trajectories, their states will be judged and the temporarily stored trajectories and the tracked trajectories will be separated. If the temporarily stored state exceeds the set maximum trajectory storage duration, the trajectory will be deleted; otherwise, the temporarily stored state will be retained until the next frame; for the unmatched high-score detections, their information will be retained.</p><p>During the second matching process, the high-score detection that failed in the first match and the trajectories that failed the match and are in the tracking state will be matched using IOU. The same three results will be obtained. If the high-score box successfully matches the tracking state trajectory, the trajectory will be updated; for the high-score detection that failed the tracking, it will be retained for the fourth match; for the tracking state trajectory that failed the match, it will be retained for the third match with the low-score detection box.</p><p>During the third matching process, the low-score detection bounding boxes are matched with the trajectories of the tracking states that failed in both matching attempts. Three results are obtained. For the trajectories of the low-score detections that were successfully matched with the tracking states, the trajectory states are updated; for the unmatched low-score detections, they are deleted; and for the unmatched tracking states of the detections, their states are changed to a pending state.</p><p>During the fourth matching process, the unmatched high-score detections retained from the second matching are matched with the undetermined trajectories using IOU (intersection over union) to obtain three results. For the high-score detections that are successfully matched, they are added to the assigned trajectory and the trajectory is updated; for the unmatched high-score detections, it is considered that a new target has entered the frame, a new trajectory is created for them, and they are marked as undetermined; for the undetermined trajectories that are not successfully matched, they are deleted.</p></sec></sec><sec id="sec4-sensors-25-05325"><title>4. Experimental Results and Analysis</title><sec id="sec4dot1-sensors-25-05325"><title>4.1. Datasets and Evaluation Criteria</title><sec id="sec4dot1dot1-sensors-25-05325"><title>4.1.1. Datasets</title><p>In line with the training set setup of CSTrack, the training set used in this paper is a mixture of six publicly available datasets for pedestrian detection and multi-object tracking. It contains the ETH dataset [<xref rid="B32-sensors-25-05325" ref-type="bibr">32</xref>], the CityPersons dataset [<xref rid="B33-sensors-25-05325" ref-type="bibr">33</xref>], the CalTech dataset [<xref rid="B34-sensors-25-05325" ref-type="bibr">34</xref>], the CUHK-SYSU dataset [<xref rid="B35-sensors-25-05325" ref-type="bibr">35</xref>], the PRW dataset [<xref rid="B36-sensors-25-05325" ref-type="bibr">36</xref>], and the MOT17 dataset [<xref rid="B37-sensors-25-05325" ref-type="bibr">37</xref>], where the MOT17 video image sequence data are identical to MOT16, and the only difference between the two is that the MOT17 annotation is more accurate and diverse. Similar to the JDE validation set, the training process used the CalTech dataset for the ReID task and the CityPersons dataset for the detection task. Finally, the MOT16 test set with no annotation information was evaluated and submitted to the MOT server for tracking effect evaluation. The number of images, the number of annotated boxes, and the number of IDs for the mixed dataset are listed in <xref rid="sensors-25-05325-t001" ref-type="table">Table 1</xref>. After data processing, the training set included 53,580 images, 275,000 bounding boxes, and 15547 ID annotations.</p></sec><sec id="sec4dot1dot2-sensors-25-05325"><title>4.1.2. Measurement</title><p>Multi-object tracking is a task that includes object detection and data association, and it is difficult to evaluate the performance of a multi-object tracking system with a single metric. The metric evaluation of this system should include the following three characteristics: timeliness, consistency, and stability. Specifically, all appearing objects should be able to be found in time. The object position should match the actual object position as much as possible. Each object should be assigned a unique ID, and this ID assigned to the object should remain constant throughout the sequence. In this paper, we use the CLEAR MOT metric from the MOT Challenge online evaluation system, which is generally accepted in academia, to evaluate the performance of the multi-object tracking system proposed. The main evaluation metrics and the meaning of each metric are shown in <xref rid="sensors-25-05325-t002" ref-type="table">Table 2</xref>, where &#8220;&#8593;&#8221; values in parentheses indicate that the higher the value, the better the performance, and &#8220;&#8595;&#8221; values in brackets indicate that the lower the value, the better the performance.</p><p>This paper selects the recognized MOTA and IDF1 indicators as the primary evaluation indicators. MOTA includes the number of times a tracker performs wrong object positioning and matching, and IDF1 includes whether a tracker can track an object for a long time. The formulations of MOTA and IDF1 are shown in Equations (9) and (10), where GT denotes the number of annotated object boxes, IDTP denotes the number of object boxes with correctly assigned IDs, and IDFP denotes the number of object boxes with incorrectly assigned IDs. IDFN denotes the number of object boxes that are not assigned IDs in the annotation.<disp-formula id="FD9-sensors-25-05325"><label>(9)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05325"><label>(10)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec4dot2-sensors-25-05325"><title>4.2. Experimental Parameter Setting</title><p>The experimental hardware setup, including an Intel Core i9-11900K CPU, PM9A1 NVMe Samsung 1024 GB SSD, NVIDIA GeForce RTX 3090 GPU, and 128 GB memory, was purchased in Changsha, China. The main software environment is Ubuntu 18.04 LTS, Pycharm2022.1.1, python3.8, torch1.8.1, and cuda toolkit 11.3. The improved version 6.0 is taken as the overall network of multi-object tracking tasks. In order to accelerate the convergence speed of the network, its pre-training weight on the COCO dataset is loaded, and the silu activation function is adopted. The predefined anchor boxes on the three feature maps are as follows: [8,24, 11,34, 16,48], [32,96, 45,135, 64,192], and [128,384, 180,540, 256,640]. In the training process, the SGD optimization algorithm is used for 100 epochs of training. The decay factor is 5 &#215; 10<sup>&#8722;4</sup>, the initial learning rate is 5 &#215; 10<sup>&#8722;3</sup>, the final learning rate is 5 &#215; 10<sup>&#8722;4</sup>, the batch size is 16, and the embedding dimension is 512. In addition, data enhancement techniques such as multi-scale input, random cropping, color dithering, and image flipping are used to prevent model overfitting. In the testing phase, since the MOT challenge limits the number of submissions and the test set does not contain annotation information, the trained weights are used to fine-tune the model by experimentally verifying the effect on MOT16-train with annotation information. The fine-tuned model experiments on MOT16-test are conducted, and finally, the obtained detection information and identity information are stored in text documents and submitted to the server for evaluation.</p></sec><sec id="sec4dot3-sensors-25-05325"><title>4.3. Experimental Results</title><p><xref rid="sensors-25-05325-t003" ref-type="table">Table 3</xref> shows the results of the proposed model RGTrack compared with the JDE improved algorithm CSTrack on the MOT16 training set. In general, RGTrack obtained a 1.70% improvement in MOTA and a 1.22% improvement in IDF1. From the results of the model predictions, RGTrack is less likely to predict false negative and false positive cases than the CSTrack model. Therefore, MOTA gained a boost, indicating that the addition of GC attention to the feature extraction network achieved some effect. From the results of the data association, RGTrack reduced the total number of ID Sw to 308. Therefore, the IDF1 metric was also improved, indicating that introducing multi-branch training in the feature fusion is practical. Compared with CSTrack, the model size of RGTrack has been reduced by 3.08%. It can effectively reduce the complexity of the model and is suitable for deployment on small mobile devices.</p><p><xref rid="sensors-25-05325-f009" ref-type="fig">Figure 9</xref> shows the training curve results. <xref rid="sensors-25-05325-f009" ref-type="fig">Figure 9</xref>a depicts the error variation in the model&#8217;s regression of target bounding boxes during the training process. From the graph, it can be seen that this loss value was relatively high in the initial stage of training, approximately 0.045, indicating that the model&#8217;s ability to locate the target position was weak at the beginning. As the training progressed, the curve rapidly decreased and stabilized after about 50 epochs, ultimately converging to approximately 0.015. This trend indicates that the model gradually learned more precise positioning features and could effectively reduce the deviation between the predicted box and the real box. The continuous decrease in the loss without significant fluctuations also indicates that the training process is relatively stable, and the optimization strategies (such as learning rate setting and gradient update) are reasonable, without experiencing severe oscillations or divergence.</p><p><xref rid="sensors-25-05325-f009" ref-type="fig">Figure 9</xref>b shows the confidence loss of the model in judging the existence of the target on the training set. At the beginning of the training, this value was approximately 0.035, indicating that the model&#8217;s ability to distinguish the target area was limited at the initial stage, and it was prone to mistakenly identifying the background as the target or failing to detect the real target. As the training progressed, the curve continued to decline and converged around 50 epochs, ultimately stabilizing at approximately 0.018. This downward trend indicates that the model gradually enhanced its sensitivity to the target area, was able to more accurately distinguish the target from the background, reflecting the stability of the training process, without signs of overfitting or training collapse, and improved the reliability of the detection.</p><p><xref rid="sensors-25-05325-f009" ref-type="fig">Figure 9</xref>c measures the proportion of actual positive samples among the predicted positive samples by the model, which represents the &#8220;accuracy&#8221; of the detection results. In the early stage of training, the precision rate was relatively low, close to 0.2, indicating that the model generated a large number of false positives. However, as training progressed, the precision rate rose rapidly, reaching above 0.8 after approximately 20 epochs, and remained at a high level in subsequent training. This significant improvement indicates that as the train/box_loss and train/obj_loss curves decrease, the model not only improves its localization ability but also significantly reduces false positive phenomena, and the output detection results are more reliable. The high and stable precision rate means that the model has strong discriminative ability and practicality under the current validation conditions.</p><p><xref rid="sensors-25-05325-t004" ref-type="table">Table 4</xref> shows the ablation experiments of the module RepVGG GC Block proposed in this paper on CSTrack. The dataset for this ablation experiment is set as follows: the first half of MOT17-train is used for the training set, MOT15-val is used for validation during the training process, and the second half of MOT17-train is finally used for model performance qualification. For subsequent comparison, the models in <xref rid="sensors-25-05325-t004" ref-type="table">Table 4</xref> are named A, B, C, and D in that order. In terms of run speed FPS, models B and D with Rep did not show a significant reduction in run speed compared to their counterparts A and C without the module (27.92 vs. 28.09, 27.52 vs. 27.56). Comparing A with B, A performs well in IDF1 and ID Sw (69.7% vs. 68.4%, 442 vs. 519). This indicates that the addition of the reconfiguration parameter convolution can better fuse features at different levels during the feature fusion phase, improving the association ability of the network. Comparing A with C, we can find that C is slightly weaker than A in predicting negative samples (16,353 vs. 16,340), and its running speed decreases from 28.09 to 27.56. At the same time, C outperforms A in all other aspects, which indicates that the C model loses some speed with the addition of the global context module. However, it can better focus on important information in the feature extraction phase and enhance the model&#8217;s detection capability. Comparing A with D, we can find that D combines the advantages of both B and C. Apart from a slight decrease in running speed (27.52 vs. 28.09), its tracking performance (61.9% vs. 61.0%), association effect (70.2% vs. 68.4%), and detection performance (3908 vs. 4151, 15,777 vs. 16,340) are all improved. This indicates that adding the reconfiguration parameter convolution module and the global context module can improve tracking performance with a slight loss of processing speed.</p><p><xref rid="sensors-25-05325-t005" ref-type="table">Table 5</xref> compares the results of the different methods on the MOT16 test set. This paper divides the networks in the table into two categories: two-stage and one-stage methods. The methods with &#8220;*&#8221; in the table are one-stage methods, and the rest are two-stage methods. Since FPSs are different with different test hardware, the running speed results in this paper are all on the hardware and software environment described in Chapter 3. The two-stage approach will have a lower FPS than the current value as the corresponding FPS term only corresponds to the correlation part of the data, and in practice, the fusion detection step will take longer. The inference speed of the one-stage methods is related to all steps in the system, from detection to association. From <xref rid="sensors-25-05325-t005" ref-type="table">Table 5</xref>, we can conclude that the JDE inference speed is much higher than that of the particular detection embedding method with comparable tracking accuracy. However, its IDF1 scores do not reach the same level, which shows that the JDE model does not effectively handle frequent occlusion cases. Compared with JDE&#8217;s improved algorithm CSTrack, the method proposed in this paper not only maintains the real-time operation speed but also increases the MOTA by 1.15% and the IDF1-related accuracy by 1.73%. Moreover, the MT of this model has increased by 6.86%, and ID Sw has decreased by 47.49%. This indicates that this model can more accurately track more targets and better maintain the consistency of target identities when dealing with complex scenarios. The FPS of this model has increased by 51.48%, and the model size has decreased by 3.08%. This demonstrates a significant improvement in the detection speed of the model, making it suitable for embedded devices. Furthermore, the proposed RGTrack model outperforms multiple existing detection models in terms of detection accuracy, detection speed, and model lightweighting. This demonstrates the novelty of the proposed model.</p></sec><sec id="sec4dot4-sensors-25-05325"><title>4.4. Visualization Results</title><p>To visually demonstrate the superiority of the RGTrack model, <xref rid="sensors-25-05325-f010" ref-type="fig">Figure 10</xref> shows the detection results of the CSTrack and the improved RGTrack models for detecting dense crowds. The left figures (a) and (c) represent the inference results of the CSTrack model, while the right figures (b) and (d) represent the inference results of the improved model RGTrack. The green boxes indicate the correct detections made by the model, the red boxes represent the missed detections, and the blue boxes are the incorrect predictions. By comparing <xref rid="sensors-25-05325-f010" ref-type="fig">Figure 10</xref>a with <xref rid="sensors-25-05325-f010" ref-type="fig">Figure 10</xref>b, it can be seen that the CSTrack model missed detecting the blue pedestrians in the scene, while the improved model detected all the targets in the scene. By comparing <xref rid="sensors-25-05325-f010" ref-type="fig">Figure 10</xref>c with <xref rid="sensors-25-05325-f010" ref-type="fig">Figure 10</xref>d, it can be found that the CSTrack model had five red boxes and three blue boxes (i.e., five missed detections and three false detections) compared to the original annotations, while the RGTrack model had five missed detections and one false detection. Overall, the RGTrack model has a certain improvement effect in detection accuracy compared to the CSTrack model.</p><p><xref rid="sensors-25-05325-f011" ref-type="fig">Figure 11</xref> shows the tracking results of CSTrack and RGTrack. The left image is the tracking result of CSTrack, and the right image is the tracking result of RGTrack. At frame 177, CSTrack detected 52 pedestrians, while RGTrack detected 59 pedestrians. In the central area of the image, due to the limited ability to detect occluded pedestrians, CSTrack identified the three pedestrians at the bottom of the picture as two targets and assigned two IDs (6, 70), while RGTrack accurately detected three pedestrians and assigned three IDs (36, 67, 78). Among the 10 pedestrians on the right side of the image, CSTrack detected 8 pedestrians, but failed to detect the occluded black-shirted and gray-shirted pedestrians, while RGTrack detected all 10 pedestrians and assigned IDs 56 and 72 to the black-shirted and gray-shirted pedestrians, respectively. Additionally, in the densely populated area of the upper left corner of the image, CSTrack detected 15 pedestrians, while RGTrack detected 19 pedestrians. At frame 204, CSTrack detected 55 pedestrians, while RGTrack detected 59 pedestrians. CSTrack still has the problem of poor detection in dense areas, only detecting one target (ID: 31) for the three pedestrians carrying bags, while RGTrack tracked the pedestrians that were severely occluded with IDs 104 and 107, and accurately tracked the three pedestrians carrying bags (ID: 6, 31, 34). At frame 495, CSTrack detected 67 pedestrians, while RGTrack detected 70 pedestrians. In the upper right corner of the image, CSTrack changed the ID of the stationary green-shirted pedestrian from 44 to 202, while RGTrack continuously tracked it, and the IDs assigned to the pink-shirted and two white-shirted pedestrians were 92, 134, and 177. Overall, the number of IDs assigned in CSTrack is excessive, resulting in inaccurate tracking. The tracking effect of RGTrack is better than that of CSTTrack, and it can generate more stable trajectories.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05325"><title>5. Discussion</title><p>Inspired by the representative model of the JDE framework, we proposed a multi-object tracking algorithm to solve the problem that the inaccurate detection ability of the JDE model in occlusion scenarios leads to unstable ID allocation in data association and unsatisfactory tracking accuracy. By introducing the global attention mechanism in the feature extraction stage and using multi-branch training in the fusion stage, the detection task and appearance embedding task are well integrated with one model. The experimental results show that the model proposed in this paper can achieve a good tracking effect with a slight loss of inference speed and a small number of parameters added, which provides support for implementing the algorithm in intelligent video surveillance.</p><p>However, this study also has limitations. For the proposed new method, although it has achieved comprehensive improvements in detection accuracy, detection speed, and model size, there is still room for further enhancement to better achieve a balance between model complexity and detection accuracy. The specific challenges are as follows:<list list-type="simple"><list-item><label>(1)</label><p>In complex scenarios, the target may be obscured by other objects or other targets, which can lead to tracking loss or confusion of identity. The existing tracking algorithms have difficulty maintaining the correct identity of the target under prolonged occlusion.</p></list-item><list-item><label>(2)</label><p>Multi-target tracking often requires processing a large number of targets in a large-scale scenario, which leads to high computational complexity and time consumption. Especially when the hardware resources are limited, how to balance the tracking accuracy and computational efficiency remains a challenge.</p></list-item><list-item><label>(3)</label><p>When the target approaches or intersects, an identity switch (ID switch) may occur, meaning that the tracker misidentifies the target&#8217;s identity. Although there are some methods to reduce ID switches, this remains a challenge in scenarios with high-density targets.</p></list-item><list-item><label>(4)</label><p>Different application scenarios, lighting conditions, background complexities, and target appearances and behavior patterns present various challenges for multi-target tracking. The generalization ability of existing models can still be enhanced.</p></list-item></list></p><p>To address the aforementioned issues, in future research directions, integrating deep learning with multimodal data (such as RGB images, depth maps, LiDAR data, etc.) can provide more comprehensive environmental and target information, thereby improving tracking accuracy and robustness. Future research can focus on how to effectively conduct long-term tracking and reidentification when the target reappears, especially in cases where the target has been out of contact for a long time. Real-time online learning and adaptive models can dynamically adjust the tracking strategy based on the changes in the target (such as appearance changes or motion changes) to enhance the accuracy and stability of tracking. Graph optimization methods can better model the spatial and temporal relationships between targets and reduce false associations and ID switches. Future research may further delve into this direction. To meet the real-time requirements of multi-target tracking in practical applications, how to improve the algorithm&#8217;s computational efficiency without sacrificing tracking accuracy is an important research direction for the future. Utilizing hardware acceleration technologies such as GPUs and TPUs may play a significant role in real-time tracking applications.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-05325"><title>6. Conclusions</title><p>In this paper, we have presented a novel multi-object tracking framework, the Reparameterized and Global Context Track (RGTrack), to address the limitations of existing joint detection and embedding (JDE) models in complex, occluded, and crowded scenes. By building upon the Correlation-Sensitive Track (CSTrack) architecture, RGTrack introduces a global attention mechanism and multi-branch training strategy to enhance feature representation, enabling more accurate detection and robust appearance embedding under challenging conditions. The integration of reparameterized convolutional blocks further strengthens the model&#8217;s capacity while maintaining efficiency. Additionally, a multiple association strategy is employed to improve cross-frame data association, significantly reducing identity switches. Experimental results demonstrate that RGTrack achieves substantial improvements over the baseline: MOTA increases by 1.15%, IDF1 by 1.73%, and MT by 6.86%, while ID switches are reduced by 47.49%. Notably, the model also achieves a 51.48% increase in FPS and a 3.08% reduction in model size, highlighting its superior balance between accuracy and efficiency. These advantages make RGTrack particularly suitable for real-time applications on embedded and resource-constrained platforms. While challenges remain in long-term occlusion, high-density interactions, and generalization across diverse environments, our work provides a strong foundation for practical multi-object tracking. Future efforts will explore multimodal fusion, online adaptation, and graph-based optimization to further advance tracking robustness and scalability.</p><p>Multi-target tracking also has many potential applications. In autonomous driving systems, it is necessary to track various targets such as vehicles, pedestrians, traffic signs, etc., on the road in real time. Efficient multi-target tracking can provide reliable dynamic environmental information for decision-making systems. In security surveillance systems, MOT can be used to track multiple suspicious individuals or objects and promptly trigger alarms, enhancing public safety. In sports events, MOT can be used to analyze the movements of athletes, team tactics, and key events during the competition. In navigation systems of robots or drones, MOT can help them identify and track dynamic obstacles or targets around them, achieving efficient path planning and obstacle avoidance. In medical image analysis, MOT can be used to track the dynamic changes in cells, organs, or tissues, providing support for disease diagnosis and treatment.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, W.L. and J.Y.; methodology, W.L. and J.Y.; software, W.L. and J.Y.; validation, F.J. and M.W.; formal analysis, W.L.; investigation, J.Y.; resources, F.J.; data curation, J.Y.; writing&#8212;original draft preparation, W.L.; writing&#8212;review and editing, F.J.; visualization, J.Y.; supervision, M.W.; project administration, F.J.; funding acquisition, F.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data used in this study are publicly available.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05325"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>L.M.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Han</surname><given-names>D.</given-names></name><name name-style="western"><surname>Moon</surname><given-names>H.</given-names></name></person-group><article-title>Multiple object tracking in deep learning approaches: A survey</article-title><source>Electronics</source><year>2021</year><volume>10</volume><elocation-id>2406</elocation-id><pub-id pub-id-type="doi">10.3390/electronics10192406</pub-id></element-citation></ref><ref id="B2-sensors-25-05325"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ciaparrone</surname><given-names>G.</given-names></name><name name-style="western"><surname>S&#225;nchez</surname><given-names>F.L.</given-names></name><name name-style="western"><surname>Tabik</surname><given-names>S.</given-names></name><name name-style="western"><surname>Troiano</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tagliaferri</surname><given-names>R.</given-names></name><name name-style="western"><surname>Herrera</surname><given-names>F.</given-names></name></person-group><article-title>Deep learning in video multi-object tracking: A survey</article-title><source>Neurocomputing</source><year>2020</year><volume>381</volume><fpage>61</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2019.11.023</pub-id></element-citation></ref><ref id="B3-sensors-25-05325"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>J.</given-names></name><name name-style="western"><surname>Milan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>T.K.</given-names></name></person-group><article-title>Multiple object tracking: A literature review</article-title><source>Artif. Intell.</source><year>2021</year><volume>293</volume><fpage>103448</fpage><pub-id pub-id-type="doi">10.1016/j.artint.2020.103448</pub-id></element-citation></ref><ref id="B4-sensors-25-05325"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ruan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Mukherjee</surname><given-names>M.</given-names></name></person-group><article-title>A survey of multiple pedestrian tracking based on tracking-by-detection framework</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2020</year><volume>31</volume><fpage>1819</fpage><lpage>1833</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2020.3009717</pub-id></element-citation></ref><ref id="B5-sensors-25-05325"><label>5.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Grigorev</surname><given-names>A.D.</given-names></name><name name-style="western"><surname>Gneushev</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Litvinchev</surname><given-names>I.S.</given-names></name></person-group><article-title>Re-Identification-Based Models for Multiple Object Tracking</article-title><source>Artificial Intelligence in Industry 4.0 and 5G Technology</source><publisher-name>John Wiley &amp; Sons</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2022</year><fpage>303</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1002/9781119798798.ch15</pub-id></element-citation></ref><ref id="B6-sensors-25-05325"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You only look once: Unified real-time object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#8211;1 July 2016</conf-date><fpage>779</fpage><lpage>788</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id></element-citation></ref><ref id="B7-sensors-25-05325"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>YOLO9000: Better, faster, stronger</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>7263</fpage><lpage>7271</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.690</pub-id></element-citation></ref><ref id="B8-sensors-25-05325"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><article-title>Towards real-time multi-object tracking</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><fpage>107</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-58621-8_7</pub-id></element-citation></ref><ref id="B9-sensors-25-05325"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>Yolov3: An incremental improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id><pub-id pub-id-type="arxiv">1804.02767</pub-id></element-citation></ref><ref id="B10-sensors-25-05325"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>W.</given-names></name></person-group><article-title>Rethinking the competition between detection and ReID in multi-object tracking</article-title><source>IEEE Trans. Image Process.</source><year>2022</year><volume>31</volume><fpage>3182</fpage><lpage>3196</lpage><pub-id pub-id-type="doi">10.1109/TIP.2022.3165376</pub-id><pub-id pub-id-type="pmid">35412982</pub-id></element-citation></ref><ref id="B11-sensors-25-05325"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bewley</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ott</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>F.</given-names></name><name name-style="western"><surname>Upcroft</surname><given-names>B.</given-names></name></person-group><article-title>Simple online and realtime tracking</article-title><source>Proceedings of the IEEE International Conference on Image Processing</source><conf-loc>Phoenix, AZ, USA</conf-loc><conf-date>25&#8211;28 September 2016</conf-date><fpage>3464</fpage><lpage>3468</lpage></element-citation></ref><ref id="B12-sensors-25-05325"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wojke</surname><given-names>N.</given-names></name><name name-style="western"><surname>Bewley</surname><given-names>A.</given-names></name><name name-style="western"><surname>Paulus</surname><given-names>D.</given-names></name></person-group><article-title>Simple online and realtime tracking with a deep association metric</article-title><source>Proceedings of the IEEE International Conference on Image Processing</source><conf-loc>Beijing, China</conf-loc><conf-date>17&#8211;20 September 2017</conf-date><fpage>3645</fpage><lpage>3649</lpage><pub-id pub-id-type="doi">10.1109/ICIP.2017.8296962</pub-id></element-citation></ref><ref id="B13-sensors-25-05325"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Feichtenhofer</surname><given-names>C.</given-names></name><name name-style="western"><surname>Pinz</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Detect to track and track to detect</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>3038</fpage><lpage>3046</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.330</pub-id></element-citation></ref><ref id="B14-sensors-25-05325"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>C.</given-names></name></person-group><article-title>Real-time multiple people tracking with deeply learned candidate selection and person re-identification</article-title><source>Proceedings of the IEEE International Conference on Multimedia and Expo</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>23&#8211;27 July 2018</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B15-sensors-25-05325"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bergmann</surname><given-names>P.</given-names></name><name name-style="western"><surname>Meinhardt</surname><given-names>T.</given-names></name><name name-style="western"><surname>Leal-Taixe</surname><given-names>L.</given-names></name></person-group><article-title>Tracking without bells and whistles</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>941</fpage><lpage>951</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2019.00103</pub-id></element-citation></ref><ref id="B16-sensors-25-05325"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kitani</surname><given-names>K.</given-names></name><name name-style="western"><surname>Weng</surname><given-names>X.</given-names></name></person-group><article-title>Joint object detection and multi-object tracking with graph neural networks</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date>30 May&#8211;5 June 2021</conf-date><fpage>13708</fpage><lpage>13715</lpage><pub-id pub-id-type="doi">10.1109/ICRA48506.2021.9561110</pub-id></element-citation></ref><ref id="B17-sensors-25-05325"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jocher</surname><given-names>G.</given-names></name><name name-style="western"><surname>Stoken</surname><given-names>A.</given-names></name><name name-style="western"><surname>Borovec</surname><given-names>J.</given-names></name><name name-style="western"><surname>Changyu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hogan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Diaconu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Dave</surname><given-names>P.</given-names></name></person-group><source>ultralytics/yolov5</source><comment>v3.0</comment><publisher-name>Zenodo</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2020</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/yolov5" ext-link-type="uri">https://github.com/ultralytics/yolov5</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-05-01">(accessed on 1 May 2024)</date-in-citation></element-citation></ref><ref id="B18-sensors-25-05325"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Minu</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Canessane</surname><given-names>R.A.</given-names></name></person-group><article-title>Deep learning-based aerial image classification model using Inception with residual network and multilayer perceptron</article-title><source>Microprocess. Microsyst.</source><year>2022</year><volume>95</volume><fpage>104652</fpage><pub-id pub-id-type="doi">10.1016/j.micpro.2022.104652</pub-id></element-citation></ref><ref id="B19-sensors-25-05325"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Machado</surname><given-names>P.</given-names></name><name name-style="western"><surname>Matic</surname><given-names>I.</given-names></name><name name-style="western"><surname>de Lemos</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ihianle</surname><given-names>I.K.</given-names></name><name name-style="western"><surname>Adama</surname><given-names>D.A.</given-names></name></person-group><article-title>Estimating the power consumption of heterogeneous devices when performing AI inference</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2207.06150</pub-id></element-citation></ref><ref id="B20-sensors-25-05325"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>M.</given-names></name></person-group><article-title>A comprehensive review of methods based on deep learning for diabetes-related foot ulcers</article-title><source>Front. Endocrinol.</source><year>2022</year><volume>13</volume><elocation-id>945020</elocation-id><pub-id pub-id-type="doi">10.3389/fendo.2022.945020</pub-id><pub-id pub-id-type="pmid">36004341</pub-id><pub-id pub-id-type="pmcid">PMC9394750</pub-id></element-citation></ref><ref id="B21-sensors-25-05325"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>A.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><article-title>Non-local neural networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>7794</fpage><lpage>7803</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00813</pub-id></element-citation></ref><ref id="B22-sensors-25-05325"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#8211;9 December 2017</conf-date></element-citation></ref><ref id="B23-sensors-25-05325"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00745</pub-id></element-citation></ref><ref id="B24-sensors-25-05325"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name></person-group><article-title>GCNet: Non-local networks meet squeeze-excitation networks and beyond</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><pub-id pub-id-type="doi">10.1109/ICCVW.2019.00246</pub-id></element-citation></ref><ref id="B25-sensors-25-05325"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>C.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.</given-names></name></person-group><article-title>Accurate and efficient traffic participant detection based on optimized features and multi-scale localization confidence</article-title><source>Proc. Inst. Mech. Eng.</source><year>2023</year><volume>237</volume><fpage>2797</fpage><lpage>2809</lpage><pub-id pub-id-type="doi">10.1177/09544070221119282</pub-id></element-citation></ref><ref id="B26-sensors-25-05325"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sutskever</surname><given-names>I.</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Commun. ACM</source><year>2017</year><volume>60</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="B27-sensors-25-05325"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#8211;1 July 2016</conf-date><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="B28-sensors-25-05325"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1409.1556</pub-id></element-citation></ref><ref id="B29-sensors-25-05325"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>N.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>RepVGG: Making VGG-style ConvNets Great Again</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>19&#8211;25 June 2021</conf-date><fpage>13733</fpage><lpage>13742</lpage><pub-id pub-id-type="doi">10.1109/CVPR46437.2021.01352</pub-id></element-citation></ref><ref id="B30-sensors-25-05325"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>A real-time detection algorithm for Kiwifruit defects based on YOLOv5</article-title><source>Electronics</source><year>2021</year><volume>10</volume><elocation-id>1711</elocation-id><pub-id pub-id-type="doi">10.3390/electronics10141711</pub-id></element-citation></ref><ref id="B31-sensors-25-05325"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>D.</given-names></name></person-group><article-title>Distance-IoU loss: Faster and better learning for bounding box regression</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>New York, NY, USA</conf-loc><conf-date>7&#8211;12 February 2020</conf-date><fpage>12993</fpage><lpage>13000</lpage></element-citation></ref><ref id="B32-sensors-25-05325"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ess</surname><given-names>A.</given-names></name><name name-style="western"><surname>Leibe</surname><given-names>B.</given-names></name><name name-style="western"><surname>Schindler</surname><given-names>K.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>A mobile vision system for robust multi-person tracking</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>23&#8211;28 June 2008</conf-date><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2008.4587581</pub-id></element-citation></ref><ref id="B33-sensors-25-05325"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Benenson</surname><given-names>R.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name></person-group><article-title>Citypersons: A diverse dataset for pedestrian detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>3213</fpage><lpage>3221</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.474</pub-id></element-citation></ref><ref id="B34-sensors-25-05325"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wojek</surname><given-names>C.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P.</given-names></name></person-group><article-title>Pedestrian detection: A benchmark</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Miami, FL, USA</conf-loc><conf-date>20&#8211;25 June 2009</conf-date><fpage>304</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206631</pub-id></element-citation></ref><ref id="B35-sensors-25-05325"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Joint detection and identification feature learning for person search</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>3415</fpage><lpage>3424</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.360</pub-id></element-citation></ref><ref id="B36-sensors-25-05325"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chandraker</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Q.</given-names></name></person-group><article-title>Person re-identification in the wild</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>1367</fpage><lpage>1376</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.357</pub-id></element-citation></ref><ref id="B37-sensors-25-05325"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Milan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Leal-Taix&#233;</surname><given-names>L.</given-names></name><name name-style="western"><surname>Reid</surname><given-names>I.</given-names></name><name name-style="western"><surname>Roth</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schindler</surname><given-names>K.</given-names></name></person-group><article-title>MOT16: A benchmark for multi-object tracking</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/arXiv.1603.00831</pub-id><pub-id pub-id-type="arxiv">1603.00831</pub-id></element-citation></ref><ref id="B38-sensors-25-05325"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mahmoudi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ahadi</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Rahmati</surname><given-names>M.</given-names></name></person-group><article-title>Multi-target tracking using CNN-based features: CNNMTT</article-title><source>Multimed. Tools Appl.</source><year>2019</year><volume>78</volume><fpage>7077</fpage><lpage>7096</lpage><pub-id pub-id-type="doi">10.1007/s11042-018-6467-6</pub-id></element-citation></ref><ref id="B39-sensors-25-05325"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>J.</given-names></name></person-group><article-title>Poi: Multiple object tracking with high performance detection and appearance feature</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>8&#8211;16 October 2016</conf-date><fpage>36</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-48881-3_3</pub-id></element-citation></ref><ref id="B40-sensors-25-05325"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>C.</given-names></name></person-group><article-title>TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training Model</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>6308</fpage><lpage>6318</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00634</pub-id></element-citation></ref><ref id="B41-sensors-25-05325"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name></person-group><article-title>Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>145</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-58548-8_9</pub-id></element-citation></ref><ref id="B42-sensors-25-05325"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>RetinaMOT: Rethinking anchor-free YOLOv5 for online multiple object tracking</article-title><source>Complex Intell. Syst.</source><year>2023</year><volume>9</volume><fpage>5115</fpage><lpage>5133</lpage><pub-id pub-id-type="doi">10.1007/s40747-023-01009-3</pub-id></element-citation></ref><ref id="B43-sensors-25-05325"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>F.</given-names></name></person-group><article-title>Quasi-Dense Similarity Learning for Multiple Object Tracking</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>164</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00023</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05325-f001" orientation="portrait"><label>Figure 1</label><caption><p>Process diagram of multi-object tracking based on detection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g001.jpg"/></fig><fig position="float" id="sensors-25-05325-f002" orientation="portrait"><label>Figure 2</label><caption><p>The overall network structure of RGTrack.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g002.jpg"/></fig><fig position="float" id="sensors-25-05325-f003" orientation="portrait"><label>Figure 3</label><caption><p>The operational flow of global context attention.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g003.jpg"/></fig><fig position="float" id="sensors-25-05325-f004" orientation="portrait"><label>Figure 4</label><caption><p>Step comparison of Conv and RepConv.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g004.jpg"/></fig><fig position="float" id="sensors-25-05325-f005" orientation="portrait"><label>Figure 5</label><caption><p>Flow diagram of reorganization parameters in the inference phase.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g005.jpg"/></fig><fig position="float" id="sensors-25-05325-f006" orientation="portrait"><label>Figure 6</label><caption><p>Diagram of the 1 &#215; 1 convolution conversion to the 3 &#215; 3 convolution.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g006.jpg"/></fig><fig position="float" id="sensors-25-05325-f007" orientation="portrait"><label>Figure 7</label><caption><p>Diagram of the BN layer conversion to a 3 &#215; 3 convolution.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g007.jpg"/></fig><fig position="float" id="sensors-25-05325-f008" orientation="portrait"><label>Figure 8</label><caption><p>Process diagram of the multiple association strategy.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g008.jpg"/></fig><fig position="float" id="sensors-25-05325-f009" orientation="portrait"><label>Figure 9</label><caption><p>Training curve results. (<bold>a</bold>) The bounding box regression loss on the training set; (<bold>b</bold>) object confidence loss on the training set; (<bold>c</bold>) the precision rate of the model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g009.jpg"/></fig><fig position="float" id="sensors-25-05325-f010" orientation="portrait"><label>Figure 10</label><caption><p>Demonstration of detection effects of CSTrack and RGTrack. (<bold>a</bold>,<bold>c</bold>) The detection effect of CSTrack; (<bold>b</bold>,<bold>d</bold>) the detection effect of RGTrack.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g010a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g010b.jpg"/></fig><fig position="float" id="sensors-25-05325-f011" orientation="portrait"><label>Figure 11</label><caption><p>The tracking results of CSTrack and RGTrack. (<bold>a</bold>) The CSTrack detection result for frame 177; (<bold>b</bold>) the RGTrack detection result for frame 177; (<bold>c</bold>) the CSTrack detection result for frame 204; (<bold>d</bold>) the RGTrack detection result for frame 204; (<bold>e</bold>) the CSTrack detection result for frame 495; (<bold>f</bold>) the RGTrack detection result for frame 495.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g011a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05325-g011b.jpg"/></fig><table-wrap position="float" id="sensors-25-05325-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05325-t001_Table 1</object-id><label>Table 1</label><caption><p>Summary of training datasets [<xref rid="B9-sensors-25-05325" ref-type="bibr">9</xref>].</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ETH</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CT</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MOT17</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">CS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">PRW</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Imgs</td><td align="center" valign="middle" rowspan="1" colspan="1">2 K</td><td align="center" valign="middle" rowspan="1" colspan="1">3 K</td><td align="center" valign="middle" rowspan="1" colspan="1">27 K</td><td align="center" valign="middle" rowspan="1" colspan="1">5 K</td><td align="center" valign="middle" rowspan="1" colspan="1">11 K</td><td align="center" valign="middle" rowspan="1" colspan="1">6 K</td><td align="center" valign="middle" rowspan="1" colspan="1">54 K</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Boxes</td><td align="center" valign="middle" rowspan="1" colspan="1">17 K</td><td align="center" valign="middle" rowspan="1" colspan="1">27 K</td><td align="center" valign="middle" rowspan="1" colspan="1">46 K</td><td align="center" valign="middle" rowspan="1" colspan="1">112 K</td><td align="center" valign="middle" rowspan="1" colspan="1">55 K</td><td align="center" valign="middle" rowspan="1" colspan="1">18 K</td><td align="center" valign="middle" rowspan="1" colspan="1">275 K</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Identities</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9 K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.5 K</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05325-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05325-t002_Table 2</object-id><label>Table 2</label><caption><p>CLEAR MOT metrics for evaluation of multi-object tracking.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Measurement</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MOTA (&#8593;)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multiple-object tracking accuracy, a standard evaluation index for multiple-object tracking, is the first index for comprehensive evaluation of trackers.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IDF1 (&#8593;)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The Identification F1 Score of ID is the first index to judge the matching performance of a tracker.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MT (&#8593;)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A maximum tracking target number is the number of tracks for which the target tracking trajectory accounts for more than 80% of the annotated track length.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FP (&#8595;)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">False positive is the total number of incorrectly detected objects in the video sequence.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FN (&#8595;)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">False negative is the video sequence&#8217;s total number of missed detection targets.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID Sw (&#8595;)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Identification switch is the total number of times a trajectory has been ID-switched.</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05325-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05325-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of RGTrack and CSTrack results on MOT16-train.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MOTA (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">IDF1 (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FN</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ID Sw</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Size (MB)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CSTrack [<xref rid="B10-sensors-25-05325" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">82.4</td><td align="center" valign="middle" rowspan="1" colspan="1">82.1</td><td align="center" valign="middle" rowspan="1" colspan="1">5126</td><td align="center" valign="middle" rowspan="1" colspan="1">13,798</td><td align="center" valign="middle" rowspan="1" colspan="1">545</td><td align="center" valign="middle" rowspan="1" colspan="1">48.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGTrack</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4971</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12,652</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">308</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05325-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05325-t004_Table 4</object-id><label>Table 4</label><caption><p>Ablation experiment of RGTrack in MOT17-train.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Rep</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GC</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MOTA (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">IDF1 (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ID Sw</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FP</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FN</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CSTrack (A)</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">61.0</td><td align="center" valign="middle" rowspan="1" colspan="1">68.4</td><td align="center" valign="middle" rowspan="1" colspan="1">519</td><td align="center" valign="middle" rowspan="1" colspan="1">4151</td><td align="center" valign="middle" rowspan="1" colspan="1">16,340</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>28.09</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">R-CSTrack (B)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">60.2</td><td align="center" valign="middle" rowspan="1" colspan="1">69.7</td><td align="center" valign="middle" rowspan="1" colspan="1">442</td><td align="center" valign="middle" rowspan="1" colspan="1">4254</td><td align="center" valign="middle" rowspan="1" colspan="1">16,774</td><td align="center" valign="middle" rowspan="1" colspan="1">27.92</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">G-CSTrack (C)</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" rowspan="1" colspan="1">61.2</td><td align="center" valign="middle" rowspan="1" colspan="1">68.9</td><td align="center" valign="middle" rowspan="1" colspan="1">504</td><td align="center" valign="middle" rowspan="1" colspan="1">4055</td><td align="center" valign="middle" rowspan="1" colspan="1">16,353</td><td align="center" valign="middle" rowspan="1" colspan="1">27.56</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGTrack (D)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8730;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">384</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3908</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15,777</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.52</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05325-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05325-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of results of different methods on MOT16 test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MOTA (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">IDF1 (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MT (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ID Sw</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Size (MB)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeepSORT-2 [<xref rid="B12-sensors-25-05325" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">61.4</td><td align="center" valign="middle" rowspan="1" colspan="1">62.2</td><td align="center" valign="middle" rowspan="1" colspan="1">32.8</td><td align="center" valign="middle" rowspan="1" colspan="1">781</td><td align="center" valign="middle" rowspan="1" colspan="1">8.1</td><td align="center" valign="middle" rowspan="1" colspan="1">137.13</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNNMTT [<xref rid="B38-sensors-25-05325" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">65.2</td><td align="center" valign="middle" rowspan="1" colspan="1">62.2</td><td align="center" valign="middle" rowspan="1" colspan="1">32.4</td><td align="center" valign="middle" rowspan="1" colspan="1">946</td><td align="center" valign="middle" rowspan="1" colspan="1">6.4</td><td align="center" valign="middle" rowspan="1" colspan="1">85.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POI [<xref rid="B39-sensors-25-05325" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">66.1</td><td align="center" valign="middle" rowspan="1" colspan="1">65.1</td><td align="center" valign="middle" rowspan="1" colspan="1">34.0</td><td align="center" valign="middle" rowspan="1" colspan="1">805</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">94.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tube_TK [<xref rid="B40-sensors-25-05325" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">63.0</td><td align="center" valign="middle" rowspan="1" colspan="1">58.6</td><td align="center" valign="middle" rowspan="1" colspan="1">32.1</td><td align="center" valign="middle" rowspan="1" colspan="1">1137</td><td align="center" valign="middle" rowspan="1" colspan="1">3.0</td><td align="center" valign="middle" rowspan="1" colspan="1">155.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GSDT [<xref rid="B16-sensors-25-05325" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">66.2</td><td align="center" valign="middle" rowspan="1" colspan="1">68.7</td><td align="center" valign="middle" rowspan="1" colspan="1">31.5</td><td align="center" valign="middle" rowspan="1" colspan="1">1318</td><td align="center" valign="middle" rowspan="1" colspan="1">4.9</td><td align="center" valign="middle" rowspan="1" colspan="1">144.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CTrackerV1 [<xref rid="B41-sensors-25-05325" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">66.6</td><td align="center" valign="middle" rowspan="1" colspan="1">57.4</td><td align="center" valign="middle" rowspan="1" colspan="1">31.2</td><td align="center" valign="middle" rowspan="1" colspan="1">1529</td><td align="center" valign="middle" rowspan="1" colspan="1">6.8</td><td align="center" valign="middle" rowspan="1" colspan="1">139.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CJTracker * [<xref rid="B42-sensors-25-05325" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">58.7</td><td align="center" valign="middle" rowspan="1" colspan="1">58.2</td><td align="center" valign="middle" rowspan="1" colspan="1">35.5</td><td align="center" valign="middle" rowspan="1" colspan="1">877</td><td align="center" valign="middle" rowspan="1" colspan="1">16.0</td><td align="center" valign="middle" rowspan="1" colspan="1">95.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">QuasiDense * [<xref rid="B43-sensors-25-05325" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">68.7</td><td align="center" valign="middle" rowspan="1" colspan="1">66.3</td><td align="center" valign="middle" rowspan="1" colspan="1">35.6</td><td align="center" valign="middle" rowspan="1" colspan="1">1378</td><td align="center" valign="middle" rowspan="1" colspan="1">17.3</td><td align="center" valign="middle" rowspan="1" colspan="1">69.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">JDE * [<xref rid="B9-sensors-25-05325" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">64.4</td><td align="center" valign="middle" rowspan="1" colspan="1">55.8</td><td align="center" valign="middle" rowspan="1" colspan="1">35.4</td><td align="center" valign="middle" rowspan="1" colspan="1">1544</td><td align="center" valign="middle" rowspan="1" colspan="1">18.8</td><td align="center" valign="middle" rowspan="1" colspan="1">45.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CSTrack * [<xref rid="B10-sensors-25-05325" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">69.4</td><td align="center" valign="middle" rowspan="1" colspan="1">69.3</td><td align="center" valign="middle" rowspan="1" colspan="1">35.0</td><td align="center" valign="middle" rowspan="1" colspan="1">958</td><td align="center" valign="middle" rowspan="1" colspan="1">16.9</td><td align="center" valign="middle" rowspan="1" colspan="1">48.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGTrack *</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">503</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.2</td></tr></tbody></table><table-wrap-foot><fn><p>* The method marked with &#8220;*&#8221; belongs to a single-stage approach.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>