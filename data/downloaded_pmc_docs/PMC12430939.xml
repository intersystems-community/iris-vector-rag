<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430939</article-id><article-id pub-id-type="pmcid-ver">PMC12430939.1</article-id><article-id pub-id-type="pmcaid">12430939</article-id><article-id pub-id-type="pmcaiid">12430939</article-id><article-id pub-id-type="doi">10.3390/s25175536</article-id><article-id pub-id-type="publisher-id">sensors-25-05536</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Robust Anchor-Aided GNSS/PDR Pedestrian Localization via Factor Graph Optimization for Remote Sighted Assistance</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-6168-0372</contrib-id><name name-style="western"><surname>Huang</surname><given-names initials="S">Sen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05536" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-0462-867X</contrib-id><name name-style="western"><surname>Zhao</surname><given-names initials="J">Jinjing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05536" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1462-3642</contrib-id><name name-style="western"><surname>Zhong</surname><given-names initials="Y">Yihan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af2-sensors-25-05536" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="Y">Yiding</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05536" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9567-5980</contrib-id><name name-style="western"><surname>Xu</surname><given-names initials="S">Shengyong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05536" ref-type="aff">1</xref><xref rid="c1-sensors-25-05536" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Zhang</surname><given-names initials="B">Baocheng</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05536"><label>1</label>School of Electronics, Peking University, Beijing 100871, China; <email>senhuang@stu.pku.edu.cn</email> (S.H.); <email>zhaojinjing@pku.edu.cn</email> (J.Z.); <email>2301213327@pku.edu.cn</email> (Y.L.)</aff><aff id="af2-sensors-25-05536"><label>2</label>Department of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic University, Hong Kong, China; <email>yi-han.zhong@connect.polyu.hk</email></aff><author-notes><corresp id="c1-sensors-25-05536"><label>*</label>Correspondence: <email>xusy@pku.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5536</elocation-id><history><date date-type="received"><day>09</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>15</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>05</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05536.pdf"/><abstract><p>Remote Sighted Assistance (RSA) systems provide visually impaired people (VIPs) with real-time guidance by connecting them with remote sighted agents to facilitate daily travel. However, unfamiliar environments often complicate decision-making for agents and can induce anxiety in VIPs, thereby reducing the effectiveness of the assistance provided. To address this challenge, this paper proposes a video-based map assistance method. By pre-recording pedestrian path videos and aligning them with geographic locations, the system enables route preview and enhances navigation guidance. This study introduces a factor graph optimization (FGO) algorithm that integrates Global Navigation Satellite System (GNSS) and pedestrian dead reckoning (PDR) data for pedestrian positioning. It incorporates road-anchor constraints, a turning-point-based anchor-matching method, and a coarse-to-fine optimization strategy to improve the positioning accuracy. GNSS provides global reference positions, PDR offers precise relative motion constraints through accurate heading estimation, and anchor factors further enhance localization accuracy by leveraging known geometric features. We collected data using a smartphone equipped with a four-camera module and conducted tests in representative urban environments. Experimental results demonstrate that the proposed anchor-aided FGO-GNSS/PDR algorithm achieves robust and accurate positioning, effectively supporting video-based map construction in complex urban settings. With anchor constraints, the mean horizontal positioning error was reduced by 42% to 65% and the maximum error by 38% to 76% across all datasets. In this study, the mean horizontal positioning error was 1.36 m.</p></abstract><kwd-group><kwd>factor graph optimization (FGO)</kwd><kwd>global navigation satellite system (GNSS)</kwd><kwd>pedestrian dead reckoning (PDR)</kwd><kwd>road-anchor</kwd><kwd>video-based map</kwd><kwd>remote sighted assistance (RSA)</kwd><kwd>smartphone</kwd></kwd-group><funding-group><award-group><funding-source>National Key R&amp;D Program of China</funding-source><award-id>2024YFC3406302</award-id><award-id>2017YFA0701302</award-id></award-group><award-group><funding-source>JK Project on Enhancement of Animal Sensing</funding-source></award-group><award-group><funding-source>Peking University</funding-source></award-group><funding-statement>This work was partially funded by the National Key R&amp;D Program of China (No. 2024YFC3406302, 2017YFA0701302, SX), the JK Project on Enhancement of Animal Sensing, and Peking University.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05536"><title>1. Introduction</title><p>Visual impairment is a prevalent global health issue. According to the data from The Lancet Global Health Commission [<xref rid="B1-sensors-25-05536" ref-type="bibr">1</xref>], as of 2020, there were 596 million visually impaired people (VIPs) globally, including 43 million who were blind. By 2050, these figures are expected to rise to 895 million and 61 million, respectively. Given that visual information accounts for approximately 80% of human perception, the large population of VIPs faces significant challenges in their daily lives and mobility.</p><p>With the rapid advancement of smartphone technology and wireless communication, Remote Sighted Assistance (RSA) systems have emerged to support VIPs in navigation and other daily tasks [<xref rid="B2-sensors-25-05536" ref-type="bibr">2</xref>]. A typical RSA system includes four core components: visual information acquisition, data transmission, remote assistance, and user feedback. In such systems, users can request help via a mobile platform, and remote sighted agents provide guidance based on real-time visual data captured from wearable or mobile cameras. RSA applications have been developed for mobility assistance [<xref rid="B3-sensors-25-05536" ref-type="bibr">3</xref>], text recognition [<xref rid="B3-sensors-25-05536" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05536" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05536" ref-type="bibr">5</xref>], and object identification [<xref rid="B3-sensors-25-05536" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05536" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05536" ref-type="bibr">5</xref>]. Notable commercial platforms include BeMyEyes [<xref rid="B3-sensors-25-05536" ref-type="bibr">3</xref>], TapTapSee [<xref rid="B4-sensors-25-05536" ref-type="bibr">4</xref>], and Aira [<xref rid="B5-sensors-25-05536" ref-type="bibr">5</xref>], while research prototypes include VizWiz [<xref rid="B6-sensors-25-05536" ref-type="bibr">6</xref>], BeSpecular [<xref rid="B7-sensors-25-05536" ref-type="bibr">7</xref>], and CrowdViz [<xref rid="B8-sensors-25-05536" ref-type="bibr">8</xref>]. Despite recent progress, RSA systems still face several critical challenges, including reliable real-time communication, precise localization in GNSS-degraded areas, environmental understanding, and intuitive user feedback design.</p><p>Recent studies have identified two key challenges in RSA systems: (1) remote sighted agents experience stress and encounter difficulties in making accurate judgments when navigating unfamiliar environments, and (2) visually impaired users often feel anxious and uneasy when navigating unknown areas [<xref rid="B2-sensors-25-05536" ref-type="bibr">2</xref>,<xref rid="B9-sensors-25-05536" ref-type="bibr">9</xref>]. Research has also shown that incorporating 3D maps can help remote sighted agents better understand the environment [<xref rid="B10-sensors-25-05536" ref-type="bibr">10</xref>]. Therefore, enabling both remote sighted agents and visually impaired users to preview route information in advance has the potential to significantly enhance RSA effectiveness and user experience. Although commercial mapping services such as Google Maps [<xref rid="B11-sensors-25-05536" ref-type="bibr">11</xref>] and OpenStreetMap (OSM) [<xref rid="B12-sensors-25-05536" ref-type="bibr">12</xref>] provide extensive road network and satellite imagery data, they are not well-aligned with real-world first-person perspectives, thereby limiting their effectiveness for pedestrian navigation. In this study, we use the term &#8220;real-world first-person perspectives&#8221; to describe video content captured from the viewpoint of a pedestrian walking along the route, closely replicating what a VIP would perceive during navigation. Unlike Google Maps or similar services, which primarily provide top-down maps or static street-view images, this approach aligns with a VIP&#8217;s forward-facing, step-by-step experience. Furthermore, publicly available imagery such as Google Street View is often outdated, lacks comprehensive coverage, and may not accurately reflect temporary environmental changes, such as construction or obstacles. Moreover, existing map data primarily focuses on the vehicular road network, resulting in sparse or incomplete coverage of pedestrian pathways. To address this limitation and support the need for prior environmental awareness in RSA services, this study proposes a video-based map, as presented in <xref rid="sensors-25-05536-f001" ref-type="fig">Figure 1</xref>. The system captures pedestrian environments through pre-recorded videos and establishes precise spatiotemporal correspondences to enhance mobility assistance in RSA scenarios. Remote sighted agents can preview the video-based map prior to the assistance session and summarize key information&#8212;such as route layout, landmarks, and potential obstacles&#8212;to visually impaired users via voice instructions. Compared with costly 3D mapping technologies, our video-based map offers a lightweight, low-cost, and perceptually aligned alternative that is well suited for outdoor remote vision assistance in real-world urban environments.</p><p>To precisely associate video frames with geographic locations, spatial positions are represented using latitude and longitude coordinates. While video acquisition is relatively straightforward, the primary challenge lies in establishing an accurate mapping between time and space. Numerous studies have investigated smartphone-based pedestrian localization methods [<xref rid="B13-sensors-25-05536" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05536" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05536" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05536" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05536" ref-type="bibr">17</xref>]. Smartphones equipped with Global Navigation Satellite System (GNSS) modules estimate absolute positions by measuring pseudoranges and applying trilateration techniques [<xref rid="B18-sensors-25-05536" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05536" ref-type="bibr">19</xref>]. However, common urban environments with buildings and trees often cause multipath propagation and Non-Line-of-Sight (NLOS) effects, leading to significant GNSS positioning errors or even failures [<xref rid="B20-sensors-25-05536" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05536" ref-type="bibr">21</xref>]. To improve GNSS accuracy, researchers have developed algorithms based on weighted least squares [<xref rid="B22-sensors-25-05536" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05536" ref-type="bibr">23</xref>], Kalman filtering (KF) [<xref rid="B24-sensors-25-05536" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05536" ref-type="bibr">25</xref>], extended Kalman filtering (EKF) [<xref rid="B26-sensors-25-05536" ref-type="bibr">26</xref>], and factor graph optimization (FGO) [<xref rid="B27-sensors-25-05536" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05536" ref-type="bibr">28</xref>]. Among these, FGO is particularly effective, as it leverages a sequence of historical measurements to produce more accurate estimates [<xref rid="B29-sensors-25-05536" ref-type="bibr">29</xref>]. However, a study from the Hong Kong Polytechnic University indicates that even FGO based solely on GNSS can result in errors exceeding 20 m in urban settings [<xref rid="B30-sensors-25-05536" ref-type="bibr">30</xref>].</p><p>In contrast to GNSS, Inertial Measurement Unit (IMU)-based localization provides continuous relative positioning without relying on external signals [<xref rid="B31-sensors-25-05536" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05536" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05536" ref-type="bibr">33</xref>]. Most modern smartphones are equipped with MEMS-based IMUs, which enable pedestrian dead reckoning (PDR) using accelerometer, gyroscope, and magnetometer data [<xref rid="B13-sensors-25-05536" ref-type="bibr">13</xref>,<xref rid="B34-sensors-25-05536" ref-type="bibr">34</xref>]. PDR typically involves three key components: step detection, step length estimation, and heading estimation. It provides relative motion estimates between consecutive steps, which are then accumulated to reconstruct the pedestrian trajectory. However, due to sensor noise and drift, PDR is susceptible to cumulative errors that can reach tens of meters [<xref rid="B13-sensors-25-05536" ref-type="bibr">13</xref>,<xref rid="B34-sensors-25-05536" ref-type="bibr">34</xref>].</p><p>To overcome the limitations of individual systems, GNSS/PDR fusion algorithms integrate the absolute positioning accuracy of GNSS with the continuous tracking capability of PDR, enabling robust localization in urban environments [<xref rid="B14-sensors-25-05536" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05536" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05536" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05536" ref-type="bibr">17</xref>,<xref rid="B35-sensors-25-05536" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05536" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05536" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05536" ref-type="bibr">38</xref>]. Recent efforts have increasingly focused on smartphone-based fusion methods [<xref rid="B14-sensors-25-05536" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05536" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05536" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05536" ref-type="bibr">17</xref>,<xref rid="B39-sensors-25-05536" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05536" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05536" ref-type="bibr">41</xref>]. Angrisano et al. [<xref rid="B39-sensors-25-05536" ref-type="bibr">39</xref>] and Basso et al. [<xref rid="B41-sensors-25-05536" ref-type="bibr">41</xref>] proposed loosely coupled fusion architectures based on EKF frameworks to integrate GNSS and PDR, thereby improving system reliability. Meng et al. [<xref rid="B15-sensors-25-05536" ref-type="bibr">15</xref>] introduced a KF-based GNSS/PDR fusion method with adaptive measurement adjustments. Zhang et al. [<xref rid="B40-sensors-25-05536" ref-type="bibr">40</xref>] applied an adaptive EKF (AEKF) to enhance robustness and mitigate cumulative errors. Jiang et al. [<xref rid="B14-sensors-25-05536" ref-type="bibr">14</xref>] pioneered the use of FGO for smartphone-based GNSS/PDR fusion, proposing both position and step-length constraint factors. Zhong et al. [<xref rid="B16-sensors-25-05536" ref-type="bibr">16</xref>] introduced PDR displacement factors derived from accelerometer data, which were jointly optimized with GNSS pseudorange and Doppler observations, achieving an average positioning accuracy of 18.51 m. Building on this work, Jiang et al. [<xref rid="B17-sensors-25-05536" ref-type="bibr">17</xref>] developed an adaptive FGO (A-FGO) framework that dynamically adjusts observation covariances to suppress outliers, achieving sub 4 m accuracy in certain test scenarios. Overall, FGO-based GNSS/PDR fusion methods have demonstrated superior localization accuracy compared to EKF-based approaches [<xref rid="B14-sensors-25-05536" ref-type="bibr">14</xref>,<xref rid="B17-sensors-25-05536" ref-type="bibr">17</xref>,<xref rid="B29-sensors-25-05536" ref-type="bibr">29</xref>,<xref rid="B42-sensors-25-05536" ref-type="bibr">42</xref>].</p><p>This study proposes a smartphone-based method for video-based map collection, motivated by the low cost and the widespread adoption of smartphones in RSA systems. In addition, we develop a high-precision pedestrian localization algorithm that builds upon existing solutions. For data acquisition, we employ a custom-designed multi-camera module and a mobile application to capture forward-facing video with a 180&#176; field of view. For localization, we adopt an FGO framework that fuses GNSS and PDR data. To enhance performance, we introduce novel road anchor factors, an anchor-matching algorithm, and a coarse-to-fine optimization strategy [<xref rid="B43-sensors-25-05536" ref-type="bibr">43</xref>]. In this framework, GNSS factors provide global position constraints to correct accumulated PDR drift; PDR factors enhance robustness by mitigating GNSS outliers; and anchor factors leverage road features to correct residual localization errors. The FGO-based fusion framework further refines trajectories by integrating multi-source historical data. The main contributions of this work are summarized as follows:<list list-type="order"><list-item><p>We propose a method for constructing a video-based map by associating video frames with corresponding spatial locations to enhance the RSA service. This map serves as a preview tool for RSA agents and provides prior environmental knowledge for visually impaired users, which helps to reduce anxiety in unfamiliar environments. It also provides valuable assistance under low-light or poor network connectivity conditions, thereby enhancing the overall safety and reliability of RSA services.</p></list-item><list-item><p>To establish accurate spatiotemporal correspondence, we propose a novel FGO-based GNSS/PDR fusion framework. This framework integrates road-anchor constraints, a turning-point-based anchor-matching method, and a coarse-to-fine optimization strategy to improve positioning accuracy. The incorporation of anchor factors further refines the localization results.</p></list-item><list-item><p>We conducted extensive real-world experiments and collected datasets from three representative urban routes to evaluate the proposed localization algorithm. The results demonstrate that our anchor-aided FGO-based GNSS/PDR fusion method achieves high localization accuracy that is sufficient for video-based map construction.</p></list-item></list></p><p>The remainder of this paper is organized as follows. <xref rid="sec2-sensors-25-05536" ref-type="sec">Section 2</xref> presents the proposed method for constructing the video-based map, with a focus on the localization framework, including the PDR algorithm, the PDR factor, the anchor factor, the GNSS factor, and the FGO-based data fusion strategy. <xref rid="sec3-sensors-25-05536" ref-type="sec">Section 3</xref> presents the experimental results obtained from two real-world test scenarios and provides a detailed performance analysis. <xref rid="sec4-sensors-25-05536" ref-type="sec">Section 4</xref> discusses the proposed PDR algorithm, the role of anchor factors, system robustness, and potential directions for future research. Finally, <xref rid="sec5-sensors-25-05536" ref-type="sec">Section 5</xref> concludes the paper.</p></sec><sec id="sec2-sensors-25-05536"><title>2. Materials and Methods</title><p>The construction of the proposed video-based map involves two primary stages: data acquisition and post-processing. In the data acquisition phase, participants are equipped with a smartphone and a multi-camera module to capture video data, GNSS positioning measurements, and sensor readings from the IMU and the attitude and heading reference system (AHRS). A major challenge in this system lies in accurately associating the captured video stream with corresponding spatial coordinates, which is the core focus of the present study. During the post-processing stage, the data are used to estimate pedestrian positions and to establish a correspondence between video timestamps and spatial locations.</p><p>The overview of the proposed anchor-aided FGO-GNSS/PDR localization algorithm is illustrated in <xref rid="sensors-25-05536-f002" ref-type="fig">Figure 2</xref>. The framework adopts a coarse-to-fine optimization strategy inspired by techniques commonly used in computer vision. Initially, pedestrian motion is estimated using a PDR algorithm driven by inertial and AHRS data from the smartphone. The PDR process includes coordinate transformation from the device frame to the East&#8211;North&#8211;Up (ENU) frame, step detection, step length estimation, and heading estimation. Next, smartphone GNSS positioning measurements are combined with the PDR results to construct an initial factor graph, producing a coarse localization estimate along the pedestrian trajectory. This preliminary trajectory is then automatically matched to predefined road anchors. The resulting anchor constraints are incorporated into the graph for fine-grained optimization, yielding the final localization result. Notably, all optimization is conducted in the ENU coordinate frame.</p><sec id="sec2dot1-sensors-25-05536"><title>2.1. PDR Mechanism</title><p>In the PDR process, the algorithm estimates the pedestrian&#8217;s relative displacement by detecting step events, estimating the step length and heading direction at each step. By accumulating these step-wise displacements, the complete pedestrian trajectory can be reconstructed. <xref rid="sensors-25-05536-f003" ref-type="fig">Figure 3</xref> illustrates the mechanism of position updates. In the ENU coordinate frame, this process can be mathematically expressed as follows:</p><p>Assuming the initial position is <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>,<disp-formula id="FD1-sensors-25-05536"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>E</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>N</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The position update at step <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> can then be expressed as<disp-formula id="FD2-sensors-25-05536"><label>(2)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>sin</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>cos</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the pedestrian position at the beginning of step <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the updated position after the step. <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> correspond to the East and North components in the ENU coordinate frame. <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the estimated step length, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the heading angle of the current step with respect to the north direction.</p><sec id="sec2dot1dot1-sensors-25-05536"><title>2.1.1. PDR Modeling</title><p>In this study, the smartphone is placed on the user&#8217;s chest, with the Y-axis aligned vertically in the walking direction. Compared to the waist, the chest offers a more stable and centrally aligned mounting position, reducing the influence of lateral sway. Although this setup may slightly compromise step-frequency sensitivity, it enhances heading stability and is more practical for real-world deployment in remote assistance scenarios. The overall PDR framework is illustrated in <xref rid="sensors-25-05536-f004" ref-type="fig">Figure 4</xref>. We use the smartphone&#8217;s accelerometer and AHRS outputs as inputs.</p><p>The accelerometer output <italic toggle="yes">a</italic> is transformed to the ENU coordinate frame using the rotation matrix <italic toggle="yes">R</italic> from the AHRS system. The transformation from the smartphone&#8217;s device frame to the ENU frame is defined as follows:<disp-formula id="FD3-sensors-25-05536"><label>(3)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>body</mml:mi><mml:mi>ENU</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mspace width="0.166667em"/><mml:mo>@</mml:mo><mml:mspace width="0.166667em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mspace width="0.166667em"/><mml:mo>@</mml:mo><mml:mspace width="0.166667em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>body</mml:mi><mml:mi>ENU</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the rotation matrix that transforms vectors from the smartphone coordinate frame to the ENU coordinate frame. <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are rotation matrices derived from the Euler angles provided by the AHRS system, and the symbol @ indicates matrix multiplication.</p><p>The PDR algorithm consists of three main steps: step detection, step length estimation, and heading estimation, which are described below:<list list-type="simple"><list-item><label>(a)</label><p>Step Detection: The algorithm detects steps by identifying valleys in the vertical acceleration signal. A fixed threshold is set to filter out noise and avoid false detections.</p></list-item><list-item><label>(b)</label><p>Step Length Estimation: Step length is estimated using the Weinberg method [<xref rid="B44-sensors-25-05536" ref-type="bibr">44</xref>] based on the peak and valley values of the vertical acceleration during a step. The empirical formula is given as<disp-formula id="FD4-sensors-25-05536"><label>(4)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mroot><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mroot></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">l</italic> denotes the estimated step length, <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a step length coefficient, and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent the maximum and minimum vertical acceleration values within a step.</p></list-item><list-item><label>(c)</label><p>Heading Estimation: The heading angle is estimated using the yaw output from the smartphone&#8217;s AHRS, corrected by an offset angle of &#8722;90&#176; in our placement scenario. To mitigate fluctuations caused by body sway, a moving average filter is applied to smooth the yaw signal. Since direct angle averaging may lead to errors near the <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#177;</mml:mo><mml:msup><mml:mn>180</mml:mn><mml:mo>&#176;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> boundary, we project the yaw angles onto the unit circle, average the projected values, and then convert the result back to an angle.</p></list-item></list></p></sec><sec id="sec2dot1dot2-sensors-25-05536"><title>2.1.2. PDR Factor</title><p>The PDR model described above produces horizontal displacements in the ENU coordinate frame at each step. Given an initial position, the full pedestrian trajectory can be obtained. In our factor graph optimization framework, the PDR factor imposes a relative motion constraint between two consecutive positions.</p><p>The horizontal displacement between two consecutive steps is defined as<disp-formula id="FD5-sensors-25-05536"><label>(5)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> denote the pedestrian positions at timestamps <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, respectively, in the ENU coordinate frame. Thus, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the relative displacement between two consecutive time steps.</p><p>The formulation of the PDR factor is then given by<disp-formula id="FD6-sensors-25-05536"><label>(6)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>PDR</mml:mi></mml:msubsup></mml:mfenced><mml:msubsup><mml:mo>&#931;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>PDR</mml:mi></mml:msubsup><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:mo>&#916;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>PDR</mml:mi></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced><mml:msubsup><mml:mo>&#931;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>PDR</mml:mi></mml:msubsup><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>PDR</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the relative displacement estimated by the PDR algorithm at time <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the actual displacement between two adjacent positions as described above. <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>PDR</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the residual of the PDR factor at timestamp <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>&#931;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>PDR</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the covariance matrix of the PDR output at time <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In our FGO framework, each PDR factor models the 2D relative displacement on the horizontal plane (E and N). To simplify the modeling, we assume that the noise in the E and N directions is uncorrelated and identically distributed. Accordingly, we adopt an isotropic noise model, represented by a fixed diagonal covariance matrix:<disp-formula id="FD7-sensors-25-05536"><label>(7)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#931;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The value of <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is empirically determined based on the observed noise characteristics of the PDR output in our datasets. While more complex noise models could be applied, this simplified isotropic formulation enables efficient optimization while maintaining robust performance.</p></sec></sec><sec id="sec2dot2-sensors-25-05536"><title>2.2. Anchor-Matching Method</title><p>To improve the localization accuracy, the proposed method incorporates anchor constraints derived from the available road geometry, specifically at turning points. These anchors serve as auxiliary constraints to correct the fused trajectory during optimization. Each anchor is placed at the geometric center of a turning region, with its true geographic coordinates (latitude and longitude) obtained through prior measurement. When the pedestrian trajectory passes through such a region, the corresponding anchor is automatically matched based on the local trajectory geometry.</p><p>To match each predefined anchor point with the corresponding pedestrian location along the trajectory, the method first selects a set of candidate points from the coarse localization result. Specifically, all trajectory points between the first and last points that lie within a 10 m radius of the anchor are selected, ensuring spatial continuity along the path.</p><p>Assume that <italic toggle="yes">N</italic> candidate points are obtained for a given anchor. From these <italic toggle="yes">N</italic> points, the method computes <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> forward-direction vectors <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> representing the pedestrian&#8217;s heading between consecutive points:<disp-formula id="FD8-sensors-25-05536"><label>(8)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, the angular differences (turning angles) between adjacent heading vectors are calculated to produce <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> direction change values <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD9-sensors-25-05536"><label>(9)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8736;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A fixed-length sliding window of size <italic toggle="yes">W</italic> is applied to the turning angle sequence. For each window, the cumulative turning angle <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is computed as</p><p>
<disp-formula id="FD10-sensors-25-05536"><label>(10)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:mi>&#948;</mml:mi><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
</p><p>As illustrated in <xref rid="sensors-25-05536-f005" ref-type="fig">Figure 5</xref>, the window with the maximum cumulative turning angle is identified, and its center point is selected as the matched location for the current anchor. Formally, the index of the matched point <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is determined as</p><p>
<disp-formula id="FD11-sensors-25-05536"><label>(11)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>anchor</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mfenced separators="" open="&#x230A;" close="&#x230B;"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>where</mml:mi><mml:mspace width="4.pt"/><mml:msup><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:munder><mml:mi>max</mml:mi><mml:mi>j</mml:mi></mml:munder><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
</p><p>This method ensures that anchor points are matched to the portions of the trajectory exhibiting the most significant local turning behavior, improving the robustness and accuracy of anchor alignment in complex environments.</p></sec><sec id="sec2dot3-sensors-25-05536"><title>2.3. Anchor Factor</title><p>To further improve localization accuracy, we introduce an anchor factor that leverages known geometric landmarks, specifically road turning points, as additional spatial constraints in the factor graph. These anchors serve as reference points to mitigate accumulated drift from GNSS and PDR estimates.</p><p>The anchor factor is defined as a residual between the estimated pedestrian position and the known ENU position of the corresponding anchor point:<disp-formula id="FD12-sensors-25-05536"><label>(12)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow><mml:mi>Anchor</mml:mi></mml:msubsup></mml:mfenced><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:msub></mml:mfenced><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the true ENU coordinate of the anchor <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is the estimated pedestrian position at time <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> when matched to this anchor. The residual <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> quantifies the positional deviation between the estimated and ground-truth anchor location. The covariance matrix <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> models the uncertainty associated with anchor matching. Similar to the PDR factor, the anchor factor also adopts an isotropic noise model. Specifically, for a matched anchor <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the anchor factor models the 2D positional deviation between the estimated pedestrian position <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and the ground-truth anchor position <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> on the horizontal plane (E and N). We assume that the uncertainties in the E and N directions are uncorrelated and identically distributed, allowing the covariance matrix <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> to be represented as a fixed diagonal matrix:&#160;<disp-formula id="FD13-sensors-25-05536"><label>(13)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>Anchor</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The parameter <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>Anchor</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is empirically determined based on the variability observed in anchor-matching performance in our datasets.</p><p>This anchor factor enhances the robustness of trajectory optimization by introducing strong spatial constraints in turning regions, where PDR performance tends to deteriorate. In our current implementation, the start and end positions are assumed to be approximately known, based on external information such as commercial maps or fixed landmarks (e.g., building entrances). This simplifying assumption reflects common RSA scenarios, where the origin and destination are often predefined. While this improves the stability of FGO by providing fixed anchor constraints, we acknowledge that it may not hold in all real-world cases. In future work, we aim to relax this assumption by integrating visual place recognition techniques, such as OrienterNet [<xref rid="B45-sensors-25-05536" ref-type="bibr">45</xref>], to support autonomous initialization and termination without relying on predefined points. These points are also treated as anchor nodes and incorporated into the factor graph as prior constraints. The formulation of their residuals follows the same structure as that of regular anchors.</p></sec><sec id="sec2dot4-sensors-25-05536"><title>2.4. GNSS Factor</title><p>GNSS positioning provides absolute spatial references, serving as global constraints in the optimization process. To incorporate this information, we introduce GNSS factors into the factor graph. In our implementation, the GNSS positions are obtained from the Android device solution [<xref rid="B46-sensors-25-05536" ref-type="bibr">46</xref>]. First, the device solution results are transformed from geodetic coordinates (latitude, longitude, height) to the Earth-Centered Earth-Fixed (ECEF) coordinate system. To simplify the transformation, a fixed height is assumed in this work, which is reasonable in flat urban areas but may introduce errors in hilly terrains. Then, given a known reference point, the rotation matrix <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>ECEF</mml:mi><mml:mi>ENU</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is computed to convert the ECEF coordinates into the local ENU frame. The GNSS factor is formulated as<disp-formula id="FD14-sensors-25-05536"><label>(14)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup></mml:mfenced><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfenced><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the GNSS position at time <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> denotes the estimated pedestrian position at the same timestamp. The residual <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> measures the deviation between the GNSS position and the current state estimate. Similar to the PDR and anchor factors, the GNSS factor also adopts an isotropic noise model. Specifically, for a GNSS observation at time <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the GNSS factor models the 2D positional deviation between the estimated pedestrian position <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and the GNSS position <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> on the horizontal plane (E and N). We assume that the errors in the E and N directions are uncorrelated and identically distributed, so the covariance matrix <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> can be represented as a fixed diagonal matrix:</p><p>
<disp-formula id="FD15-sensors-25-05536"><label>(15)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#931;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>GNSS</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
</p><p>The value of <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>GNSS</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is empirically determined from the observed noise characteristics of GNSS positioning in our datasets.</p></sec><sec id="sec2dot5-sensors-25-05536"><title>2.5. Factor Graph Integration</title><p>In the proposed optimization framework, we adopt a coarse-to-fine strategy. Initially, the GNSS factor, PDR factor, and start&amp;end anchor constraints are used to perform a coarse optimization and obtain an initial trajectory estimate. The corresponding loss function is defined as<disp-formula id="FD16-sensors-25-05536"><label>(16)</label><mml:math id="mm73" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:munder><mml:mi>min</mml:mi><mml:mi>P</mml:mi></mml:munder><mml:munder><mml:mo>&#8721;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:munder><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>PDR</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>&amp;</mml:mo><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mi>Anchor</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The resulting trajectory is then used as input to the anchor-matching algorithm, which determines matched anchor positions. These anchor constraints are subsequently incorporated into the factor graph for fine-grained optimization. The final objective function for the complete factor graph optimization is given by<disp-formula id="FD17-sensors-25-05536"><label>(17)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:munder><mml:mi>min</mml:mi><mml:mi>P</mml:mi></mml:munder><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow></mml:munder><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>PDR</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>Anchor</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>GNSS</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The optimized trajectory <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> represents the best estimate of the pedestrian position at each timestamp <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> based on the fused constraints. The structure of the final factor graph is illustrated in <xref rid="sensors-25-05536-f006" ref-type="fig">Figure 6</xref>. It consists of three types of factors: PDR, anchor, and GNSS. In our implementation, we use the Georgia Tech Smoothing and Mapping (GTSAM) library [<xref rid="B47-sensors-25-05536" ref-type="bibr">47</xref>], an open-source optimization framework developed by Georgia Institute of Technology, to perform factor graph optimization. The Levenberg&#8211;Marquardt algorithm [<xref rid="B48-sensors-25-05536" ref-type="bibr">48</xref>] is applied to solve the entire nonlinear optimization problem.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05536"><title>3. Results</title><sec id="sec3dot1-sensors-25-05536"><title>3.1. Experiment Setup</title><p>To validate the effectiveness of the proposed method, we collected real-world data in three challenging urban environments. These environments feature dense buildings and vegetation, which induce GNSS positioning errors due to multipath propagation and NLOS conditions. These characteristics pose significant challenges for GNSS localization.</p><p>In addition, we intentionally used a consumer-grade smartphone (OPPO PESM10, manufactured by OPPO Guangdong Mobile Telecommunications Corp., Ltd., Dongguan, China) rather than the high-end devices commonly adopted in similar studies [<xref rid="B14-sensors-25-05536" ref-type="bibr">14</xref>,<xref rid="B16-sensors-25-05536" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05536" ref-type="bibr">17</xref>]. The device supports multi-constellation GNSS, including GPS, GLONASS, BeiDou, Galileo, QZSS, and A-GPS. GNSS data were recorded at a frequency of 1 Hz using the Android fused location provider. The smartphone is also equipped with a typical suite of inertial sensors: a 3-axis accelerometer, a 3-axis gyroscope, and a 3-axis magnetometer. The smartphone data were collected using the GetSensorData app [<xref rid="B46-sensors-25-05536" ref-type="bibr">46</xref>], which records inertial and AHRS outputs at 50 Hz. The GNSS data were logged at a frequency of 1 Hz.</p><p>The data acquisition setup is depicted in <xref rid="sensors-25-05536-f007" ref-type="fig">Figure 7</xref>. The smartphone was chest-mounted on the user, following the configuration described in the methodology section. For ground-truth acquisition, we employed a Vision-RTK 2 system that integrates visual-inertial odometry with RTK corrections to achieve centimeter-level accuracy. As shown in <xref rid="sensors-25-05536-f007" ref-type="fig">Figure 7</xref>, the RTK receiver was mounted on a wheeled platform moving alongside the user, enabling accurate reference trajectory collection at 10 Hz. All experiments were performed on a desktop computer equipped with an Intel i5-13490F 2.5 GHz CPU and 32 GB of RAM, running Ubuntu 24.04.</p></sec><sec id="sec3dot2-sensors-25-05536"><title>3.2. Evaluation Metrics and Methods</title><p>We adopt four commonly used evaluation metrics to evaluate the performance of the proposed method: mean error (MEAN), standard deviation (STD), root mean square error (RMSE), and maximum error (MAX). All evaluations are performed in the ENU coordinate frame. To validate the effectiveness of the proposed anchor factor, we compare the localization results of the following three methods:<list list-type="order"><list-item><p>GNSS: The GNSS positions are obtained from the on-chip fused GNSS location estimates provided by the Android device via the GetSensorData app.</p></list-item><list-item><p>FGO-GNSS/PDR: Localization results obtained from the coarse optimization stage using GNSS, PDR, and start&amp;end anchor constraints.</p></list-item><list-item><p>FGO-GNSS/PDR + Anchor: The final localization results obtained by incorporating the proposed anchor factor into the factor graph and applying the complete coarse-to-fine optimization strategy.</p></list-item></list></p><p>We acknowledge that all experiments in this study were conducted using a single smartphone model (OPPO PESM10), which may limit the generalizability of the results across different devices. Variations in GNSS hardware, antenna quality, and sensor fusion implementations among smartphone models could influence positioning performance. In future work, we plan to investigate the robustness and adaptability of the proposed method across multiple device types to assess cross-device generalization.</p></sec><sec id="sec3dot3-sensors-25-05536"><title>3.3. Experiment 1 in Urban Areas</title><p>To evaluate the effectiveness of the proposed method, we selected a representative and challenging urban environment. The scene map and corresponding trajectory layout is shown in <xref rid="sensors-25-05536-f008" ref-type="fig">Figure 8</xref>. The path includes five turning anchors, where PDR performance typically deteriorates due to heading estimation errors and changes in gait dynamics introduced by turning motions. This makes the environment particularly suitable for evaluating the effectiveness of the proposed anchor factor. In this area, we collected three walking datasets with durations of 376 s, 396 s, and 397 s and corresponding step counts of 608, 616, and 629, respectively.</p><p><xref rid="sensors-25-05536-f009" ref-type="fig">Figure 9</xref> presents the corresponding pedestrian trajectories and the horizontal positioning errors for the first dataset. In the trajectory plot, the red dashed line denotes the ground-truth trajectory, the purple solid line shows the raw GNSS trajectory, the green solid line illustrates the result of FGO-GNSS/PDR, and the orange solid line depicts the final output after incorporating the proposed anchor factor. As observed, the FGO-GNSS/ PDR + Anchor method yields a trajectory that closely aligns with the ground-truth trajectory, while both the raw GNSS and FGO-GNSS/PDR trajectories exhibit noticeable deviations. In particular, the GNSS result shows substantial drift. This observation is further supported by the horizontal error plot, where the raw GNSS solution reaches a peak error of 33.95 m. Additional results are visualized in <xref rid="secAdot1-sensors-25-05536" ref-type="sec">Appendix A.1</xref>.</p><p><xref rid="sensors-25-05536-t001" ref-type="table">Table 1</xref> summarizes the localization errors of the three methods using four standard metrics: RMSE, MEAN, STD, and MAX. <xref rid="sensors-25-05536-t002" ref-type="table">Table 2</xref> presents the performance improvement of the FGO-GNSS/PDR + Anchor method compared to the baseline FGO-GNSS/PDR approach. The performance gap between raw GNSS and the optimized methods highlights the limitations of GNSS localization in urban environments. The raw GNSS trajectories suffer from large deviations, particularly in areas surrounded by tall buildings or vegetation, with maximum errors exceeding 30 m. Although the FGO-GNSS/PDR method effectively suppresses noise by leveraging inertial data, it still experiences drift over time due to the absence of reliable global constraints, as is evident in the third dataset, where the maximum error remains above 11 m.</p><p>In contrast, the proposed FGO-GNSS/PDR + Anchor method consistently achieves lower localization errors across all datasets. The maximum errors are reduced to below 3.1 m, and the error distributions are notably tighter. The significantly lower standard deviations indicate not only improved accuracy but also greater stability and robustness. This demonstrates the value of incorporating anchor constraints derived from known road geometry, especially in turning regions where PDR is prone to degradation.</p><p>The third dataset exhibits the most substantial improvement, with RMSE reduced by 71%, MEAN by 65%, STD by 82%, and MAX error by 76% compared to the FGO-GNSS/PDR baseline. These results suggest that the proposed anchor factor is particularly beneficial in complex urban scenarios involving multiple turns, where it can provide tighter constraints on trajectory estimation.</p><p>Moreover, the consistent improvements observed across all three datasets demonstrate the potential of the proposed method in structured urban environments. The error reduction rates suggest that anchor-aided optimization not only decreases average errors but also effectively suppresses large deviations. Nevertheless, further evaluation across more diverse routes and scenarios is needed to validate its generalization and robustness.</p></sec><sec id="sec3dot4-sensors-25-05536"><title>3.4. Experiment 2 in Urban Areas</title><p>In Experiment 2, we selected a more challenging long-range test environment featuring curves and an extended walking path. The scene map and corresponding trajectory layout are shown in <xref rid="sensors-25-05536-f010" ref-type="fig">Figure 10</xref>. This setting allows for a more comprehensive evaluation of the robustness of the proposed anchor-aided localization approach. Three datasets were collected with walking durations of 871 s, 904 s, and 890 s and corresponding step counts of 1274, 1324, and 1317, respectively.</p><p>Similar to Experiment 1, <xref rid="sensors-25-05536-f011" ref-type="fig">Figure 11</xref> presents the pedestrian trajectories and horizontal positioning errors for the third dataset. <xref rid="sensors-25-05536-t003" ref-type="table">Table 3</xref> summarizes the positioning performance (RMSE, MEAN, STD, MAX) of all three methods, while <xref rid="sensors-25-05536-t004" ref-type="table">Table 4</xref> shows the improvement rates of the FGO-GNSS/PDR + Anchor method compared to the baseline FGO-GNSS/PDR. Consistent with previous results, the proposed FGO-GNSS/PDR + Anchor method achieves the highest accuracy across all datasets. Additional results are provided in <xref rid="secAdot2-sensors-25-05536" ref-type="sec">Appendix A.2</xref>.</p><p>As shown in the horizontal error plot, the FGO-GNSS/PDR method still exhibits considerable deviations from the ground-truth trajectory, particularly in the third dataset, where the maximum error reaches 10.44 m. In contrast, the FGO-GNSS/PDR + Anchor method effectively reduces the maximum error to 4.49 m&#8212;a reduction of 57%. Likewise, the mean error decreases from 3.79 m to 1.63 m, representing a 57% improvement. These results confirm the effectiveness of anchor constraints in correcting accumulated drift in complex scenes. In the first dataset, the best mean error achieved is only 1.54 m, and the STD is as low as 0.93 m. These results further confirm the effectiveness of the proposed anchor factor. These findings demonstrate that the proposed anchor factor significantly improves both the accuracy and stability of position estimation.</p><p>An interesting observation can be made in <xref rid="sensors-25-05536-f011" ref-type="fig">Figure 11</xref>b, where the fine-optimized trajectory exhibits a slightly higher positioning error than the coarse-optimized trajectory during the 0&#8211;200 s interval of the third dataset. This behavior may be attributed to the redistribution of constraint weights in the factor graph. Specifically, during fine optimization, the relative influence of GNSS constraints decreases due to the incorporation of additional anchor constraints. While this adjustment improves robustness in challenging areas, it reduces the contribution of reliable GNSS data in well-performing segments. Since the GNSS signal quality is relatively high in the initial 0&#8211;200 s, the reduced contribution of GNSS data may lead to a marginal degradation in accuracy. This trade-off highlights the importance of appropriately balancing the contributions of anchor and GNSS factors.</p><p>Overall, the results of Experiment 2 demonstrate the effectiveness of our approach under similar walking conditions. While the method shows robustness to small path deviations and heading variations, further evaluation on more diverse walking patterns is necessary to fully validate its generalizability. It also shows that the proposed anchor-aided FGO-GNSS/PDR remains effective in extended-range, geometrically complex urban settings, thereby highlighting the method&#8217;s suitability for real-world pedestrian localization applications.</p></sec><sec id="sec3dot5-sensors-25-05536"><title>3.5. Experiment 3 in Urban Areas</title><p>In Experiment 3, we collected a dataset that contains a partial trajectory overlapping with that in Experiment 1. The corresponding scene map and trajectory layout are illustrated in <xref rid="sensors-25-05536-f012" ref-type="fig">Figure 12</xref>. This trajectory includes three anchor points from Experiment 1 and two additional newly deployed anchors, enabling a denser spatial constraint distribution. The dataset was recorded with a total walking duration of 434 s, covering a step count of 658.</p><p>The pedestrian trajectories and horizontal positioning errors for this dataset are presented in <xref rid="sensors-25-05536-f013" ref-type="fig">Figure 13</xref>, while the quantitative positioning performance is summarized in <xref rid="sensors-25-05536-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-05536-t006" ref-type="table">Table 6</xref>. As depicted in <xref rid="sensors-25-05536-f013" ref-type="fig">Figure 13</xref>a, the PDR trajectory exhibits large angular deviations similar to those observed in dataset 3 of Experiment 1, indicating accumulated heading drift. Nevertheless, after fine optimization, the resulting trajectory closely matches the ground truth, highlighting the corrective effect of the anchor constraints. <xref rid="sensors-25-05536-f013" ref-type="fig">Figure 13</xref>b further shows that the fine optimized solution maintains low error levels throughout most of the trajectory, whereas the GNSS and standalone PDR results suffer from significant fluctuations and drifts.</p><p>The RMSE, MEAN, STD, and MAX errors are compared across the three evaluated methods. It can be clearly observed that the proposed FGO-GNSS/PDR + Anchor method yields the most accurate results, with trajectories that are well-aligned with the ground-truth path. Specifically, as shown in <xref rid="sensors-25-05536-t005" ref-type="table">Table 5</xref>, the maximum horizontal error achieved by the proposed method is only 2.10 m, while the mean horizontal error is reduced to 1.19 m. Compared with the GNSS-only and FGO-GNSS/PDR methods, the proposed anchor-assisted approach consistently achieves substantial reductions in all error metrics. The RMSE, MEAN, STD, and MAX values are reduced by 58%, 55%, 70%, and 68%, respectively, relative to the FGO-GNSS/PDR baseline. These results verify that incorporating anchor factors not only improves overall accuracy but also enhances the stability of trajectory estimation in challenging urban environments.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05536"><title>4. Discussion</title><sec id="sec4dot1-sensors-25-05536"><title>4.1. PDR Results</title><p><xref rid="sensors-25-05536-f014" ref-type="fig">Figure 14</xref> shows the trajectory and horizontal error distribution of the PDR algorithm in Experiment 2. This figure is intended to illustrate the standalone positioning performance of the PDR algorithm. Blue, orange, and green curves correspond to the results of the first, second, and third datasets, respectively. From the trajectory plots, we observe that the estimated PDR trajectories generally align well with the ground-truth path in shape, although deviations of varying magnitudes exist. Notably, near the turning points, the proposed PDR algorithm accurately and smoothly reconstructs the original curves, providing reliable inputs for anchor matching.</p><p>Statistical results (<xref rid="sensors-25-05536-t007" ref-type="table">Table 7</xref>) for the three datasets show that the third dataset yields the best performance, with a mean error of 8.53 m and RMSE of 9.17 m. The first dataset exhibits the largest deviation, with a mean error of 13.93 m and RMSE of 15.07 m. These results demonstrate that the proposed PDR algorithm provides accurate local relative motion estimates, particularly suitable for reconstructing the shape of walking trajectories.</p><p>Nevertheless, as PDR errors accumulate over time, it cannot ensure long-term global accuracy. To mitigate this issue, absolute position constraints from GNSS can be integrated to correct large-scale drifts. The proposed anchor factor enables more precise localization corrections by leveraging known anchor points. Therefore, this paper introduces a coarse-to-fine optimization strategy: we first use GNSS data to roughly align the trajectory to a global reference, and then we apply anchor matching based on the adjusted trajectory to obtain accurate localization constraints.</p><p>Improving the quality of PDR results hinges on the ability to obtain accurate heading estimates. Smartphones equipped with high-grade inertial sensors are expected to enable more precise heading estimates and yield lower-error PDR trajectories. Besides improving hardware, developing robust heading estimation algorithms under varying walking postures is equally important.</p><p>Future research should explore methods to reduce heading estimation errors caused by body movement and instability. In addition, reliable step detection remains a critical component of PDR systems, and developing approaches to automatically adjust step detection thresholds is an important open question. With the advancement of deep learning techniques, future work can also investigate how learning-based methods may improve step detection, heading estimation, and trajectory reconstruction. Ultimately, the integration of model-based and learning-based approaches represents a viable strategy for enhancing the robustness and accuracy of PDR solutions.</p></sec><sec id="sec4dot2-sensors-25-05536"><title>4.2. Anchor Effect</title><p>To achieve more accurate pedestrian localization, we proposed the anchor factor, which leverages prior knowledge of road geometry to impose constraints during optimization. We also proposed an anchor-matching algorithm that automatically aligns trajectory points with predefined anchors. Taking Experiment 1 as an example, <xref rid="sensors-25-05536-t008" ref-type="table">Table 8</xref> summarizes the anchor-matching errors across the three datasets using the anchor-matching method proposed in this study.</p><p>Statistical results (<xref rid="sensors-25-05536-t009" ref-type="table">Table 9</xref>) show that the average anchor-matching error across all datasets remains below 1.1 m. Particularly in the first dataset, the matching RMSE is only 0.21 m and MEAN is 0.19 m. These results demonstrate the effectiveness of the proposed matching algorithm in achieving accurate anchor correspondence.</p><p>To further evaluate the effectiveness of the anchor factor, we refined the anchor-matching results for the second dataset in Experiment 1, which exhibited the largest anchor-matching errors. <xref rid="sensors-25-05536-f015" ref-type="fig">Figure 15</xref> illustrates the horizontal positioning errors before and after anchor refinement. The red line represents the errors using raw anchors obtained directly from the angle-based matching algorithm, while the blue line shows the errors after refinement with more accurate anchor positions selected from video frames. Orange dashed lines mark the timestamps of raw anchors, green dashed lines indicate the timestamps of refined anchors, and purple dashed lines represent timestamps shared by both raw and refined anchors.</p><p>As shown in the error plot, the horizontal positioning errors significantly decrease after refining the anchors with previously large matching errors. In particular, between Anchor<sub>1</sub> and Anchor<sub>2</sub>, the horizontal errors are noticeably reduced following the refinement. According to <xref rid="sensors-25-05536-t010" ref-type="table">Table 10</xref>, the mean error decreased from 1.45 m to 1.14 m, representing a 21% improvement, while the RMSE dropped from 1.65 m to 1.38 m, corresponding to a 16% reduction. Although the maximum error increased slightly by 0.13 m, this change is negligible and does not affect the overall improvement. These results confirm that accurate anchor matching contributes to better trajectory optimization, significantly reducing localization error. They also highlight the effectiveness of the proposed anchor factor in improving overall localization accuracy.</p><p>Nevertheless, the anchor-matching algorithm still has certain limitations. In some scenarios, relatively large matching errors may persist. Experimental results confirm that more accurate anchor positions lead to better localization performance. Therefore, future work may focus on improving the anchor-matching algorithm and exploring strategies for selecting or refining anchor positions to achieve optimal performance.</p></sec><sec id="sec4dot3-sensors-25-05536"><title>4.3. Another-User Test</title><p>We further evaluated the robustness of our method by collecting an additional dataset from another user in the same test environment as Experiment 1. <xref rid="sensors-25-05536-f016" ref-type="fig">Figure 16</xref> shows the trajectory and horizontal error comparisons among different methods.</p><p>As shown in the trajectory plot (<xref rid="sensors-25-05536-f016" ref-type="fig">Figure 16</xref>a), the FGO-GNSS/PDR + Anchor method yields the trajectory that closely follows the ground-truth path, consistent with the performance observed in Experiments 1 and 2. In the corresponding horizontal positioning errors plot (<xref rid="sensors-25-05536-f016" ref-type="fig">Figure 16</xref>b), the error distribution of the FGO-GNSS/PDR + Anchor method is significantly more stable than that of the GNSS and FGO-GNSS/PDR methods. The STD is only 0.58 m, representing a 50% reduction compared to the FGO-GNSS/PDR method. Notably, the original GNSS trajectory exhibits seven major error spikes, all of which are effectively mitigated by the proposed method.</p><p><xref rid="sensors-25-05536-t011" ref-type="table">Table 11</xref> summarizes the RMSE, MEAN, STD, and MAX errors for this dataset. Additionally, <xref rid="sensors-25-05536-t012" ref-type="table">Table 12</xref> presents the corresponding performance improvements of the FGO-GNSS/PDR + Anchor method relative to the FGO-GNSS/PDR baseline. The dataset exhibits substantial improvement, with RMSE reduced by 52%, MEAN by 53%, STD by 50%, and MAX error by 47% compared to the FGO-GNSS/PDR baseline. These results highlight the accuracy of the proposed method, even when applied to data collected from another user.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05536"><title>5. Conclusions</title><p>In this paper, we introduce the concept of a video-based map in the context of RSA and propose a pedestrian localization method based on FGO-GNSS/PDR integrated with anchor constraints. To enable automatic anchor matching, we design a coarse-to-fine optimization strategy: GNSS positions, PDR results, and start&amp;end anchors are first fused using factor graph optimization to yield coarse localization estimates. Then, trajectory turning angle cues are leveraged to match anchor points, and anchor-based constraints are incorporated to obtain refined localization results through second-stage optimization. We collected multiple real-world datasets in three representative urban scenarios and conducted both single-user repeated experiments and another-user trials. The experimental results demonstrate that the proposed anchor factor significantly improves localization accuracy across diverse scenarios. Specifically, the introduction of anchor constraints reduced the mean horizontal positioning error by 42% to 65% and the maximum error by 38% to 76% across all datasets. In this study, the mean horizontal positioning error was 1.36 m. Moreover, in the another-user experiment, the method maintained strong robustness, achieving a 47% to 53% reduction in RMSE, MEAN, STD, and MAX errors. Furthermore, the experiments demonstrate the effectiveness of the proposed PDR method in providing accurate local motion constraints. For anchor matching, the trajectory-turning-angle-based approach shows promising performance in structured urban environments, enabling reasonable anchor correspondence under typical conditions. While the method supports video-based map construction and enhances remote assistance capabilities for visually impaired users, its generalizability and robustness across more diverse users and walking patterns require further validation in future studies.</p><p>In future work, we plan to incorporate GNSS raw measurements into our framework to achieve higher positioning accuracy. We will enhance the PDR algorithm through improved step detection and heading estimation, enabling more reliable displacement constraints in the FGO framework. In addition, the anchor positions and types will be more precisely defined, and a more robust anchor-matching algorithm will be developed. Finally, we aim to implement a real-time, sliding window-based FGO framework for GNSS/PDR integration to enable real-time pedestrian localization in RSA applications.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, S.H. and S.X.; methodology, S.H. and Y.Z.; software, S.H.; validation, S.H., J.Z. and Y.L.; formal analysis, S.H.; investigation, S.H. and Y.Z.; resources, S.X.; data curation, S.H.; writing&#8212;original draft preparation, S.H. and J.Z.; writing&#8212;review and editing, S.H., J.Z. and S.X.; visualization, S.H.; supervision, S.X.; project administration, S.X.; funding acquisition, S.X. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study and the source code are available on request from the corresponding author due to privacy reasons.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><app-group><app id="app1-sensors-25-05536"><title>Appendix A</title><sec id="secAdot1-sensors-25-05536"><title>Appendix A.1</title><p>Detailed visualizations of the results from Experiment 1 are provided in <xref rid="sensors-25-05536-f0A1" ref-type="fig">Figure A1</xref>. Subfigures (a), (c), and (e) show the trajectory plots for Datasets 1, 2, and 3, respectively. Subfigures (b), (d), and (f) illustrate the corresponding horizontal position errors for these datasets. In these plots, the red dashed line denotes the ground truth, the blue solid line indicates the PDR result, the purple solid line represents the GNSS result, the green solid line shows the FGO-GNSS/PDR result, and the orange solid line corresponds to the FGO-GNSS/PDR + Anchor result.</p><fig position="anchor" id="sensors-25-05536-f0A1" orientation="portrait"><label>Figure A1</label><caption><p>Visualizations of Experiment 1 results. (<bold>a</bold>,<bold>c</bold>,<bold>e</bold>) show the estimated trajectories for Datasets 1, 2, and 3, respectively. (<bold>b</bold>,<bold>d</bold>,<bold>f</bold>) present the corresponding horizontal position errors for the same datasets. In these plots, the red dashed line denotes the ground truth, the blue solid line indicates the PDR result, the purple solid line represents the GNSS result, the green solid line shows the FGO-GNSS/PDR result, and the orange solid line corresponds to the FGO-GNSS/PDR + Anchor result.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g0A1.jpg"/></fig></sec><sec id="secAdot2-sensors-25-05536"><title>Appendix A.2</title><p>Detailed visualizations of the results from Experiment 2 are provided in <xref rid="sensors-25-05536-f0A2" ref-type="fig">Figure A2</xref>. Subfigures (a), (c), and (e) show the trajectory plots for Datasets 1, 2, and 3, respectively. Subfigures (b), (d), and (f) illustrate the corresponding horizontal position errors for these datasets. In these plots, the red dashed line denotes the ground truth, the blue solid line indicates the PDR result, the purple solid line represents the GNSS result, the green solid line shows the FGO-GNSS/PDR result, and the orange solid line corresponds to the FGO-GNSS/PDR + Anchor result.</p><fig position="anchor" id="sensors-25-05536-f0A2" orientation="portrait"><label>Figure A2</label><caption><p>Visualizations of Experiment 2 results. (<bold>a</bold>,<bold>c</bold>,<bold>e</bold>) show the estimated trajectories for Datasets 1, 2, and 3, respectively. (<bold>b</bold>,<bold>d</bold>,<bold>f</bold>) present the corresponding horizontal position errors for the same datasets. In these plots, the red dashed line denotes the ground truth, the blue solid line indicates the PDR result, the purple solid line represents the GNSS result, the green solid line shows the FGO-GNSS/PDR result, and the orange solid line corresponds to the FGO-GNSS/PDR + Anchor result.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g0A2.jpg"/></fig></sec></app></app-group><ref-list><title>References</title><ref id="B1-sensors-25-05536"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burton</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Ramke</surname><given-names>J.</given-names></name><name name-style="western"><surname>Marques</surname><given-names>A.P.</given-names></name><name name-style="western"><surname>Bourne</surname><given-names>R.R.A.</given-names></name><name name-style="western"><surname>Congdon</surname><given-names>N.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>I.</given-names></name><name name-style="western"><surname>Ah Tong</surname><given-names>B.A.M.</given-names></name><name name-style="western"><surname>Arunga</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bachani</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bascaran</surname><given-names>C.</given-names></name><etal/></person-group><article-title>The Lancet Global Health Commission on Global Eye Health: Vision beyond 2020</article-title><source>Lancet Glob. Health</source><year>2021</year><volume>9</volume><fpage>e489</fpage><lpage>e551</lpage><pub-id pub-id-type="doi">10.1016/S2214-109X(20)30488-5</pub-id><pub-id pub-id-type="pmid">33607016</pub-id><pub-id pub-id-type="pmcid">PMC7966694</pub-id></element-citation></ref><ref id="B2-sensors-25-05536"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Reddie</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tsai</surname><given-names>C.H.</given-names></name><name name-style="western"><surname>Beck</surname><given-names>J.</given-names></name><name name-style="western"><surname>Rosson</surname><given-names>M.B.</given-names></name><name name-style="western"><surname>Carroll</surname><given-names>J.M.</given-names></name></person-group><article-title>The Emerging Professional Practice of Remote Sighted Assistance for People with Visual Impairments</article-title><source>Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>25&#8211;30 April 2020</conf-date><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/3313831.3376591</pub-id></element-citation></ref><ref id="B3-sensors-25-05536"><label>3.</label><element-citation publication-type="webpage"><article-title>Be My Eyes</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.bemyeyes.com/" ext-link-type="uri">https://www.bemyeyes.com/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-24">(accessed on 24 June 2025)</date-in-citation></element-citation></ref><ref id="B4-sensors-25-05536"><label>4.</label><element-citation publication-type="webpage"><article-title>TapTapSee</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://taptapseeapp.com/" ext-link-type="uri">https://taptapseeapp.com/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-24">(accessed on 24 June 2025)</date-in-citation></element-citation></ref><ref id="B5-sensors-25-05536"><label>5.</label><element-citation publication-type="webpage"><article-title>Aira</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://aira.io/" ext-link-type="uri">https://aira.io/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-24">(accessed on 24 June 2025)</date-in-citation></element-citation></ref><ref id="B6-sensors-25-05536"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bigham</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Jayant</surname><given-names>C.</given-names></name><name name-style="western"><surname>Miller</surname><given-names>A.</given-names></name><name name-style="western"><surname>White</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>T.</given-names></name></person-group><article-title>VizWiz::LocateIt&#8212;Enabling blind people to locate objects in their environment</article-title><source>Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition&#8212;Workshops</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>13&#8211;18 June 2010</conf-date><fpage>65</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1109/CVPRW.2010.5543821</pub-id></element-citation></ref><ref id="B7-sensors-25-05536"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Holton</surname><given-names>B.</given-names></name></person-group><article-title>BeSpecular: A New Remote Assistant Service</article-title><source>Access World Mag.</source><year>2016</year><volume>17</volume><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.afb.org/aw/17/7/15313" ext-link-type="uri">https://www.afb.org/aw/17/7/15313</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-24">(accessed on 24 June 2025)</date-in-citation></element-citation></ref><ref id="B8-sensors-25-05536"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Holton</surname><given-names>B.</given-names></name></person-group><article-title>CrowdViz: Remote Video Assistance on your iPhone</article-title><source>Access World Mag.</source><year>2015</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.afb.org/aw/16/11/15507" ext-link-type="uri">https://www.afb.org/aw/16/11/15507</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-24">(accessed on 24 June 2025)</date-in-citation></element-citation></ref><ref id="B9-sensors-25-05536"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>J.</given-names></name><name name-style="western"><surname>Billah</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Carroll</surname><given-names>J.M.</given-names></name></person-group><article-title>Opportunities for Human-AI Collaboration in Remote Sighted Assistance</article-title><source>Proceedings of the 27th International Conference on Intelligent User Interfaces</source><conf-loc>Helsinki, Finland</conf-loc><conf-date>22&#8211;25 March 2022</conf-date><fpage>63</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1145/3490099.3511113</pub-id><pub-id pub-id-type="pmcid">PMC11755352</pub-id><pub-id pub-id-type="pmid">39850496</pub-id></element-citation></ref><ref id="B10-sensors-25-05536"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kamikubo</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kato</surname><given-names>N.</given-names></name><name name-style="western"><surname>Higuchi</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yonetani</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sato</surname><given-names>Y.</given-names></name></person-group><article-title>Support Strategies for Remote Guides in Assisting People with Visual Impairments for Effective Indoor Navigation</article-title><source>Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>25&#8211;30 April 2020</conf-date><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/3313831.3376823</pub-id></element-citation></ref><ref id="B11-sensors-25-05536"><label>11.</label><element-citation publication-type="webpage"><article-title>Google Maps</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.google.com/maps/" ext-link-type="uri">https://www.google.com/maps/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-24">(accessed on 24 June 2025)</date-in-citation></element-citation></ref><ref id="B12-sensors-25-05536"><label>12.</label><element-citation publication-type="webpage"><article-title>OpenStreetMap</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.openstreetmap.org/" ext-link-type="uri">https://www.openstreetmap.org/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-24">(accessed on 24 June 2025)</date-in-citation></element-citation></ref><ref id="B13-sensors-25-05536"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Han</surname><given-names>Y.</given-names></name></person-group><article-title>SmartPDR: Smartphone-Based Pedestrian Dead Reckoning for Indoor Localization</article-title><source>IEEE Sens. J.</source><year>2015</year><volume>15</volume><fpage>2906</fpage><lpage>2916</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2014.2382568</pub-id></element-citation></ref><ref id="B14-sensors-25-05536"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hyypp&#228;</surname><given-names>J.</given-names></name></person-group><article-title>Smartphone PDR/GNSS Integration via Factor Graph Optimization for Pedestrian Navigation</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2022</year><volume>71</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/TIM.2022.3186082</pub-id></element-citation></ref><ref id="B15-sensors-25-05536"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>X.</given-names></name></person-group><article-title>Smartphone-based GNSS/PDR integration navigation enhanced by measurements resilient adjustment under challenging scenarios</article-title><source>GPS Solut.</source><year>2024</year><volume>29</volume><fpage>23</fpage><pub-id pub-id-type="doi">10.1007/s10291-024-01777-6</pub-id></element-citation></ref><ref id="B16-sensors-25-05536"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>L.T.</given-names></name></person-group><article-title>Trajectory Smoothing Using GNSS/PDR Integration via Factor Graph Optimization in Urban Canyons</article-title><source>IEEE Internet Things J.</source><year>2024</year><volume>11</volume><fpage>25425</fpage><lpage>25439</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2024.3396360</pub-id></element-citation></ref><ref id="B17-sensors-25-05536"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hyypp&#228;</surname><given-names>J.</given-names></name></person-group><article-title>Walking Gaits Aided Mobile GNSS for Pedestrian Navigation in Urban Areas</article-title><source>IEEE Internet Things J.</source><year>2024</year><volume>11</volume><fpage>8499</fpage><lpage>8510</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2023.3319014</pub-id></element-citation></ref><ref id="B18-sensors-25-05536"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suzuki</surname><given-names>T.</given-names></name></person-group><article-title>Precise Position Estimation Using Smartphone Raw GNSS Data Based on Two-Step Optimization</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>1205</elocation-id><pub-id pub-id-type="doi">10.3390/s23031205</pub-id><pub-id pub-id-type="pmid">36772245</pub-id><pub-id pub-id-type="pmcid">PMC9919037</pub-id></element-citation></ref><ref id="B19-sensors-25-05536"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>N.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.</given-names></name></person-group><article-title>Real-time GNSS precise point positioning with smartphones for vehicle navigation</article-title><source>Satell. Navig.</source><year>2022</year><volume>3</volume><fpage>19</fpage><pub-id pub-id-type="doi">10.1186/s43020-022-00079-x</pub-id></element-citation></ref><ref id="B20-sensors-25-05536"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>L.T.</given-names></name></person-group><article-title>3D Vision Aided GNSS Real-Time Kinematic Positioning for Autonomous Systems in Urban Canyons</article-title><source>Navigation</source><year>2023</year><volume>70</volume><fpage>navi.590</fpage><pub-id pub-id-type="doi">10.33012/navi.590</pub-id></element-citation></ref><ref id="B21-sensors-25-05536"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Gan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>W.</given-names></name></person-group><article-title>NLOS signal detection and correction for smartphone using convolutional neural network and variational mode decomposition in urban environment</article-title><source>GPS Solut.</source><year>2023</year><volume>27</volume><fpage>31</fpage><pub-id pub-id-type="doi">10.1007/s10291-022-01369-2</pub-id></element-citation></ref><ref id="B22-sensors-25-05536"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>M.</given-names></name></person-group><article-title>Doppler-Aided GNSS Position Estimation With Weighted Least Squares</article-title><source>IEEE Trans. Veh. Technol.</source><year>2011</year><volume>60</volume><fpage>3615</fpage><lpage>3624</lpage><pub-id pub-id-type="doi">10.1109/TVT.2011.2163738</pub-id></element-citation></ref><ref id="B23-sensors-25-05536"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>M.</given-names></name></person-group><article-title>The Improvement of Positioning Accuracy with Weighted Least Square Based on SNR</article-title><source>Proceedings of the 2009 5th International Conference on Wireless Communications, Networking and Mobile Computing</source><conf-loc>Beijing, China</conf-loc><conf-date>24&#8211;26 September 2009</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/WICOM.2009.5302600</pub-id></element-citation></ref><ref id="B24-sensors-25-05536"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Grewal</surname><given-names>M.S.</given-names></name></person-group><article-title>Kalman Filtering</article-title><source>International Encyclopedia of Statistical Science</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2011</year><fpage>705</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-04898-2_321</pub-id></element-citation></ref><ref id="B25-sensors-25-05536"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petovello</surname><given-names>M.G.</given-names></name><name name-style="western"><surname>O&#8217;Keefe</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lachapelle</surname><given-names>G.</given-names></name><name name-style="western"><surname>Cannon</surname><given-names>M.E.</given-names></name></person-group><article-title>Consideration of time-correlated errors in a Kalman filter applicable to GNSS</article-title><source>J. Geod.</source><year>2009</year><volume>83</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1007/s00190-008-0231-z</pub-id></element-citation></ref><ref id="B26-sensors-25-05536"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Stephenson</surname><given-names>S.</given-names></name><name name-style="western"><surname>Peltola</surname><given-names>P.</given-names></name></person-group><article-title>A closed-loop EKF and multi-failure diagnosis approach for cooperative GNSS positioning</article-title><source>GPS Solut.</source><year>2016</year><volume>20</volume><fpage>795</fpage><lpage>805</lpage><pub-id pub-id-type="doi">10.1007/s10291-015-0489-6</pub-id></element-citation></ref><ref id="B27-sensors-25-05536"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pfeifer</surname><given-names>T.</given-names></name><name name-style="western"><surname>Protzel</surname><given-names>P.</given-names></name></person-group><article-title>Robust Sensor Fusion with Self-Tuning Mixture Models</article-title><source>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Madrid, Spain</conf-loc><conf-date>1&#8211;5 October 2018</conf-date><fpage>3678</fpage><lpage>3685</lpage><pub-id pub-id-type="doi">10.1109/IROS.2018.8594459</pub-id></element-citation></ref><ref id="B28-sensors-25-05536"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pfeifer</surname><given-names>T.</given-names></name><name name-style="western"><surname>Protzel</surname><given-names>P.</given-names></name></person-group><article-title>Expectation-Maximization for Adaptive Mixture Models in Graph Optimization</article-title><source>Proceedings of the 2019 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>20&#8211;24 May 2019</conf-date><fpage>3151</fpage><lpage>3157</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2019.8793601</pub-id></element-citation></ref><ref id="B29-sensors-25-05536"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Pfeifer</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>L.T.</given-names></name></person-group><article-title>Factor graph optimization for GNSS/INS integration: A comparison with the extended Kalman filter</article-title><source>Navigation</source><year>2021</year><volume>68</volume><fpage>315</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1002/navi.421</pub-id></element-citation></ref><ref id="B30-sensors-25-05536"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>L.T.</given-names></name></person-group><article-title>Towards Robust GNSS Positioning and Real-time Kinematic Using Factor Graph Optimization</article-title><source>Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date>30 May&#8211;5 June 2021</conf-date><fpage>5884</fpage><lpage>5890</lpage><pub-id pub-id-type="doi">10.1109/ICRA48506.2021.9562037</pub-id></element-citation></ref><ref id="B31-sensors-25-05536"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name></person-group><article-title>Sensing strides using EMG signal for pedestrian navigation</article-title><source>GPS Solut.</source><year>2011</year><volume>15</volume><fpage>161</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1007/s10291-010-0180-x</pub-id></element-citation></ref><ref id="B32-sensors-25-05536"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>An enhanced foot-mounted PDR method with adaptive ZUPT and multi-sensors fusion for seamless pedestrian navigation</article-title><source>GPS Solut.</source><year>2021</year><volume>26</volume><fpage>13</fpage><pub-id pub-id-type="doi">10.1007/s10291-021-01196-x</pub-id></element-citation></ref><ref id="B33-sensors-25-05536"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>R.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name></person-group><article-title>Large-Scale Indoor Localization Solution for Pervasive Smartphones Using Corrected Acoustic Signals and Data-Driven PDR</article-title><source>IEEE Internet Things J.</source><year>2023</year><volume>10</volume><fpage>15338</fpage><lpage>15349</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2023.3263551</pub-id></element-citation></ref><ref id="B34-sensors-25-05536"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kuang</surname><given-names>J.K.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Robust Pedestrian Dead Reckoning Based on MEMS-IMU for Smartphones</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>1391</elocation-id><pub-id pub-id-type="doi">10.3390/s18051391</pub-id><pub-id pub-id-type="pmid">29724003</pub-id><pub-id pub-id-type="pmcid">PMC5982656</pub-id></element-citation></ref><ref id="B35-sensors-25-05536"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hyyppa</surname><given-names>J.</given-names></name></person-group><article-title>A Probabilistic Method-Based Smartphone GNSS Fault Detection and Exclusion System Utilizing PDR Step Length</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>4993</elocation-id><pub-id pub-id-type="doi">10.3390/rs15204993</pub-id></element-citation></ref><ref id="B36-sensors-25-05536"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Walker: Continuous and Precise Navigation by Fusing GNSS and MEMS in Smartphone Chipsets for Pedestrians</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>139</elocation-id><pub-id pub-id-type="doi">10.3390/rs11020139</pub-id></element-citation></ref><ref id="B37-sensors-25-05536"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><article-title>Hybrid Urban Canyon Pedestrian Navigation Scheme Combined PDR, GNSS and Beacon Based on Smartphone</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>2174</elocation-id><pub-id pub-id-type="doi">10.3390/rs11182174</pub-id></element-citation></ref><ref id="B38-sensors-25-05536"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rehman</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shahid</surname><given-names>H.</given-names></name><name name-style="western"><surname>Afzal</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Bhatti</surname><given-names>H.M.A.</given-names></name></person-group><article-title>Accurate and Direct GNSS/PDR Integration Using Extended Kalman Filter for Pedestrian Smartphone Navigation</article-title><source>Gyroscopy Navig.</source><year>2020</year><volume>11</volume><fpage>124</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1134/S2075108720020054</pub-id></element-citation></ref><ref id="B39-sensors-25-05536"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Angrisano</surname><given-names>A.</given-names></name><name name-style="western"><surname>Vultaggio</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gaglione</surname><given-names>S.</given-names></name><name name-style="western"><surname>Crocetto</surname><given-names>N.</given-names></name></person-group><article-title>Pedestrian localization with PDR supplemented by GNSS</article-title><source>Proceedings of the 2019 European Navigation Conference (ENC)</source><conf-loc>Warsaw, Poland</conf-loc><conf-date>9&#8211;12 April 2019</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/EURONAV.2019.8714150</pub-id></element-citation></ref><ref id="B40-sensors-25-05536"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Mi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name></person-group><article-title>A Continuous PDR and GNSS Fusing Algorithm for Smartphone Positioning</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>5171</elocation-id><pub-id pub-id-type="doi">10.3390/rs14205171</pub-id></element-citation></ref><ref id="B41-sensors-25-05536"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Basso</surname><given-names>M.</given-names></name><name name-style="western"><surname>Martinelli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Morosi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sera</surname><given-names>F.</given-names></name></person-group><article-title>A Real-Time GNSS/PDR Navigation System for Mobile Devices</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>1567</elocation-id><pub-id pub-id-type="doi">10.3390/rs13081567</pub-id></element-citation></ref><ref id="B42-sensors-25-05536"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hyyppa</surname><given-names>J.</given-names></name></person-group><article-title>Implementation and performance analysis of the PDR/GNSS integration on a smartphone</article-title><source>GPS Solut.</source><year>2022</year><volume>26</volume><fpage>81</fpage><pub-id pub-id-type="doi">10.1007/s10291-022-01260-0</pub-id></element-citation></ref><ref id="B43-sensors-25-05536"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bruhn</surname><given-names>A.</given-names></name><name name-style="western"><surname>Papenberg</surname><given-names>N.</given-names></name><name name-style="western"><surname>Weickert</surname><given-names>J.</given-names></name></person-group><article-title>High Accuracy Optical Flow Estimation Based on a Theory for Warping</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2004</source><conf-loc>Prague, Czech Republic</conf-loc><conf-date>11&#8211;14 May 2004</conf-date><fpage>25</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1007/978-3-540-24673-2_3</pub-id></element-citation></ref><ref id="B44-sensors-25-05536"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weinberg</surname><given-names>H.</given-names></name></person-group><article-title>Using the ADXL202 in Pedometer and Personal Navigation Applications</article-title><source>Analog Devices AN-602 Appl. Note</source><year>2002</year><volume>2</volume><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B45-sensors-25-05536"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sarlin</surname><given-names>P.E.</given-names></name><name name-style="western"><surname>DeTone</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Avetisyan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Straub</surname><given-names>J.</given-names></name><name name-style="western"><surname>Malisiewicz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bulo</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Newcombe</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kontschieder</surname><given-names>P.</given-names></name><name name-style="western"><surname>Balntas</surname><given-names>V.</given-names></name></person-group><article-title>Orienternet: Visual localization in 2d public maps with neural matching</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>21632</fpage><lpage>21642</lpage></element-citation></ref><ref id="B46-sensors-25-05536"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jim&#233;nez</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Seco</surname><given-names>F.</given-names></name><name name-style="western"><surname>Torres-Sospedra</surname><given-names>J.</given-names></name></person-group><article-title>Tools for smartphone multi-sensor data registration and GT mapping for positioning applications</article-title><source>Proceedings of the 2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN)</source><conf-loc>Pisa, Italy</conf-loc><conf-date>30 September&#8211;3 October 2019</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B47-sensors-25-05536"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dellaert</surname><given-names>F.</given-names></name></person-group><article-title>Factor graphs and GTSAM: A hands-on introduction</article-title><source>Ga. Inst. Technol. Tech. Rep.</source><year>2012</year><volume>2</volume><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://sites.cc.gatech.edu/home/dellaert/FrankDellaert/Frank_Dellaert/Entries/2013/5/10_Factor_Graphs_Tutorial_files/gtsam.pdf" ext-link-type="uri">https://sites.cc.gatech.edu/home/dellaert/FrankDellaert/Frank_Dellaert/Entries/2013/5/10_Factor_Graphs_Tutorial_files/gtsam.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-24">(accessed on 24 June 2025)</date-in-citation></element-citation></ref><ref id="B48-sensors-25-05536"><label>48.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gavin</surname><given-names>H.P.</given-names></name></person-group><source>The Levenberg-Marquardt Algorithm for Nonlinear Least Squares Curve-Fitting Problems</source><publisher-name>Department of Civil and Environmental Engineering, Duke University</publisher-name><publisher-loc>Durham, NC, USA</publisher-loc><year>2019</year><volume>Volume 3</volume><fpage>1</fpage><lpage>23</lpage></element-citation></ref><ref id="B49-sensors-25-05536"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lisle</surname><given-names>R.J.</given-names></name></person-group><article-title>Google Earth: A new geological resource</article-title><source>Geol. Today</source><year>2006</year><volume>22</volume><fpage>29</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2451.2006.00546.x</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05536-f001" orientation="portrait"><label>Figure 1</label><caption><p>A schematic of the video-based map proposed in this study. Spatial positions are represented using latitude and longitude coordinates, with associations established between video frames and geographic locations. The background map layer is derived from OSM data [<xref rid="B12-sensors-25-05536" ref-type="bibr">12</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g001.jpg"/></fig><fig position="float" id="sensors-25-05536-f002" orientation="portrait"><label>Figure 2</label><caption><p>An overview of the proposed FGO-GNSS/PDR localization algorithm. The framework consists of three main components: PDR, GNSS, and a coarse-to-fine optimization strategy. The PDR module estimates pedestrian motion based on inertial and AHRS data, while the GNSS module provides satellite-based absolute positioning. The coarse-to-fine strategy progressively refines the trajectory by integrating multi-source information to optimize localization accuracy.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g002.jpg"/></fig><fig position="float" id="sensors-25-05536-f003" orientation="portrait"><label>Figure 3</label><caption><p>PDR mechanism.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g003.jpg"/></fig><fig position="float" id="sensors-25-05536-f004" orientation="portrait"><label>Figure 4</label><caption><p>An overview of the proposed PDR algorithm framework, including coordinate conversion, step detection, step length estimation, and heading estimation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g004.jpg"/></fig><fig position="float" id="sensors-25-05536-f005" orientation="portrait"><label>Figure 5</label><caption><p>Anchor-matching process: Turning angle accumulation is computed within a fixed-length sliding window. The middle point of the window with the largest cumulative angle change is selected as the anchor match.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g005.jpg"/></fig><fig position="float" id="sensors-25-05536-f006" orientation="portrait"><label>Figure 6</label><caption><p>The structure of the final factor graph, including GNSS factors (green), PDR factors (light yellow), and anchor factors (gray).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g006.jpg"/></fig><fig position="float" id="sensors-25-05536-f007" orientation="portrait"><label>Figure 7</label><caption><p>Data acquisition setup. The system consists of a smartphone and a multi-camera module for data collection and a Vision-RTK 2 unit for recording ground-truth trajectories.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g007.jpg"/></fig><fig position="float" id="sensors-25-05536-f008" orientation="portrait"><label>Figure 8</label><caption><p>A scene map and trajectory layout for Experiment 1, visualized in Google Earth [<xref rid="B49-sensors-25-05536" ref-type="bibr">49</xref>]. The trajectory shown represents the ground truth, collected using the Vision-RTK 2, with red circles denoting pedestrian positions sampled at one-second intervals.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g008.jpg"/></fig><fig position="float" id="sensors-25-05536-f009" orientation="portrait"><label>Figure 9</label><caption><p>Results for the first dataset in Experiment 1. (<bold>a</bold>) A comparison of pedestrian trajectories obtained using the three methods; (<bold>b</bold>) corresponding horizontal positioning errors over time. The red dashed line denotes the ground truth, the blue solid line indicates the PDR result, the purple solid line represents the GNSS result, the green solid line shows the FGO-GNSS/PDR result, and the orange solid line corresponds to the FGO-GNSS/PDR + Anchor result.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g009.jpg"/></fig><fig position="float" id="sensors-25-05536-f010" orientation="portrait"><label>Figure 10</label><caption><p>Scene map and trajectory layout for Experiment 2, visualized in Google Earth. The trajectory shown represents the ground truth, collected using the Vision-RTK 2, with red circles denoting pedestrian positions sampled at one-second intervals.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g010.jpg"/></fig><fig position="float" id="sensors-25-05536-f011" orientation="portrait"><label>Figure 11</label><caption><p>Results for the third dataset in Experiment 2. (<bold>a</bold>) A comparison of pedestrian trajectories obtained using the three methods; (<bold>b</bold>) corresponding horizontal positioning errors over time. The red dashed line denotes the ground truth, the blue solid line indicates the PDR result, the purple solid line represents the GNSS result, the green solid line shows the FGO-GNSS/PDR result, and the orange solid line corresponds to the FGO-GNSS/PDR + Anchor result.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g011.jpg"/></fig><fig position="float" id="sensors-25-05536-f012" orientation="portrait"><label>Figure 12</label><caption><p>Scene map and trajectory layout for Experiment 3, visualized in Google Earth. The trajectory shown represents the ground truth, collected using the Vision-RTK 2, with red circles denoting pedestrian positions sampled at one-second intervals.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g012.jpg"/></fig><fig position="float" id="sensors-25-05536-f013" orientation="portrait"><label>Figure 13</label><caption><p>Results for the dataset in Experiment 3. (<bold>a</bold>) A comparison of pedestrian trajectories obtained using the three methods; (<bold>b</bold>) corresponding horizontal positioning errors over time. The red dashed line denotes the ground truth, the blue solid line indicates the PDR result, the purple solid line represents the GNSS result, the green solid line shows the FGO-GNSS/PDR result, and the orange solid line corresponds to the FGO-GNSS/PDR + Anchor result.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g013.jpg"/></fig><fig position="float" id="sensors-25-05536-f014" orientation="portrait"><label>Figure 14</label><caption><p>PDR performance in Experiment 2. (<bold>a</bold>) Reconstructed trajectory using the proposed PDR algorithm; (<bold>b</bold>) corresponding horizontal positioning errors over time for three datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g014.jpg"/></fig><fig position="float" id="sensors-25-05536-f015" orientation="portrait"><label>Figure 15</label><caption><p>Horizontal positioning errors before and after anchor refinement for the 2<inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula> dataset in Experiment 1. The red line represents errors using the raw anchor positions, while the blue line shows errors after refinement. Orange, green, and purple dashed lines indicate the timestamps of raw anchors, refined anchors, and their shared anchors, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g015.jpg"/></fig><fig position="float" id="sensors-25-05536-f016" orientation="portrait"><label>Figure 16</label><caption><p>Results for the another-user dataset. (<bold>a</bold>) A comparison of pedestrian trajectories obtained using the three methods; (<bold>b</bold>) corresponding horizontal positioning errors over time. The red dashed line denotes the ground truth, the blue solid line indicates the PDR result, the purple solid line represents the GNSS result, the green solid line shows the FGO-GNSS/PDR result, and the orange solid line corresponds to the FGO-GNSS/PDR + Anchor result.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05536-g016.jpg"/></fig><table-wrap position="float" id="sensors-25-05536-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t001_Table 1</object-id><label>Table 1</label><caption><p>Positioning performance of the listed methods in Experiment 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Group</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">1<inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mi>st</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">5.69</td><td align="center" valign="middle" rowspan="1" colspan="1">4.12</td><td align="center" valign="middle" rowspan="1" colspan="1">3.93</td><td align="center" valign="middle" rowspan="1" colspan="1">33.95</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">2.57</td><td align="center" valign="middle" rowspan="1" colspan="1">2.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">4.47</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.77</td></tr><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">2<inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">5.10</td><td align="center" valign="middle" rowspan="1" colspan="1">4.74</td><td align="center" valign="middle" rowspan="1" colspan="1">1.88</td><td align="center" valign="middle" rowspan="1" colspan="1">11.64</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">4.00</td><td align="center" valign="middle" rowspan="1" colspan="1">3.68</td><td align="center" valign="middle" rowspan="1" colspan="1">1.58</td><td align="center" valign="middle" rowspan="1" colspan="1">6.52</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.08</td></tr><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">3<inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mi>rd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">6.84</td><td align="center" valign="middle" rowspan="1" colspan="1">5.26</td><td align="center" valign="middle" rowspan="1" colspan="1">4.37</td><td align="center" valign="middle" rowspan="1" colspan="1">16.56</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">4.93</td><td align="center" valign="middle" rowspan="1" colspan="1">3.76</td><td align="center" valign="middle" rowspan="1" colspan="1">3.19</td><td align="center" valign="middle" rowspan="1" colspan="1">11.04</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.61</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t002_Table 2</object-id><label>Table 2</label><caption><p>Improvement rates of GNSS and FGO-GNSS/PDR + Anchor compared to the baseline in Experiment 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Group</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">1<inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mi>st</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;121%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;72%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;314%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;659%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38%</td></tr><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">2<inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;27%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;29%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;19%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;79%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53%</td></tr><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">3<inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mi>rd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;39%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;40%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;37%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;50%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t003_Table 3</object-id><label>Table 3</label><caption><p>Positioning performance of the listed methods in Experiment 2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Group</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">1<inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mi>st</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">6.47</td><td align="center" valign="middle" rowspan="1" colspan="1">5.59</td><td align="center" valign="middle" rowspan="1" colspan="1">3.26</td><td align="center" valign="middle" rowspan="1" colspan="1">24.56</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">4.02</td><td align="center" valign="middle" rowspan="1" colspan="1">3.51</td><td align="center" valign="middle" rowspan="1" colspan="1">1.97</td><td align="center" valign="middle" rowspan="1" colspan="1">7.89</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.00</td></tr><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">2<inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">7.38</td><td align="center" valign="middle" rowspan="1" colspan="1">4.90</td><td align="center" valign="middle" rowspan="1" colspan="1">5.52</td><td align="center" valign="middle" rowspan="1" colspan="1">39.81</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">3.71</td><td align="center" valign="middle" rowspan="1" colspan="1">2.83</td><td align="center" valign="middle" rowspan="1" colspan="1">2.39</td><td align="center" valign="middle" rowspan="1" colspan="1">9.57</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.08</td></tr><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">3<inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mi>rd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">6.42</td><td align="center" valign="middle" rowspan="1" colspan="1">5.35</td><td align="center" valign="middle" rowspan="1" colspan="1">3.53</td><td align="center" valign="middle" rowspan="1" colspan="1">15.75</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">4.58</td><td align="center" valign="middle" rowspan="1" colspan="1">3.79</td><td align="center" valign="middle" rowspan="1" colspan="1">2.57</td><td align="center" valign="middle" rowspan="1" colspan="1">10.44</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.49</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t004_Table 4</object-id><label>Table 4</label><caption><p>Improvement rates of GNSS and FGO-GNSS/PDR + Anchor compared to the baseline in Experiment 2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Group</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">1<inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mi>st</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;61%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;59%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;65%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;211%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49%</td></tr><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">2<inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;99%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;73%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;131%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;316%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57%</td></tr><tr><td align="center" valign="middle" rowspan="3" style="border-bottom:solid thin" colspan="1">3<inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mi>rd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;40%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;41%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;37%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;51%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t005_Table 5</object-id><label>Table 5</label><caption><p>Positioning performance of the listed methods in Experiment 3.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (m)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">5.72</td><td align="center" valign="middle" rowspan="1" colspan="1">4.58</td><td align="center" valign="middle" rowspan="1" colspan="1">3.42</td><td align="center" valign="middle" rowspan="1" colspan="1">36.35</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">2.97</td><td align="center" valign="middle" rowspan="1" colspan="1">2.66</td><td align="center" valign="middle" rowspan="1" colspan="1">1.31</td><td align="center" valign="middle" rowspan="1" colspan="1">6.55</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.10</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t006_Table 6</object-id><label>Table 6</label><caption><p>Improvement rates of GNSS and FGO-GNSS/PDR + Anchor compared to the baseline in Experiment 3.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;93%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;72%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;162%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;455%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t007_Table 7</object-id><label>Table 7</label><caption><p>PDR-only localization errors (RMSE, MEAN, STD, MAX) in Experiment 2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Group</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1<inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mi>st</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">15.07</td><td align="center" valign="middle" rowspan="1" colspan="1">13.93</td><td align="center" valign="middle" rowspan="1" colspan="1">5.76</td><td align="center" valign="middle" rowspan="1" colspan="1">26.48</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2<inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">12.00</td><td align="center" valign="middle" rowspan="1" colspan="1">11.08</td><td align="center" valign="middle" rowspan="1" colspan="1">4.61</td><td align="center" valign="middle" rowspan="1" colspan="1">19.50</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3<inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mi>rd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.32</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t008_Table 8</object-id><label>Table 8</label><caption><p>Anchor-matching errors (in meters) across five anchors in Experiment 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Anchor<sub>1</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Anchor<sub>2</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Anchor<sub>3</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Anchor<sub>4</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Anchor<sub>5</sub></th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1<inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mi>st</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">0.19</td><td align="center" valign="middle" rowspan="1" colspan="1">0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.22</td><td align="center" valign="middle" rowspan="1" colspan="1">0.32</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2<inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">1.27</td><td align="center" valign="middle" rowspan="1" colspan="1">2.54</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" rowspan="1" colspan="1">0.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.70</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3<inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mi>rd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.30</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t009_Table 9</object-id><label>Table 9</label><caption><p>Anchor-matching performance in Experiment 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1<inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mi>st</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">0.21</td><td align="center" valign="middle" rowspan="1" colspan="1">0.19</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">0.32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2<inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">1.34</td><td align="center" valign="middle" rowspan="1" colspan="1">1.06</td><td align="center" valign="middle" rowspan="1" colspan="1">0.82</td><td align="center" valign="middle" rowspan="1" colspan="1">2.54</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3<inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mi>rd</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.75</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t010_Table 10</object-id><label>Table 10</label><caption><p>Positioning performance before and after anchor refinement for the 2<inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mi>nd</mml:mi></mml:mrow></mml:math></inline-formula> dataset in Experiment 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Stage</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Before Refinement</td><td align="center" valign="middle" rowspan="1" colspan="1">1.65</td><td align="center" valign="middle" rowspan="1" colspan="1">1.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.78</td><td align="center" valign="middle" rowspan="1" colspan="1">3.08</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">After Refinement</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.21</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t011" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t011_Table 11</object-id><label>Table 11</label><caption><p>Positioning performance of the listed methods in the another-user dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (m)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">6.75</td><td align="center" valign="middle" rowspan="1" colspan="1">5.14</td><td align="center" valign="middle" rowspan="1" colspan="1">4.38</td><td align="center" valign="middle" rowspan="1" colspan="1">41.87</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">2.76</td><td align="center" valign="middle" rowspan="1" colspan="1">2.51</td><td align="center" valign="middle" rowspan="1" colspan="1">1.17</td><td align="center" valign="middle" rowspan="1" colspan="1">4.85</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.56</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05536-t012" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05536-t012_Table 12</object-id><label>Table 12</label><caption><p>Improvement rates of GNSS and FGO-GNSS/PDR + Anchor compared to the baseline in the another-user dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MEAN (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">STD (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAX (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;144%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;105%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;276%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8722;763%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FGO-GNSS/PDR</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8212;</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FGO-GNSS/PDR + Anchor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47%</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>