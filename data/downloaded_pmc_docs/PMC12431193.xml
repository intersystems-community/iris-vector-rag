<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431193</article-id><article-id pub-id-type="pmcid-ver">PMC12431193.1</article-id><article-id pub-id-type="pmcaid">12431193</article-id><article-id pub-id-type="pmcaiid">12431193</article-id><article-id pub-id-type="doi">10.3390/s25175290</article-id><article-id pub-id-type="publisher-id">sensors-25-05290</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Rapid Segmentation Method Based on Few-Shot Learning: A Case Study on Roadways</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Cai</surname><given-names initials="H">He</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-5384-1710</contrib-id><name name-style="western"><surname>Chen</surname><given-names initials="J">Jiangchuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="c1-sensors-25-05290" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4098-8522</contrib-id><name name-style="western"><surname>Yin</surname><given-names initials="Y">Yunfei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yu</surname><given-names initials="J">Junpeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Dong</surname><given-names initials="Z">Zejiao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Kwan</surname><given-names initials="C">Chiman</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05290">School of Transportation Science and Engineering, Harbin Institute of Technology, Nangang District, Harbin 150006, China; <email>20b932024@stu.hit.edu.cn</email> (H.C.); <email>yunfeiyin@hit.edu.cn</email> (Y.Y.); <email>22s132079@stu.hit.edu.cn</email> (J.Y.); <email>hitdzj@hit.edu.cn</email> (Z.D.)</aff><author-notes><corresp id="c1-sensors-25-05290"><label>*</label>Correspondence: <email>22b932014@stu.hit.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5290</elocation-id><history><date date-type="received"><day>05</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>09</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>23</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>26</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05290.pdf"/><abstract><p>Currently, deep learning-based segmentation methods are capable of achieving accurate segmentation. However, their deployment and training are costly and resource-intensive. To reduce deployment costs and facilitate the application of segmentation models for road imagery, this paper introduces a novel road segmentation algorithm based on few-shot learning. The algorithm consists of the back-projection module (BPM), responsible for generating target probabilities, and the segmentation module (SM), which performs image segmentation based on these probabilities. To achieve precise segmentation, the paper proposes a learning mechanism that simultaneously considers both positive and negative samples, effectively capturing the color features of the environment and objects. Additionally, through the workflow design, the algorithm can rapidly perform segmentation tasks across different scenarios without requiring transfer learning and with minimal sample prompts. Experimental results show that the algorithm achieves intersection over union segmentation accuracies of 94.9%, 92.7%, 94.9%, and 94.7% across different scenarios. Compared to state-of-the-art methods, it delivers precise segmentation with fewer local road image prompts, enabling efficient edge deployment.</p></abstract><kwd-group><kwd>road segmentation</kwd><kwd>unmanned aerial vehicles</kwd><kwd>back-projection</kwd><kwd>few-shot learning</kwd></kwd-group><funding-group><award-group><funding-source>National Key R&amp;D Program of China</funding-source><award-id>2024YFE0214600</award-id></award-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62303134</award-id></award-group><award-group><funding-source>China Postdoctoral Science Foundation</funding-source><award-id>2022M710963</award-id><award-id>2024T171157</award-id></award-group><award-group><funding-source>Heilongjiang Postdoctoral Science Foundation</funding-source><award-id>LBH-Z22160</award-id></award-group><award-group><funding-source>Fundamental Research Funds for the Central Universities</funding-source><award-id>XNJKKGYDJ2024012</award-id></award-group><funding-statement>This work was supported in part by the National Key R&amp;D Program of China (Grant No. 2024YFE0214600), the National Natural Science Foundation of China (No. 62303134), the China Postdoctoral Science Foundation (No. 2022M710963 and 2024T171157), the Heilongjiang Postdoctoral Science Foundation (No. LBH-Z22160), and the Fundamental Research Funds for the Central Universities (XNJKKGYDJ2024012).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05290"><title>1. Introduction</title><p>The development of computer vision technology has led to its increasingly widespread application in road systems. In computer vision, semantic segmentation is a crucial technology that can provide both semantic information and localization information for objects within an image. With the advancement of drone technology, its application in road vision systems is becoming increasingly widespread. The unmanned aerial vehicles (UAVs) can capture images of the road surface from multiple angles and heights, which can provide a comprehensive view of the condition of the road. In addition, UAVs can also reach all areas of the road without disrupting traffic, which have great potential for road inspection [<xref rid="B1-sensors-25-05290" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05290" ref-type="bibr">2</xref>] and tasks such as post-disaster road reconstruction [<xref rid="B3-sensors-25-05290" ref-type="bibr">3</xref>]. However, there are many background areas in the images captured by the UAVs, and only road areas are interested. The segmentation of roads is needed to obtain road knowledge for detection and maintenance [<xref rid="B4-sensors-25-05290" ref-type="bibr">4</xref>].</p><p>Road segmentation involves dividing a digital image into segments or sections, where each segment represents a part of a road. The road network information can be extracted from remote sensing images to serve smart navigation, autonomous driving, smart cities and intelligent transportation [<xref rid="B5-sensors-25-05290" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05290" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05290" ref-type="bibr">7</xref>]. In road region segmentation, traditional solutions achieve segmentation by extracting road image features based on manually designed algorithms, such as mathematical morphology [<xref rid="B8-sensors-25-05290" ref-type="bibr">8</xref>], genetic algorithms [<xref rid="B9-sensors-25-05290" ref-type="bibr">9</xref>], fuzzy connectedness [<xref rid="B10-sensors-25-05290" ref-type="bibr">10</xref>], etc. These methods exhibit accuracy and efficiency in solving specific categories such as low-resolution SAR images or roads with linear characteristics. However, they highly dependent on specific models and assumptions, which limit the use of adaptability and stability.</p><p>With the rapid growth of the data scale, traditional model-driven methods are gradually becoming unable to meet the needs of big data applications [<xref rid="B11-sensors-25-05290" ref-type="bibr">11</xref>]. In order to improve the accuracy of algorithms in different scenarios, many scholars begin to improve algorithms through the application of deep learning (DL). In 2015, Long et al. [<xref rid="B12-sensors-25-05290" ref-type="bibr">12</xref>] replaced the fully-connected layer of CNN with the convolutional layer and proposed a fully convolutional neural network (FCNN) model. The proposal of FCNN has significantly advanced the development of DL for semantic segmentation. After that, Henry et al. [<xref rid="B13-sensors-25-05290" ref-type="bibr">13</xref>] used FCNN to segment roads in TerraSAR images. Although it can successfully accomplish the segmentation task in the majority of scenarios at that time, it has exhibited sensitivity to forest boundaries. Since then, Deeplab [<xref rid="B14-sensors-25-05290" ref-type="bibr">14</xref>], U-Net [<xref rid="B15-sensors-25-05290" ref-type="bibr">15</xref>], and other proposed models [<xref rid="B16-sensors-25-05290" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05290" ref-type="bibr">17</xref>] have also been successively applied to road segmentation. Overall, such end-to-end DL models can achieve automatic feature extraction and classification, thereby avoiding complex algorithm design and providing relatively high accuracy. However, the methods ignore spatial information and suffer from sample imbalance. Furthermore, the performance of DL is expected to decline when there is a shift in study domains, as indicated by Neupane et al. [<xref rid="B18-sensors-25-05290" ref-type="bibr">18</xref>]. The existence of these drawbacks also limits the application of DL in other scenarios.</p><p>Although deep learning-based methods have achieved notable accuracy in road segmentation tasks, their generalization ability across different scenes remains limited. In UAVs imagery, roads exhibit inhomogeneous colors and varying widths, often appearing unpaved, as shown in <xref rid="sensors-25-05290-f001" ref-type="fig">Figure 1</xref>. Consequently, segmentation methods tailored for specific types of roads may not be suitable for others [<xref rid="B19-sensors-25-05290" ref-type="bibr">19</xref>]. In addition, constructing datasets for transfer learning requires a substantial amount of effort as well as computational resources. In summary, segmentation still remains difficult for different types of roads under variable lighting conditions.</p><p>Training and tuning the network for different application scenarios is time consuming and costly. Therefore, a more convenient method is proposed for the efficient segmentation of road images captured by UAVs in this paper. The segmentation of roads in a video can be achieved by learning from several image frames and be quickly adapted to different road scenes. The main contributions of this paper are as follows: An improved back-projection method that uses multiple positive and negative samples is proposed. The features of multiple samples can be integrated and eliminated by the different operators proposed in this paper. An easy-to-deploy road segmentation algorithm is designed to quickly adapt and realize road video segmentation for different environments. The algorithm combines the color features and morphological connectivity features of roads, which enables the accurate segmentation of roads at different scales of images.</p><p>The rest of this paper is organized as follows: <xref rid="sec2-sensors-25-05290" ref-type="sec">Section 2</xref> reviews related work relevant to the proposed algorithm; <xref rid="sec3-sensors-25-05290" ref-type="sec">Section 3</xref> details the design of the learning mechanism and parameter optimization process; <xref rid="sec4-sensors-25-05290" ref-type="sec">Section 4</xref> presents experiments applying the algorithm under various parameters and environments; finally, <xref rid="sec5-sensors-25-05290" ref-type="sec">Section 5</xref> concludes the paper and discusses potential application scenarios.</p></sec><sec id="sec2-sensors-25-05290"><title>2. Related Works</title><sec id="sec2dot1-sensors-25-05290"><title>2.1. Road Segmentation Method</title><p>The primary task of road segmentation is to utilize computer vision techniques to analyze the entire image and extract the road regions, thereby providing scene environment information for downstream tasks. Relevant technologies can be broadly categorized into traditional segmentation methods based on image features and end-to-end segmentation approaches driven by deep learning. Their application scenarios include remote sensing imagery [<xref rid="B20-sensors-25-05290" ref-type="bibr">20</xref>], autonomous driving [<xref rid="B21-sensors-25-05290" ref-type="bibr">21</xref>], and surveying and mapping [<xref rid="B22-sensors-25-05290" ref-type="bibr">22</xref>].</p><p>Traditional road segmentation methods primarily rely on image processing techniques and probabilistic graphical models with the core idea of segmenting regions based on low-level visual features of pixels. These methods can be mainly categorized into threshold- and edge-based approaches [<xref rid="B23-sensors-25-05290" ref-type="bibr">23</xref>], clustering- and graph theory-based methods [<xref rid="B24-sensors-25-05290" ref-type="bibr">24</xref>], as well as implementations such as random decision forests and contour detection [<xref rid="B25-sensors-25-05290" ref-type="bibr">25</xref>]. While these methods offer high computational efficiency, their generalization ability in complex scenes is limited, making them suitable primarily for relatively simple tasks today.</p><p>With the advancement of deep learning technology, convolutional neural networks, owing to their hierarchical feature extraction capabilities, have become the dominant architecture for road segmentation. Many leading semantic segmentation models have been adapted and applied to road image segmentation. For example, Zhang et al. [<xref rid="B26-sensors-25-05290" ref-type="bibr">26</xref>] introduced a boundary-constrained multi-scale segmentation method based on U-Net tailored for remote sensing images, focusing on enhancing image analysis for land use classification. Xiao et al. [<xref rid="B27-sensors-25-05290" ref-type="bibr">27</xref>] proposed a novel C-DeepLabV3+ algorithm that incorporates a Coordinate Attention module and a Cascade Feature Fusion module to improve road segmentation accuracy in UAV aerial images.</p><p>In recent years, with the introduction of the Transformer architecture, networks have been able to overcome the local receptive field limitations of CNNs through global modeling capabilities. Based on this, many researchers have incorporated attention mechanisms into segmentation models, significantly enhancing robustness in complex road scene segmentation. For example, Wu et al. [<xref rid="B28-sensors-25-05290" ref-type="bibr">28</xref>] proposed TC-Net, a lightweight Transformer&#8211;Convolutional network for real-time road segmentation that employs Transformer-Conv and PatchMerging-Conv modules to reduce parameters while maintaining accuracy on the KITTI dataset. Tao et al. [<xref rid="B29-sensors-25-05290" ref-type="bibr">29</xref>] introduced Seg-Road, a Transformer&#8211;CNN segmentation network for remote sensing road extraction that integrates a pixel connectivity structure to reduce fragmentation, achieving 67.2% IoU on DeepGlobe and 68.4% IoU on Massachusetts.</p><p>Deep learning-based road segmentation methods exhibit strong generalization capabilities, enabling extensive applications in scenarios such as aerial remote sensing and autonomous driving. However, for certain low-altitude scenarios where the overall image features vary complexly, fine-tuning networks to adapt to diverse conditions is time consuming and labor-intensive. Additionally, the substantial computational overhead of deep networks limits their deployment in real-time inference and edge computing environments. Therefore, there is a need to develop a road surface segmentation algorithm with low computational cost that facilitates deployment and transferability.</p></sec><sec id="sec2dot2-sensors-25-05290"><title>2.2. Few-Shot Semantic Segmentation</title><p>Traditional fully supervised semantic segmentation methods rely on large amounts of precisely annotated training data, with the annotation process being both time consuming and costly. Moreover, these models are typically limited to the categories present in the training set and struggle to generalize to new classes. Few-shot semantic segmentation has emerged as a novel research direction to address these challenges, enabling models to quickly adapt to and segment new categories using only a small number of annotated samples [<xref rid="B30-sensors-25-05290" ref-type="bibr">30</xref>].</p><p>Current mainstream approaches can be categorized into two major technical routes: prototype-based learning and affinity-based learning. Prototype-based methods compress support features into class prototypes via masked average pooling and then perform segmentation by comparing these prototypes with query features, offering computational efficiency but suffering from spatial information loss and insufficient context awareness [<xref rid="B31-sensors-25-05290" ref-type="bibr">31</xref>]. Affinity-based methods construct pixel-level feature matching directly, preserving fine-grained spatial relationships, but they are susceptible to background interference and may cause semantic ambiguity [<xref rid="B32-sensors-25-05290" ref-type="bibr">32</xref>]. To overcome these limitations, recent research has begun exploring hybrid architectures that combine the strengths of both approaches. For example, Zhang et al. [<xref rid="B33-sensors-25-05290" ref-type="bibr">33</xref>] proposed integrating Model-Agnostic Meta-Learning with SegNet and U-Net for the few-shot semantic segmentation of buildings and roads in remote sensing imagery.</p><p>Although deep learning-based few-shot segmentation methods perform well in data-scarce scenarios, their inherent model complexity may result in excessive computational resource consumption, leading to unnecessary efficiency loss in simple image segmentation tasks [<xref rid="B34-sensors-25-05290" ref-type="bibr">34</xref>]. HSV is a color model based on human visual perception, consisting of three dimensions: hue (H), saturation (S), and value (V). Roads in images often exhibit a regular color distribution, as shown in <xref rid="sensors-25-05290-f002" ref-type="fig">Figure 2</xref>; even across different scenes, their distribution in the HSV color space maintains a certain pattern. Based on this characteristic, road segmentation can be achieved at low cost by extracting the color distribution through back-projection.</p><p>Based on this characteristic, this paper innovatively proposes a color-learning mechanism that simultaneously considers both positive and negative samples, effectively extracting the color features of the road while eliminating environmental interference. Furthermore, addressing the specific features of road scenes, a mask growth algorithm is introduced to further optimize the segmentation results and enhance the algorithm&#8217;s performance. Finally, these improvements are integrated into a segmentation algorithm workflow. The algorithm boasts low floating-point computational complexity and ease of deployment, enabling rapid segmentation across different scenarios with minimal sample prompts and without the need for transfer learning.</p></sec></sec><sec id="sec3-sensors-25-05290"><title>3. Road Segmentation Based on Improved Back-Projection Algorithm</title><p>The traditional back-projection algorithm has poor generalization and low accuracy, making it difficult to apply in road segmentation tasks. In this section, a learning mechanism is proposed to improve the generalization ability of the algorithm by learning multiple positive and negative images. Based on the improved learning mechanism, an algorithmic framework for road surface segmentation through few-shot learning has been designed and proposed, as illustrated in <xref rid="sensors-25-05290-f003" ref-type="fig">Figure 3</xref>.</p><p>The algorithm consists of two modules: a back-projection module (BPM), which is responsible for feature extraction and learning, and a segmentation module (SM), which realizes image segmentation based on feature distribution. In the BPM, the segmented video is first sampled, containing positive samples that include the road area and negative samples that include the road environment. After sampling, the histograms of the color distributions of the two types of samples are fused to obtain the projection model. Then, all frames within the video are back-projected to obtain the target distribution probability information. The role of the SM is to use the feature distribution obtained from the BPM for road segmentation. First, the features are Gaussian convolved and binarized using an adaptive threshold to obtain a segmentation mask. Then, the mask undergoes N-times of region growth. Voids are filled to obtain the main body of the road through CCA. Finally, the mask is multiplied with the road image to obtain the segmentation result. The details of each part will be described in the following subsections.</p><sec id="sec3dot1-sensors-25-05290"><title>3.1. Histogram Learning Mechanism</title><p>In order to make the model close to the color characteristics of the segmented video, it is recommended that the samples be selected using a portion of the key frames in the video or a similar pavement image. Positive samples are selected from the image blocks of the road area, while negative samples are selected from the outside area. The colors of samples are converted from the RGB to HSV color space. Then, a 2D histogram is constructed with H and S values of the remainder. Given the total number of histogram bins in the H and S dimensions as M and N, respectively, the computation of the 2D histogram is achieved through the following equation:<disp-formula id="FD1-sensors-25-05290"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="normal">&#937;</mml:mi></mml:mrow></mml:munder><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>&#948;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>b</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>s</mml:mi></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>h</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>s</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="&#x230A;" close="&#x230B;"><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">M</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msub><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="&#x230A;" close="&#x230B;"><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">N</mml:mi></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula></p><p>In the above equation, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the value of the 2D histogram in the <italic toggle="yes">h</italic> interval of hue and <italic toggle="yes">s</italic> interval of saturation; <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the H and S values of the pixel located at coordinates <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> respectively; the notation <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mfenced open="&#x230A;" close="&#x230B;"><mml:mo>&#183;</mml:mo></mml:mfenced></mml:mrow></mml:math></inline-formula> indicates that the values within the brackets are rounded down to the nearest integer and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#937;</mml:mi></mml:mrow></mml:math></inline-formula> indicates the coordinate area of the sample.</p><p>After obtaining all the sample histograms, the next step involves learning about the color distribution among the samples. For positive samples <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the features are merged using intersection and union operations. The equation for the intersection operation is<disp-formula id="FD2-sensors-25-05290"><label>(2)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Intersection</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="normal">&#937;</mml:mi></mml:mrow></mml:msub><mml:mi>min</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>min</mml:mi><mml:mo>(</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The intersection operation preserves similar characteristics among positive samples, enabling the compression and refinement of feature information within the samples. Consequently, this process reduces the impact of debris or shadows on road segmentation across diverse samples. The equation for the union operation is<disp-formula id="FD3-sensors-25-05290"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Union</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="normal">&#937;</mml:mi></mml:mrow></mml:msub><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo>(</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The union operation can extend the features among samples, enhancing the generalization capability of the model. Additionally, this operation consolidates diverse road surface features, enabling the segmentation model to concurrently finish multiple segmentation tasks across varied scenarios.</p><p>For positive sample <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and negative sample <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the subtraction operation serves to adjust the features, eliminating those encompassing environmental color attributes within the samples. The equation for the subtraction operation is<disp-formula id="FD4-sensors-25-05290"><label>(4)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Subtraction</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="normal">&#937;</mml:mi></mml:mrow></mml:msub><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo>(</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The subtraction operation is able to remove the same color features from the road and the environment, making the segmentation result more accurate.</p><p>Through the above three types of operations, the general characteristics of the road can be learned from the samples of the same scene by intersection operations. Also, the model can learn from the samples from different scenes to realize segmentation in different types of roads by union operations. For the wrong results of the segmentation, the use of subtraction operations can be used for the removal of the interference color in the road, making the segmentation results more accurate.</p><p>In the BPM, its basic calculation process is shown in <xref rid="sensors-25-05290-f004" ref-type="fig">Figure 4</xref>. Firstly, some of the road images are selected as positive samples from the input video or similar road images. The features are merged using an intersection and union operation between the sample histograms. Then, a portion of the environment image is selected from the input video as negative samples. A subtraction operation is used on the positive samples that have been learned to finally obtain the projection model. After that, the project is modeled to the video to finally obtain the segmentation probability of the road.</p></sec><sec id="sec3dot2-sensors-25-05290"><title>3.2. Mask Generation and Growth</title><p>In BPM, the target probability distribution in discrete form is obtained. However, the regional probability distribution is needed to realize the segmentation of road; thus, the probability needs to be regionally weighted. In the application of the back-projection to the tracking task, the mean-shift algorithm is often used to extract the probability distribution of the region and find the target for tracking in the form of a sliding window [<xref rid="B35-sensors-25-05290" ref-type="bibr">35</xref>]. Similarly, in order to give higher weights to the pixels close to the center, this paper uses the Gaussian convolution method to replace the sliding window for regional probability extraction. After completing the convolution operation, a road map is found according to the mean and variance of the Gaussian distribution. The road map is calculated as<disp-formula id="FD5-sensors-25-05290"><label>(5)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Map</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8805;</mml:mo><mml:mi>&#956;</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>&#963;</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> represent the variance and mean of the distributional probabilities <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is a parameter controling the threshold range. Note that the larger it is, the more pixels will be retained, while the accuracy decreases.</p><p>However, it is difficult for the map obtained by global mean and variance to reflect local color features. Certain shadows, dust, and other conditions can cause local color distribution zones to shift, resulting in voids of mask. These voids can be resolved through mask growth. The basic idea of the mask growth is to calculate the data distribution in the region, using a sliding window and expanding the mask in areas near the neighborhood of the average value, which can be written as<disp-formula id="FD6-sensors-25-05290"><label>(6)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Map</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>if</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mover accent="true"><mml:mi mathvariant="normal">a</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>P</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>&#956;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#8804;</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#183;</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mi>for</mml:mi><mml:mspace width="4.pt"/><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mspace width="4.pt"/><mml:mi>in</mml:mi><mml:mspace width="4.pt"/><mml:mi>W</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mi>Map</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mover accent="true"><mml:mi>&#956;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mi>Map</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mi>Sum</mml:mi><mml:mo>(</mml:mo><mml:mi>Map</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mstyle scriptlevel="0" displaystyle="false"><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>Map</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>P</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>&#956;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle><mml:mrow><mml:mi>Sum</mml:mi><mml:mo>(</mml:mo><mml:mi>Map</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
<list list-type="simple"><list-item><p>In the above equation, <italic toggle="yes">W</italic> is a sliding window of size <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the coordinates of the window; <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>&#956;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the probability mean of the intersection of <italic toggle="yes">W</italic> and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> represents the variance. In the growth of the map, first, the mean and variance of the probability of retained pixels within the window are calculated. Then, the pixels not in the map whose probability is within <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> times the variance of the mean are retained, and their corresponding maps are filled. Finally, the whole process is repeated N times to expand the map.</p></list-item></list></p></sec><sec id="sec3dot3-sensors-25-05290"><title>3.3. Connected Component Reservation and Filling</title><p>Through the probabilistic projection, mask generation and growth, the basic segmentation masks of the road are obtained. However, due to the complex color environment, the masks tend to have the holes inside the road and some false segmentation results, as shown in <xref rid="sensors-25-05290-f005" ref-type="fig">Figure 5</xref>. In order to solve this problem, CCA is used to fill the holes and remove discrete regions. The forward scan mask is introduced to combine components of the road area of the image.</p><p>Through CCA, the connectivity regions can be obtained in the image. Meanwhile, the different connectivity regions are labeled with different labels. The road area mask can be obtained by counting each label and keeping the highest number one, which can be written as<disp-formula id="FD7-sensors-25-05290"><label>(7)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Mask</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mi>label</mml:mi><mml:mo>=</mml:mo><mml:mi>argmax</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>label</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4.pt"/><mml:mi>otherwise</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>label</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the number of labels counted by number and argmax denotes the label corresponding to the maximum value from <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>label</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The acquisition rule for the mask is to keep the pixels corresponding to the labels with the largest share in each image to obtain the segmentation of the road.</p><p>After obtaining the connected component, contour finding is performed on it, and the computational method proposed by Suzuki et al. [<xref rid="B36-sensors-25-05290" ref-type="bibr">36</xref>] is chosen here. The algorithm first performs a raster scan of the binary image on the input to find the border following starting points. Then after that, it performs boundary tracking to find the complete boundary. The boundary is filled to obtain the road segmentation mask without the voids through the boundary search.</p><p>Through the connected component reservation and filling, the image mask is finally obtained to realize the road image segmentation. In the SM, its basic algorithm processing flow is shown in <xref rid="sensors-25-05290-f006" ref-type="fig">Figure 6</xref>. Firstly, Gaussian convolution is performed on the probability distribution information obtained in BPM to extract the probability distribution features of the zones. Then, adaptive threshold and region growth are used to eliminate the influence of the brightness of the region on the segmentation accuracy. Finally, CCA is used to retain the maximum connectivity domain. Contours will be extracted and the voids within will be filled, which is followed by segmenting the image using the filled mask to achieve road segmentation.</p></sec><sec id="sec3dot4-sensors-25-05290"><title>3.4. Parameter Optimization Methods</title><p>In order to obtain the segmentation probability distribution, it is necessary to determine the HS histogram scale as well as the threshold coefficients for classification. Appropriate parameters can improve color differentiation in projection, improving the accuracy of segmentation. The value of parameters can be determined by calculating the segmentation effect indicator.</p><p>The scale of the histogram includes the number of M and N. To evaluate the performance of the algorithm, manual segmentation is used to acquire the binary mask and ground truth (GT) of the road. For the obtained segmentation probabilities, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the performance of BPM is evaluated by the distinction (DT) obtained by calculating the probability sum in the correct region, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">&#937;</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the background region, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">&#937;</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The DT can be calculated as<disp-formula id="FD8-sensors-25-05290"><label>(8)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>DT</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="normal">&#937;</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="normal">&#937;</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="normal">&#937;</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi mathvariant="normal">&#937;</mml:mi><mml:mi>GT</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>GT</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#937;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">&#937;</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">&#937;</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
<list list-type="simple"><list-item><p>It should be noted that in (<xref rid="FD8-sensors-25-05290" ref-type="disp-formula">8</xref>), the first part represents the recognition accuracy of the model, where the small probability or area of incorrect segmentation means a large value. The second term represents the recall of the probability, which is used to represent the model&#8217;s recognition of pavement areas.</p></list-item></list></p><p>In the SM, the threshold of segmentation is mainly determined by <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>, which is obtained based on the assumption that the overall probability conforms to a Gaussian distribution. The determination of <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> can be achieved by calculating intersection over union (IoU). The IoU value estimates the amount of overlap between the segmentation area and marker area (ground truth area), which can assess the correctness of a prediction as well as the rationality of <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>More precise results can be obtained by means of parameter tuning. The optimal parameters intervals are confirmed by calculating the indicator with different values, being suitable for most scenarios at the same time.</p></sec></sec><sec id="sec4-sensors-25-05290"><title>4. Experimental Results</title><p>The experimental data were collected using a DJI Mavic 3 drone. The collection sites included urban roads and suburban highways in Heilongjiang Province. Each video segment covered a distance of 1 km with a flight altitude of 20 m. Videos were sampled at equal time intervals, resulting in 20 images per scene category for validating the segmentation performance of the algorithm. For each scene, four pixel blocks were selected for environmental feature learning, and ground truth road surface regions were manually annotated to evaluate the algorithm&#8217;s segmentation accuracy. Both tested scenarios have the effect of environmental disturbances or shadows of water, as shown in <xref rid="sensors-25-05290-f007" ref-type="fig">Figure 7</xref>. The proposed algorithm has been tested in the acquired data. In each video, several parts of the road area from one or two frames are taken for sampling and model learning.</p><sec id="sec4dot1-sensors-25-05290"><title>4.1. Experimental Parameter Optimization</title><p>To enhance segmentation effectiveness, refined parameters were meticulously selected for distinct scenarios based on the methodology outlined in <xref rid="sec3dot4-sensors-25-05290" ref-type="sec">Section 3.4</xref>. As can be seen in <xref rid="sensors-25-05290-f002" ref-type="fig">Figure 2</xref>, the road color space has a greater differentiation on hue, showing a concentrated distribution around 0.15 and 0.6 in the selected scene while having more coverage domains on saturation. Taking this into account, more bins such as 20, 40, 80, 100, and 200 were chosen for the values of M, while 10, 20, 40, 80, and 100 were chosen for N. DT is calculated under the combinations of these values, and the results are shown in <xref rid="sensors-25-05290-f008" ref-type="fig">Figure 8</xref>a. At different values of (M, N), DT achieves a better performance without exhibiting significant variations as M increases at (100, 80) in the selected scene. Considering that fewer partitions can achieve better generalization and less computation, the recommended values of M and N in this paper are M = 100 and N = 80.</p><p>In order to select the suitable value of <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>, it is sampled at intervals of 0.05 from 0 to 1 in this paper, calculating IoU at different values. From <xref rid="sensors-25-05290-f008" ref-type="fig">Figure 8</xref>b, it can be seen that when the value of <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is between 0.6 and 0.9, the IoU is stable at about 0.95, which has good segmentation ability. When the value of <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is too small, indicating a high segmentation threshold, it results in a reduced area for road recognition. On the contrary, a low segmentation threshold may result in inaccurate recognition. The recommended value of <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> in this paper is 0.65.</p></sec><sec id="sec4dot2-sensors-25-05290"><title>4.2. Algorithm Testing Under Complex Environment</title><p>In the road scene segmentation task, there are many interferences, most of them come from environment and the texture of road itself. Therefore the algorithm must have anti-interference ability in these scenes. In <xref rid="sensors-25-05290-f009" ref-type="fig">Figure 9</xref>a, the first scene detected is roads with puddles, accompanied by shadows cast due to the presence of water on the road. Four areas of the road are selected in two frames to extract road features.</p><p>These samples are crossed through the intersection operation, which is accompanied with the union operation to merge the features to obtain the projection model. The obtained model is used to segment the rest of the frames in the video by the algorithm. The average IoU is 94.89%, and the average precision is 92.73% in 10 frames of different scenes from videos. In <xref rid="sensors-25-05290-f009" ref-type="fig">Figure 9</xref>b, the detection scene is roads with similar color to the environment. Three areas of the road and one area of the environment in one frame are selected, which are followed by the union operation to merge the features of the road. Finally, the subtraction operation is used to remove the same features in the road as in the environment. The average IoU and precision in the segmentation of the remaining frames is 94.89% and 94.73%.</p><p>In the experiment, more than 10 images not containing the same scene are extracted from each video. The ground truth values in these images are obtained through manual segmentation while removing areas obscured by trees in the road. The test results are shown in <xref rid="sensors-25-05290-t001" ref-type="table">Table 1</xref>. From the results, it can be seen that just by learning from the four samples of roads and environments, images can achieve good results in video segmentation, and such advantages can greatly reduce the deployment and training time in UAV road inspection and road area extraction, etc. Meanwhile, as shown in <xref rid="sensors-25-05290-f009" ref-type="fig">Figure 9</xref>a, some misidentified environmental regions appear along the road edges when negative samples are not selected. In <xref rid="sensors-25-05290-f009" ref-type="fig">Figure 9</xref>b, the introduction of the subtraction operation effectively mitigates the issue of background misidentification. Therefore, by covering as many features of the road color as possible, the accuracy of segmentation can be improved.</p></sec><sec id="sec4dot3-sensors-25-05290"><title>4.3. Comparative Experiments</title><p>Considering the application scenario of this algorithm under small sample learning, the traditional back-projection (BP) algorithm and automatic segmentation of the segment anything model (SAM) [<xref rid="B37-sensors-25-05290" ref-type="bibr">37</xref>] are tested. Roads with puddles are chosen for the test scenario.</p><p>The tested BP algorithm is a wrapper function within the opencv library. Road segmentation is realized only by convolution and thresholding; meanwhile, the samples are selected from the road mask of one frame in the video, as shown in <xref rid="sensors-25-05290-f009" ref-type="fig">Figure 9</xref>a. The results show that the probabilistic discretization through single template projection greatly affects the accuracy of segmentation with 21.39% average IoU and 21.43% average precision, as shown in <xref rid="sensors-25-05290-t002" ref-type="table">Table 2</xref>. Compared to our algorithm, the traditional BP method lacks a mechanism for learning the color distribution across multiple road regions, making it difficult to simultaneously extract color features from different parts of the road area and resulting in numerous voids in the detection outcomes. Additionally, the absence of convolutional steps restricts it to performing segmentation based solely on discrete data distribution rather than from an overall probabilistic perspective.</p><p>Since deep learning-based few-shot segmentation methods do not incorporate a negative sample learning mechanism, for the experiments involving SAM, we only used positive samples as prompts, which is consistent with our algorithm. SAM acquires scene semantic understanding through training on large-scale datasets, enabling few-shot segmentation with minimal prompts. The experimental results of SAM and SAM 2 on the same validation data are shown in <xref rid="sensors-25-05290-f010" ref-type="fig">Figure 10</xref>b. As illustrated, the scene understanding capability of SAM allows it to produce smoother edges. However, in road scenes with complex color textures, its performance is affected by road markings and water stains. Although this issue can be addressed through transfer learning, it requires substantial data and computational resources. While SAM 2 demonstrates certain advantages in overall segmentation metrics, achieving an IOU of 0.9692 and accuracy of 0.9813, its high computational demand limits its deployment on edge devices.</p><p>The algorithm was executed on an Intel(R) Core(TM) i5-10400F CPU using the MATLAB platform without involving GPU computation or complex environmental deployment. During operation, the algorithm only includes Gaussian convolution and morphological operations. At the same input resolution (1024 &#215; 1024), the FLOPs of SAM differ by an entire order of magnitude compared to the algorithm proposed in this paper. Meanwhile, the algorithm contains no learnable parameters and does not rely on GPU computation. In dataset testing, the average inference time per image was 55.4 ms. These design advantages enable the algorithm to be easily deployed on most edge computing devices.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05290"><title>5. Conclusions</title><p>Considering the difficulty of training and deploying deep learning networks in tasks of road area extraction and segmentation in low-altitude UAV viewpoints, a small-sample road video segmentation method is proposed based on few-shot learning. To address the problems regarding the traditional back-projection algorithms&#8217; poor generalization ability and inability to adequately represent the road color information, this paper proposes the feature learning mechanism to achieve the feature fusion and subtraction of environment color. The road is segmented by designing the algorithm. Experiments demonstrate that the algorithm is capable of realizing the task of segmenting road videos in different environments by just learning from several road area images. Also, the algorithm has a huge improvement over traditional back-projection.</p><p>The proposed algorithm can also be used for some segmentation tasks of the objects with distinctive color features, such as smoke, sky and leaves. Compared with the DL methods in different environments at high resolution, the proposed algorithm proves simpler to implement and remains unaffected by image texture. In summary, this paper expects to achieve efficient segmentation by extracting target features, and the obtained results can serve as a priori knowledge to direct UAVs toward areas of interest during road inspections.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.C. and H.C.; methodology, J.C.; software, J.C.; validation, Y.Y.; formal analysis, J.Y.; investigation, H.C.; resources, Z.D.; data curation, Y.Y.; writing&#8212;original draft preparation, J.C.; writing&#8212;review and editing, Y.Y.; visualization, J.C.; supervision, Z.D.; project administration, H.C.; funding acquisition, Y.Y. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05290"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Silva</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Leithardt</surname><given-names>V.R.Q.</given-names></name><name name-style="western"><surname>Batista</surname><given-names>V.F.L.</given-names></name><name name-style="western"><surname>Gonz&#225;lez</surname><given-names>G.V.</given-names></name><name name-style="western"><surname>Santana</surname><given-names>J.F.D.P.</given-names></name></person-group><article-title>Automated Road Damage Detection using UAV Images and Deep Learning Techniques</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>62918</fpage><lpage>62931</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3287770</pub-id></element-citation></ref><ref id="B2-sensors-25-05290"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nappo</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mavrouli</surname><given-names>O.</given-names></name><name name-style="western"><surname>Nex</surname><given-names>F.</given-names></name><name name-style="western"><surname>van Westen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gambillara</surname><given-names>R.</given-names></name><name name-style="western"><surname>Michetti</surname><given-names>A.M.</given-names></name></person-group><article-title>Use of UAV-based photogrammetry products for semi-automatic detection and classification of asphalt road damage in landslide-affected areas</article-title><source>Eng. Geol.</source><year>2021</year><volume>294</volume><fpage>106363</fpage><pub-id pub-id-type="doi">10.1016/j.enggeo.2021.106363</pub-id></element-citation></ref><ref id="B3-sensors-25-05290"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sebasco</surname><given-names>N.P.</given-names></name><name name-style="western"><surname>Sevil</surname><given-names>H.E.</given-names></name></person-group><article-title>Graph-Based Image Segmentation for Road Extraction from Post-Disaster Aerial Footage</article-title><source>Drones</source><year>2022</year><volume>6</volume><elocation-id>315</elocation-id><pub-id pub-id-type="doi">10.3390/drones6110315</pub-id></element-citation></ref><ref id="B4-sensors-25-05290"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mahmud</surname><given-names>M.N.</given-names></name><name name-style="western"><surname>Osman</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Ismail</surname><given-names>A.P.</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>K.A.</given-names></name><name name-style="western"><surname>Ibrahim</surname><given-names>A.</given-names></name></person-group><article-title>Road Image Segmentation using Unmanned Aerial Vehicle Images and DeepLab V3+ Semantic Segmentation Model</article-title><source>Proceedings of the 2021 11th IEEE International Conference on Control System, Computing and Engineering (ICCSCE)</source><conf-loc>Penang, Malaysia</conf-loc><conf-date>27&#8211;28 August 2021</conf-date><fpage>176</fpage><lpage>181</lpage></element-citation></ref><ref id="B5-sensors-25-05290"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ahmed</surname><given-names>M.I.</given-names></name><name name-style="western"><surname>Foysal</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chaity</surname><given-names>M.D.</given-names></name><name name-style="western"><surname>Hossain</surname><given-names>A.A.</given-names></name></person-group><article-title>DeepRoadNet: A deep residual based segmentation network for road map detection from remote aerial image</article-title><source>IET Image Process.</source><year>2023</year><volume>18</volume><fpage>265</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1049/ipr2.12948</pub-id></element-citation></ref><ref id="B6-sensors-25-05290"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>G.</given-names></name><name name-style="western"><surname>Long</surname><given-names>R.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>W.</given-names></name></person-group><article-title>SPNet: An RGB-D Sequence Progressive Network for Road Semantic Segmentation</article-title><source>Proceedings of the 2023 IEEE 25th International Workshop on Multimedia Signal Processing (MMSP)</source><conf-loc>Poitiers, France</conf-loc><conf-date>27&#8211;29 September 2023</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B7-sensors-25-05290"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dewangan</surname><given-names>D.K.</given-names></name><name name-style="western"><surname>Sahu</surname><given-names>S.P.</given-names></name><name name-style="western"><surname>Sairam</surname><given-names>B.</given-names></name><name name-style="western"><surname>Agrawal</surname><given-names>A.</given-names></name></person-group><article-title>VLDNet: Vision-based lane region detection network for intelligent vehicle system using semantic segmentation</article-title><source>Computing</source><year>2021</year><volume>103</volume><fpage>2867</fpage><lpage>2892</lpage><pub-id pub-id-type="doi">10.1007/s00607-021-00974-2</pub-id></element-citation></ref><ref id="B8-sensors-25-05290"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Q.</given-names></name></person-group><article-title>High-resolution SAR image road network extraction combining statistics and shape features</article-title><source>J. Wuhan Univ.</source><year>2013</year><volume>38</volume><fpage>1308</fpage><lpage>1312</lpage></element-citation></ref><ref id="B9-sensors-25-05290"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jia</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Kuang</surname><given-names>G.</given-names></name></person-group><article-title>Automatic road extraction from SAR imagery based on genetic algorithm</article-title><source>J. Image Graph.</source><year>2008</year><volume>6</volume><fpage>1134</fpage><lpage>1142</lpage></element-citation></ref><ref id="B10-sensors-25-05290"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Y.</given-names></name></person-group><article-title>Automatic road extraction from high resolution SAR images based on fuzzy connectedness</article-title><source>J. Comput. Appl.</source><year>2015</year><volume>35</volume><fpage>523</fpage></element-citation></ref><ref id="B11-sensors-25-05290"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Geng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Scherer</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wo&#378;niak</surname><given-names>M.</given-names></name></person-group><article-title>Review of road segmentation for SAR images</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>1011</elocation-id><pub-id pub-id-type="doi">10.3390/rs13051011</pub-id></element-citation></ref><ref id="B12-sensors-25-05290"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shelhamer</surname><given-names>E.</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully convolutional networks for semantic segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="B13-sensors-25-05290"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Henry</surname><given-names>C.</given-names></name><name name-style="western"><surname>Azimi</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Merkle</surname><given-names>N.</given-names></name></person-group><article-title>Road segmentation in SAR satellite images with deep fully convolutional neural networks</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2018</year><volume>15</volume><fpage>1867</fpage><lpage>1871</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2018.2864342</pub-id></element-citation></ref><ref id="B14-sensors-25-05290"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Quan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>Improved Deeplabv3 for better road segmentation in remote sensing images</article-title><source>Proceedings of the 2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)</source><conf-loc>Shanghai, China</conf-loc><conf-date>27&#8211;29 August 2021</conf-date><fpage>331</fpage><lpage>334</lpage></element-citation></ref><ref id="B15-sensors-25-05290"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Abderrahim</surname><given-names>N.Y.Q.</given-names></name><name name-style="western"><surname>Abderrahim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rida</surname><given-names>A.</given-names></name></person-group><article-title>Road segmentation using u-net architecture</article-title><source>Proceedings of the 2020 IEEE International Conference of Moroccan Geomatics (Morgeo)</source><conf-loc>Casablanca, Morocco</conf-loc><conf-date>11&#8211;13 May 2020</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B16-sensors-25-05290"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Rao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>S.</given-names></name></person-group><article-title>A Y-Net deep learning method for road segmentation using high-resolution visible remote sensing images</article-title><source>Remote Sens. Lett.</source><year>2019</year><volume>10</volume><fpage>381</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1080/2150704X.2018.1557791</pub-id></element-citation></ref><ref id="B17-sensors-25-05290"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yousri</surname><given-names>R.</given-names></name><name name-style="western"><surname>Elattar</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Darweesh</surname><given-names>M.S.</given-names></name></person-group><article-title>A Deep Learning-Based Benchmarking Framework for Lane Segmentation in the Complex and Dynamic Road Scenes</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>117565</fpage><lpage>117580</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3106377</pub-id></element-citation></ref><ref id="B18-sensors-25-05290"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Neupane</surname><given-names>B.</given-names></name><name name-style="western"><surname>Horanont</surname><given-names>T.</given-names></name><name name-style="western"><surname>Aryal</surname><given-names>J.</given-names></name></person-group><article-title>Deep learning-based semantic segmentation of urban features in satellite images: A review and meta-analysis</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>808</elocation-id><pub-id pub-id-type="doi">10.3390/rs13040808</pub-id></element-citation></ref><ref id="B19-sensors-25-05290"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>L.</given-names></name><name name-style="western"><surname>Creighton</surname><given-names>D.</given-names></name><name name-style="western"><surname>Nahavandi</surname><given-names>S.</given-names></name></person-group><article-title>On Detecting Road Regions in a Single UAV Image</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2017</year><volume>18</volume><fpage>1713</fpage><lpage>1722</lpage><pub-id pub-id-type="doi">10.1109/TITS.2016.2622280</pub-id></element-citation></ref><ref id="B20-sensors-25-05290"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lian</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Mustafa</surname><given-names>N.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name></person-group><article-title>Road Extraction Methods in High-Resolution Remote Sensing Images: A Comprehensive Review</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2020</year><volume>13</volume><fpage>5489</fpage><lpage>5507</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2020.3023549</pub-id></element-citation></ref><ref id="B21-sensors-25-05290"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chougula</surname><given-names>B.</given-names></name><name name-style="western"><surname>Tigadi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Manage</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kulkarni</surname><given-names>S.</given-names></name></person-group><article-title>Road segmentation for autonomous vehicle: A review</article-title><source>Proceedings of the 2020 3rd International Conference on Intelligent Sustainable Systems (ICISS)</source><conf-loc>Thoothukudi, India</conf-loc><conf-date>3&#8211;5 December 2020</conf-date><fpage>362</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1109/ICISS49785.2020.9316090</pub-id></element-citation></ref><ref id="B22-sensors-25-05290"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>C.</given-names></name></person-group><article-title>A Robust Ground Point Cloud Segmentation Algorithm Based on Region Growing in a Fan-shaped Grid Map</article-title><source>Proceedings of the 2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)</source><conf-loc>Jinghong, China</conf-loc><conf-date>5&#8211;9 December 2022</conf-date><fpage>1359</fpage><lpage>1364</lpage><pub-id pub-id-type="doi">10.1109/ROBIO55434.2022.10011824</pub-id></element-citation></ref><ref id="B23-sensors-25-05290"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>G.b.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>S.y.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>Y.x.</given-names></name></person-group><article-title>An Optimalizing Threshold Segmentation Algorithm for Road Images Based on Mathematical Morphology</article-title><source>Proceedings of the 2009 Third International Symposium on Intelligent Information Technology Application</source><conf-loc>Nanchang, China</conf-loc><conf-date>21&#8211;22 November 2009</conf-date><volume>Volume 2</volume><fpage>518</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1109/IITA.2009.310</pub-id></element-citation></ref><ref id="B24-sensors-25-05290"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mittal</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pandey</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Saraswat</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pal</surname><given-names>R.</given-names></name><name name-style="western"><surname>Modwel</surname><given-names>G.</given-names></name></person-group><article-title>A comprehensive survey of image segmentation: Clustering methods, performance parameters, and benchmark datasets</article-title><source>Multimed. Tools Appl.</source><year>2022</year><volume>81</volume><fpage>35001</fpage><lpage>35026</lpage><pub-id pub-id-type="doi">10.1007/s11042-021-10594-9</pub-id><pub-id pub-id-type="pmid">33584121</pub-id><pub-id pub-id-type="pmcid">PMC7870780</pub-id></element-citation></ref><ref id="B25-sensors-25-05290"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>L.</given-names></name></person-group><article-title>Research on road information extraction from high resolution imagery based on global precedence</article-title><source>Proceedings of the 2014 Third International Workshop on Earth Observation and Remote Sensing Applications (EORSA)</source><conf-loc>Changsha, China</conf-loc><conf-date>11&#8211;14 June 2014</conf-date><fpage>151</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1109/EORSA.2014.6927868</pub-id></element-citation></ref><ref id="B26-sensors-25-05290"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wijaya</surname><given-names>R.B.M.A.A.</given-names></name><name name-style="western"><surname>Wahyono</surname></name></person-group><article-title>Enhancing Road Segmentation in Satellite Images via Double U-Net with Advanced Pre-Processing</article-title><source>Proceedings of the 2024 Ninth International Conference on Informatics and Computing (ICIC)</source><conf-loc>Medan, Indonesia</conf-loc><conf-date>24&#8211;25 October 2024</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ICIC64337.2024.10957410</pub-id></element-citation></ref><ref id="B27-sensors-25-05290"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Min</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Y.</given-names></name></person-group><article-title>A Novel Network Framework on Simultaneous Road Segmentation and Vehicle Detection for UAV Aerial Traffic Images</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3606</elocation-id><pub-id pub-id-type="doi">10.3390/s24113606</pub-id><pub-id pub-id-type="pmid">38894397</pub-id><pub-id pub-id-type="pmcid">PMC11175345</pub-id></element-citation></ref><ref id="B28-sensors-25-05290"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Song</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>H.</given-names></name></person-group><article-title>TC-Net: Transformer-Convolutional Networks for Road Segmentation</article-title><source>Proceedings of the 2022 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>18&#8211;22 July 2022</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ICME52920.2022.9859734</pub-id></element-citation></ref><ref id="B29-sensors-25-05290"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ran</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shibasaki</surname><given-names>R.</given-names></name></person-group><article-title>Graph Encoding based Hybrid Vision Transformer for Automatic Road Network Extraction</article-title><source>Proceedings of the IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium</source><conf-loc>Pasadena, CA, USA</conf-loc><conf-date>16&#8211;21 July 2023</conf-date><fpage>3656</fpage><lpage>3658</lpage><pub-id pub-id-type="doi">10.1109/IGARSS52108.2023.10283247</pub-id></element-citation></ref><ref id="B30-sensors-25-05290"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ran</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Few-shot semantic segmentation: A review on recent approaches</article-title><source>Neural Comput. Appl.</source><year>2023</year><volume>35</volume><fpage>18251</fpage><lpage>18275</lpage><pub-id pub-id-type="doi">10.1007/s00521-023-08758-9</pub-id></element-citation></ref><ref id="B31-sensors-25-05290"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Diao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name></person-group><article-title>AgMTR: Agent mining transformer for few-shot segmentation in remote sensing</article-title><source>Int. J. Comput. Vis.</source><year>2025</year><volume>133</volume><fpage>1780</fpage><lpage>1807</lpage><pub-id pub-id-type="doi">10.1007/s11263-024-02252-y</pub-id></element-citation></ref><ref id="B32-sensors-25-05290"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>B.</given-names></name></person-group><article-title>Weakly Supervised Semantic Segmentation of Remote Sensing Images Using Siamese Affinity Network</article-title><source>Remote Sens.</source><year>2025</year><volume>17</volume><elocation-id>808</elocation-id><pub-id pub-id-type="doi">10.3390/rs17050808</pub-id></element-citation></ref><ref id="B33-sensors-25-05290"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Han</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>Few-Shot Semantic Segmentation for Building Detection and Road Extraction Based on Remote Sensing Imagery Using Model-Agnostic Meta-Learning</article-title><source>Advances in Guidance, Navigation and Control</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2022</year><fpage>1973</fpage><lpage>1983</lpage></element-citation></ref><ref id="B34-sensors-25-05290"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>W.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Han</surname><given-names>Q.L.</given-names></name></person-group><article-title>Visual Semantic Segmentation Based on Few/Zero-Shot Learning: An Overview</article-title><source>IEEE/CAA J. Autom. Sin.</source><year>2024</year><volume>11</volume><fpage>1106</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1109/JAS.2023.123207</pub-id></element-citation></ref><ref id="B35-sensors-25-05290"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>I.</given-names></name><name name-style="western"><surname>Farbiz</surname><given-names>F.</given-names></name></person-group><article-title>A back projection scheme for accurate mean shift based tracking</article-title><source>Proceedings of the 2010 IEEE International Conference on Image Processing, Image Processing (ICIP)</source><conf-loc>Hong Kong, China</conf-loc><conf-date>26&#8211;29 September 2010</conf-date><fpage>33</fpage><lpage>36</lpage></element-citation></ref><ref id="B36-sensors-25-05290"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suzuki</surname><given-names>S.</given-names></name><name name-style="western"><surname>be</surname><given-names>K.</given-names></name></person-group><article-title>Topological structural analysis of digitized binary images by border following</article-title><source>Comput. Vision Graph. Image Process.</source><year>1985</year><volume>30</volume><fpage>32</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/0734-189X(85)90016-7</pub-id></element-citation></ref><ref id="B37-sensors-25-05290"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mintun</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ravi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Rolland</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gustafson</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Whitehead</surname><given-names>S.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Lo</surname><given-names>W.Y.</given-names></name><etal/></person-group><article-title>Segment anything</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.02643</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05290-f001" orientation="portrait"><label>Figure 1</label><caption><p>Road scenes in different environments, where differences in the color characteristics of pavements make it difficult to achieve accurate segmentation with a common model. (<bold>a</bold>) Pavement with puddles after rain. (<bold>b</bold>) Gray pavement with potholes due to long service life. (<bold>c</bold>) Renovated asphalt pavement.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g001.jpg"/></fig><fig position="float" id="sensors-25-05290-f002" orientation="portrait"><label>Figure 2</label><caption><p>Distribution of pixels in three types of scenes; pavements are most distinguishable in the H and S color space. (<bold>a</bold>) Distribution of pixels in HSV color space. (<bold>b</bold>) Distribution of pixels in H and S space.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g002.jpg"/></fig><fig position="float" id="sensors-25-05290-f003" orientation="portrait"><label>Figure 3</label><caption><p>Algorithm flow of road projection segmentation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g003.jpg"/></fig><fig position="float" id="sensors-25-05290-f004" orientation="portrait"><label>Figure 4</label><caption><p>Process of the back-projection module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g004.jpg"/></fig><fig position="float" id="sensors-25-05290-f005" orientation="portrait"><label>Figure 5</label><caption><p>Connected component analysis label results.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g005.jpg"/></fig><fig position="float" id="sensors-25-05290-f006" orientation="portrait"><label>Figure 6</label><caption><p>Process of the segmentation module.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g006.jpg"/></fig><fig position="float" id="sensors-25-05290-f007" orientation="portrait"><label>Figure 7</label><caption><p>The tested videos collected in different scenarios. (<bold>a</bold>) Images taken over school roads with shadows of water. (<bold>b</bold>) Images taken over country roads with environmental disturbance.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g007.jpg"/></fig><fig position="float" id="sensors-25-05290-f008" orientation="portrait"><label>Figure 8</label><caption><p>Segmentation effect under different parameters. (<bold>a</bold>) Calculation of DT. (<bold>b</bold>) Calculation of IoU.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g008.jpg"/></fig><fig position="float" id="sensors-25-05290-f009" orientation="portrait"><label>Figure 9</label><caption><p>Segmentation effect of the algorithm in different scenarios. (<bold>a</bold>) Segmentation effect under wet road surface. (<bold>b</bold>) Calculation of IoU with different parameters.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g009.jpg"/></fig><fig position="float" id="sensors-25-05290-f010" orientation="portrait"><label>Figure 10</label><caption><p>Segmentation effect of different models. (<bold>a</bold>) Segmentation effect of the traditional BP algorithm. (<bold>b</bold>) Segmentation effect of the SAM.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05290-g010.jpg"/></fig><table-wrap position="float" id="sensors-25-05290-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05290-t001_Table 1</object-id><label>Table 1</label><caption><p>Segmentation results for different road scenes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Scenes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Minimum IoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Minimum Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average IoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average Precision</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Roads with puddles</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8437</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8633</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9273</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9489</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Roads with similar color</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9277</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.93022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9473</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9489</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05290-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05290-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of segmentation results between different models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Scenes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ave IoU</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ave Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPS</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters (M)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Proposed algorithm</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9273</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9489</td><td align="center" valign="middle" rowspan="1" colspan="1">146 M</td><td align="center" valign="middle" rowspan="1" colspan="1">0.008</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Traditional algorithm</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2139</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2143</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9685</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9809</td><td align="center" valign="middle" rowspan="1" colspan="1">746.4 G</td><td align="center" valign="middle" rowspan="1" colspan="1">93.7</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SAM2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9692</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9813</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">533.9 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.8</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>