<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430991</article-id><article-id pub-id-type="pmcid-ver">PMC12430991.1</article-id><article-id pub-id-type="pmcaid">12430991</article-id><article-id pub-id-type="pmcaiid">12430991</article-id><article-id pub-id-type="doi">10.3390/s25175323</article-id><article-id pub-id-type="publisher-id">sensors-25-05323</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Virtual Reality as a Stress Measurement Platform: Real-Time Behavioral Analysis with Minimal Hardware</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7501-7809</contrib-id><name name-style="western"><surname>Rah</surname><given-names initials="A">Audrey</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="c1-sensors-25-05323" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3218-1304</contrib-id><name name-style="western"><surname>Chen</surname><given-names initials="Y">Yuhua</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Vatankhah Barenji</surname><given-names initials="A">Ali</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Boboc</surname><given-names initials="RG">R&#259;zvan Gabriel</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05323">Department of Electrical and Computer Engineering, University of Houston, Houston, TX 77204, USA; <email>yuhuachen@uh.edu</email></aff><author-notes><corresp id="c1-sensors-25-05323"><label>*</label>Correspondence: <email>arahimi@uh.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>27</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5323</elocation-id><history><date date-type="received"><day>10</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>12</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>18</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>27</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05323.pdf"/><abstract><p>With the growing use of digital technologies and interactive games, there is rising interest in how people respond to challenges, stress, and decision-making in virtual environments. Studying human behavior in such settings helps to improve design, training, and user experience. Instead of relying on complex devices, Virtual Reality (VR) creates new ways to observe and understand these responses in a simple and engaging format. This study introduces a lightweight method for monitoring stress levels that uses VR as the primary sensing platform. Detection relies on behavioral signals from VR. A minimal sensor such as Galvanic Skin Response (GSR), which measures skin conductance as a sign of physiological body response, supports the Sensor-Assisted Unity Architecture. The proposed Sensor-Assisted Unity Architecture focuses on analyzing the user&#8217;s behavior inside the virtual environment along with physical sensory measurements. Most existing systems rely on physiological wearables, which add both cost and complexity. The Sensor-Assisted Unity Architecture shifts the focus to behavioral analysis in VR supplemented by minimal physiological input. Behavioral cues captured within the VR environment are analyzed in real time by an embedded processor, which then triggers simple physical feedback. Results show that combining VR behavioral data with a minimal sensor can improve detection in cases where behavioral or physiological signals alone may be insufficient. While this study does not quantitatively compare the Sensor-Assisted Unity Architecture to multi-sensor setups, it highlights VR as the main platform, with sensor input offering targeted enhancements without significantly increasing system complexity.</p></abstract><kwd-group><kwd>virtual reality</kwd><kwd>stress detection</kwd><kwd>behavioral analysis</kwd><kwd>physiological sensors</kwd><kwd>GSR</kwd><kwd>low-cost sensors</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05323"><title>1. Introduction</title><p>Real-time stress detection can enhance training and safety in high-pressure environments by providing timely feedback and performance monitoring [<xref rid="B1-sensors-25-05323" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05323" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05323" ref-type="bibr">3</xref>].</p><p>Two primary modalities are commonly used: behavioral and physiological data. Behavioral signals arise from a user&#8217;s interactions within Virtual Reality (VR) environments and may manifest as hesitation, trembling, or repeated task failure [<xref rid="B4-sensors-25-05323" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05323" ref-type="bibr">5</xref>]. By contrast, physiological signals are captured with wearable sensors such as skin-conductance electrodes or heart rate monitors that track autonomic changes in sweat activity or cardiovascular functions. Previous studies have demonstrated that autonomic arousal can be inferred from skin conductance alone, supporting closed-loop applications without requiring complex sensor arrays [<xref rid="B6-sensors-25-05323" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05323" ref-type="bibr">7</xref>]. Physiological data such as heart rate and sweating can provide helpful information, but may not always be clear, as changes may also result from excitement, caffeine intake, exercise, or hot weather [<xref rid="B8-sensors-25-05323" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05323" ref-type="bibr">9</xref>].</p><p>In VR, the environment is fully programmable, allowing researchers to introduce well-defined high-stress conditions such as flashing alarms, countdown timers, and sensory overload with precise timing. Because the exact moment a stressor is introduced is known, behavioral responses can be interpreted with confidence [<xref rid="B9-sensors-25-05323" ref-type="bibr">9</xref>]. If a user hesitates, commits repeated errors, or shows physical signs such as hand tremors immediately after a stressor, these become strong behavioral indicators of cognitive load. The behavioral response is tied to a controlled VR trigger. When physiological sensors are used alongside VR, their readings gain contextual meaning. If skin conductance and heart rate signals rise as behavioral markers appear during a known stressor, then it is possible to confidently conclude that a real stress response occurred [<xref rid="B7-sensors-25-05323" ref-type="bibr">7</xref>]. Conversely, when behavioral signs are weak or absent but internal physiological changes are detected by sensors, this can reveal hidden stress that might otherwise be missed.</p><p>No prior system has combined precise VR-triggered behavioral monitoring with on-demand physiological sensing in a single low-latency framework. Therefore, in this paper, we propose the design of a Sensor-Assisted Unity Architecture that uses standard VR headsets and controllers to monitor natural reactions such as task failure, delay, or non-responsiveness. Our proposed Sensor-Assisted Unity Architecture incorporates a minimal, low cost wearable sensor to improve accuracy. This architecture maintains system simplicity while enhancing precision. A combined approach is particularly well-suited for environments that demand rapid feedback, precise tracking, and scalable tools. A system that mostly uses VR behavior with few sensors provides a useful and reliable way to measure stress. The main contributions of this paper are as follows:<list list-type="bullet"><list-item><p>A Sensor-Assisted Unity Architecture for real-time stress detection without bulky wearables.</p></list-item><list-item><p>A decision-level Sensor-Assisted Unity Architecture algorithm that invokes a single low-cost Galvanic Skin Response (GSR) sensor.</p></list-item><list-item><p>An end-to-end pipeline achieving sub-120 ms latency.</p></list-item></list></p><p>The structure of this paper is as follows: <xref rid="sec1-sensors-25-05323" ref-type="sec">Section 1</xref> introduces the motivation and objectives of the study; <xref rid="sec2-sensors-25-05323" ref-type="sec">Section 2</xref> reviews the research background on physiological and behavioral sensing in VR, including limitations of wearable-based systems and advances in lightweight behavioral modeling; <xref rid="sec3-sensors-25-05323" ref-type="sec">Section 3</xref> presents the proposed Sensor-Assisted Unity Architecture, outlining its main components and data flow; <xref rid="sec4-sensors-25-05323" ref-type="sec">Section 4</xref> describes the implementation, covering behavioral feature extraction, logic, and real-time feedback mechanisms; <xref rid="sec5-sensors-25-05323" ref-type="sec">Section 5</xref> presents the experimental setup and results, including detection accuracy, latency measurements, and architectural comparisons across various stress scenarios; finally, <xref rid="sec6-sensors-25-05323" ref-type="sec">Section 6</xref> reflects on the findings and offers concluding remarks.</p><p>While the primary focus of this study is behavioral analysis in VR, we acknowledge the importance of validating stress states against established references. In this work, stress conditions were validated through synchronization with controlled VR triggers and confirmed by GSR fluctuations. To support future extensions, we suggest incorporating validated psychological scales such as the State&#8211;Trait Anxiety Inventory (STAI) [<xref rid="B10-sensors-25-05323" ref-type="bibr">10</xref>] or the Perceived Stress Scale (PSS) [<xref rid="B11-sensors-25-05323" ref-type="bibr">11</xref>] along with physiological baselines including Heart Rate Variability (HRV) [<xref rid="B12-sensors-25-05323" ref-type="bibr">12</xref>] and cortisol sampling [<xref rid="B13-sensors-25-05323" ref-type="bibr">13</xref>] as independent ground truths for more rigorous confirmation of stress. This limitation is further discussed in <xref rid="sec5-sensors-25-05323" ref-type="sec">Section 5</xref>. In addition, the Sensor-Assisted Unity Architecture was evaluated using the Wearable Stress and Affect Detection (WESAD) dataset, demonstrating its ability to generalize beyond VR-generated data. However, the primary validation in this study remains based on controlled VR triggers, with stress confirmed through GSR fluctuations. The module used was the Grove GSR sensor (Model 101020052; Seed Studio, Shenzhen, China).</p></sec><sec id="sec2-sensors-25-05323"><title>2. Research Background</title><sec id="sec2dot1-sensors-25-05323"><title>2.1. Physiological Sensing in Virtual Environments</title><p>VR is widely adopted for simulation and high-risk training tasks. Accurate monitoring of cognitive stress and mental workload is essential in these scenarios. Existing VR monitoring systems often rely on physiological signals such as Electroencephalography (EEG), heart rate (HR), and oxygen saturation, which show strong correlations with self-reported stress levels [<xref rid="B14-sensors-25-05323" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>]. Wickramasuriya et al. [<xref rid="B7-sensors-25-05323" ref-type="bibr">7</xref>] introduced a deconvolution-based method to extract neural impulses from skin conductance signals, enabling real-time sympathetic arousal estimation in closed-loop feedback systems. Their results showed that changes in skin conductivity due to perspiration are reliable and low-cost indicators of stress and fatigue during cognitive tasks. Using standard VR hardware such as headsets and hand controllers, it is possible to capture behavioral signals such as reaction time, mistakes, and unusual movements without requiring invasive body sensors [<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>]. VR environments can reproduce high-stakes tasks, time pressure, and sensory overloads that induce measurable cognitive and emotional responses in users [<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>], making VR a useful tool for examining stress responses in scenarios that are difficult or unsafe to reproduce in real life.</p></sec><sec id="sec2dot2-sensors-25-05323"><title>2.2. Limitations of Wearable-Based Systems</title><p>Despite their usefulness, wearable devices can introduce discomfort and may interfere with natural motion. They often require frequent calibration, are sensitive to placement errors, and may limit user immersion during training. These limitations have motivated researchers to explore alternatives that are less invasive and more adaptable. Wearable sensors such as EEG caps and HR monitors have been extensively used for stress detection in both laboratory and field settings [<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>,<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. While these sensors provide valuable physiological data, their integration in VR environments often introduces practical challenges; for example, EEG systems can be sensitive to electrical noise from VR headsets, and motion artifacts can degrade signal quality during active tasks [<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. Signal accuracy and maintaining proper contact can also be considerations in dynamic VR tasks [<xref rid="B19-sensors-25-05323" ref-type="bibr">19</xref>]. Several studies have highlighted the tradeoffs between sensor accuracy and user comfort in immersive environments. It is reported that EEG-based systems often require users to remain still for optimal data collection, which may not be feasible in dynamic VR training scenarios [<xref rid="B14-sensors-25-05323" ref-type="bibr">14</xref>]. Physiological signals can be influenced by external factors such as temperature, humidity, and user-specific characteristics such as skin type or hydration levels [<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. These factors introduce variability and complicate data interpretation; moreover, wearable sensors can increase the complexity and cost of VR training systems. Maintaining proper sensor calibration, ensuring reliable data transmission, and preventing hardware interference can also impose additional burdens on system designers and users [<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. In time-sensitive or high-pressure training scenarios such as emergency response simulations, these issues can limit the feasibility of physiological sensing approaches. Therefore, there is growing interest in alternative methods that rely on behavioral signals and embedded system integration to provide practical, scalable, and user-friendly stress detection solutions in VR environments [<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>].</p></sec><sec id="sec2dot3-sensors-25-05323"><title>2.3. Shift Towards Behavioral Indicators</title><p>Behavioral signals such as reaction time, task completion accuracy, and motion irregularities have emerged as viable alternatives. These indicators can be passively captured using standard VR headsets and controllers. Unlike physiological sensors, behavioral data acquisition does not require specialized hardware. This enables lightweight, scalable, and cost-effective systems suitable for long-duration sessions. Recent research has demonstrated the value of behavioral signals as indirect but reliable proxies for cognitive load and stress. Task repetition, hesitation, and delays in VR simulations can accurately predict user stress levels, aligning well with self-reported data [<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>]. Motion irregularities, such as hand tremors and erratic movements are common in high-stress scenarios. These can be systematically analyzed for real-time feedback [<xref rid="B20-sensors-25-05323" ref-type="bibr">20</xref>]. Integrating behavioral metrics into VR applications is feasible as well. These data streams can be captured continuously without interfering with immersion or comfort [<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>,<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. Moreover, behavioral indicators can provide a holistic understanding of user states by combining multiple metrics, such as response latency, error frequency, and path deviations [<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. This multifaceted approach enables the development of robust and adaptive feedback systems that adjust task difficulty or trigger warnings based on detected stress patterns [<xref rid="B14-sensors-25-05323" ref-type="bibr">14</xref>,<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>]. Recent studies in stress detection have relied heavily on physiological measurements such as heart rate variability, electrodermal activity, and EEG signals. While these methods offer high-resolution insight into internal states, they often require complex, costly, or intrusive equipment. Although physiological signals offer granular insights into biological processes, behavioral indicators present a practical solution for many training scenarios, especially when ease of use, cost, and long-term wearability are critical factors [<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>]. Therefore, the use of behavioral data has become an attractive modality for stress monitoring in VR, supporting the design of scalable systems that promote user engagement while minimizing hardware complexity.</p></sec><sec id="sec2dot4-sensors-25-05323"><title>2.4. Enhancing Behavioral Models with Machine Learning</title><p>Recent advancements have integrated behavioral analysis with Machine Learning (ML) techniques to improve stress detection accuracy in virtual environments. ML models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and decision trees have shown strong performance in identifying patterns of cognitive load, stress, and fatigue [<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>,<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. CNNs have been successfully applied to classify stress levels based on motion trajectories and task errors in VR simulations, achieving over 85% accuracy [<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. One key advantage of ML-based models is their ability to adapt across users, identifying subtle differences in behavior that traditional threshold-based systems may overlook. These advancements support the development of scalable, user-centered VR training systems capable of real-time adaptation and continuous learning based on behavioral feedback [<xref rid="B14-sensors-25-05323" ref-type="bibr">14</xref>,<xref rid="B19-sensors-25-05323" ref-type="bibr">19</xref>]. As ML models continue to evolve, their integration into VR-based behavioral monitoring systems holds significant promise for improving safety, engagement, and personalized user experiences in high-stakes training environments.</p></sec><sec id="sec2dot5-sensors-25-05323"><title>2.5. Ethical Considerations and Data Privacy</title><p>The increasing use of behavioral monitoring in VR and haptic systems introduces unique challenges for protecting sensitive user information [<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. Behavioral signals such as reaction patterns, hand tremors, and task errors can reveal intimate details about a user&#8217;s cognitive state, emotional responses, and even medical conditions [<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>,<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. Ethical frameworks emphasize the importance of informed consent, transparency in data usage, and clear communication of risks and benefits to users [<xref rid="B19-sensors-25-05323" ref-type="bibr">19</xref>]. To address these concerns, researchers are exploring decentralized data processing methods that enable real-time analysis without transmitting raw sensor data to external servers. Techniques such as on-device processing, federated learning, and differential privacy can reduce the risk of data breaches while maintaining system functionality [<xref rid="B14-sensors-25-05323" ref-type="bibr">14</xref>,<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. Zhang et al. demonstrated a VR stress detection system that processed behavioral signals locally on an Arduino microcontroller, eliminating the need for cloud-based storage [<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. In addition to technical solutions, ethical design principles encourage the minimization of data collection, secure storage practices, and anonymization techniques to protect user identities [<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>,<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. As VR systems become more immersive and data-rich, incorporating robust privacy safeguards will be essential for maintaining user trust and enabling widespread adoption across clinical, educational, and industrial settings.</p></sec><sec id="sec2dot6-sensors-25-05323"><title>2.6. Broader Applications and Haptic Interfaces</title><p>Behavior-driven monitoring systems have been applied beyond gaming to areas such as rehabilitation, surgical training, and robotics control [<xref rid="B21-sensors-25-05323" ref-type="bibr">21</xref>]. Haptic feedback plays a critical role in these systems by enhancing realism and reinforcing behavioral cues [<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. For example, vibration patterns or resistive forces can support motor learning, procedural accuracy, and situational awareness in high-pressure scenarios [<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>]. Haptic interfaces also enable adaptive feedback control such as increasing vibration intensity when stress is detected or adjusting resistance based on user fatigue [<xref rid="B14-sensors-25-05323" ref-type="bibr">14</xref>]. This multimodal approach enhances user engagement, safety, and performance across diverse VR applications, including medical simulations, emergency training, and collaborative robotics [<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>]. As VR technologies advance, integrating behavior-driven monitoring with haptic feedback holds significant potential for creating immersive, responsive, and user-centered training systems.</p></sec><sec id="sec2dot7-sensors-25-05323"><title>2.7. Towards Sensor-Assisted Unity Architectures and Minimal Systems</title><p>Current trends favor architectures that combine behavioral cues with simple physical sensors such as sweat patches. These systems balance detection accuracy and system complexity while improving usability and cost-effectiveness. This shift supports the broader goal of developing lightweight and user-friendly stress monitoring tools suitable for diverse VR applications. The proposed Sensor-Assisted Unity Architecture integrates basic sensors such as GSR, electrodes or temperature sensors with VR-based behavioral analysis to enhance detection accuracy while keeping system overhead low [<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>,<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. Kirschbaum and Hellhammer have demonstrated that combining skin conductance data with task error metrics improves sensitivity to stress variations compared to using either method alone. Minimal GSR integration has also been shown to enable adaptive feedback systems that adjust VR task difficulty based on real-time user states [<xref rid="B19-sensors-25-05323" ref-type="bibr">19</xref>]. Sensor-Assisted Unity Architectures offer a practical way to develop scalable stress detection frameworks that work effectively. Such architectures can adapt to a variety of VR scenarios without requiring complex hardware or advanced technical expertise [<xref rid="B14-sensors-25-05323" ref-type="bibr">14</xref>,<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. In contrast to existing systems, our work introduces a VR-centered framework in which behavioral cues serve as the primary input. Minimal sensor data are used only when necessary, making our Sensor-Assisted Unity Architecture lighter, more scalable, and better suited for real-time stress detection with minimal hardware. By using VR as the main platform for behavioral analysis and relying on sensors, these two sources of information are combined in a complementary and efficient way. This setup improves the system&#8217;s accuracy, reliability, and overall performance.</p></sec></sec><sec id="sec3-sensors-25-05323"><title>3. System Architecture</title><sec id="sec3dot1-sensors-25-05323"><title>3.1. System Overview</title><p>This section introduces the design of our real-time stress detection called the Sensor-Assisted Unity Architecture. The system implements the Sensor-Assisted Unity Architecture algorithm, which integrates behavioral monitoring in a virtual reality environment with input from lightweight physiological sensors. The goal is to maintain a clear and consistent structure without switching between different system setups.</p><p><xref rid="sensors-25-05323-f001" ref-type="fig">Figure 1</xref> illustrates the key components of the proposed VR-based stress detection system. The system operates in a Unity-based VR environment, running the Sensor-Assisted Unity Architecture algorithm. Behavioral indicators, such as hesitation and tremor intensity, are always monitored as users interact with the virtual world. In addition, physiological sensing provides further information about user stress when available. The algorithm integrates all available information and delivers real-time feedback based on detected stress.</p><p>As shown in <xref rid="sensors-25-05323-f001" ref-type="fig">Figure 1</xref>, the main components of the system are:<list list-type="bullet"><list-item><p>Behavioral Indicators: Monitors hesitation and tremor intensity during VR tasks.</p></list-item><list-item><p>Physiological Sensing: Collects physiological signals to supplement behavioral data.</p></list-item><list-item><p>Data: Integrates sensor data and behavioral indicators for robust detection using the Sensor-Assisted Unity Architecture algorithm.</p></list-item><list-item><p>VR Environment: The immersive Unity space in which all interactions and monitoring occur.</p></list-item><list-item><p>Controlled Setup: The system can be configured for VR-only or sensor-assisted experiments.</p></list-item><list-item><p>Feedback Mechanisms: Provides real-time feedback to users based on detected stress.</p></list-item></list></p></sec><sec id="sec3dot2-sensors-25-05323"><title>3.2. Positioning This Work and What Is New</title><p>Many past stress detection systems have used wearables such a EEG or heart rate sensors as their main source of data [<xref rid="B14-sensors-25-05323" ref-type="bibr">14</xref>,<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. These systems often require multiple sensors, special setup, and complex software, which can limit their use in real-time VR applications. In our work, instead of starting from sensors, we begin with VR behavior; the VR headset and controllers capture signs such as hesitations or tremors, along with a small GSR sensor. This &#8220;behavior-first&#8221; approach is implemented within the proposed Sensor-Assisted Unity Architecture algorithm, making the system simple, fast, and low-cost. Most systems with both VR and sensors use complex machine learning models that require training on large datasets [<xref rid="B17-sensors-25-05323" ref-type="bibr">17</xref>,<xref rid="B19-sensors-25-05323" ref-type="bibr">19</xref>]. In contrast, we use a clear, rule-based Sensor-Assisted Unity Architecture algorithm that works in real time and provides feedback in under 120 ms. This makes it better suited for fast training scenarios.</p><p>What is new in our system:<list list-type="bullet"><list-item><p>Behavior-First Design: GSR is optional and is only used when behavioral signs are weak, as defined in the Sensor-Assisted Unity Architecture algorithm.</p></list-item><list-item><p>Low Hardware Needs: Works with just standard VR and a single GSR sensor.</p></list-item><list-item><p>Fast and Simple: Real-time response under 120 ms without machine learning using the rule-based Sensor-Assisted Unity Architecture algorithm.</p></list-item><list-item><p>Easy to Understand: Uses a Sensor-Assisted Unity Architecture decision process instead of a black-box algorithm.</p></list-item><list-item><p>Runs on any VR Platform: Works on both PC-VR and standalone devices.</p></list-item></list></p><p>The proposed system shows that real-time stress detection can be accomplished using mostly VR behavior and minimal sensors through the Sensor-Assisted Unity Architecture algorithm, offering a simpler and more scalable option than most prior systems.</p></sec><sec id="sec3dot3-sensors-25-05323"><title>3.3. System Architecture</title><p><xref rid="sensors-25-05323-f002" ref-type="fig">Figure 2</xref> shows the system architecture of the proposed Sensor-Assisted Unity Architecture, which is implemented through the Sensor-Assisted Unity Architecture algorithm. Physiological signals are collected through external sensors and combined with behavioral data from the Unity environment. A communication interface manages data exchange between Unity and an external analysis module, where the algorithm fuses the inputs and triggers real-time feedback when stress is detected.</p><p>The system includes the following components:<list list-type="bullet"><list-item><p>Unity VR Environment: This provides a virtual space where users perform tasks; it monitors user behavior and interaction patterns to identify signs of stress, such as hesitation, tremor, or task errors.</p></list-item><list-item><p>Sensor Input Module: Collects physiological data related to stress; these sensors connect to a small embedded processor and support behavioral monitoring by providing complementary information.</p></list-item><list-item><p>Communication Interface: Manages the connection and data exchange between Unity and the external processor, enabling further analysis without overloading the VR system.</p></list-item><list-item><p>Sensor-Assisted Unity Architecture Algorithm: Combines behavioral and physiological signals using a decision-making process to detect stress based on deviations from expected user response patterns.</p></list-item><list-item><p>Feedback Module: Delivers real-time feedback in the VR environment when stress is detected, including changes in visuals, sounds, or controller responses to alert or guide the user.</p></list-item></list></p><p>This unified architecture ensures that the Sensor-Assisted Unity Architecture algorithm operates consistently across both high-end and portable VR platforms, maintaining balanced contributions from behavioral and physiological components while delivering reliable performance during testing.</p></sec></sec><sec id="sec4-sensors-25-05323"><title>4. Implementation</title><p>This section describes how the proposed Sensor-Assisted Unity Architecture was implemented. The virtual environment and system logic were developed using Unity (version 2021.3.45f1; Unity Technologies, San Francisco, CA, USA) and designed to work in real time, allowing detection of stress during VR interaction based on user behavior and sensor input.</p><sec id="sec4dot1-sensors-25-05323"><title>4.1. Behavioral Signal Acquisition and Processing</title><p>We first describe how behavioral signals are acquired and processed in the VR system. The system observes behavioral patterns such as hesitation, tremors, and inactivity directly through the Unity engine. These indicators are monitored in real time during task execution inside the VR environment. When stress cues are detected, the system triggers feedback through vibration, sound, or visual changes, completing the behavioral feedback loop within the VR-based stress detection system.</p><p><xref rid="sensors-25-05323-f003" ref-type="fig">Figure 3</xref> shows that behavioral data are collected through standard VR input devices. These behavioral features are analyzed in Unity, where a threshold-based logic detects signs of stress. If stress is detected, the system triggers real-time feedback.</p><p><xref rid="sensors-25-05323-f004" ref-type="fig">Figure 4</xref> shows the stress detection pipeline within the VR system. The process begins with VR headsets capturing the user&#8217;s actions during task execution. Unity then extracts behavioral metrics such as hesitations or tremors from this input. These metrics are analyzed by a feedback processing module to detect potential stress. If stress is identified, immediate cues are delivered through visual, auditory, or haptic feedback to assist the user in real time.</p></sec><sec id="sec4dot2-sensors-25-05323"><title>4.2. Sensor-Assisted Unity Architecture Algorithm Design</title><p>The model combines behavioral cues from VR with physiological input from sensors to improve detection confidence.</p><p><xref rid="sensors-25-05323-f005" ref-type="fig">Figure 5</xref> illustrates how combining behavioral and physiological data improves stress detection. On the left, a rule-based model uses behavioral cues alone for fast and easy interpretation; on the right, a Sensor-Assisted Unity Architecture algorithm combines VR behavior with GSR input for a more balanced multimodal analysis. By using both sources together, the proposed system applies Sensor-Assisted Unity Architecture thresholds, increasing accuracy and helping to confirm cases where behavioral signals alone may be unclear. This strategy supports more reliable decision-making, especially in cases where one signal alone may not be enough. By combining both behavioral and physiological inputs, the system can adapt to different stress patterns and reduce uncertainty. The next step is to define how these inputs interact through a clear set of rules.</p><p><xref rid="sensors-25-05323-f006" ref-type="fig">Figure 6</xref> explains how the system decides whether or not stress is present. If both behavioral and sensor signals indicate stress, the system immediately triggers feedback. If either of them is strong, whether behavioral or GSR, the system continues to respond; however, if both signals are weak or uncertain, the system holds back to avoid false alarms. This ensures that feedback is only provided when there is reliable evidence of stress.</p><p>The system looks at both behavioral and physiological signals to decide whether stress is present. It checks how strong and clear each signal is. If one of the signals clearly shows stress, the system provides feedback such as a sound or vibration. If both signals are weak or unclear, the system might stay silent to avoid false alarms. This setup is designed to make reliable decisions in real time, but could also be changed in the future.</p><p>The core of this architecture is the Sensor-Assisted Unity Architecture algorithm, which combines both data streams and evaluates their strength and consistency. When stress is detected from either or both sources, the system generates immediate feedback through visual, auditory, or haptic channels. A feedback connection is also utilized to inform the physiological module, enabling adaptive analysis and continuous validation.</p><p><xref rid="sensors-25-05323-f007" ref-type="fig">Figure 7</xref> provides an overview of how behavioral and physiological signals are combined to improve stress detection in VR. The system integrates user behaviors such as reaction time, tremors, or task performance with physiological input from a GSR sensor worn on the skin. When used separately, either signal can lead to inaccurate results or false alarms. By fusing both sources of information through the Sensor-Assisted Unity Architecture algorithm, the system is able to reduce false positives and improve the accuracy of stress detection. The goal is to support real-time decisions that are both timely and meaningful, helping users to stay aware of their stress levels during VR experiences. The figure emphasizes this transition from high false positive rates towards more accurate and reliable feedback.</p><p>This modular Sensor-Assisted Unity Architecture design supports reliable and responsive stress detection while remaining flexible for future expansion. Each layer of the system, from input to feedback, is structured to allow new sensor modalities, more advanced techniques, and adaptive feedback strategies in future implementations.</p><p><xref rid="sensors-25-05323-f008" ref-type="fig">Figure 8</xref> illustrates the complete stress detection pipeline within the Sensor-Assisted Unity Architecture. The process begins with VR input devices such as headsets and controllers, which capture the user&#8217;s physical actions and interactions. These inputs are used to detect behavioral signals, including tremors, hesitations, or task execution errors. These behavioral data are passed to the Unity-based processing layer for real-time analysis. This core layer also accepts physiological signals such as GSR data.</p><p>By integrating data collection, decision-making, and feedback into one unified Sensor-Assisted Unity Architecture environment, the proposed system maintains high responsiveness and adaptability without increasing complexity.</p></sec><sec id="sec4dot3-sensors-25-05323"><title>4.3. System Integration and Feedback Mechanism</title><p>The Sensor-Assisted Unity Architecture is designed to operate in real time. Behavioral data are captured directly from standard VR input devices such as headsets and controllers. These data include user actions such as reaction delays, tremors, or task errors. The Unity engine processes these behavioral signals internally, ensuring fast and consistent detection without additional hardware dependencies. When physiological inputs such as GSR signals are available, they are integrated into the decision logic to support stronger or more uncertain cases. This flexible combination of behavioral and physiological data enhances the system&#8217;s reliability across varied user responses.</p><p>Once stress is detected, the system responds immediately with feedback to assist the user. Unity delivers this feedback through multimodal outputs such as controller vibration, visual flashes, or audio alerts. The modular nature of the Sensor-Assisted Unity Architecture allows the system to be deployed across different VR platforms and is readily expandable. By integrating data collection, decision-making, and feedback in one unified environment, the system maintains high responsiveness and adaptability without increased complexity. It allows the system to be deployed across different VR platforms and expanded in future versions to include additional sensors, more advanced logic, or personalized feedback strategies.</p></sec><sec id="sec4dot4-sensors-25-05323"><title>4.4. Algorithm Implementation</title><p>The Sensor-Assisted Unity Architecture integrates behavioral features extracted from VR with physiological signals from a GSR sensor to improve stress detection robustness.</p><p>Inputs:<list list-type="bullet"><list-item><p>Four binary behavioral indicators (repeated errors, hesitation, inactivity, and trembling) derived from task performance logs in Unity.</p></list-item><list-item><p>A continuous GSR signal sampled at 5 Hz, filtered and normalized per user.</p></list-item></list></p><p>Decision Logic: Each behavioral indicator is assigned a value of 1 if it crosses its defined threshold:<list list-type="bullet"><list-item><p>Hesitation: Delay &gt; 2 s.</p></list-item><list-item><p>Repeated error: &#8805;2 failed attempts.</p></list-item><list-item><p>Inactivity: No input for &gt;3 s.</p></list-item><list-item><p>Trembling: Sudden unintentional micro-movements (measured via controller jitter).</p></list-item></list></p><p>The binary scores are then summed to create a behavioral score <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (range: 0&#8211;4).</p><p>The GSR signal is processed using a slope detector. If a steep rise (&gt;0.05 &#956;S/s within 3 s) is detected, then a physiological flag <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is set to 1.</p><p>Rule: The final decision score is computed as follows:<disp-formula><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in our baseline model.</p><p>Output: A stress state is triggered if <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>&#8805;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This rule ensures that mild behavioral signs must be supported by GSR rise to qualify as real stress, thereby reducing false positives.</p></sec></sec><sec id="sec5-sensors-25-05323"><title>5. Experimental Setup and Results</title><p>This section reports the experimental findings obtained with the proposed Sensor-Assisted Unity Architecture, a behavior-first system that integrates physiological signals when necessary to enhance stress detection.</p><sec id="sec5dot1-sensors-25-05323"><title>5.1. System Detection Framework</title><p>The system is built to work quickly and reliably in VR. All parts, from signal detection to feedback, are performed in real time. Unity&#8217;s built-in tools are used to minimize delay. The design can also be updated later to include more signals or feedback. In the Sensor-Assisted Unity Architecture algorithm, GSR signals are not directly used to validate behavioral stress indicators. Instead, they serve as a parallel inputs when behavioral cues are ambiguous, such as distinguishing between intentional pauses and stress-induced hesitation. We acknowledge that both GSR and behavioral patterns reflect sympathetic nervous system activation, and relying on one to confirm the other may introduce circular reasoning; to mitigate this, our Sensor-Assisted Unity Architecture logic treats GSR as a supplementary input rather than as a ground truth validator. For future iterations, we recommend incorporating external independent validation methods such as psychological self-report instruments or biological markers such as cortisol. All processing occurs within Unity, enabling rapid feedback delivery during VR interactions.</p><p><xref rid="sensors-25-05323-f009" ref-type="fig">Figure 9</xref> shows how the VR stress detection system works during training tasks. Users experience different stress scenarios such as red lights, alarm sounds, or time pressure. The system watches for behavioral signs such as hesitation or task failure, and uses GSR sensors to measure physical stress. All data are processed inside the VR environment, and feedback is provided in real time. The sensor input is only used when behavioral signs are unclear, helping to confirm whether stress is truly present.</p></sec><sec id="sec5dot2-sensors-25-05323"><title>5.2. Simulation Scenarios and Stress Conditions</title><p>The system was tested in several VR scenarios with different types of stressors, including changes in light, alarms, and time pressure. These trials helped to evaluate how well the system could detect stress in different situations.</p><p>As <xref rid="sensors-25-05323-f010" ref-type="fig">Figure 10</xref> shows, different VR conditions were designed to test stress levels by varying combinations of visual, auditory, and time-based stressors. The matrix includes four main scenarios: (1) a baseline condition with normal white lighting and no sound, (2) a red light condition without additional stressors, (3) a time pressure condition with countdown tasks, and (4) a high-stress condition simultaneously combining red lighting, alarm sounds, and time pressure.</p><p>These controlled variations helped to evaluate how the system responds to different levels of cognitive load and sensory pressure. By testing across these distinct conditions, the experiment assessed the system&#8217;s sensitivity and robustness under increasing stress intensity.</p><p>The selected stress stimuli (red lighting, alarm sounds, and countdown timers) are commonly used in psychological and human factors studies to induce time-sensitive cognitive stress. Prior studies have demonstrated the effectiveness of these stimuli in triggering measurable stress responses such as elevated GSR and impaired performance [<xref rid="B15-sensors-25-05323" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05323" ref-type="bibr">16</xref>]. These stressors were incorporated in combination to simulate escalating levels of perceived urgency and discomfort in the VR environment.</p><p>The &#8220;Normal&#8221; condition in this study served as a controlled non-stress baseline. Participants experienced this condition at the beginning of each session in a calm virtual environment with neutral lighting, no alarms, and no time constraints. All comparisons of behavioral and physiological changes under stress scenarios were made relative to this baseline, allowing for the detection of deviations in user response that corresponded to induced stress conditions.</p><sec><title>Participant</title><p>This study was conducted using simulated users in a controlled virtual environment. No real human participants were involved, and no demographic or consent data were collected. The scenarios and behaviors were scripted to model stress responses for system validation purposes. Future studies will involve human participants with appropriate institutional review and consent procedures.</p></sec></sec><sec id="sec5dot3-sensors-25-05323"><title>5.3. Stressor Validation</title><p>To ensure the validity of our chosen VR stressors (red lights, alarms, and time pressure), we selected them based on the established literature linking these stimuli to measurable physiological stress responses. For instance, emergency alarms have been shown to cause rapid increases in heart rate and cortisol levels [<xref rid="B20-sensors-25-05323" ref-type="bibr">20</xref>]. The color red is associated with heightened arousal and urgency, and can influence performance depending on exposure timing [<xref rid="B22-sensors-25-05323" ref-type="bibr">22</xref>]. Finally, time pressure is a widely recognized cognitive stressor that can increase error rates and impair decision-making [<xref rid="B23-sensors-25-05323" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05323" ref-type="bibr">24</xref>].</p></sec><sec id="sec5dot4-sensors-25-05323"><title>5.4. Stress Detection and Comparative Analysis</title><p>Confusion Matrix and Classification Metrics. To evaluate the performance of our stress detection system, we constructed a three-class confusion matrix (Negative, Neutral, Positive) that shows the number of correctly classified samples and misclassifications. Our system achieved precision scores of 0.89, 0.81, and 0.77 for the Negative, Neutral, and Positive classes, respectively The recall values for these classes were 0.78, 0.78, and 0.93, respectively. For the F1-score, which balances precision and recall, the results were 0.83, 0.80, and 0.84, respectively. The overall classification accuracy was 82%, demonstrating effective performance for different stress levels. These results confirm that combining behavioral and physiological signals improves stress detection accuracy, supporting practical applications in VR-based training systems.</p><p>In our VR experiments, GSR readings showed consistent event-related increases within 1&#8211;3 s of stressor onset, aligning with established Electrodermal Activity (EDA) latency windows [<xref rid="B20-sensors-25-05323" ref-type="bibr">20</xref>]. To further support generalizability, we applied our Sensor-Assisted Unity Architecture to the externally validated WESAD dataset [<xref rid="B25-sensors-25-05323" ref-type="bibr">25</xref>], achieving 96% mean accuracy and an Area Under the Receiver Operating Characteristic (AUC) of 0.95 using physiological signals (EDA and HR). This alignment with a gold-standard physiological dataset supports the effectiveness of our chosen stressors in eliciting measurable stress responses.</p></sec><sec id="sec5dot5-sensors-25-05323"><title>5.5. Threshold Selection and Justification</title><p>The threshold values used in this study were selected through pilot testing, Unity&#8217;s hand-tracking sensitivity, and the established literature on stress detection and motor control. The 2-s hesitation threshold was derived from pilot trials, where stressed participants consistently took longer than 2 s to initiate object interactions. This aligns with criteria reported in prior VR motor control studies (1&#8211;3 s ranges) [<xref rid="B26-sensors-25-05323" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05323" ref-type="bibr">27</xref>]. The tremor amplitude thresholds were set based on Unity&#8217;s tracking resolution, with high-frequency jitter above 0.03 Unity units frequently observed under stress but rarely during baseline. This was confirmed against VR/AR tremor detection parameters in motor disorder assessment research [<xref rid="B28-sensors-25-05323" ref-type="bibr">28</xref>]. The 0.7 <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S GSR conductance threshold corresponds to the minimum event-related skin conductance response magnitude described in core electrodermal references [<xref rid="B29-sensors-25-05323" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05323" ref-type="bibr">30</xref>], and has previously been used in VR stress-induction protocols [<xref rid="B31-sensors-25-05323" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05323" ref-type="bibr">32</xref>]. This combination of empirical pilot data and established results from the literature ensures that all thresholds are technically valid and scientifically supported. Future work will include Receiver Operating Characteristic (ROC) curve optimization and individualized baselining for improved generalizability.</p></sec><sec id="sec5dot6-sensors-25-05323"><title>5.6. ROC Curve Analysis</title><p>We assessed the system&#8217;s ability to distinguish between stress classes (Negative, Neutral, Positive) ROC curves as shown <xref rid="sensors-25-05323-f011" ref-type="fig">Figure 11</xref>. The calculated AUC was 0.88 for both the Negative and Neutral classes and 0.96 for the Positive class. An AUC value close to 1.0 indicates the model is highly effective at telling the classes apart. These high AUC scores demonstrate that the system can reliably classify different stress levels, especially for the Positive class. This strong performance is achieved by combining both behavioral and physiological features in our hybrid model, allowing the system to capture a wide range of stress indicators and improve the overall classification accuracy.</p></sec><sec id="sec5dot7-sensors-25-05323"><title>5.7. Statistical Analysis of Stress Classes</title><p>The boxplots of hesitation time, tremble amplitude, and GSR for each stress class (Negative, Neutral, Positive) showed clear differences between groups. The Neutral class consistently exhibited the highest median and mean values across all indicators hesitation time (median: 1.69 s, mean: 1.68 s), tremble amplitude (median: 0.0160, mean: 0.0170), and GSR (median: 0.77 <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S, mean: 0.76 <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S).</p><p>In contrast, the Negative and Positive classes had notably lower medians hesitation time (0.99 s and 0.91 s), tremble amplitude (0.0060 and 0.0050), and GSR (0.62 <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S and 0.61 <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S), respectively. These results indicate that participants in the Neutral group showed the strongest behavioral and physiological responses, while the Negative and Positive groups showed weaker reactions. This clear separation among all three features demonstrates that combining behavioral and physiological data enables the system to distinguish stress levels more effectively than using a single type of signal.</p><p>As shown in <xref rid="sensors-25-05323-f012" ref-type="fig">Figure 12</xref>, the boxplots illustrate the distribution of stress-related features across different classes. The central line of each box represents the median, the box edges indicate the interquartile range, and the whiskers extend to capture variability in the data. This plot shows the statistical distribution of each indicator for the Negative, Neutral, and Positive stress classes. As observed in the plot, the Neutral class exhibits the highest median values across all three features; in contrast, the Negative and Positive classes show notably lower values. This clear separation of statistical distributions for all three features demonstrates that combining behavioral (hesitation, trembling) and physiological (GSR) data is effective in distinguishing between different stress levels.</p></sec><sec id="sec5dot8-sensors-25-05323"><title>5.8. Scatterplot and Pearson Value</title><p>This subsection highlights the relationship between tremble amplitude and hesitation time across different stress classes.</p><p><xref rid="sensors-25-05323-f013" ref-type="fig">Figure 13</xref> demonstrates the strong linear relationship between the two behavioral signals, with the upper panel showing the relationship between GSR and hesitation time (Pearson r = 0.94, <italic toggle="yes">p</italic> = 1.2 &#215; 10<sup>&#8722;68</sup>) and the lower panel displaying the correlation between GSR and tremble amplitude (Pearson r = 0.92, <italic toggle="yes">p</italic> = 4.7 &#215; 10<sup>&#8722;59</sup>). Both plots demonstrate strong positive linear relationships between physiological and behavioral indicators. These high correlation values indicate that hesitation and trembling are not independent measures, but rather highly correlated signals that both increase with rising stress levels. The synergistic relationship between these signals enables more robust stress detection, as changes in one signal are consistently reinforced by changes in the other. By combining these signals, our system can use their synergistic relationship to more robustly detect stress, as a change in one signal is consistently reinforced by a change in the other.</p></sec><sec id="sec5dot9-sensors-25-05323"><title>5.9. Machine Learning Model Comparison and Performance Evaluation</title><p>Our Sensor-Assisted Unity method uses straightforward interpretable thresholds for GSR, hesitation time, and tremble amplitude to classify stress levels. This approach achieved strong AUC values (0.88 for Negative and Neutral classes, and 0.96 for the Positive class), showing that it can reliably distinguish different stress states. Unlike machine learning models, this method does not require large amounts of data or intensive training, and is also robust to noise. Its rule-based design makes the results easier to interpret for users and clinicians. Our proposed approach is fast and computationally efficient, making it well suited for real-time VR applications where quick feedback is needed. While machine learning models can improve with more data and find subtle patterns, the proposed Sensor-Assisted Unity method provides consistently high and interpretable performance, offering a practical and reliable solution for stress detection.</p><p><xref rid="sensors-25-05323-f014" ref-type="fig">Figure 14</xref> shows a visual comparison of the performance of the Sensor-Assisted Unity method against various machine learning models for stress detection. The plot illustrates the accuracy and macro F1-scores, demonstrating that the proposed approach outperforms traditional ML models in this context.</p><p><xref rid="sensors-25-05323-t001" ref-type="table">Table 1</xref> summarizes the comparative performance of all evaluated stress detection approaches. Among all tested methods, our Sensor-Assisted Unity approach achieved the best overall performance, reaching 82.0% accuracy. In comparison, Gradient Boosting achieved 78.6% accuracy, while both the Stacked (LR + Extra Trees) and K-Nearest Neighbors models reached 75.0% accuracy. Extra Trees achieved 71.4% accuracy, Logistic Regression reached 67.9%, SVM achieved 64.3%, and Naive Bayes obtained 60.7%. These results indicate that integrating both behavioral and physiological features through our method leads to the most accurate and reliable stress detection in virtual reality.</p></sec><sec id="sec5dot10-sensors-25-05323"><title>5.10. Ground Truth Considerations</title><p>To supplement simulation-based validation, we applied our Sensor-Assisted Unity Architecture logic to real-world physiological data using the WESAD dataset. This dataset contains labeled stress and baseline conditions collected from fifteen participants using wearable sensors. The WESAD dataset comprises fifteen healthy adult participants (eight male, seven female) with a mean age of 27.5 &#177; 2.4 years. Subjects are labeled S2&#8211;S17 (excluding S4), with numbering not indicating recruitment order. We selected Subject S16 for this study, as the wrist-worn EDA and Blood Volume Pulse (BVP) data were complete and clearly labeled for both the baseline (label 1) and stress (label 2) conditions. No demographic information specific to S16 was provided in WESAD in order to preserve participant anonymity, and it was not required for the validation analysis, which relied solely on sensor signals (EDA and HR derived from BVP).</p><p>As shown in <xref rid="sensors-25-05323-f015" ref-type="fig">Figure 15</xref>, the plot visualizes the temporal changes in electrodermal activity (green) and heart rate (blue) throughout the experiment. The results indicate that EDA tends to increase during stressful periods and decrease afterwards, while HR displays stronger noise and fluctuations. This comparison confirms that EDA provides a clearer signal of stress, whereas HR adds complementary but subtler information.</p><p>We selected subject S16 for analysis due to completeness and clarity of data, and focused only on wrist-based signals for EDA, GSR and HR, derived from BVP. The inclusion of HR as an orthogonal physiological signal helped to ensure that external validation did not rely solely on the same modality as used in the simulations, further reducing the risk of circular validation and demonstrating the architecture&#8217;s ability to integrate independent biosignals. In the present implementation, behavioral stress detection was validated through alignment with controlled stimuli introduced in the VR environment, including visual (red light), auditory (alarm), and temporal (time pressure) stressors. Physiological confirmation was conducted via GSR threshold crossings recorded during high-stress segments. However, no standardized clinical instruments were applied to define ground truth labels. To enhance future validation, we recommend incorporating psychological scales such as the State&#8211;Trait Anxiety Inventory (STAI) [<xref rid="B10-sensors-25-05323" ref-type="bibr">10</xref>] or Perceived Stress Scale (PSS) [<xref rid="B11-sensors-25-05323" ref-type="bibr">11</xref>] as well as physiological benchmarks such as Heart Rate Variability (HRV) [<xref rid="B12-sensors-25-05323" ref-type="bibr">12</xref>] and salivary cortisol [<xref rid="B13-sensors-25-05323" ref-type="bibr">13</xref>]. These independent measures would support more rigorous classification and validation of user stress responses.</p><p>As shown in <xref rid="sensors-25-05323-f016" ref-type="fig">Figure 16</xref>, the Sensor-Assisted Unity model achieved AUC of 0.95 on the WESAD dataset (subject S16), demonstrating strong separation between stress and non-stress conditions.</p><p>Our threshold-based model achieved 96% classification accuracy, with only 289 misclassified samples out of 7412.</p><p>The precision for stress detection reached 1.00, and the recall was 0.89. The F1-scores were 0.97 for baseline and 0.94 for stress. ROC analysis further confirmed strong performance, with an AUC of 0.95.</p><p>These results validate that our system can generalize to real labeled stress data, supporting the claim that combining behavioral and physiological indicators yields reliable detection. This external validation reinforces the robustness of our Sensor-Assisted Unity Architecture model in both simulated and real-world settings.</p><p>As presented in <xref rid="sensors-25-05323-f017" ref-type="fig">Figure 17</xref>, the confusion matrix and classification report for WESAD subject S16 using our Sensor-Assisted Union classifier demonstrate robust performance metrics. The proposed model achieved 96% accuracy across 7412 samples, with precision = 1.00 and recall = 0.89 for stress detection. This external validation supports the system&#8217;s reliability on real-world labeled stress data.</p><p>As illustrated in <xref rid="sensors-25-05323-f016" ref-type="fig">Figure 16</xref>, the Sensor-Assisted Unity model achieved an AUC of 0.95 on the WESAD dataset (subject S16), demonstrating strong discrimination between stress and non-stress conditions.</p><p>Normalization to Address Individual Differences: To address concerns regarding individual variability, we implemented a normalization approach that accounts for differences in physiological and behavioral baselines. Instead of comparing absolute values directly, we computed the relative change of each stress indicator from a dataset-derived baseline.</p><p>The baseline was established by calculating the mean value of each feature (GSR, hesitation time, and tremble amplitude) across all non-stress conditions, specifically the Negative and Neutral classes in our VR dataset.</p><p>For each trial, the relative change from this baseline was computed using the following formula: <disp-formula><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">Value</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="italic">Baseline</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi mathvariant="italic">Baseline</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This method allowed us to capture stress-induced changes in a way that inherently reduces the effect of natural variation across individuals. Analyzing these relative changes provided clear evidence of the system&#8217;s ability to differentiate between stress levels. The findings demonstrate that the normalization method increases the robustness of our system to individual differences. In future work, we plan to implement a per-user calibration step to establish personalized baselines, further enhancing the system&#8217;s accuracy and reliability.</p></sec><sec id="sec5dot11-sensors-25-05323"><title>5.11. WESAD Dataset Performance</title><p>To validate our Sensor-Assisted Unity classifier, we utilized WESAD dataset. This publicly available dataset is ideal for our purposes, as it contains physiological data from individuals in different states, including rest, stress, and amusement, captured via wearable sensors on the chest and wrist. For this study, we focused on data from subject S16 due to completeness and clear labeling, specifically using wrist-based signals to simulate our VR system.</p><p>Our analysis concentrated on two key signals: EDA and HR&#8212;the latter of which was estimated from BVP signal. We observed how these signals changed over time, noting that EDA showed a more pronounced increase during stress intervals before returning to baseline. While HR exhibited more fluctuations, its changes were also aligned with the stress labels. A key finding from our exploratory data analysis was the weak Pearson correlation between EDA and HR (0.02 for baseline, 0.00 for stress), which supports the rationale for this approach. Because the signals capture independent physiological responses, their combination provides a more robust and reliable indicator of stress than either signal alone.</p><p>We trained our Sensor-Assisted Unity classifier on the WESAD data, treating the task as a binary classification problem in order to distinguish between baseline (label 1) and stress (label 2) conditions. The model demonstrated strong performance, achieving AUC of 0.95 on the ROC curve, which indicates excellent separation between the two classes.</p><p>The confusion matrix and classification report further confirm the model&#8217;s effectiveness:<list list-type="bullet"><list-item><p>Accuracy: The model achieved an overall accuracy of 96% across 7412 samples.</p></list-item><list-item><p>Correct Classifications: It correctly identified 4720 baseline samples and 2403 stress samples.</p></list-item><list-item><p>Errors: Only 289 classification errors were made.</p></list-item><list-item><p>Precision and Recall: The model showed a precision of 1.00 and a recall of 0.89 for stress detection, meaning that it rarely misidentified a non-stress state as stress and successfully captured a large majority of actual stress cases.</p></list-item><list-item><p>F1-Scores: The F1-scores were 0.97 for baseline and 0.94 for stress.</p></list-item></list></p><p>These results demonstrate that our simple threshold-based model is highly reliable and provide strong external validation for its potential in real-time VR stress detection.</p></sec><sec id="sec5dot12-sensors-25-05323"><title>5.12. Parameters</title><p>Evaluation focused on four key aspects: recognition of behavioral signals, response latency, task performance under stress, and the contribution of physiological input to decision reliability.</p><list list-type="simple"><list-item><label>(i)</label><p>Hesitation Delay: Delay of &#8805;2 s before initiating an action.</p></list-item><list-item><label>(ii)</label><p>Repeated Repair Failures: Consecutive unsuccessful task attempts.</p></list-item><list-item><label>(iii)</label><p>Tremor Above Threshold: High-frequency hand vibration detected.</p></list-item><list-item><label>(iv)</label><p>Inactivity: No controller movement for &#8805;3 s.</p></list-item></list><p>Detection performance was analyzed under three input conditions within the same architecture:<list list-type="bullet"><list-item><p>Behavioral Only: Detection based solely on interaction data from VR.</p></list-item><list-item><p>Physiological Only: Detection based solely on GSR sensor data (used in auditory stress scenario to evaluate sensor-only limitations).</p></list-item><list-item><p>Combined Input (Sensor-Assisted Unity Architecture): Behavioral data fused with physiological input when behavioral evidence is inconclusive.</p></list-item></list></p></sec><sec id="sec5dot13-sensors-25-05323"><title>5.13. Parameters and Detection Logic</title><p>To evaluate the effectiveness of the Sensor-Assisted Unity Architecture, the system used a combination of real-time features, namely, tremor amplitude, hand movement Root Mean Square (RMS), response delay, and task failure count. These features were compared against preset thresholds to detect stress states.</p><p>Thresholds were applied within Unity using ScriptableObjects and C# coroutines. In the Sensor-Assisted Unity Architecture logic, behavioral detections were verified using a GSR sensor. A binary flag was used to resolve whether elevated tremor was due to stress or non-stressful hand instability. If GSR conductance exceeded 0.7 <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S in sync with high behavioral indicators, then the stress state was confirmed, enhancing classification accuracy in edge cases.</p><p>The system maintained modular thresholds to allow real-time adjustment during trials. Stress detection accuracy was assessed based on detection timing, number of false positives, and agreement across sensor-assisted detections.</p><p>The Sensor-Assisted Unity Architecture logic operates on a three-tier structure that balances speed, accuracy, and responsiveness:<list list-type="bullet"><list-item><p>Tier 1&#8211;Immediate Alert:</p><p>When a strong behavioral or physiological signal is present, such as tremor RMS &gt; 30 Hz or GSR &gt; 0.7 <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S, the system immediately flags high stress without additional computation. This tier minimizes latency for high-confidence cases.</p></list-item><list-item><p>Tier 2&#8211;Selective Fusion (Gray Zone):</p><p>If neither signal reaches a critical threshold but both show partial signs, a combined stress index is computed; for example, if the reaction delay is around 0.8 s and GSR lies between 0.61 and 0.65 <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S, then the fusion score <italic toggle="yes">S</italic> is calculated as<disp-formula><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mn>0.4</mml:mn><mml:msub><mml:mi>G</mml:mi><mml:mi>norm</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">B</italic> is the normalized behavioral score and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>norm</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the normalized GSR signal. An alert is issued only if <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.65</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This tier ensures sensitivity while avoiding false positives.</p></list-item><list-item><p>Tier 3&#8211;No Alert:</p><p>If neither behavioral nor GSR cues indicate elevated stress, then the system remains idle. All behavioral indicators, such as hesitation, tremor, inactivity and GSR samples, remain within normal operating bounds.</p></list-item></list></p><p>This approach offers both responsiveness and selectivity in real-time VR stress detection, and follows the threshold&#8211;rule methodology and wearable refinement presented in previous work [<xref rid="B33-sensors-25-05323" ref-type="bibr">33</xref>].</p><p>The threshold values used in this study were selected through pilot testing and guided by prior work in stress detection. The 2-s hesitation threshold was observed during preliminary trials, where participants under stress consistently took longer than 2 s to initiate object interactions. Tremor amplitude thresholds were set based on Unity&#8217;s hand-tracking sensitivity: high-frequency jitters exceeding 0.03 Unity units were frequently observed during stress-inducing tasks, but rarely during baseline. For the GSR threshold, a value of 0.7 <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mi mathvariant="sans-serif">&#956;</mml:mi></mml:mrow></mml:math></inline-formula>S was chosen based on established studies that associate sympathetic nervous system activation with conductance changes exceeding this level [<xref rid="B9-sensors-25-05323" ref-type="bibr">9</xref>,<xref rid="B18-sensors-25-05323" ref-type="bibr">18</xref>]. While these values were effective in our environment, they may require adaptation for broader populations. Future studies should incorporate ROC curve analysis and individualized baselining to improve generalizability.</p></sec><sec id="sec5dot14-sensors-25-05323"><title>5.14. Performance Evaluation</title><p>Profiling on a PC with an RTX 3070 GPU (NVIDIA Corporation, Santa Clara, CA, USA) and a Valve Index headset (Valve Corporation, Bellevue, WA, USA), headset showed only a 0.9&#8211;1.3 ms increase in CPU time per frame and a 5&#8211;8% GPU overhead.</p><p>This design achieves end-to-end latency below 120 ms, ensuring rapid cognitive stress response. Profiling on a Meta Quest 3 showed a 45% reduction in Unity&#8217;s CPU/GPU load at the cost of a modest increase in round-trip feedback latency (150 &#177; 12 ms), which remains within acceptable bounds for cognitive stress feedback.</p><p>This tradeoff improves frame rate stability and overall performance on mobile platforms, where battery life and thermal throttling are concerns. The Sensor-Assisted Unity Architecture adapts across platforms by balancing load between the Unity engine and lightweight embedded processing. All system components share the same behavioral input signals and stress detection logic, ensuring consistent performance while enabling deployment on both PC-based and standalone headsets. By distributing the processing efficiently within the same architecture, Unity&#8217;s CPU/GPU load is reduced by approximately 60% on standalone devices such as the Meta Quest 3. Although this increases average latency to about 150 ms, the Sensor-Assisted Unity Architecture still provides smooth frame rates and maintains compatibility across different VR platforms.</p><p>In the current system, GSR data are used to assist the interpretation of behavioral indicators such as hesitations and tremors. While this improves real-time robustness, it introduces a limitation in that both behavioral and GSR data originate from the same underlying stress response. Without external validation, relying on GSR to validate behavioral stress could potentially create a circular reasoning situation in which two correlated signals are each used to confirm the other. To address this, future versions of the Sensor-Assisted Unity Architecture should incorporate independent validation sources. These could include psychological scales such as STAI or PSS as well as orthogonal physiological signals such as HRV. These additions would support more objective stress detection and reduce the risk of overfitting to internal signal patterns.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05323"><title>6. Conclusions</title><p>This work presents a VR-based stress detection system that primarily relies on behavioral signals, supported by a minimal physiological sensor. By focusing on hesitation, delays, and repeated errors, the proposed Sensor-Assisted Unity Architecture demonstrates the potential of VR-based behavioral monitoring as a foundation for detecting stress. The current findings are based on controlled VR scenarios with simulated participants, and no human participant data. Within these constraints, the proposed approach helps to distinguish between performance issues arising from inexperience, distraction, or cognitive load, particularly when behavioral or physiological cues alone may be ambiguous. While the Sensor-Assisted Unity Architecture does not include direct comparisons against full multi-sensor arrays, results suggest that this lightweight configuration can improve detection coverage in simulated scenarios. The inclusion of real-time visual and auditory feedback supports its applicability in adaptive training settings, and future work will validate the approach with diverse populations and independent stress references.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.R. and Y.C.; methodology, A.R. and Y.C.; software, A.R.; validation, A.R.; formal analysis, A.R.; investigation, A.R.; resources, A.R.; data curation, A.R.; writing&#8212;original draft preparation, A.R.; writing&#8212;review and editing, A.R. and Y.C.; visualization, A.R.; supervision, Y.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data supporting the results are available within the article and its figures.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05323"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sevdalis</surname><given-names>N.</given-names></name><name name-style="western"><surname>Nestel</surname><given-names>D.</given-names></name><name name-style="western"><surname>Tierney</surname><given-names>T.</given-names></name><name name-style="western"><surname>Woloshynowych</surname><given-names>M.</given-names></name><name name-style="western"><surname>Darzi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kneebone</surname><given-names>R.</given-names></name></person-group><article-title>The impact of stress on surgical performance: A systematic review of the literature</article-title><source>Surgery</source><year>2009</year><volume>147</volume><fpage>318</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1016/j.surg.2009.10.007</pub-id><pub-id pub-id-type="pmid">20004924</pub-id></element-citation></ref><ref id="B2-sensors-25-05323"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeBlanc</surname><given-names>V.R.</given-names></name></person-group><article-title>The effects of acute stress on performance: Implications for health professions education</article-title><source>Acad. Med.</source><year>2009</year><volume>84</volume><issue>(Suppl. S10)</issue><fpage>S25</fpage><lpage>S33</lpage><pub-id pub-id-type="doi">10.1097/ACM.0b013e3181b37b8f</pub-id><pub-id pub-id-type="pmid">19907380</pub-id></element-citation></ref><ref id="B3-sensors-25-05323"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Filaire</surname><given-names>E.</given-names></name><name name-style="western"><surname>Treuvelot</surname><given-names>P.</given-names></name><name name-style="western"><surname>Toumi</surname><given-names>H.</given-names></name></person-group><article-title>Anxiety, stress and depression in elite athletes: A narrative review</article-title><source>Sci. Sport.</source><year>2009</year><volume>24</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1016/j.scispo.2009.03.005</pub-id></element-citation></ref><ref id="B4-sensors-25-05323"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hernandez</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Rehg</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Picard</surname><given-names>R.W.</given-names></name></person-group><article-title>BioWatch: Estimating breathing and heart rate from wrist motions</article-title><source>Proceedings of the 6th International Conference on Pervasive Computing Technologies for Healthcare, ICST</source><conf-loc>New York, NY, USA</conf-loc><conf-date>20&#8211;23 May 2014</conf-date><fpage>169</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.4108/icst.pervasivehealth.2014.254959</pub-id></element-citation></ref><ref id="B5-sensors-25-05323"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Healey</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Picard</surname><given-names>R.W.</given-names></name></person-group><article-title>Detecting stress during real-world driving tasks using physiological sensors</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2005</year><volume>6</volume><fpage>156</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1109/TITS.2005.848368</pub-id></element-citation></ref><ref id="B6-sensors-25-05323"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Amin</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Faghih</surname><given-names>R.T.</given-names></name></person-group><article-title>Inferring Autonomic Nervous System Stimulation from Hand and Foot Skin Conductance Measurements</article-title><source>Proceedings of the 52nd Asilomar Conference on Signals, Systems, and Computers (ACSSC)</source><conf-loc>Pacific Grove, CA, USA</conf-loc><conf-date>28&#8211;31 October 2018</conf-date><fpage>655</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1109/acssc.2018.8645408</pub-id></element-citation></ref><ref id="B7-sensors-25-05323"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wickramasuriya</surname><given-names>D.S.</given-names></name><name name-style="western"><surname>Amin</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Faghih</surname><given-names>R.T.</given-names></name></person-group><article-title>Skin Conductance as a Viable Alternative for Closing the Deep-Brain-Stimulation Loop in Neuropsychiatric Disorders</article-title><source>Front. Neurosci.</source><year>2019</year><volume>13</volume><elocation-id>780</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2019.00780</pub-id><pub-id pub-id-type="pmid">31447627</pub-id><pub-id pub-id-type="pmcid">PMC6692489</pub-id></element-citation></ref><ref id="B8-sensors-25-05323"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ghaffari</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Rogers</surname><given-names>J.A.</given-names></name></person-group><article-title>State-of-the-Art Soft Microfluidic Patches for Multiplexed Sweat Analysis</article-title><source>Adv. Healthc. Mater.</source><year>2021</year><volume>10</volume><fpage>2101234</fpage><pub-id pub-id-type="doi">10.1016/j.cobme.2019.01.003</pub-id></element-citation></ref><ref id="B9-sensors-25-05323"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>H.-G.</given-names></name><name name-style="western"><surname>Song</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>B.H.</given-names></name><name name-style="western"><surname>Jang</surname><given-names>D.P.</given-names></name></person-group><article-title>Deep learning-based stress detection for daily life use using single-channel EEG and GSR in a virtual reality interview paradigm</article-title><source>PLoS ONE</source><year>2024</year><volume>19</volume><elocation-id>e0305864</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0305864</pub-id><pub-id pub-id-type="pmid">38959272</pub-id><pub-id pub-id-type="pmcid">PMC11221693</pub-id></element-citation></ref><ref id="B10-sensors-25-05323"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Spielberger</surname><given-names>C.D.</given-names></name><name name-style="western"><surname>Gorsuch</surname><given-names>R.L.</given-names></name><name name-style="western"><surname>Lushene</surname><given-names>R.E.</given-names></name></person-group><source>Manual for the State-Trait Anxiety Inventory STAI (Form Y)</source><publisher-name>Consulting Psychologists Press</publisher-name><publisher-loc>Palo Alto, CA, USA</publisher-loc><year>1983</year></element-citation></ref><ref id="B11-sensors-25-05323"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cohen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kamarck</surname><given-names>T.</given-names></name><name name-style="western"><surname>Mermelstein</surname><given-names>R.</given-names></name></person-group><article-title>A global measure of perceived stress</article-title><source>J. Health Soc. Behav.</source><year>1983</year><volume>24</volume><fpage>385</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.2307/2136404</pub-id><pub-id pub-id-type="pmid">6668417</pub-id></element-citation></ref><ref id="B12-sensors-25-05323"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cheon</surname><given-names>E.J.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>D.S.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>Y.H.</given-names></name><name name-style="western"><surname>Koo</surname><given-names>B.H.</given-names></name></person-group><article-title>Stress and heart rate variability: A meta-analysis and review of the literature</article-title><source>Psychiatry Investig.</source><year>2018</year><volume>15</volume><fpage>235</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.30773/pi.2017.08.17</pub-id><pub-id pub-id-type="pmcid">PMC5900369</pub-id><pub-id pub-id-type="pmid">29486547</pub-id></element-citation></ref><ref id="B13-sensors-25-05323"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hellhammer</surname><given-names>D.H.</given-names></name><name name-style="western"><surname>W&#252;st</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kudielka</surname><given-names>B.M.</given-names></name></person-group><article-title>Salivary cortisol as a biomarker in stress research</article-title><source>Psychoneuroendocrinology</source><year>2009</year><volume>34</volume><fpage>163</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.psyneuen.2008.10.026</pub-id><pub-id pub-id-type="pmid">19095358</pub-id></element-citation></ref><ref id="B14-sensors-25-05323"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Halbig</surname><given-names>A.</given-names></name><name name-style="western"><surname>Latoschik</surname><given-names>M.E.</given-names></name></person-group><article-title>A Systematic Review of Physiological Measurements, Factors, Methods, and Applications in Virtual Reality</article-title><source>Front. Virtual Real.</source><year>2021</year><volume>2</volume><elocation-id>694567</elocation-id><pub-id pub-id-type="doi">10.3389/frvir.2021.694567</pub-id></element-citation></ref><ref id="B15-sensors-25-05323"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hirt</surname><given-names>C.</given-names></name><name name-style="western"><surname>Eckard</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kunz</surname><given-names>A.</given-names></name></person-group><article-title>Stress Generation and Non-Intrusive Measurement in Virtual Environments Using Eye Tracking</article-title><source>J. Ambient Intell. Human Comput.</source><year>2020</year><volume>11</volume><fpage>5977</fpage><lpage>5989</lpage><pub-id pub-id-type="doi">10.1007/s12652-020-01845-y</pub-id></element-citation></ref><ref id="B16-sensors-25-05323"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Finseth</surname><given-names>T.</given-names></name><name name-style="western"><surname>Dorneich</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Keren</surname><given-names>N.</given-names></name><name name-style="western"><surname>Franke</surname><given-names>W.D.</given-names></name><name name-style="western"><surname>Vardeman</surname><given-names>S.B.</given-names></name></person-group><article-title>Manipulating Stress Responses during Spaceflight Training with Virtual Stressors</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>2289</elocation-id><pub-id pub-id-type="doi">10.3390/app12052289</pub-id></element-citation></ref><ref id="B17-sensors-25-05323"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name></person-group><article-title>Challenges of EEG Data Quality in VR Environments: A Review of Contributing Factors, Assessment Methods, and Potential Solutions</article-title><source>Front. Psychol.</source><year>2023</year><volume>14</volume><elocation-id>1289816</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2023.1289816</pub-id><pub-id pub-id-type="pmid">38239464</pub-id><pub-id pub-id-type="pmcid">PMC10794660</pub-id></element-citation></ref><ref id="B18-sensors-25-05323"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mudassar</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kalatian</surname><given-names>A.</given-names></name><name name-style="western"><surname>Farooq</surname><given-names>B.</given-names></name></person-group><article-title>Analysis of Pedestrian Stress Level Using Skin-Conductance Sensors in Virtual Immersive Reality</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2111.11492</pub-id></element-citation></ref><ref id="B19-sensors-25-05323"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nechyporenko</surname><given-names>A.</given-names></name><name name-style="western"><surname>Frohme</surname><given-names>M.</given-names></name><name name-style="western"><surname>Strelchuk</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Omelchenko</surname><given-names>V.</given-names></name><name name-style="western"><surname>Gargin</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ishchenko</surname><given-names>L.</given-names></name><name name-style="western"><surname>Alekseeva</surname><given-names>V.</given-names></name></person-group><article-title>Galvanic Skin Response and Photoplethysmography for Stress Recognition Using Machine Learning and Wearable Sensors</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>11997</elocation-id><pub-id pub-id-type="doi">10.3390/app142411997</pub-id></element-citation></ref><ref id="B20-sensors-25-05323"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kirschbaum</surname><given-names>C.</given-names></name><name name-style="western"><surname>Pirke</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Hellhammer</surname><given-names>D.H.</given-names></name></person-group><article-title>Acute Physiological Stress Response to Emergency Alarms</article-title><source>Psychoneuroendocrinology</source><year>2016</year><volume>71</volume><fpage>207</fpage><lpage>215</lpage></element-citation></ref><ref id="B21-sensors-25-05323"><label>21.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Kent State University</collab></person-group><article-title>Mixed Virtual and Haptic Technologies for Simulations</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kent.edu/today/news/new-lab-combines-mixed-virtual-and-haptic-technologies-provide-realistic-simulations" ext-link-type="uri">https://www.kent.edu/today/news/new-lab-combines-mixed-virtual-and-haptic-technologies-provide-realistic-simulations</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-04-03">(accessed on 3 April 2025)</date-in-citation></element-citation></ref><ref id="B22-sensors-25-05323"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Plitnick</surname><given-names>B.</given-names></name><name name-style="western"><surname>Figueiro</surname><given-names>M.G.</given-names></name><name name-style="western"><surname>Wood</surname><given-names>B.</given-names></name><name name-style="western"><surname>Rea</surname><given-names>M.S.</given-names></name></person-group><article-title>The effects of red and blue light on alertness and mood at night</article-title><source>Light. Res. Technol.</source><year>2010</year><volume>42</volume><fpage>449</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1177/1477153509360887</pub-id></element-citation></ref><ref id="B23-sensors-25-05323"><label>23.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Number Analytics</collab></person-group><article-title>Mastering Time Pressure in Human Factors</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.numberanalytics.com/blog/ultimate-guide-time-pressure-human-factors" ext-link-type="uri">https://www.numberanalytics.com/blog/ultimate-guide-time-pressure-human-factors</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-04-03">(accessed on 3 April 2025)</date-in-citation></element-citation></ref><ref id="B24-sensors-25-05323"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Poolton</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Wilson</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Malhotra</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ngo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Masters</surname><given-names>R.S.</given-names></name></person-group><article-title>A comparison of evaluation, time pressure, and multitasking as stressors of psychomotor operative performance</article-title><source>Surgery</source><year>2011</year><volume>149</volume><fpage>776</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1016/j.surg.2010.12.005</pub-id><pub-id pub-id-type="pmid">21310451</pub-id></element-citation></ref><ref id="B25-sensors-25-05323"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Schmidt</surname><given-names>P.</given-names></name><name name-style="western"><surname>Reiss</surname><given-names>A.</given-names></name><name name-style="western"><surname>Duerichen</surname><given-names>R.</given-names></name><name name-style="western"><surname>Van Laerhoven</surname><given-names>K.</given-names></name></person-group><article-title>Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection</article-title><source>Proceedings of the 20th ACM International Conference on Multimodal Interaction (ICMI &#8217;18)</source><conf-loc>Boulder, CO, USA</conf-loc><conf-date>16&#8211;20 October 2018</conf-date><fpage>400</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1145/3242969.3242985</pub-id></element-citation></ref><ref id="B26-sensors-25-05323"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boland</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ruiz</surname><given-names>A.</given-names></name><name name-style="western"><surname>Garcia</surname><given-names>L.</given-names></name><name name-style="western"><surname>Henderson</surname><given-names>T.</given-names></name></person-group><article-title>Analyzing dwell time thresholds in immersive VR interactions</article-title><source>J. Virtual Real. Broadcast.</source><year>2020</year><volume>17</volume><fpage>112</fpage><lpage>125</lpage></element-citation></ref><ref id="B27-sensors-25-05323"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>J.</given-names></name><name name-style="western"><surname>Neff</surname><given-names>M.</given-names></name></person-group><article-title>Hesitation and dwell periods in VR motor tasks</article-title><source>Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST)</source><conf-loc>Tokyo, Japan</conf-loc><conf-date>1 December 2018</conf-date><fpage>45</fpage><lpage>52</lpage></element-citation></ref><ref id="B28-sensors-25-05323"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Held</surname><given-names>R.</given-names></name><name name-style="western"><surname>Meier</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schneider</surname><given-names>F.</given-names></name></person-group><article-title>Quantifying tremor amplitude in VR/AR motor-control applications</article-title><source>Proceedings of the IEEE International Conference on Human-Computer Interaction (HCI)</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>26&#8211;31 July 2019</conf-date><fpage>321</fpage><lpage>330</lpage></element-citation></ref><ref id="B29-sensors-25-05323"><label>29.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Dawson</surname><given-names>M.E.</given-names></name><name name-style="western"><surname>Schell</surname><given-names>A.M.</given-names></name><name name-style="western"><surname>Filion</surname><given-names>D.L.</given-names></name></person-group><article-title>The electrodermal system</article-title><source>Handbook of Psychophysiology</source><edition>4th ed.</edition><person-group person-group-type="editor"><name name-style="western"><surname>Cacioppo</surname><given-names>J.T.</given-names></name><name name-style="western"><surname>Tassinary</surname><given-names>L.G.</given-names></name><name name-style="western"><surname>Berntson</surname><given-names>G.G.</given-names></name></person-group><publisher-name>Cambridge University Press</publisher-name><publisher-loc>Cambridge, UK</publisher-loc><year>2017</year><comment>Chapter 5</comment><fpage>217</fpage><lpage>243</lpage></element-citation></ref><ref id="B30-sensors-25-05323"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Boucsein</surname><given-names>W.</given-names></name></person-group><source>Electrodermal Activity</source><edition>2nd ed.</edition><publisher-name>Springer</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2012</year></element-citation></ref><ref id="B31-sensors-25-05323"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shiban</surname><given-names>K.</given-names></name><name name-style="western"><surname>Pauli</surname><given-names>A.</given-names></name><name name-style="western"><surname>M&#252;hlberger</surname><given-names>M.</given-names></name></person-group><article-title>VR-based stress induction with GSR measurement</article-title><source>Behav. Res. Methods</source><year>2016</year><volume>48</volume><fpage>341</fpage><lpage>350</lpage></element-citation></ref><ref id="B32-sensors-25-05323"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kinateder</surname><given-names>N.</given-names></name><name name-style="western"><surname>Wissner</surname><given-names>J.</given-names></name><name name-style="western"><surname>Weiss</surname><given-names>T.</given-names></name></person-group><article-title>Alarm simulation in VR: Physiological responses including skin conductance</article-title><source>Proceedings of the IEEE Virtual Reality Conference (VR)</source><conf-loc>Munich, Germany</conf-loc><conf-date>10&#8211;12 September 2014</conf-date><fpage>227</fpage><lpage>230</lpage></element-citation></ref><ref id="B33-sensors-25-05323"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Rogers</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kotz</surname><given-names>D.</given-names></name></person-group><article-title>Stress Detection Using Context-Aware Sensor Fusion from Wearable Devices</article-title><source>IEEE Internet Things J.</source><year>2021</year><volume>8</volume><fpage>12148</fpage><lpage>12161</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2023.3265768</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05323-f001" orientation="portrait"><label>Figure 1</label><caption><p>Key components of the VR-based stress detection system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g001.jpg"/></fig><fig position="float" id="sensors-25-05323-f002" orientation="portrait"><label>Figure 2</label><caption><p>Sensor-Assisted Unity Architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g002.jpg"/></fig><fig position="float" id="sensors-25-05323-f003" orientation="portrait"><label>Figure 3</label><caption><p>Block diagram of the VR-based stress detection system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g003.jpg"/></fig><fig position="float" id="sensors-25-05323-f004" orientation="portrait"><label>Figure 4</label><caption><p>VR stress detection pipeline.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g004.jpg"/></fig><fig position="float" id="sensors-25-05323-f005" orientation="portrait"><label>Figure 5</label><caption><p>Combining behavioral and physiological signals improves detection accuracy and helps to confirm uncertain cases.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g005.jpg"/></fig><fig position="float" id="sensors-25-05323-f006" orientation="portrait"><label>Figure 6</label><caption><p>Decision matrix showing how behavioral and physiological data work together to guide the system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g006.jpg"/></fig><fig position="float" id="sensors-25-05323-f007" orientation="portrait"><label>Figure 7</label><caption><p>Overview of how behavior and physiological data are fused in the Sensor-Assisted Unity Architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g007.jpg"/></fig><fig position="float" id="sensors-25-05323-f008" orientation="portrait"><label>Figure 8</label><caption><p>Sensor-Assisted Unity Architecture stress detection pipeline.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g008.jpg"/></fig><fig position="float" id="sensors-25-05323-f009" orientation="portrait"><label>Figure 9</label><caption><p>System framework, showing stressors and signals in a VR training task.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g009.jpg"/></fig><fig position="float" id="sensors-25-05323-f010" orientation="portrait"><label>Figure 10</label><caption><p>VR Conditions Matrix: stress levels used during testing.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g010.jpg"/></fig><fig position="float" id="sensors-25-05323-f011" orientation="portrait"><label>Figure 11</label><caption><p>Multi-Class ROC Curves for Stress Detection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g011.jpg"/></fig><fig position="float" id="sensors-25-05323-f012" orientation="portrait"><label>Figure 12</label><caption><p>Boxplots of Hesitation Time, Tremble Amplitude, and GSR by Stress Class of hesitation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g012.jpg"/></fig><fig position="float" id="sensors-25-05323-f013" orientation="portrait"><label>Figure 13</label><caption><p>Scatterplot analysis of GSR vs. hesitation time (<bold>top</bold>) and GSR vs. tremble amplitude (<bold>bottom</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g013.jpg"/></fig><fig position="float" id="sensors-25-05323-f014" orientation="portrait"><label>Figure 14</label><caption><p>ROC Curve Comparison of Machine Learning Models for Stress Detection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g014.jpg"/></fig><fig position="float" id="sensors-25-05323-f015" orientation="portrait"><label>Figure 15</label><caption><p>Ground Truth Validation and WESAD Dataset Analysis.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g015.jpg"/></fig><fig position="float" id="sensors-25-05323-f016" orientation="portrait"><label>Figure 16</label><caption><p>ROC curve for Sensor-Assisted Unity model on the WESAD dataset curve.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g016.jpg"/></fig><fig position="float" id="sensors-25-05323-f017" orientation="portrait"><label>Figure 17</label><caption><p>Confusion Matrix for the proposed classification model. The matrix demonstrates the performance of the sensor-assisted threshold classifier on the WESAD dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05323-g017.jpg"/></fig><table-wrap position="float" id="sensors-25-05323-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05323-t001_Table 1</object-id><label>Table 1</label><caption><p>Stress detection model performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Rank</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="left" valign="middle" rowspan="1" colspan="1">Sensor-Assisted Unity</td><td align="center" valign="middle" rowspan="1" colspan="1">82.0%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="left" valign="middle" rowspan="1" colspan="1">Gradient Boosting</td><td align="center" valign="middle" rowspan="1" colspan="1">78.6%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="left" valign="middle" rowspan="1" colspan="1">Stacked (LR + Extra Trees)</td><td align="center" valign="middle" rowspan="1" colspan="1">75.0%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="left" valign="middle" rowspan="1" colspan="1">K-Nearest Neighbors</td><td align="center" valign="middle" rowspan="1" colspan="1">75.0%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">4</td><td align="left" valign="middle" rowspan="1" colspan="1">Extra Trees</td><td align="center" valign="middle" rowspan="1" colspan="1">71.4%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">5</td><td align="left" valign="middle" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="middle" rowspan="1" colspan="1">67.9%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">6</td><td align="left" valign="middle" rowspan="1" colspan="1">SVM</td><td align="center" valign="middle" rowspan="1" colspan="1">64.3%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Naive Bayes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.7%</td></tr></tbody></table><table-wrap-foot><fn><p>Sensor-Assisted Unity is a rule-based approach, not a trained machine learning model; it is included here for the purposes of performance comparison.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>