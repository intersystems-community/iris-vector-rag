<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431545</article-id><article-id pub-id-type="pmcid-ver">PMC12431545.1</article-id><article-id pub-id-type="pmcaid">12431545</article-id><article-id pub-id-type="pmcaiid">12431545</article-id><article-id pub-id-type="doi">10.3390/s25175540</article-id><article-id pub-id-type="publisher-id">sensors-25-05540</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>UHF RFID Sensing for Dynamic Tag Detection and Behavior Recognition: A Multi-Feature Analysis and Dual-Path Residual Network Approach</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="H">Honggang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-8816-8972</contrib-id><name name-style="western"><surname>Liu</surname><given-names initials="X">Xinyi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="c1-sensors-25-05540" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="L">Lei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Qin</surname><given-names initials="B">Bo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1494-8922</contrib-id><name name-style="western"><surname>Pan</surname><given-names initials="R">Ruoyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Pang</surname><given-names initials="S">Shengli</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>L&#225;zaro</surname><given-names initials="A">Antonio</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05540">College of Communication and Information Engineering, Xi&#8217;an University of Posts and Telecommunications, Xi&#8217;an 710121, China; <email>wanghonggang@xupt.edu.cn</email> (H.W.); <email>liulei@stu.xupt.edu.cn</email> (L.L.); <email>qinbo@stu.xupt.edu.cn</email> (B.Q.); <email>panruoyu@xupt.edu.cn</email> (R.P.); <email>pangshengli@xupt.edu.cn</email> (S.P.)</aff><author-notes><corresp id="c1-sensors-25-05540"><label>*</label>Correspondence: <email>lxy829@stu.xupt.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5540</elocation-id><history><date date-type="received"><day>29</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>01</day><month>9</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>05</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 15:25:32.480"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05540.pdf"/><abstract><p>To address the challenges of dynamic coupling interference and time-frequency feature degradation in current approaches to Ultra-High-Frequency Radio-Frequency Identification (UHF RFID) behavior recognition, this study proposes a novel behavior recognition method integrating multi-feature analysis with a dual-path residual network. The proposed method mitigates interference by using phase difference methods to eliminate signal errors and cross-correlation, as well as adaptive equalization algorithms to decouple interfering signals. To identify the target tags participating in behavioral interactions, we construct a three-dimensional feature space and apply an improved weighted isolated forest algorithm to detect active tags during interactions. Subsequently, Doppler shift analysis extracts behavioral features, and multiscale wavelet-packet decomposition generates time-frequency representations. The dual-path residual network then fuses global and local features from these time-frequency representations for behavioral classification, thereby identifying interaction behaviors such as &#8216;taking away&#8217;, &#8216;putting back&#8217;, and &#8216;hesitation&#8217; (characterized by picking up, then putting back). Experimental results demonstrate that the proposed scheme achieves behavioral recognition accuracy of 94% in complex scenarios, significantly enhancing the overall robustness of interaction behavior recognition.</p></abstract><kwd-group><kwd>behavior recognition</kwd><kwd>dynamic tag detection</kwd><kwd>dual-path residual network</kwd><kwd>isolated forest</kwd><kwd>UHF RFID</kwd></kwd-group><funding-group><award-group><funding-source>Key Research and Development Plan of Shaanxi Province</funding-source><award-id>2024GX-ZDCYL-01-33</award-id><award-id>2024PT-ZCK-25</award-id><award-id>2024CY2-GJHX-63</award-id></award-group><award-group><funding-source>Key Industry Innovation Chain Project of Shaanxi Province</funding-source><award-id>2021ZDLGY07-10</award-id><award-id>2021ZDLNY03-08</award-id></award-group><award-group><funding-source>Science and Technology Plan Project of Shaanxi Province</funding-source><award-id>2022GY-045</award-id></award-group><award-group><funding-source>Scientific Research Program Funded by Shaanxi Provincial Education Department</funding-source><award-id>21JC030</award-id></award-group><award-group><funding-source>Graduate Innovation Fund of Xi&#8217;an University of Posts and Telecommunications</funding-source><award-id>CXJJZL2024004</award-id></award-group><funding-statement>This work is supported by the Key Research and Development Plan of Shaanxi Province (No. 2024GX-ZDCYL-01-33, No. 2024PT-ZCK-25, No. 2024CY2-GJHX-63), the Key Industry Innovation Chain Project of Shaanxi Province (No. 2021ZDLGY07-10, No. 2021ZDLNY03-08), the Science and Technology Plan Project of Shaanxi Province (No. 2022GY-045), Scientific Research Program Funded by Shaanxi Provincial Education Department (Program No. 21JC030), and Graduate Innovation Fund of Xi&#8217;an University of Posts and Telecommunications (No. CXJJZL2024004).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05540"><title>1. Introduction</title><p>Human behavior recognition, as&#160;one of the core technologies of pervasive computing, plays a crucial role in transforming service models in fields such as smart homes, medical monitoring, and retail [<xref rid="B1-sensors-25-05540" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05540" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05540" ref-type="bibr">3</xref>]. Compared with traditional computer vision solutions, which suffer from issues such as light sensitivity and privacy leaks [<xref rid="B4-sensors-25-05540" ref-type="bibr">4</xref>], RFID-based technology offers significant advantages, owing to its properties of non-contact sensing and strong environmental adaptability. In particular, device-free sensing methods [<xref rid="B5-sensors-25-05540" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05540" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05540" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05540" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05540" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05540" ref-type="bibr">10</xref>] achieve behavior recognition by analyzing electromagnetic field disturbances caused by RFID tags deployed in the environment, thereby avoiding user compliance issues associated with wearable devices.</p><p>Existing RFID behavior recognition technology has undergone two evolutionary phases: early wearable solutions require fixed tags on the human body [<xref rid="B11-sensors-25-05540" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05540" ref-type="bibr">12</xref>], leading to limited usability; later device-free methods, while freeing users from physical constraints [<xref rid="B13-sensors-25-05540" ref-type="bibr">13</xref>], still face significant challenges, particularly with respect to signal interference. In particular, in scenarios with dense tag deployment, accurately identifying specific tags that interact with the human body is a critical challenge.</p><p>This paper proposes a novel behavior recognition method for ultra-high-frequency passive RFID systems, aiming to address the technical challenges of severe multipath interference and inaccurate target identification caused by densely distributed tags in crowded environments. The method establishes a recognition framework based on a dual-path residual network with multi-feature fusion. The processing pipeline begins with preprocessing pf the raw phase and RSSI signals using an autocorrelation algorithm and an adaptive filter to effectively suppress multipath effects. Subsequently, an improved isolation forest algorithm is employed to automatically screen and identify dynamic tags. Finally, wavelet transform and the dual-path network are integrated to deeply fuse spatiotemporal features, achieving high-precision and highly robust behavior recognition in complex scenarios. This study provides an effective and scalable technical solution for typical applications such as intelligent security and warehouse management.</p><p>The research scenario is illustrated in <xref rid="sensors-25-05540-f001" ref-type="fig">Figure 1</xref>. By attaching RFID tags to the sides of shelf items and optimizing antenna installation positions slightly above the shelf to avoid obstruction, the system achieves precise recognition of user interaction behaviors such as &#8216;take-away&#8217;, &#8216;put-back&#8217;, and &#8216;hesitation&#8217;.</p><p>The main contributions of this system are summarized below.</p><list list-type="order"><list-item><p>We develop a diversity suppression model to reduce interference with the signal from tag and human characteristics.</p></list-item><list-item><p>We construct a 3D feature space incorporating the IQR temporal fluctuation index, cumulative phase difference, and tag-state evaluation metrics, combined with an improved isolation forest algorithm, to achieve precise dynamic tag detection.</p></list-item><list-item><p>We develop a behavior recognition model to segment behavior streams and use Doppler frequency analysis to identify specific behavior characteristics and changes. We also construct a time-frequency map based on wavelet transform to illustrate energy changes in behavior and propose a dual-path residual network to recognize behaviors such as taking away, putting back, picking up, and putting back.</p></list-item><list-item><p>We develop an optimization engine for marketing strategies in a retail scenario to convert behavior recognition results into actionable business decisions.</p></list-item></list><p>The remainder of this paper is organized as follows. <xref rid="sec2-sensors-25-05540" ref-type="sec">Section 2</xref> introduces related work. <xref rid="sec3-sensors-25-05540" ref-type="sec">Section 3</xref> introduces an overview of the proposed system. <xref rid="sec4-sensors-25-05540" ref-type="sec">Section 4</xref> introduces the specific implementation of the system. <xref rid="sec5-sensors-25-05540" ref-type="sec">Section 5</xref> proposes marketing strategies for human pattern analysis and comprehensively evaluates the performance of the proposed system.</p></sec><sec id="sec2-sensors-25-05540"><title>2. Related Work</title><p>Related research has mainly focused on multipath effect mitigation, Doppler shift-based motion recognition, and deep learning behavior recognition methods.</p><sec id="sec2dot1-sensors-25-05540"><title>2.1. Autocorrelation Method to Mitigate Multipath Effects</title><p>Existing research on RFID often inadequately addresses the multipath effect, resulting in raw signals (such as phase and RSSI) containing substantial noise caused by environmental and human reflections, which leads to a low signal-to-noise ratio and directly compromises the accuracy of subsequent analysis. Multipath effects are a common challenge in wireless communications, and mature autocorrelation suppression techniques have been developed in the GNSS field. Reference [<xref rid="B14-sensors-25-05540" ref-type="bibr">14</xref>] proposed a multipath suppression technique based on a partial autocorrelation function to suppress short-delay multipath signals. Reference [<xref rid="B15-sensors-25-05540" ref-type="bibr">15</xref>] focused on multi-GNSS research, proposed a unified parameter suppression method based on autocorrelation, and verified its effectiveness in terms of the correlation between different GNSS frequencies and the validity of the unified parameters. These methods provide important insights for RFID research. On the one hand, partial autocorrelation techniques can be directly applied to suppress short-delay interference; on the other hand, the correlation between multi-frequency RFID systems can be explored to develop unified parameter multipath suppression schemes, thereby simplifying signal processing workflows in multi-frequency environments.</p></sec><sec id="sec2dot2-sensors-25-05540"><title>2.2. Behavior Recognition Based on Doppler Frequency Shift</title><p>Doppler frequency-shift curves are of significant value in human behavior analysis, as they can capture shifts in signal frequency caused by human movement, thereby distinguishing between different actions/behaviors. For example, RF-RES [<xref rid="B16-sensors-25-05540" ref-type="bibr">16</xref>] uses Doppler frequency shift to track chest displacement caused by breathing; RFPass [<xref rid="B17-sensors-25-05540" ref-type="bibr">17</xref>] captures Doppler frequency shift and utilizes gait features for user authentication; Tag-Fall [<xref rid="B18-sensors-25-05540" ref-type="bibr">18</xref>] combines Doppler frequency shift, velocity, and position information to determine falls; FEMO [<xref rid="B19-sensors-25-05540" ref-type="bibr">19</xref>] uses Doppler frequency shift to identify free-weight exercise types; CBID [<xref rid="B20-sensors-25-05540" ref-type="bibr">20</xref>] uses the Doppler effect to detect tag movement for shopping behavior recognition; the method proposed by Chen et al. [<xref rid="B21-sensors-25-05540" ref-type="bibr">21</xref>] uses RFID tags attached to key skeletal nodes to identify various human activities through Doppler frequency shift, RSSI, and phase information; and FGSA [<xref rid="B22-sensors-25-05540" ref-type="bibr">22</xref>] uses Doppler frequency shift with dual-tag phase calibration to identify shopping behavior. However, most of the aforementioned studies are limited to ideal scenarios (e.g., with a sparse number of tags or known target tags), making it difficult to automatically identify moving targets in dense tag environments. The dynamic tag detection method proposed in this paper, combined with Doppler shift analysis, enables autonomous screening of moving tags and behavior recognition in complex scenarios, significantly improving the system&#8217;s universality and environmental adaptability.</p></sec><sec id="sec2dot3-sensors-25-05540"><title>2.3. Behavior Recognition Based on Deep Learning</title><p>In recent years, deep learning has garnered significant attention in the field of human behavior recognition and has been integrated with classical machine learning to enhance model robustness and generalization capabilities. For example, SATCN [<xref rid="B23-sensors-25-05540" ref-type="bibr">23</xref>] combines TCN and self-attention mechanisms to improve the accuracy of RFID-based indoor human behavior recognition; TSCNN [<xref rid="B24-sensors-25-05540" ref-type="bibr">24</xref>] uses 3D convolutional neural networks to extract spatio-temporal features from RFID RSSI data for behavior classification; the method proposed in Ref. [<xref rid="B25-sensors-25-05540" ref-type="bibr">25</xref>] comprises a non-wearable human motion recognition scheme based on STGCN integrating phase and RSSI data; and the TagSee [<xref rid="B26-sensors-25-05540" ref-type="bibr">26</xref>] system uses single-antenna RFID imaging and deep learning to analyze customer blocking signals, enabling contactless customer behavior analysis. Ding et al. [<xref rid="B27-sensors-25-05540" ref-type="bibr">27</xref>] validated the effectiveness of multimodal feature fusion in human interaction behavior classification by coupling a 2D-CNN with ResNet. RFnet [<xref rid="B28-sensors-25-05540" ref-type="bibr">28</xref>] applies a dual-path structure to RFID temporal signal processing, using the spatial residual path to capture static poses and the temporal residual path to model dynamic evolution. Building upon the aforementioned deep learning approaches, this paper proposes a dual-path residual network-based method for action recognition. By integrating both global and local motion features, it achieves complementary enhancement of motion information, thereby improving the accuracy of action recognition.</p></sec></sec><sec id="sec3-sensors-25-05540"><title>3. System Overview</title><p>The core research process is shown in <xref rid="sensors-25-05540-f002" ref-type="fig">Figure 2</xref> and includes the following:<list list-type="simple"><list-item><label>(1)</label><p>Preprocessing of labeled data using phase unwrapping and smoothing filtering;</p></list-item><list-item><label>(2)</label><p>Diversity suppression to effectively reduce tag hardware characteristics and environmental and human interference;</p></list-item><list-item><label>(3)</label><p>Dynamic tag detection, using a three-dimensional feature matrix containing IQR time-domain fluctuations, cumulative phase difference phase-domain dynamics, and tag-state evaluation indicators, combined with an improved isolation forest algorithm for dynamic tag detection;</p></list-item><list-item><label>(4)</label><p>Behavior recognition utilizing Doppler frequency to analyze behavior characteristics, combined with wavelet transform to generate time-frequency maps of behavior energy features and a dual-path residual network for behavior classification;</p></list-item><list-item><label>(5)</label><p>Analysis of key indicators, such as the number of times tags are picked up or put back and the duration of behavior, to analyze human behavior patterns.</p></list-item></list></p></sec><sec id="sec4-sensors-25-05540"><title>4. System Design</title><sec id="sec4dot1-sensors-25-05540"><title>4.1. Data Preprocessing</title><p>Phase signals have periodic characteristics that can easily lead to phase entanglements during measurement [<xref rid="B29-sensors-25-05540" ref-type="bibr">29</xref>]. Therefore, phase disentanglement is used to restore the true phase-change trajectory, which can be achieved by using (<xref rid="FD1-sensors-25-05540" ref-type="disp-formula">1</xref>).<disp-formula id="FD1-sensors-25-05540"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#981;</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="sans-serif">&#960;</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the unwrapped phase (rad), <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#981;</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the wrapped measurement (rad), and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> counts the <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> phase jumps.</p><p>As shown in <xref rid="sensors-25-05540-f003" ref-type="fig">Figure 3</xref>, the disentangled phase signal and the simultaneously obtained RSSI signal were filtered and smoothed using a Savitzky&#8211;Golay filter to effectively reduce noise.</p></sec><sec id="sec4dot2-sensors-25-05540"><title>4.2. Feature Diversity Suppression</title><p>The hardware characteristics of tags not only determine their sensitivity to the environment but also directly affect the signal transmission efficiency and reliability. Simultaneously, differences in human height and weight can alter signal propagation paths and intensity, affecting signal stability and accuracy.</p><sec id="sec4dot2dot1-sensors-25-05540"><title>4.2.1. Tag Hardware Feature Suppression</title><p>As shown in <xref rid="sensors-25-05540-f004" ref-type="fig">Figure 4</xref>a, although most static tags exhibit stable characteristics, some exhibit abnormal fluctuations. To improve the accuracy of environmental perception, it is necessary to effectively suppress the inherent characteristics of the tags. During the static detection phase, the phase information (<inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>) of sample <italic toggle="yes">j</italic> of the <italic toggle="yes">i</italic>-th tag includes the influence of environmental factors and tag characteristics.</p><p>The RF phase measurement model is expressed as follows:<disp-formula id="FD2-sensors-25-05540"><label>(2)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>&#955;</mml:mi></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>tag</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mspace width="0.277778em"/><mml:mo form="prefix">mod</mml:mo><mml:mspace width="0.277778em"/><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The signal travels a round-trip path (from the reader to the tag and back to the reader), so the actual propagation distance is <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, resulting in a phase change that is twice that of a one-way trip. Hence, the coefficient is <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> is the signal wavelength, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the distance between the <italic toggle="yes">j</italic>-th sample of the <italic toggle="yes">i</italic>-th tag and the antenna (m). The first part reflects the phase delay of the electromagnetic wave along the propagation path. <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>tag</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the intrinsic phase offset of the tag hardware (rad), and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> captures the environmental phase noise with <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The phase value <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of the <italic toggle="yes">j</italic>-th sample of the <italic toggle="yes">i</italic>-th tag during the behavior process includes the characteristics of the environment and tags, as well as the influence of the human hand.<disp-formula id="FD3-sensors-25-05540"><label>(3)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>&#955;</mml:mi></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>tag</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>hand</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mspace width="0.277778em"/><mml:mo form="prefix">mod</mml:mo><mml:mspace width="0.277778em"/><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>hand</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is hand-induced phase perturbation (rad).</p><p>Environmental and tag-specific influences are eliminated by subtracting the static tag means, yielding the adjusted dynamic phase (<inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>).<disp-formula id="FD4-sensors-25-05540"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow><mml:mi>&#955;</mml:mi></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>hand</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mspace width="0.277778em"/><mml:mo form="prefix">mod</mml:mo><mml:mspace width="0.277778em"/><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">Z</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the distance for the <italic toggle="yes">i</italic>-th tag (m) and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">Z</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is a set of positive integers.</p><p>As shown in <xref rid="sensors-25-05540-f004" ref-type="fig">Figure 4</xref>b, the starting points of the processed phase signals are aligned, which greatly improves the comparability of the data and the accuracy of the subsequent analysis.</p></sec><sec id="sec4dot2dot2-sensors-25-05540"><title>4.2.2. Suppression of Human Characteristics</title><p>To address the issue of multipath interference caused by the human body in wireless signals, this study proposes a joint suppression scheme integrating partial autocorrelation estimation (PACE ) and adaptive equalization. The scheme first uses the PACE algorithm to extract the delay and amplitude characteristics of the multipath signals, thereby constructing a multipath interference model and achieving signal reconstruction. Subsequently, a variable-step-length LMS adaptive equalizer is employed to suppress residual interference, and finally, polarity correction is applied to maintain the signal integrity. As shown in <xref rid="sensors-25-05540-f005" ref-type="fig">Figure 5</xref>a, the originally fluctuating signal error of the static tag was mitigated, resulting in a smoother state. As shown in <xref rid="sensors-25-05540-f005" ref-type="fig">Figure 5</xref>b, the phase data of the processed dynamic tag still clearly shows the fluctuation characteristics caused by this behavior.</p></sec></sec><sec id="sec4dot3-sensors-25-05540"><title>4.3. Dynamic Tag Detection</title><sec id="sec4dot3dot1-sensors-25-05540"><title>4.3.1. Volatility Analysis</title><p>After feature diversity suppression, the phase data were unified and aligned to zero, eliminating the original scale differences. However, as shown in the box plot in <xref rid="sensors-25-05540-f006" ref-type="fig">Figure 6</xref>a, the phase data still exhibits significant variability during the behavioral process. To robustly quantify this variability, the study employed the interquartile range (IQR). The IQR provides a more robust measure of the core dispersion of data than the standard deviation, particularly for highly variable phase data containing outliers. As shown in <xref rid="sensors-25-05540-f006" ref-type="fig">Figure 6</xref>b, there were notable differences in the IQR values across different tags. Higher IQR values indicate greater variability in tag data, indicating more pronounced changes in phase features during behavioral processes.</p></sec><sec id="sec4dot3dot2-sensors-25-05540"><title>4.3.2. Phase Characteristic Analysis</title><p>When a tag is moved, changes in its position cause alterations in the signal propagation path, leading to fluctuations in phase data. Dynamic tags exhibit significant phase variation characteristics. By calculating the cumulative phase difference, the system can amplify the effects of small but continuous or intense but transient phase changes caused by human movement of the tag. Therefore, monitoring the cumulative effects of phase changes can quantify tag movement caused by human operation.</p><p>The grayscale image of the cumulative phase difference values shown in <xref rid="sensors-25-05540-f007" ref-type="fig">Figure 7</xref> indicates that the areas of the tag affected by human movement typically exhibit larger cumulative phase difference values. Consequently, the areas of the tag influenced by human movement are clearly depicted in darker colors.</p></sec><sec id="sec4dot3dot3-sensors-25-05540"><title>4.3.3. Analysis of the Human Body&#8217;s Impact on Signal Characteristics</title><p>In this study, the space between the tag and antenna is modeled as the first Fresnel zone. Under ideal line-of-sight (LOS) propagation conditions, there are no obstacles between the tag and antenna, and signal attenuation is primarily due to free-space loss. In practical applications, the human hand enters the first Fresnel zone, resulting in non-line-of-sight (NLOS) propagation. As shown in <xref rid="sensors-25-05540-f008" ref-type="fig">Figure 8</xref>a, when the hand is located at boundary position H of the first Fresnel zone, the signal emitted by the antenna is blocked by the hand before reaching the tag, resulting in signal attenuation.</p><p>Therefore, the actual received signal is the signal blocked by the hand (NLOS) and the signal transmitted by the tag in free space (LOS); thus, the true received signal is <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>real</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD5-sensors-25-05540"><label>(5)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>real</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>los</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>nlos</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where the <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>los</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> component represents free-space propagation and the <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>nlos</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> component represents the hand-blocked signal.</p><p>Human hand movements can interfere with tag signals. To quantify and analyze their impact on the tag array, a theoretical spatial transmission power loss model was established, as shown in <xref rid="sensors-25-05540-f008" ref-type="fig">Figure 8</xref>b.</p><p>Therefore, the LOS signal <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:msub><mml:mi>los</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and NLOS <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:msub><mml:mi>nlos</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> signal of the <italic toggle="yes">i</italic>-th tag in the tag array are affected by the hand.<disp-formula id="FD6-sensors-25-05540"><label>(6)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>los</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05540"><label>(7)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>nlos</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-05540"><label>(8)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>&#952;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the signal transmitted directly from the antenna to the <italic toggle="yes">i</italic>-th tag, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the signal transmitted from the antenna that is blocked by the hand, <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the signal transmitted to the i-th tag after being obstructed by the hand, and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> is channel attenuation and its phase shift.</p><p>When the signal is transmitted in free space, assuming there is no other interference (<inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), the phase shift (<inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) is related to the reader&#8217;s operating frequency. Then, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is represented as follows:<disp-formula id="FD9-sensors-25-05540"><label>(9)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the antenna-to-hand scenario, the hand is treated as an environmental obstacle, and its overall attenuation effect on the signal can be approximated as a constant value (<italic toggle="yes">C</italic>). In the hand-to-tag scenario, the hand dynamically influences the tag, and subtle hand movements can cause significant variations in the tag&#8217;s signal data; therefore, the attenuation (<inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:msub><mml:mi>HT</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>) in signal transmission is approximated as <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>HT</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>AH</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the phase shift caused by the antenna transmitting to the hand. <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:msub><mml:mi>HT</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> represents the phase shift caused by the signal returning from hand to tag. Then, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are represented as follows: <disp-formula id="FD10-sensors-25-05540"><label>(10)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>&#183;</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>AH</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05540"><label>(11)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:msub><mml:mi>HT</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:msub><mml:mi>&#952;</mml:mi><mml:msub><mml:mi>HT</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Therefore, the power loss for the <italic toggle="yes">i</italic>-th tag is obtained as follows:<disp-formula id="FD12-sensors-25-05540"><label>(12)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>hand</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#8594;</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>HT</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In (<xref rid="FD12-sensors-25-05540" ref-type="disp-formula">12</xref>), it can be seen that the tag&#8217;s reflection power is affected by the distance between the hand and the tag. Because RSSI is a measure of the received power, the likelihood of the tag being picked up can be assessed by comparing the predicted value from the power correlation model with the actual RSSI. The Pearson correlation coefficient (I) was used to quantify the consistency between the two variables.<disp-formula id="FD13-sensors-25-05540"><label>(13)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:munderover><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfrac></mml:mstyle></mml:mfenced><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of samples for a single tag, <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the power model for the tag, <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> are the sample means of <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> are the standard sample deviations of <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>As shown in <xref rid="sensors-25-05540-f009" ref-type="fig">Figure 9</xref>, the probability of picking up different tags was quantified.</p></sec><sec id="sec4dot3dot4-sensors-25-05540"><title>4.3.4. Dynamic Tag Detection</title><p>Accurately locating dynamic tags in dense RFID environments is essential for passive behavior recognition. To address this, we propose a tag-state evaluation method that integrates multi-feature fusion with an improved isolation forest algorithm. The approach begins by constructing a probabilistic evaluation metric based on a spatial transmission power loss model, combined with time-domain features (IQR and cumulative phase deviation), to achieve quantitative assessment of tag states. This provides high-quality input for the subsequent improved isolation forest process. The feature vector of the <italic toggle="yes">i</italic>-th tag can be represented as <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD14-sensors-25-05540"><label>(14)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#8942;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the status evaluation index, cumulative phase difference, and IQR, respectively, for the <italic toggle="yes">k</italic>-th tag.</p><p>In the dynamic tag detection phase, <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is used as the input, and the weighted isolated forest (W-IForest) algorithm is employed, which detects dynamic tags as outliers. he pseudocode of this algorithm is presented as Algorithm 1.</p><p>The core innovation of this algorithm lies in transforming dynamic tag detection into a weighted anomaly detection problem. By quantifying the ratio of signal dynamic range to volatility and the motion activity level of tags, the algorithm achieves weighted processing of IQR, cumulative phase difference, and signal correlation. During the construction of isolation trees, it prioritizes the splitting of feature dimensions with higher weights. The&#160;algorithm calculates anomaly scores through weighted path length, significantly enhancing the detection sensitivity for dynamic tags. This method markedly improves the accuracy and real-time performance of RFID dynamic tag detection in complex environments.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1:</bold>&#160;Weighted Isolation Forest (W-IF).</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><label><bold>Require:</bold>&#160;</label><p>Phase signals <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, RSSI <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, Timestamps <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> Initial contamination <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>init</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label><bold>Ensure:</bold>&#160;</label><p>Anomalous tags <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>anomaly</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with confidence scores</p></list-item><list-item><label>1:</label><p><bold>for</bold> each tag <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>2:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8592;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>std</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#949;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>3:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>motion_ratio</mml:mi></mml:mrow><mml:mo>&#8592;</mml:mo><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>diff</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>thresh</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>len</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>4:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>IQR</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>cumsum</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>diff</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>corr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>5:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>weighted</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.3</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>motion_ratio</mml:mi><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>6:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>sample</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:mi>motion_ratio</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>var</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>7:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Store</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>weighted</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>sample</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>8:</label><p><bold>end for</bold></p></list-item><list-item><label>9:</label><p><bold>for</bold>&#160;<inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>estimators</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>10:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:mi>BuildTree</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Features</mml:mi><mml:mo>,</mml:mo><mml:mi>Weights</mml:mi><mml:mo>,</mml:mo><mml:mi>strategy</mml:mi><mml:mo>=</mml:mo><mml:mo>&#8216;</mml:mo><mml:mi>weighted_gain</mml:mi><mml:mo>&#8217;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>11:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>AnomalyScores</mml:mi><mml:mo>&#8592;</mml:mo><mml:mi>AnomalyScores</mml:mi><mml:mo>+</mml:mo><mml:mi>ScoreSamples</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>12:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>adj</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>init</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>mean</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Weights</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>13:</label><p>&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>anomaly</mml:mi></mml:msub><mml:mo>&#8592;</mml:mo><mml:mi>FilterScores</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>AnomalyScores</mml:mi><mml:mo>,</mml:mo><mml:mi>percentile</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>adj</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>14:</label><p><bold>end for</bold></p></list-item><list-item><label>15:</label><p><bold>return</bold>&#160;<inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>anomaly</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item></list></td></tr></tbody></array></p></sec></sec><sec id="sec4dot4-sensors-25-05540"><title>4.4. Behavioral Recognition&#160;Model</title><sec id="sec4dot4dot1-sensors-25-05540"><title>4.4.1. Behavioral&#160;Analysis</title><p>Following the detection of dynamic tags, the target tags were identified. For accurate analysis of consecutively occurring behaviors, this paper employs a behavior flow segmentation method based on a sliding window. Initially, this method constructs adaptive thresholds by calculating the mean and standard deviation of phase differences within the window, thereby achieving preliminary segmentation. To further enhance the robustness and accuracy of segmentation, the following post-processing steps are introduced: The integration of a minimum amplitude threshold and a minimum duration threshold effectively suppresses noise interference and filters out non-significant instantaneous phase changes. Long-interval detection ensures the completeness of behavior events, preventing premature segmentation due to brief interruptions. Peak point detection precisely captures behavior turning points, such as key moments of picking up or putting down. By synthesizing the aforementioned filtering conditions, continuous phase data is successfully segmented into a series of independent behavior fragments (as depicted in <xref rid="sensors-25-05540-f010" ref-type="fig">Figure 10</xref>). These segmented fragments clearly characterize specific behavior events. <xref rid="sensors-25-05540-t001" ref-type="table">Table 1</xref> showcases the specific behavior of each fragment. Based on the properties of the phase, during the process of human movement of the tag, the distance between the tag and the antenna continuously changes, which induces a change in phase, according to (<xref rid="FD2-sensors-25-05540" ref-type="disp-formula">2</xref>). Specifically, as illustrated in <xref rid="sensors-25-05540-f011" ref-type="fig">Figure 11</xref>, during the process of the tag being removed from the shelf (assuming the tag moves horizontally away from the antenna along the X-axis at this time), the distance (d) between the tag and the antenna gradually increases, leading to an increase in phase until the tag is moved outside the antenna&#8217;s reading range. Conversely, when the tag is put back on the shelf (moving horizontally towards the antenna along the X-axis), the signal re-emerges, and the distance (d) between the tag and the antenna gradually decreases, causing a corresponding change in phase. Therefore, during the process of the tag being horizontally taken away, then put back, the distance between the tag and the antenna undergoes a progression from far to near, followed by a return to stability. For vertical pick-up and put-down actions (e.g., the tag is lifted from a surface, then placed back onto the same surface), although the change in horizontal position may not be significant, the vertical distance between the tag and the antenna experiences a substantial variation. Specifically, when the tag is picked up, the distance between the tag and the antenna decreases, and the phase consequently reduces; when the tag is put down, the distance between the tag and the antenna increases, and the phase correspondingly increases.</p><p>To specifically analyze the relative changes between tags and antennas caused by human behavior, this study introduces the Doppler effect as a key analytical tool. The relative motion between the tag and the antenna causes an offset in the signal frequency. This offset in frequency change is known as the Doppler frequency shift. By accurately capturing and quantifying this Doppler frequency shift, we can indirectly reflect the relative motion state between the tag and the antenna, thereby identifying and distinguishing different behavioral patterns.</p><p>The Doppler shift arises from the relative motion between the transmitter and receiver. In this scenario, it occurs between a stationary reader and a moving tag. Assuming the reader receives two consecutive signals from the moving tag at times <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, with phase readings of <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, respectively, let <italic toggle="yes">v</italic> represent the tag&#8217;s velocity during time interval <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Since the time interval between consecutive tag readings is very short, <italic toggle="yes">v</italic> can be considered constant. Therefore, the distance (<italic toggle="yes">d</italic>) moved by the tag equals <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#183;</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>In backscatter communication, the signal propagation distance is twice the actual movement distance (2<italic toggle="yes">d</italic>). Combining this with Equation (<xref rid="FD2-sensors-25-05540" ref-type="disp-formula">2</xref>), we obtain the following:<disp-formula id="FD15-sensors-25-05540"><label>(15)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi><mml:mo>&#183;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>v</mml:mi><mml:mo>&#183;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The change in this path difference leads to a corresponding shift in phase, from which the Doppler frequency shift can be derived by analyzing the rate of phase change over time. The quantification of the Doppler effect is presented in (<xref rid="FD16-sensors-25-05540" ref-type="disp-formula">16</xref>).<disp-formula id="FD16-sensors-25-05540"><label>(16)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#957;</mml:mi><mml:mi>&#955;</mml:mi></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#952;</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi><mml:mo>&#183;</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#952;</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the phase difference of the <italic toggle="yes">i</italic>-th tag after suppression of tag diversity and <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the time difference of the <italic toggle="yes">i</italic>-th tag.</p><p>Therefore, by utilizing the Doppler frequency variation over time, we can obtain the relative changes between the tag and the reader and analyze behavioral characteristics.Three primary motion behavior patterns can be identified. As illustrated in <xref rid="sensors-25-05540-f012" ref-type="fig">Figure 12</xref>, the tag starts moving at point A. The patterns of take away&#8221; and &#8220;put back&#8221; and the combined actions of &#8220;pick up&#8221; and &#8220;put down&#8221; all exhibit unique and distinguishable change characteristics.</p><p>Compared to the traditional Doppler behavior analysis method introduced in <xref rid="sec2-sensors-25-05540" ref-type="sec">Section 2</xref>, which can only analyze known static tags in a controlled environment, the analysis technology proposed in this study combining dynamic tag detection with Doppler has significant advantages: it can not only accurately identify target tags and achieve precise behavior analysis but also work stably in complex environments with dense tags. Through the aforementioned tag detection algorithm, the system can distinguish between static and dynamic tags, effectively filter environmental interference, and significantly enhance its applicability and reliability in real-world scenarios.</p></sec><sec id="sec4dot4dot2-sensors-25-05540"><title>4.4.2. CWT-Frequency Analysis of Behavioral&#160;Characteristics</title><p>To illustrate behavioral characteristics more intuitively, this study a employs continuous wavelet transform (CWT) combined with Morlet wavelet basis functions for time-frequency feature extraction of Doppler signals. By precisely calculating the actual frequency range of the signal, an appropriate scale sequence is constructed, and a time-frequency plot is generated that clearly illustrates the process of signal energy variation over time. The plot shown in <xref rid="sensors-25-05540-f013" ref-type="fig">Figure 13</xref> visually represents the distribution of the signal energy through color changes, with darker regions corresponding to the occurrence of actions or changes in intensity, effectively revealing the dynamic characteristics within the behavioral segmentation intervals. The intensity of time-frequency energy directly reflects the degree of change in behavioral characteristics, providing a visual basis for action analysis.</p></sec><sec id="sec4dot4dot3-sensors-25-05540"><title>4.4.3. Behavior Recognition Model Based on Dual-Path Residual&#160;Network</title><p>To achieve behavior recognition, this study employs the Dual-Path Res-Net model to enable end-to-end learning and recognition of behavioral features. As shown in the model diagram in <xref rid="sensors-25-05540-f014" ref-type="fig">Figure 14</xref>, the proposed model adopts a dual-modal parallel processing architecture.</p><p><bold>Main Path:</bold> This path focuses on extracting the global spectral features of the behavior. By processing the wavelet time-frequency map using ResNet18 and extracting 512-dimensional global spectral features through four residual blocks, it captures macro patterns in the time-frequency domain.</p><p><bold>Edge Path:</bold> This path focuses on capturing the local dynamic edge information of the behaviors. We applied adaptive edge detection to preprocessed data (grayscale/Gaussian filtering) and extracted local dynamic features using a lightweight CNN.</p><p><bold>Feature Fusion and Classification:</bold> The features extracted from both paths were concatenated in the fusion layer to form a 6784-dimensional comprehensive feature vector. This vector was then input into the fully connected layer to predict the behavior categories.</p><p><bold>Model Optimization and Training:</bold> To optimize the model&#8217;s performance, this study employs an SGD optimizer combined with cross-entropy loss for training, utilizing hyperparameter search and early stopping mechanisms to optimize the model, ultimately achieving high-precision end-to-end behavior recognition.</p></sec></sec></sec><sec id="sec5-sensors-25-05540"><title>5. System Implementation and&#160;Evaluation</title><p>This section proposes marketing strategies tailored for human behavior pattern analysis, with a comprehensive experimental evaluation conducted to assess performance.</p><sec id="sec5dot1-sensors-25-05540"><title>5.1. Human Behavior Pattern&#160;Analysis</title><p>This study breaks through the limitations of traditional sales data analysis by accurately recording key indicators such as the number of interactions between customers and goods, the duration of interactions, and interaction behavior, thereby constructing a multi-dimensional behavioral analysis system. This method not only enables real-time tracking of changes in the status of goods on shelves but also captures behavioral characteristics in the customer decision-making process with respect to purchasing, providing more comprehensive and accurate data support for retail behavior analysis.</p><p>Based on the statistics shown in <xref rid="sensors-25-05540-f015" ref-type="fig">Figure 15</xref> and <xref rid="sensors-25-05540-t002" ref-type="table">Table 2</xref>, regarding the number of times products were picked up and their seasonal changes from January to June, retailers can adopt a refined strategy: for products with high pick-up frequencies and strong sales performance, they should increase inventory levels and place them in core promotional areas to ensure supply and maximize sales; for products with lower pick-up frequencies and poor performance, they should employ diversified promotional tactics (such as discounts) and re-plan their shelf locations, making them more visible and accessible to enhance exposure and stimulate purchases, ultimately achieving efficient store resource allocation and improved sales performance.</p><p>The study precisely identifies the moment when customers pick up and put back items, indicating the start and end of the behavior. As shown in <xref rid="sensors-25-05540-f016" ref-type="fig">Figure 16</xref>, on 15 November 2024, at 21:03:39, a certain tag was put down, and at 21:03:42, the tag was picked up, indicating that the customer interacted with the product for three seconds, thereby quantifying the customer&#8217;s interest in the product.</p><p>By analyzing customers&#8217; contact frequency with products, holding duration, and behavioral sequences, this method deeply decodes product attractiveness and purchase-decision barriers. High-frequency, brief interactions indicate price sensitivity or hesitation in the product style. Such behavioral characteristics can guide retailers to optimize merchandise strategies (price adjustments and style iterations). This approach provides a quantifiable decision-making basis for consumer behavior analysis.</p></sec><sec id="sec5dot2-sensors-25-05540"><title>5.2. Experimental&#160;Evaluation</title><sec id="sec5dot2dot1-sensors-25-05540"><title>5.2.1. Experimental&#160;Setup</title><p>This study was conducted using a COTS UHF Impinj R420 RFID reader and multiple AZ-9662 tags, operating at a fixed frequency of 920.625 MHz and compatible with the EPC Global C1G2 standard, using a circularly polarized antenna with dimensions of 25 &#215; 25 cm and a gain of 9 dBi. When the reader queries the deployed tags, information including the EPC, timestamp, channel frequency, antenna ID, RSSI value, and phase value is transmitted to the host computer via Ethernet. Data is read using Item Test software and statistically analyzed using Python software. The experimental parameters are set as shown in <xref rid="sensors-25-05540-t003" ref-type="table">Table 3</xref>.</p><p>An experimental setup was constructed to validate the performance of the algorithm, as shown in <xref rid="sensors-25-05540-f017" ref-type="fig">Figure 17</xref>. The antennas were deployed at an appropriate height above the shelves to ensure effective coverage and a comprehensive reading of all tags on the shelves. Ten items with tags were placed on the second and third layers of the shelves, with tags affixed to the sides of the items. In the experiment, a human subject entered the area formed by the antennas and tags in any manner and performed actions such as taking away, putting back, picking up, and putting down.</p></sec><sec id="sec5dot2dot2-sensors-25-05540"><title>5.2.2. Evaluation of Experimental&#160;Results</title><p>To evaluate the robustness of the algorithm in handling human body diversity, this study designed and conducted a user experiment. Three volunteers with distinct body types were recruited to perform interactive behaviors, following their natural habits while reader data was collected. The raw data, after only phase unwrapping, is shown in <xref rid="sensors-25-05540-f018" ref-type="fig">Figure 18</xref>. It can be observed that the initial phase points vary significantly across different volunteers, and adjacent tags (such as A229 and A225) exhibit abnormal fluctuations, which severely interfere with the interpretation of the target interactive behavior.</p><p>Therefore, the previously described diversity suppression method was applied to process the data: first, hardware calibration was used to unify the initial phase, eliminating baseline differences and the effects caused by tag hardware characteristics, as shown in <xref rid="sensors-25-05540-f019" ref-type="fig">Figure 19</xref>; then, to address interference from nearby tags due to hand movements, a human feature-based suppression algorithm was employed, effectively highlighting the phase variation patterns of the target tags (A226, A336, and A223). The processed signals shown in <xref rid="sensors-25-05540-f020" ref-type="fig">Figure 20</xref> significantly reduce the difficulty of target tag identification and provide a clear and reliable data foundation for subsequent interactive behavior analysis. The experimental results verify that the algorithm maintains high accuracy and stability across users with different body types.</p><p>Subsequently, as shown in <xref rid="sensors-25-05540-f020" ref-type="fig">Figure 20</xref>, to address interference from hand movements associated with adjacent tags(A229/A225, etc.), a human-body characteristic suppression algorithm was applied. This approach effectively enhanced the distinctive phase variations of the target tags (A226, A336, and A223), significantly facilitating their accurate identification.</p><p>This study input the optimized three-dimensional feature matrix into an improved weighted isolated forest model to identify A226, A336, and A223 as dynamic tags (<xref rid="sensors-25-05540-f021" ref-type="fig">Figure 21</xref>), where the red &#8220;&#215;&#8221; mark indicates a dynamic tag and the &#8220;o&#8221; mark indicates a static tag.</p><p>Based on this, combined with Doppler frequency shift analysis, the experiment accurately captured the Doppler feature changes caused by the interactive behaviors of the three volunteers at different time points, as shown in <xref rid="sensors-25-05540-f022" ref-type="fig">Figure 22</xref>, with the behavior flow of the volunteers segmented into corresponding action segments.</p><p>The Doppler-based behavioral analysis methods described in the FEMO [<xref rid="B19-sensors-25-05540" ref-type="bibr">19</xref>] and ShopMiner [<xref rid="B13-sensors-25-05540" ref-type="bibr">13</xref>] literature typically depend on data analysis of specific target tags. In contrast, our proposed system implements tag diversity suppression and dynamic tag detection mechanisms, enabling effective recognition and analysis of arbitrary pick-up or put-down actions without prior knowledge of tag states. This breakthrough provides a more flexible and universal solution for fine-grained behavioral perception in unconstrained scenarios.</p><p>Finally, to validate the effectiveness of the proposed Dual-Path Res-Net in behavior recognition tasks, an experimental dataset was constructed using data collected from 30 participants with different physical characteristics (height, weight, and behavioral habits), yielding a total of 900 complete behavior flow samples covering three typical types of shelf interaction behaviors.</p><p>As shown in <xref rid="sensors-25-05540-f023" ref-type="fig">Figure 23</xref>a, this model exhibits outstanding performance in recognizing three types of actions, achieving an overall accuracy of 0.94 and an F1 score between 0.94 and 0.95. Among these, the F1 scores for the actions of putting back and taking away both exceed 0.94, and the accuracy for the actions of picking up and putting down is 1.00, although the recall rate is slightly lower (0.84). <xref rid="sensors-25-05540-f023" ref-type="fig">Figure 23</xref>b indicates that the primary misclassifications involve taking away being incorrectly classified as picking up and putting down, along with some misclassification of the putting-back action. This is due to inaccurate boundary determination in behavior segmentation, which leads to distortion of the input data. Segmentation errors not only affect endpoint localization but also decrease the effective training samples for picking up and putting down, resulting in a lower recall rate and increased classification confusion. Future work will focus on optimizing the segmentation algorithm, particularly improving boundary determination accuracy and segmentation robustness.</p><p>This study validates the superior performance of the dual-path residual network in behavior recognition through comparative experiments. As shown in the left subplot of <xref rid="sensors-25-05540-f024" ref-type="fig">Figure 24</xref>, the model achieves a macro-averaged F1 score of 0.94 (an 8.2%improvement over the single-path model) and an accuracy of 95%(a 5.3%improvement compared to the CNN baseline). The right subplot of <xref rid="sensors-25-05540-f024" ref-type="fig">Figure 24</xref> presents the ROC curve, which demonstrates an AUC value of 0.93 and maintains a true-positive rate of 89%at a false-positive rate of 0.2. By synergistically integrating global spectral features and local edge information through its residual and attention pathways, the model achieves an interaction recognition accuracy of 95%while reducing the false-positive rate by 40%under challenging conditions (FPR &lt; 0.3).</p></sec></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05540"><title>6. Conclusions</title><p>This study proposes a novel UHF RFID-based behavior recognition method integrating dual-path residual networks with multi-feature fusion. Addressing the challenge of the vulnerability of UHF RFID signals to interference, this research innovatively employs static baseline subtraction, PACE analysis, and adaptive equalization techniques to effectively suppress environmental noise, human-body interference, and multipath effects. An improved isolation forest model accurately detects dynamic tags, whereas Doppler shift analysis captures behavioral characteristics by converting the acquired time-frequency information into wavelet time-frequency maps. The dual-path residual network subsequently performs deep feature learning, significantly enhancing recognition performance through the fusion of global spectral and local edge features. Experimental validation with 30 participants demonstrated the system&#8217;s reliability and practicality, achieving 95% recognition accuracy with an F1 score of 0.94. This technology provides an innovative solution for implicit user behavior perceptions in smart retail and intelligent warehousing applications.</p><p>However, faced with more complex behavioral combinations and contextual constraints in real retail environments, this study still has the following limitations and corresponding future research directions:</p><p>The current method performs well in recognizing simple continuous behaviors, but its temporal action segmentation capability remains insufficient when dealing with complex continuous, combined, and interactive behaviors. Future work could introduce more powerful temporal modeling mechanisms to effectively capture long-range dependencies between actions. Secondly, existing experiments were conducted in controlled environments, whereas real retail scenarios often involve multiple customers moving simultaneously, frequent occlusions, and device signal interference. Subsequent research should validate system robustness in open and dynamic environments and explore strategies such as adversarial training or meta-learning to enhance model generalization. Furthermore, although the current method can recognize macro-level behaviors (e.g., picking up or placing goods), it struggles to distinguish between fine-grained actions with similar intentions (such as &#8220;shopping&#8221; vs. &#8220;stealing&#8221; or &#8220;organizing&#8221; vs. &#8220;searching&#8221;). Future efforts should incorporate contextual scene information and causal reasoning methods to improve the discrimination and interpretability of behavioral semantics. Finally, although RFID technology offers the advantages of non-visual perception and privacy protection, its limited sensing dimensions restrict the depth of complex behavior understanding. Future research could explore the integration of RFID with low-cost infrared, acoustic, or sparse visual sensors to construct cross-modal behavioral cognition models while maintaining privacy protection, thereby enhancing overall perceptual capabilities.</p><p>In summary, while this study has achieved promising results within its defined scope, further advancements are needed in areas such as complex behavior modeling, generalization to open environments, fine-grained behavioral semantic understanding, system optimization, and multimodal fusion to promote the widespread application and implementation of RFID-based behavioral sensing technology in real retail scenarios.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, X.L. and H.W.; methodology, X.L.; software, X.L.; validation, X.L., L.L. and B.Q.; formal analysis, X.L.; investigation, S.P.; resources, X.L.; data curation, R.P.; writing&#8212;original draft preparation, X.L.; writing&#8212;review and editing, X.L.; visualization, H.W.; supervision, X.L.; project administration, X.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data supporting the results reported in this study are not publicly available due to privacy restrictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05540"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nikpour</surname><given-names>B.</given-names></name><name name-style="western"><surname>Sinodinos</surname><given-names>D.</given-names></name><name name-style="western"><surname>Armanfard</surname><given-names>N.</given-names></name></person-group><article-title>Deep Reinforcement Learning in Human Activity Recognition: A Survey and Outlook</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2025</year><volume>36</volume><fpage>4267</fpage><lpage>4278</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2024.3360990</pub-id><pub-id pub-id-type="pmid">38373132</pub-id></element-citation></ref><ref id="B2-sensors-25-05540"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hussain</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>Q.Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.E.</given-names></name></person-group><article-title>A review and categorization of techniques on device-free human activity recognition</article-title><source>J. Netw. Comput. Appl.</source><year>2020</year><volume>167</volume><fpage>102738</fpage><pub-id pub-id-type="doi">10.1016/j.jnca.2020.102738</pub-id></element-citation></ref><ref id="B3-sensors-25-05540"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lara</surname><given-names>O.D.</given-names></name><name name-style="western"><surname>Labrador</surname><given-names>M.A.</given-names></name></person-group><article-title>A Survey on Human Activity Recognition using Wearable Sensors</article-title><source>IEEE Commun. Surv. Tutor.</source><year>2013</year><volume>15</volume><fpage>1192</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1109/SURV.2012.110112.00192</pub-id></element-citation></ref><ref id="B4-sensors-25-05540"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Padilla-Lopez</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Chaaraoui</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Florez-Revuelta</surname><given-names>F.</given-names></name></person-group><article-title>Visual Privacy Protection Methods: A Survey</article-title><source>Expert Syst. Appl.</source><year>2015</year><volume>42</volume><fpage>4177</fpage><lpage>4195</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2015.01.041</pub-id></element-citation></ref><ref id="B5-sensors-25-05540"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Twins: Device-free Object Tracking using Passive Tags</article-title><source>IEEE/ACM Trans. Netw.</source><year>2016</year><volume>24</volume><fpage>1605</fpage><lpage>1617</lpage><pub-id pub-id-type="doi">10.1109/TNET.2015.2429657</pub-id></element-citation></ref><ref id="B6-sensors-25-05540"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ni</surname><given-names>L.M.</given-names></name></person-group><article-title>GRfid: A Device-Free RFID-Based Gesture Recognition System</article-title><source>IEEE Trans. Mob. Comput.</source><year>2017</year><volume>16</volume><fpage>381</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1109/TMC.2016.2549518</pub-id></element-citation></ref><ref id="B7-sensors-25-05540"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>Q.Z.</given-names></name><name name-style="western"><surname>Ruan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name></person-group><article-title>Unobtrusive Posture Recognition via Online Learning of Multi-dimensional RFID Received Signal Strength</article-title><source>Proceedings of the International Conference on Parallel and Distributed Systems</source><conf-loc>Melbourne, VIC, Australia</conf-loc><conf-date>14&#8211;17 December 2015</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ICPADS.2015.23</pub-id></element-citation></ref><ref id="B8-sensors-25-05540"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ruan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ruan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>Q.Z.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Falkner</surname><given-names>N.J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name></person-group><article-title>Device-Free Human Localization and Tracking with UHF Passive RFID Tags: A Data-Driven Approach</article-title><source>J. Netw. Comput. Appl.</source><year>2018</year><volume>104</volume><fpage>78</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.jnca.2017.12.010</pub-id></element-citation></ref><ref id="B9-sensors-25-05540"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>X.J.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>Q.R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>B.Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name></person-group><article-title>A low-cost device-free activity recognition method</article-title><source>J. Northwest Univ. (Nat. Sci. Ed.)</source><year>2018</year><volume>48</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.16152/j.cnki.xdxbzr.2018-02-005</pub-id></element-citation></ref><ref id="B10-sensors-25-05540"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>W.</given-names></name></person-group><article-title>RF-AcSense: Device-Free Activity Localization and Recognition via Passive RFID Tag Array</article-title><source>Proceedings of the 49th IEEE Conference on Local Computer Networks (LCN)</source><conf-loc>Normandy, France</conf-loc><conf-date>8&#8211;10 October 2024</conf-date><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/LCN60385.2024.10639663</pub-id></element-citation></ref><ref id="B11-sensors-25-05540"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Golipoor</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sigg</surname><given-names>S.</given-names></name></person-group><article-title>RFID-based Human Activity Recognition Using Multimodal Convolutional Neural Networks</article-title><source>Proceedings of the 29th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)</source><conf-loc>Padova, Italy</conf-loc><conf-date>10&#8211;13 September 2024</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ETFA61755.2024.10710698</pub-id></element-citation></ref><ref id="B12-sensors-25-05540"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name></person-group><article-title>Skeleton-Based Human Activities Fine-grained Recognition with RFID Technology</article-title><source>Proceedings of the 2024 9th International Conference on Signal and Image Processing (ICSIP)</source><conf-loc>Nanjing, China</conf-loc><conf-date>12&#8211;14 July 2024</conf-date><fpage>6</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/ICSIP61881.2024.10671507</pub-id></element-citation></ref><ref id="B13-sensors-25-05540"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shangguan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name></person-group><article-title>ShopMiner: Mining Customer Shopping Behavior in Physical Clothing Stores with COTS RFID Devices</article-title><source>Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>1&#8211;4 November 2015</conf-date><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/2809695.2809710</pub-id></element-citation></ref><ref id="B14-sensors-25-05540"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Law</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Gunawan</surname><given-names>E.</given-names></name></person-group><article-title>Multipath Mitigation Technique Based on Partial Autocorrelation Function</article-title><source>Wirel. Pers. Commun.</source><year>2007</year><volume>41</volume><fpage>145</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1007/s11277-006-9134-6</pub-id></element-citation></ref><ref id="B15-sensors-25-05540"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ruan</surname><given-names>X.</given-names></name></person-group><article-title>Analysis of Multi-GNSS Multipath for Parameter-Unified Autocorrelation-Based Mitigation and the Impact of Constellation Shifts</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>4009</elocation-id><pub-id pub-id-type="doi">10.3390/rs16214009</pub-id></element-citation></ref><ref id="B16-sensors-25-05540"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>X.</given-names></name></person-group><article-title>RF-RES: Respiration Monitoring with COTS RFID Tags by Dopplershift</article-title><source>IEEE Sens. J.</source><year>2021</year><volume>21</volume><fpage>24844</fpage><lpage>24854</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2021.3114091</pub-id></element-citation></ref><ref id="B17-sensors-25-05540"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>F.</given-names></name></person-group><article-title>Sensing Human Gait for Environment-Independent User Authentication Using Commodity RFID Devices</article-title><source>IEEE Trans. Mob. Comput.</source><year>2024</year><volume>23</volume><fpage>6304</fpage><lpage>6317</lpage><pub-id pub-id-type="doi">10.1109/TMC.2023.3318753</pub-id></element-citation></ref><ref id="B18-sensors-25-05540"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Tag-Fall: A Doppler Shift-Based Fall Detection Method Using RFID Passive Tags</article-title><source>IEEE J. Radio Freq. Identif.</source><year>2024</year><volume>8</volume><fpage>252</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1109/JRFID.2024.3393242</pub-id></element-citation></ref><ref id="B19-sensors-25-05540"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shangguan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name></person-group><article-title>FEMO: A Platform for Free-weight Exercise Monitoring with RFIDs</article-title><source>Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>1&#8211;4 November 2015</conf-date><fpage>83</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1145/2809695.2809708</pub-id></element-citation></ref><ref id="B20-sensors-25-05540"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name></person-group><article-title>CBID: A Customer Behavior Identification System Using Passive Tags</article-title><source>IEEE/Acm Trans. Netw.</source><year>2016</year><volume>24</volume><fpage>2885</fpage><lpage>2898</lpage><pub-id pub-id-type="doi">10.1109/TNET.2015.2501103</pub-id></element-citation></ref><ref id="B21-sensors-25-05540"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name></person-group><article-title>High-Accuracy and Fine-Granularity Human Activity Recognition Method Based on Body RFID Skeleton</article-title><source>IEEE Trans. Consum. Electron.</source><year>2024</year><volume>70</volume><fpage>1040</fpage><lpage>1051</lpage><pub-id pub-id-type="doi">10.1109/TCE.2023.3340752</pub-id></element-citation></ref><ref id="B22-sensors-25-05540"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Enabling Fine-Grained Shopping Behavior Information Acquisition with Dual-RFID-Tags</article-title><source>IEEE Trans. Mob. Comput.</source><year>2020</year><volume>7</volume><fpage>7539</fpage><lpage>7549</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2020.2985852</pub-id></element-citation></ref><ref id="B23-sensors-25-05540"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name></person-group><article-title>RFID-Based Indoor Human Behavior Recognition Using SATCN: A Self-Attention Enhanced Temporal Convolutional Network</article-title><source>Proceedings of the 2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)</source><conf-loc>Tianjin, China</conf-loc><conf-date>8&#8211;10 May 2024</conf-date><fpage>2852</fpage><lpage>2857</lpage><pub-id pub-id-type="doi">10.1109/CSCWD61410.2024.10580071</pub-id></element-citation></ref><ref id="B24-sensors-25-05540"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>TSCNN: A 3D Convolutional Activity Recognition Network Based on RFID RSSI</article-title><source>Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN)</source><conf-loc>Glasgow, UK</conf-loc><conf-date>19&#8211;24 July 2020</conf-date><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/IJCNN48605.2020.9207590</pub-id></element-citation></ref><ref id="B25-sensors-25-05540"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>F.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Su</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name></person-group><article-title>RFID-Based Human Action Recognition Through Spatiotemporal Graph Convolutional Neural Network</article-title><source>IEEE Internet Things J.</source><year>2023</year><volume>10</volume><fpage>19898</fpage><lpage>19912</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2023.3282680</pub-id></element-citation></ref><ref id="B26-sensors-25-05540"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ali</surname><given-names>K.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>A.X.</given-names></name><name name-style="western"><surname>Chai</surname><given-names>E.</given-names></name><name name-style="western"><surname>Sundaresan</surname><given-names>K.</given-names></name></person-group><article-title>Monitoring Browsing Behavior of Customers in Retail Stores via RFID Imaging</article-title><source>IEEE Trans. Mob. Comput.</source><year>2022</year><volume>21</volume><fpage>1034</fpage><lpage>1048</lpage><pub-id pub-id-type="doi">10.1109/TMC.2020.3019652</pub-id></element-citation></ref><ref id="B27-sensors-25-05540"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name></person-group><article-title>RF-Net: A Unified Meta-learning Framework for RF-enabled One-shot Human Activity Recognition</article-title><source>Proceedings of the 18th ACM Conference on Embedded Networked Sensor Systems</source><conf-loc>ACM, Virtual Event</conf-loc><conf-date>16&#8211;19 November 2021</conf-date><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1145/3384419.3430735</pub-id></element-citation></ref><ref id="B28-sensors-25-05540"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khean</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ryu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>E.Y.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Nam</surname><given-names>Y.</given-names></name></person-group><article-title>Human Interaction Recognition in Surveillance Videos Using Hybrid Deep Learning and Machine Learning Models</article-title><source>Comput. Mater. Contin.</source><year>2024</year><volume>81</volume><fpage>773</fpage><pub-id pub-id-type="doi">10.32604/cmc.2024.056767</pub-id></element-citation></ref><ref id="B29-sensors-25-05540"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>H.</given-names></name></person-group><article-title>A Fast UHF RFID Localization Method Using Unwrapped Phase-Position Model</article-title><source>IEEE Trans. Autom. Sci. Eng.</source><year>2019</year><volume>16</volume><fpage>1698</fpage><lpage>1707</lpage><pub-id pub-id-type="doi">10.1109/TASE.2019.2895104</pub-id></element-citation></ref><ref id="B30-sensors-25-05540"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maguire</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pappu</surname><given-names>R.</given-names></name></person-group><article-title>An optimal Q-algorithm for the ISO 18000-6C RFID protocol</article-title><source>IEEE Trans. Autom. Sci. Eng.</source><year>2009</year><volume>6</volume><fpage>16</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1109/TASE.2008.2007266</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05540-f001" orientation="portrait"><label>Figure 1</label><caption><p>Schematic diagram of UHF RFID tags and antenna layout scenarios in the shelf area.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g001.jpg"/></fig><fig position="float" id="sensors-25-05540-f002" orientation="portrait"><label>Figure 2</label><caption><p>System architecture diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g002.jpg"/></fig><fig position="float" id="sensors-25-05540-f003" orientation="portrait"><label>Figure 3</label><caption><p>The first plot shows the raw, periodic phase signal. The second shows the unwrapped and smoothed result, revealing the true trajectory with suppressed noise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g003.jpg"/></fig><fig position="float" id="sensors-25-05540-f004" orientation="portrait"><label>Figure 4</label><caption><p>(<bold>a</bold>) The distribution of the standard phase deviation of individual tags at rest in the tag matrix. (<bold>b</bold>) The phase change of the tag after it has been suppressed by the hardware characteristics.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g004.jpg"/></fig><fig position="float" id="sensors-25-05540-f005" orientation="portrait"><label>Figure 5</label><caption><p>(<bold>a</bold>) Phase change of a static tag after inhibition of human characteristics. (<bold>b</bold>) Phase change of a dynamic tag after human trait suppression.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g005.jpg"/></fig><fig position="float" id="sensors-25-05540-f006" orientation="portrait"><label>Figure 6</label><caption><p>(<bold>a</bold>) Volatility characteristics of different tags. The boxplots display the distribution of phase values (in radians) for each EPC tag. The central box represents the interquartile range (IQR), the horizontal line within the box marks the median, and the whiskers extend to 1.5 &#215; IQR. The circles beyond the whiskers indicate outliers&#8212;individual phase readings that deviate significantly from the majority of the data.(<bold>b</bold>) The volatility characteristics of the tag are quantified by IQR values.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g006.jpg"/></fig><fig position="float" id="sensors-25-05540-f007" orientation="portrait"><label>Figure 7</label><caption><p>Quantification of the phase change caused by human movement by accumulating phase differences.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g007.jpg"/></fig><fig position="float" id="sensors-25-05540-f008" orientation="portrait"><label>Figure 8</label><caption><p>(<bold>a</bold>) The first Fresnel zone model consists of a tag and an antenna. (<bold>b</bold>) A loss model of the power that the tag transmits in space.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g008.jpg"/></fig><fig position="float" id="sensors-25-05540-f009" orientation="portrait"><label>Figure 9</label><caption><p>The dynamic possibility of labeling is quantified by constructing a spatial transmission loss model to obtain the correlation between the lost power and the signal strength.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g009.jpg"/></fig><fig position="float" id="sensors-25-05540-f010" orientation="portrait"><label>Figure 10</label><caption><p>The phase data stream and the Doppler data stream are divided into behavior segments: A&#8211;B is take away, B&#8211;C is put back, and C&#8211;D is pick up and put down.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g010.jpg"/></fig><fig position="float" id="sensors-25-05540-f011" orientation="portrait"><label>Figure 11</label><caption><p>Changes in the relative distance between the tag and the antenna during tag movement.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g011.jpg"/></fig><fig position="float" id="sensors-25-05540-f012" orientation="portrait"><label>Figure 12</label><caption><p>The estimated Doppler frequencies are used to analyze the characteristics of the three behaviors in detail.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g012.jpg"/></fig><fig position="float" id="sensors-25-05540-f013" orientation="portrait"><label>Figure 13</label><caption><p>Time-frequency plot of continuous wavelet transform based on Morlet wavelet: (<bold>a</bold>) Stationary. (<bold>b</bold>) Put back. (<bold>c</bold>) Take away. (<bold>d</bold>) Pick up and put down.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g013.jpg"/></fig><fig position="float" id="sensors-25-05540-f014" orientation="portrait"><label>Figure 14</label><caption><p>Dual-path residual network architecture.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g014.jpg"/></fig><fig position="float" id="sensors-25-05540-f015" orientation="portrait"><label>Figure 15</label><caption><p>Pick-up count monitoring with shelf status visualization.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g015.jpg"/></fig><fig position="float" id="sensors-25-05540-f016" orientation="portrait"><label>Figure 16</label><caption><p>Time identification of behavioral events: Doppler frequency after wavelet smoothing with a time range of 0&#8211;14 s. The red triangle indicates the drop action, and the green triangle indicates the pick-up action. The dotted line marks the start and end time points of the behavior, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g016.jpg"/></fig><fig position="float" id="sensors-25-05540-f017" orientation="portrait"><label>Figure 17</label><caption><p>Experimental scenario of the system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g017.jpg"/></fig><fig position="float" id="sensors-25-05540-f018" orientation="portrait"><label>Figure 18</label><caption><p>The results of the original phase signal after only phase unwrapping.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g018.jpg"/></fig><fig position="float" id="sensors-25-05540-f019" orientation="portrait"><label>Figure 19</label><caption><p>Phase change of the tag array after inhibition by tag diversity.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g019.jpg"/></fig><fig position="float" id="sensors-25-05540-f020" orientation="portrait"><label>Figure 20</label><caption><p>Phase change after inhibition of human diversity.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g020.jpg"/></fig><fig position="float" id="sensors-25-05540-f021" orientation="portrait"><label>Figure 21</label><caption><p>Dynamic tag detection results through isolated forests: the red &#8220;x&#8221; indicates the detected dynamic tags. &#8220;o&#8221; indicates normal tags.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g021.jpg"/></fig><fig position="float" id="sensors-25-05540-f022" orientation="portrait"><label>Figure 22</label><caption><p>Analysis and segmentation of distinct behavior streams executed by three volunteers at different times and through varying methods.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g022.jpg"/></fig><fig position="float" id="sensors-25-05540-f023" orientation="portrait"><label>Figure 23</label><caption><p>(<bold>a</bold>) Heatmap classification report based on behavior recognition of dual-path residual networks. (<bold>b</bold>) Behavior identification confusion matrix of a two-path residual network. The dark-blue area (diagonal) of the matrix shows the correct classification of the three behavior categories.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g023.jpg"/></fig><fig position="float" id="sensors-25-05540-f024" orientation="portrait"><label>Figure 24</label><caption><p>Performance Comparison of Behavior Recognition Models: The left subplot compares evaluation metrics across different models, with the bar chart representing accuracy and the line graph corresponding to the F1 score. The results demonstrate that the Dual-ResNet model outperforms other methods on both metrics. The right subplot shows the ROC curves of various models, where Dual-ResNet achieves an AUC value of 0.983, significantly higher than those of the SVM and CNN models, further confirming its superior classification capability.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05540-g024.jpg"/></fig><table-wrap position="float" id="sensors-25-05540-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05540-t001_Table 1</object-id><label>Table 1</label><caption><p>Behavioral flow segmentation explanation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Split Interval</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Behavior</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">A&#8211;B</td><td align="center" valign="middle" rowspan="1" colspan="1">Take&#160;away</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">B&#8211;C</td><td align="center" valign="middle" rowspan="1" colspan="1">Put&#160;back</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C&#8211;D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pick up and put&#160;down</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05540-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05540-t002_Table 2</object-id><label>Table 2</label><caption><p>Monthly pick-up counts of retail items (January to June).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Product</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">January</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">February</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">March</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">April</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">May</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">June</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Thermos cup</td><td align="center" valign="middle" rowspan="1" colspan="1">480</td><td align="center" valign="middle" rowspan="1" colspan="1">450</td><td align="center" valign="middle" rowspan="1" colspan="1">380</td><td align="center" valign="middle" rowspan="1" colspan="1">250</td><td align="center" valign="middle" rowspan="1" colspan="1">180</td><td align="center" valign="middle" rowspan="1" colspan="1">150</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Glass cup</td><td align="center" valign="middle" rowspan="1" colspan="1">250</td><td align="center" valign="middle" rowspan="1" colspan="1">260</td><td align="center" valign="middle" rowspan="1" colspan="1">280</td><td align="center" valign="middle" rowspan="1" colspan="1">320</td><td align="center" valign="middle" rowspan="1" colspan="1">380</td><td align="center" valign="middle" rowspan="1" colspan="1">420</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Towel</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td><td align="center" valign="middle" rowspan="1" colspan="1">310</td><td align="center" valign="middle" rowspan="1" colspan="1">320</td><td align="center" valign="middle" rowspan="1" colspan="1">350</td><td align="center" valign="middle" rowspan="1" colspan="1">380</td><td align="center" valign="middle" rowspan="1" colspan="1">450</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Umbrella</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td><td align="center" valign="middle" rowspan="1" colspan="1">150</td><td align="center" valign="middle" rowspan="1" colspan="1">350</td><td align="center" valign="middle" rowspan="1" colspan="1">380</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td><td align="center" valign="middle" rowspan="1" colspan="1">250</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Flip-flops</td><td align="center" valign="middle" rowspan="1" colspan="1">50</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td><td align="center" valign="middle" rowspan="1" colspan="1">150</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td><td align="center" valign="middle" rowspan="1" colspan="1">450</td><td align="center" valign="middle" rowspan="1" colspan="1">550</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cotton slippers</td><td align="center" valign="middle" rowspan="1" colspan="1">500</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shirt</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">350</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">380</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">450</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">520</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">650</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05540-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05540-t003_Table 3</object-id><label>Table 3</label><caption><p>Reader parameter settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Reader Type</td><td align="center" valign="middle" rowspan="1" colspan="1">Impinj Speedway</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tag Type</td><td align="center" valign="middle" rowspan="1" colspan="1">AZ-9662</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tag Size</td><td align="center" valign="middle" rowspan="1" colspan="1">70&#160;mm &#215; 17&#160;mm</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Protocol</td><td align="center" valign="middle" rowspan="1" colspan="1">ISO-18000-6C [<xref rid="B30-sensors-25-05540" ref-type="bibr">30</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reader Read Power</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26&#160;dBm</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>