<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431004</article-id><article-id pub-id-type="pmcid-ver">PMC12431004.1</article-id><article-id pub-id-type="pmcaid">12431004</article-id><article-id pub-id-type="pmcaiid">12431004</article-id><article-id pub-id-type="doi">10.3390/s25175368</article-id><article-id pub-id-type="publisher-id">sensors-25-05368</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>High-Fidelity NIR-LED Direct-View Display System for Authentic Night Vision Goggle Simulation Training</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9632-8534</contrib-id><name name-style="western"><surname>Zeng</surname><given-names initials="Y">Yixiong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="c1-sensors-25-05368" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Xu</surname><given-names initials="B">Bo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Qiu</surname><given-names initials="K">Kun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Jechow</surname><given-names initials="A">Andreas</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05368">School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China</aff><author-notes><corresp id="c1-sensors-25-05368"><label>*</label>Correspondence: <email>zyxiong@std.uestc.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>30</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5368</elocation-id><history><date date-type="received"><day>22</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>20</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>30</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05368.pdf"/><abstract><p>Current simulation training for pilots wearing night vision goggles (NVGs) (e.g., night landings and tactical reconnaissance) faces fidelity limitations from conventional displays. This study proposed a novel dynamic NIR-LED direct-view display system for authentic nighttime scene simulation. Through comparative characterization of NVG response across LED wavelengths under ultra-low-current conditions, 940 nm was identified as the optimal wavelength. Quantification of inherent nonlinear response in NVG observation enabled derivation of a mathematical model that provides the foundation for inverse gamma correction compensation. A prototype NIR-LED display was engineered with 1.25 mm pixel pitch and 1280 &#215; 1024 resolution at 60 Hz refresh rate, achieving &gt;90% uniformity and &gt;2000:1 contrast. Subjective evaluations confirmed exceptional simulation fidelity. This system enables high-contrast, low-power NVG simulation for both full-flight simulators and urban low-altitude reconnaissance training systems, providing the first quantified analysis of NVG-LED nonlinear interactions and establishing the technical foundation for next-generation LED-based all-weather visual displays.</p></abstract><kwd-group><kwd>NVGs</kwd><kwd>NVG training</kwd><kwd>NIR-LED display</kwd><kwd>night vision simulation</kwd><kwd>nonlinear response</kwd><kwd>inverse gamma correction</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>U22A2086</award-id></award-group><funding-statement>This work was supported by National Natural Science Foundation of China (U22A2086).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05368"><title>1. Introduction</title><p>Night vision goggles (NVGs) constitute a category of optoelectronic devices that amplify faint light signals through photon amplification and photoelectric conversion to produce visible imagery [<xref rid="B1-sensors-25-05368" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05368" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05368" ref-type="bibr">3</xref>]. This capacity for generating clear images under darkness or low-light conditions enhances human night vision, enabling extensive deployment in military operations, security surveillance, law enforcement, and wilderness exploration. Through technological advancements, modern military systems widely employ third-generation (Gen III) NVGs, which feature Gallium Arsenide (GaAs) photocathodes. These photocathodes provide significantly higher sensitivity&#8212;particularly in the near-infrared (NIR) spectrum&#8212;compared to earlier generations (e.g., Gen II devices with multialkali photocathodes). This enhanced sensitivity translates into superior image resolution and markedly improved performance under low-light conditions, including starlight [<xref rid="B4-sensors-25-05368" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05368" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05368" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05368" ref-type="bibr">7</xref>]. Particularly in aerial aviation, NVGs serve as critical equipment that augment pilots&#8217; situational awareness and operational effectiveness during nighttime missions. By providing visual assistance at dusk or night, these significantly improve aircrew all-weather combat capabilities [<xref rid="B8-sensors-25-05368" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05368" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05368" ref-type="bibr">10</xref>].</p><p>However, NVG deployment presents significant challenges for pilots. First, the constrained field-of-view (FOV)&#8212;typically &#8776;40&#176;&#8212;limits pilots&#8217; visual scanning capabilities during operations [<xref rid="B11-sensors-25-05368" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05368" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05368" ref-type="bibr">13</xref>]. Second, monochromatic imaging output (typically green or white, determined by photocathode materials) combined with optical artifacts&#8212;halation, blooming, and shading&#8212;degrades image clarity [<xref rid="B14-sensors-25-05368" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05368" ref-type="bibr">15</xref>]. This manifests as reduced image gradation, low contrast ratios, and increased visual interference. Such chromatic distortion and image degradation can trigger NVG-induced visual illusions, elevating the risk of spatial misjudgments that compromise flight safety. Finally, NVG usage increases pilot workload, accelerating fatigue development&#8212;a critical aviation safety concern [<xref rid="B16-sensors-25-05368" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05368" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05368" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05368" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05368" ref-type="bibr">20</xref>]. Consequently, global air forces have implemented comprehensive NVG training programs to address these challenges, focusing on developing pilot proficiency in NVG operation, adapting to NVG-induced visual perception changes, and mitigating physiological impacts.</p><p>Following the initial NVG deployment in the 1970s, the United States has established dedicated NVG training research that evolved from early studies on ground troop effectiveness to sophisticated, aviation-centric curricula. These curricula encompass soldier competency in NVG operation for nocturnal missions, equipment integration methods, and distance estimation enhancement under NVG conditions [<xref rid="B21-sensors-25-05368" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05368" ref-type="bibr">22</xref>]. Simultaneously, NATO Standardization Agreement STANAG 7147 governs NVG training standards through a structured framework: Initial Qualification Training (ground), Recurrent Training (ground), and Flight Training. The Initial Qualification phase integrates theoretical instruction with hand-on NVG configuration/inspection and simulator-based training [<xref rid="B23-sensors-25-05368" ref-type="bibr">23</xref>]. Globally, the military aviation community recognizes ground-based simulated NVG training&#8212;particularly live goggle drill&#8212;as critically valuable. This training enhances mission readiness, operational familiarity, and flight perception adaptation, substantially improving aviation safety [<xref rid="B16-sensors-25-05368" ref-type="bibr">16</xref>,<xref rid="B24-sensors-25-05368" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05368" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05368" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05368" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05368" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05368" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05368" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05368" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05368" ref-type="bibr">32</xref>].</p><p>Consequently, devices capable of delivering realistic video imagery for NVGs have become critical for effective ground-based training. Currently, ground-based NVG simulation training primarily relies on two methodologies, (1) Image Generator (IG) direct simulation, which renders synthetic NVG-like imagery&#8212;characterized by monochrome green/white, noise and halos&#8212;on conventional displays [<xref rid="B33-sensors-25-05368" ref-type="bibr">33</xref>], and (2) physical simulation, in which trainees wear actual NVGs and view a specialized display system engineered to stimulate the image intensifier tube of the goggles [<xref rid="B34-sensors-25-05368" ref-type="bibr">34</xref>]. While IG simulation offers advantages in implementation simplicity and minimal hardware requirements, it fundamentally fails to replicate the critical physical interactions and perceptual characteristics inherent in real-world NVG operations. Specifically, it does not accurately reproduce the FOV, spatial awareness constraints imposed by helmet-mounted devices, authentic optical artifacts (e.g., dynamic halation), or the realistic visual experience provided through optical eyepieces. Consequently, IG simulation falls short in preparing pilots for the operational challenges of NVG handling and the visual&#8211;spatial adaptations required during actual missions. In contrast, physical simulation using real NVGs delivers the most immersive and operationally relevant training experience. It faithfully replicates the complete sensory and physical state of a pilot observing the external environment through NVGs during flight. This includes accurate simulation of the device&#8217;s spectral response, nonlinear luminance conversion, optical distortions, and system-specific artifacts, as well as realistic viewing posture/environmental constraints. Consequently, physical simulation enables comprehensive training in physical interaction, perceptual fidelity, and physiological adaptation. However, the adoption of physical simulation introduces a significant technical challenge: the high-fidelity design and implementation of the display system serving as the NVG stimulator source. Crucially, developing a single display system capable of supporting all-weather flight simulation&#8212;including daytime and nighttime scenarios&#8212;for a unified platform remains a significant technical challenge. Current research indicates that projection-based display systems&#8211;&#8211;primary solution for NVG compatible flight simulators&#8211;&#8211;integrate auxiliary light sources to simultaneously present visible imagery and stimulate NVG image intensifiers (e.g., Barco FS40/FS70/FS400 series) [<xref rid="B34-sensors-25-05368" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05368" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05368" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05368" ref-type="bibr">37</xref>]. Despite advantages in retrofit flexibility, continuous operability, and display integration, such projection systems fundamentally grapple with contrast ratio limitations. Night scene simulation often suffers from diminished image clarity and loss of grayscale resolution due to intrinsic technological constraints [<xref rid="B38-sensors-25-05368" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05368" ref-type="bibr">39</xref>].</p><p>Notably, LED-based visual display systems have been documented in flight simulation training applications [<xref rid="B40-sensors-25-05368" ref-type="bibr">40</xref>]. While they deliver exceptional visual performance through superior luminance and contrast, their implementations have been confined to daytime scenario simulation; no NVG-compatible configurations have been documented. To address the limitations of projection technology&#8211;&#8211;particularly its insufficient contrast for authentic NVG simulation&#8211;&#8211;and to expand the applicability of LED-based displays into the critical domain of night vision training, this paper proposes a novel direct-view LED display system configured as an NVG imaging stimulator source. The core motivation behind developing this NIR-LED direct-view system is to harness the inherent advantages of LED technology&#8212;such as high contrast ratios, fast response times, and precise luminance control&#8212;to deliver a significantly higher-fidelity image stimulation source for NVGs. This directly addresses a critical gap in existing solutions for authentic NVG simulation training. Simultaneously, this research aims to establish the theoretical foundation and technical pathway for extending advanced LED-based dome visual display systems&#8212;currently restricted to daytime simulation&#8212;into the realm of all-weather training applications. Leveraging inherently high contrast ratios and rapid response characteristics, this system enables accurate night scene reproduction, particularly during dynamic scenario transitions, achieving unprecedented simulation fidelity. Such advancements directly contribute to enhanced pilot training efficacy during ground-based NVG exercises. Through the development of this system, we investigate the response characteristics of NVGs to NIR-LEDs, validating the feasibility and identifying key challenges of using LED display technology as an NVG stimulation source under ultra-low-light conditions. We characterize and model the nonlinear response of NVGs and propose an inverse gamma correction method to accurately compensate for its impact on display performance&#8212;significantly improving simulation fidelity, as confirmed by subjective evaluations of NVG imagery. Furthermore, this work presents the first nonlinear interaction mechanism between NVGs and LEDs, establishing a critical technical foundation for the development of next-generation, all-weather LED-based visual display systems.</p><p>This paper is structured as follows: <xref rid="sec2-sensors-25-05368" ref-type="sec">Section 2</xref> presents the design and implementation of the NIR-LED direct-view display, with particular emphasis on the spectral selection process that establishes 940 nm as the optimal wavelength. <xref rid="sec3-sensors-25-05368" ref-type="sec">Section 3</xref> characterizes the inherent nonlinear response of NVG image intensifiers and proposes an inverse gamma correction method for compensation. <xref rid="sec4-sensors-25-05368" ref-type="sec">Section 4</xref> quantifies display performance through uniformity and contrast measurements, complemented by visual validation using authentic night vision scenes, collectively validating its operational viability.</p></sec><sec id="sec2-sensors-25-05368"><title>2. Design of an NIR-LED Direct-View Display</title><sec id="sec2dot1-sensors-25-05368"><title>2.1. LED Spectral Selection for Night Vision Simulation</title><p>Spectrum selection proves critical for engineering direct-view LED displays in NVG-compatible training applications, driven by dual imperatives:<list list-type="simple"><list-item><label>(1)</label><p>The display must simulate human nocturnal vision to create authentically low-illuminance conditions, enabling non-NVG-equipped pilots to experience authentic nocturnal immersion.</p></list-item><list-item><label>(2)</label><p>LED emission must both trigger NVG detection and accurately replicate NVG spectral responses to natural night-sky irradiance.</p></list-item></list></p><p>Consequently, optimal LED spectral selection necessitates integrated consideration of the interrelationship between human visual perception, NVG spectral response characteristics, and night-sky radiation profiles. To establish the analytic framework, we referenced three datasets: the photopic luminosity function standardized by the Commission Internationale de l&#8217;&#201;clairage (CIE) for human spectral sensitivity [<xref rid="B41-sensors-25-05368" ref-type="bibr">41</xref>], the NVG response characteristics codified in U.S. Department of Defense specification MIL-STD-3009 [<xref rid="B42-sensors-25-05368" ref-type="bibr">42</xref>], and the night-sky irradiance distributions documented by Vatsia et al. [<xref rid="B43-sensors-25-05368" ref-type="bibr">43</xref>]. Comparative spectral diagrams (380&#8211;1100 nm wavelength, <xref rid="sensors-25-05368-f001" ref-type="fig">Figure 1</xref>) delineate correlations between night-sky radiation, human perceptual thresholds, and Generation III NVG spectral sensitivity.</p><p>As evidenced by the blue-shaded night-sky radiation profile in <xref rid="sensors-25-05368-f001" ref-type="fig">Figure 1</xref>, natural nocturnal radiation intensity predominantly concentrates in the near-infrared (NIR) spectrum beyond 700 nm. Consequently, NVGs are engineered to achieve peak responsivity within the NIR band. For instance, third-generation devices exhibit peak spectral response between 580 nm and 950 nm. Therefore, NIR-band LEDs were chosen as the optimal spectral solution to simultaneously achieve the following objectives:<list list-type="simple"><list-item><label>(1)</label><p>Match the natural night-sky irradiance spectrum (dominant above 700 nm) [<xref rid="B35-sensors-25-05368" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05368" ref-type="bibr">36</xref>];</p></list-item><list-item><label>(2)</label><p>Align with NVG peak responsibility (580&#8211;950 nm) [<xref rid="B1-sensors-25-05368" ref-type="bibr">1</xref>,<xref rid="B4-sensors-25-05368" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05368" ref-type="bibr">5</xref>,<xref rid="B42-sensors-25-05368" ref-type="bibr">42</xref>];</p></list-item><list-item><label>(3)</label><p>Avoid visual interference for unaided human observers during nighttime training scenarios (human vision has negligible sensitivity beyond 700 nm).</p></list-item></list></p><p>This triple-alignment strategy&#8212;spanning night-sky physics, NVG sensitivity, and human physiological constraints&#8212;ensures ecological valid nocturnal simulation and operational compatibility with dual-user (NVG-equipped/unaided) training.</p><p>To identify optimal wavelengths, we characterized seven LEDs (780&#8211;940 nm, manufactured by Shenzhen Baoyoute Industrial Development Co., Ltd., Shenzhen, China) with identical package dimensions, total radiant power, and viewing angles using an ADCMT 6146 precision current source (ADCMT Corporation, Saitama, Japan; voltage accuracy: &#177;1 &#956;V, current accuracy: &#177;0.1 &#956;A) to apply forward voltage and a Gwinstek GDM-8261 digital multimeter (6&#189;-digit) for current measurement (accuracy: &#177;0.1 &#956;A) by evaluating their current-voltage behavior under ultra-low current conditions (&#956;A-range) and measuring the corresponding NVG-observed luminance&#8211;current relationships under photon amplification. Results in <xref rid="sensors-25-05368-f002" ref-type="fig">Figure 2</xref>a reveal consistent characteristics across wavelengths; current exhibits exceptional growth beyond a forward voltage threshold, confirming significant challenges in achieving high-gray-level display at low luminance values. When observed through third-generation NVGs (Onick NVG-D3, Onick Optics (Wuhan) Co., Ltd.,Wuhan, China) (with a 42&#176; field of view, a photocathode radiant sensitivity of 500&#8211;600 &#956;A/lm, 1&#215; magnification, a limiting resolution of 51&#8211;57 lp/mm, and a GaAs photocathode material), the photon-amplified luminance exhibited heightened sensitivity to current variations. <xref rid="sensors-25-05368-f002" ref-type="fig">Figure 2</xref>b demonstrates luminance saturation at currents as low as 20 &#956;A (810 nm) and 60 &#956;A (900 nm), indicating extreme difficulty in rendering high-bit-depth grayscale imagery. Critically, these saturation currents inversely correlate with NVG spectral sensitivity shown in <xref rid="sensors-25-05368-f001" ref-type="fig">Figure 1</xref>: wavelengths aligning with sensitivity peaks saturate at lower currents. The 940 nm LED demonstrates saturation at 500 &#956;A, where its attenuated spectral responsivity enables precise current gradation control. This facilitates superior low-brightness high-gray-level rendering, making it particularly advantageous for generating high-fidelity grayscale imagery in NVG-compatible direct-view displays.</p><p>Notably, while human vision perceives 380&#8211;780 nm wavelengths, experimental characterization revealed that near-infrared LEDs emit visible red light (&#8220;red glow&#8221; effect) when driven beyond threshold currents. This phenomenon poses critical operational risks in direct-view displays integrating millions of NIR-LEDs, potentially inducing pilot physiological discomfort and psychological distress&#8212;manifested as tension, irritability, and impulsiveness. To mitigate this risk, we quantified red glow thresholds for all seven NIR-LEDs within a darkroom environment (&lt;10<sup>&#8722;5</sup> lx illuminance). <xref rid="sensors-25-05368-t001" ref-type="table">Table 1</xref> details the minimum currents required to elicit perceptible red glow, demonstrating progressive threshold current elevation with wavelength; 780 nm LEDs exhibited visible glow at 35 &#956;A, 810&#8211;850 nm variants required &#956;A-range currents, 880/900 nm devices reached thresholds at 2 mA, while 940 nm LEDs showed negligible emission. This current threshold was not recorded in <xref rid="sensors-25-05368-t001" ref-type="table">Table 1</xref> for 940 nm LEDs because, during our experiments, we applied currents up to the maximum allowable limit of 25 mA without observing the &#8220;red glow&#8221; phenomenon. Further increases in current would have risked permanent damage to the devices. Therefore, under normal operating conditions, the red glow was not observed for 940 nm LEDs, and no corresponding threshold value is listed in <xref rid="sensors-25-05368-t001" ref-type="table">Table 1</xref>. Consequently, 940 nm LEDs emerge as the optimal solution, delivering dual advantages, enabling low-brightness high-gray-level rendering while eliminating red glow interference in NVG simulation displays.</p></sec><sec id="sec2dot2-sensors-25-05368"><title>2.2. System Hardware Design</title><p>Building upon the established spectral and LED selection rationale, this section details the implementation of the system hardware design. For direct-view displays utilizing 940 nm NIR-LEDs (Dongguan Lanjin Optoelectronics Co., Ltd., Dongguan, China), this work proposed an optimized hardware architecture comprising three core components: a video processing module executing RGB-to-grayscale conversion, luminance regulation, gamma correction, image segmentation/distribution, and video interface adaptation; a driver module paring image data into scanning control signals; and an LED array unit (<xref rid="sensors-25-05368-f003" ref-type="fig">Figure 3</xref>) comprising matrix scan driver ICs and LED devices that convert control signals into PWM signals. In practical implementations, display panels are composed of multiple tiled LED array units. A single driver module typically controls multiple arrays, with its load capacity determined by pixel count, grayscale depth, and refresh rate. Video processing module configuration scales with display resolution, requiring only one module for systems at or below 4K resolution (3840 &#215; 2160).</p><p>During simulation training with pilots positioned 2.5&#8211;3.5 m from the display, this design ensures high-definition imagery by first determining pixel pitch based on spatial resolution (arcmin/pixel), then defining resolution, and finally establishing screen dimensions. Although 1 arcmin/pixel (60 ppd) delivers optimal visual acuity, established LED display engineering practice confirms resolutions &#8804;2 arcmin/pixel (30 ppd) maintain excellent visual quality while substantially reducing hardware resource demands and power consumption [<xref rid="B44-sensors-25-05368" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05368" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05368" ref-type="bibr">46</xref>]. Accordingly, this implementation adopted a 1.25 mm pixel pitch with 1280 &#215; 1024 resolution. For a cylindrical screen configuration of radius R = 2500 mm, this yields a horizontal field of view (HFOV) of 32.62&#176; and vertical field of view (VFOV) of 28.72&#176;, achieving a calculated spatial resolution of 1.72 arcmin/pixel&#8211;&#8211;delivering high-definition NVG-compatible imagery. The finalized NIR-LED direct-view display rendering is demonstrated in <xref rid="sensors-25-05368-f004" ref-type="fig">Figure 4</xref>.</p></sec></sec><sec id="sec3-sensors-25-05368"><title>3. NVG Response Characteristics and Gamma Correction for NIR-LED Direct-View Displays</title><sec id="sec3dot1-sensors-25-05368"><title>3.1. Modeling of NVG Response Characteristics to NIR-LED Displays</title><p>Functioning as a monochromatic system, the developed NIR-LED direct-view display prioritized grayscale reproduction fidelity&#8212;particularly under NVG observation&#8212;as its primary performance metric. Accordingly, display performance was evaluated using a 0&#8211;255 grayscale step wedge test pattern comprising 80 discrete blocks. The gray level of these blocks increased incrementally by steps of three or four from 0 to 255. As illustrated in <xref rid="sensors-25-05368-f005" ref-type="fig">Figure 5</xref>b, IR-captured images demonstrate close conformity to original images, validating high-fidelity grayscale resolution. The IR images were captured using a commercially available infrared camera (Model IUA8300KME, Tupu Photoelectric Technology Co., Ltd., Hangzhou, China). This camera is equipped with a CMOS image sensor having a spectral response range of 400 nm&#8211;1000 nm, a pixel size of 2.0 &#181;m &#215; 2.0 &#181;m, and an optical format of 1/1.8&#8221;. It offers a 12-bit dynamic range and captures images at a resolution of 3840 &#215; 2160 pixels, with a frame rate of 45 frames per second (fps). However, as shown in <xref rid="sensors-25-05368-f005" ref-type="fig">Figure 5</xref>c, under NVG observation, the on-screen grayscale transition no longer progresses smoothly as in <xref rid="sensors-25-05368-f005" ref-type="fig">Figure 5</xref>a. Instead, beginning at gray level 6, the brightness of the blocks increases sharply (indicated by the red arrow). By the time the gray level reaches 129&#8212;corresponding to the initial block of the fifth row&#8212;the luminance effectively saturates, making adjacent blocks visually indistinguishable. Consequently, establishing an NVG response model is essential for developing effective luminance compensation solutions.</p><p>LED display operation adheres to pulse-width modulation (PWM) principles governed by <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>out</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes LED luminance, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents full-duty-cycle luminance, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is PWM period, and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> designates on-time duration. With constant <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, luminance <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>out</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> varies linearly with <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, establishing proportionality to displayed grayscale levels. To characterize grayscale response under NVG observation with identical PWM driving conditions, we conducted an experiment where an FPGA controller drove an 8 &#215; 8 LED matrix (1.25 mm pitch, 940 nm) with fixed <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> = 51,200 ns while increasing <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> from 0 to 51,200 ns in 200 ns steps (generating 256 grayscale levels; measurements at 51 points). Infrared irradiance was measured in a darkroom using an Instrument Systems GmbH (M&#252;nchen, Germany) CAS140CT-156 spectroradiometer with EOP146 probe (spectral response range: 300&#8211;1100 nm), while NVG output luminance was quantified via Konica Minolta CA-310 color analyzer (spectral response range: 380&#8211;780 nm). Finally, PWM-controlled luminance values were mapped to grayscale levels with normalized irradiance and luminance data, producing the results shown in <xref rid="sensors-25-05368-f006" ref-type="fig">Figure 6</xref>.</p><p>Both curves in <xref rid="sensors-25-05368-f006" ref-type="fig">Figure 6</xref> exhibit monotonic increase from 0 to 1, yet the NVG-observed curve consistently exceeds direct measurements in normalized values, confirming NVGs&#8217; irradiance amplification throughout the input range. This enhancement is particularly pronounced at low irradiance (grayscale 0&#8211;50), indicating enhanced NVG responsivity under diminished irradiance. Conversely, in high-irradiance regions (grayscale 210&#8211;255), NVG-observed outputs demonstrate minimal increments with progressive stabilization, confirming asymptotic convergence toward saturation in this region. Mid-irradiance zones approximate linearity. These observations establish that NVGs exhibit inherent non-linear response characterized by low-irradiance enhancement and high-irradiance compression.</p><p>For effective compensation of NVG-induced effects, we developed mathematical models to precisely characterize their response. Based on the NVG response curve characteristics in <xref rid="sensors-25-05368-f006" ref-type="fig">Figure 6</xref>, we proposed a preliminary power-law mapping model expressed as:<disp-formula id="FD1-sensors-25-05368"><label>(1)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote input and output grayscale values, respectively, and <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are positive constants. For simplify fitting, we assumed <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and employed nonlinear least-squares regression to fit <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Given data points <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (excluding (0,0)), the objective minimizes residual sum of squares by optimizing <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The objective function is defined as:<disp-formula id="FD2-sensors-25-05368"><label>(2)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:munder><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#947;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of data points. To establish mathematical optimality, we verified the first-order condition by differentiating the objective function <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>&#947;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD3-sensors-25-05368"><label>(3)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8706;</mml:mo><mml:mi>&#947;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:munderover><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>&#8901;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The optimal solution <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> satisfies the vanishing gradient condition. We therefore implemented the Gauss&#8211;Newton method to solve this nonlinear least-squares problem [<xref rid="B47-sensors-25-05368" ref-type="bibr">47</xref>,<xref rid="B48-sensors-25-05368" ref-type="bibr">48</xref>], with parameter update rule:<disp-formula id="FD4-sensors-25-05368"><label>(4)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>&#932;</mml:mi></mml:mrow></mml:msup><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>&#932;</mml:mi></mml:mrow></mml:msup><mml:mi>J</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes iteration index, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>&#932;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the residual vector with components <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>&#932;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> the Jacobian vector defined by <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Numerical solution in MATLAB R2016a used initial estimate <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, maximum 100 iterations, and convergence tolerance <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for successive parameter increments. The fitted solution yielded <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>&#8776;</mml:mo><mml:mn>0.35</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> with residual sum of squares <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>&#947;</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.1047</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, establishing the final model as <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0.35</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec3dot2-sensors-25-05368"><title>3.2. Gamma Correction for Direct-View Displays Compensating NVG Nonlinear Response Characteristics</title><p><xref rid="sensors-25-05368-f007" ref-type="fig">Figure 7</xref>a demonstrates close alignment between the fitted curve and empirical NVG response. In digital image processing, the exponent in Model (1) is conventionally designated as gamma [<xref rid="B49-sensors-25-05368" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-05368" ref-type="bibr">50</xref>]. The NVG response effectively applies gamma correction with <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.35</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to input images, as evidenced by the <xref rid="sensors-25-05368-f007" ref-type="fig">Figure 7</xref>b (&#9312;&#8594;&#9313;) transformation. This gamma adjustment induces significant output luminance elevation. To counteract this effect, we implemented inverse gamma transformation by setting the exponent to <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> in Model (1). For the NVG response discussed herein, calculated gamma compensation with <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>2.85</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> generates the <xref rid="sensors-25-05368-f007" ref-type="fig">Figure 7</xref>b (&#9312;&#8594;&#9314;) transformation. Subsequent NVG observation of image &#9314; produces output &#9315;, achieving perceptual equivalence with the original image &#9312; (as indicated by the green arrow in the figure). Due to equipment availability constraints, this study focused exclusively on the NVG-D3 night vision goggle. As a result, the proposed inverse gamma correction method remains relatively straightforward. Future research should extend validation to a broader range of night vision devices to gather more comprehensive data. By integrating these findings with dynamic gamma correction models and JND quantification methods&#8212;such as those proposed by Qian et al. [<xref rid="B51-sensors-25-05368" ref-type="bibr">51</xref>]&#8212;it may be possible to develop a more universal and accurate mathematical compensation framework. Such a model would provide a stronger theoretical foundation for generating realistic night vision imagery using NIR-LED direct-view displays.</p><p>We further validated grayscale rendering fidelity on the NIR-LED direct-view display by implementing gamma compensation at &#947; = 2.85. Results in <xref rid="sensors-25-05368-f008" ref-type="fig">Figure 8</xref> demonstrate significant similarity between images (b) and (d), both exhibiting compressed shadow regions and expanded highlight zones. These characteristics align with the effects of &#947; &#8776; 0.35 transformation observed during NVG processing. Crucially, the output (e) replicates the perceptual gradation of reference (a), while clearly resolving each row of grayscale blocks&#8212;unlike the saturation and loss of contrast observed beyond the fifth row in (d). Simultaneously, it preserves low-luminance grayscale differentiation to enhance dark-scene detail visibility. These findings verify that &#947; = 2.85 compensation prior to NVG observation achieves perceptual equivalence to the original scene, thereby demonstrating effective mitigation of NVG nonlinearity through inverse gamma transformation.</p></sec></sec><sec id="sec4-sensors-25-05368"><title>4. Experimental Results</title><sec id="sec4dot1-sensors-25-05368"><title>4.1. Display Uniformity and Contrast Ratio Characterization</title><p>To quantitatively evaluate the proposed NIR-LED direct-view display performance, we assessed two critical parameters: display uniformity and contrast ratio. Uniformity testing followed IEC 62922 specification [<xref rid="B52-sensors-25-05368" ref-type="bibr">52</xref>] under full-field illumination at maximum brightness, with luminance measured at nine positions in a standardized 3 &#215; 3 grid. Luminance uniformity was calculated as:<disp-formula id="FD5-sensors-25-05368"><label>(5)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes display uniformity and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent maximum and minimum luminance among the nine test points, respectively. Given the display&#8217;s 940 nm emission wavelength, infrared irradiance measurement replaced luminance for performance characterization. Darkroom measurements employed a CAS140CT-156 spectroradiometer with EOP146 probe in direct screen contact, with test images depicted in <xref rid="sensors-25-05368-f009" ref-type="fig">Figure 9</xref>. <xref rid="sensors-25-05368-f009" ref-type="fig">Figure 9</xref>a shows the designed test pattern (1280 &#215; 1024), matching the resolution of the NIR-LED direct-view display to ensure pixel-to-pixel rendering. <xref rid="sensors-25-05368-f009" ref-type="fig">Figure 9</xref>b presents the infrared camera capture, confirming high sharpness and faithful imaging under point-to-point mapping. Despite lower resolution due to sensor limitations, <xref rid="sensors-25-05368-f009" ref-type="fig">Figure 9</xref>c shows that the NVG-resolved image remains discernible, with clear edges and legible text, indicating adequate resolution performance for practical NVG simulation.</p><p>Testing data from points P1 to P9 identified P2 and P8 as the maximum and minimum irradiance points, respectively. Substituting these values into Equation (5) (<xref rid="sensors-25-05368-t002" ref-type="table">Table 2</xref>) yields a display uniformity of 92.12% for the NIR-LED direct-view screen. This high uniformity confirms excellent spatial consistency, generally eliminating luminance compensation requirements in practical applications.</p><p>Higher display contrast ratios expand dynamic range representation&#8212;particularly crucial for low-light night vision scenarios where enhanced contrast preserves shadow detail while preventing image washout. Following IEC 61947-1 Electronic projection&#8212;Measurement of performance standards [<xref rid="B53-sensors-25-05368" ref-type="bibr">53</xref>], we evaluated the NIR-LED direct-view screen using a 16-cell checkerboard test pattern comprising eight white and eight black alternating squares (<xref rid="sensors-25-05368-f010" ref-type="fig">Figure 10</xref>). The contrast ratio is calculated as:<disp-formula id="FD6-sensors-25-05368"><label>(6)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>&#160;</mml:mtext><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the average irradiance of all white and black squares, respectively. Infrared irradiance measurements yielded <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>9.138</mml:mn><mml:mtext>&#160;</mml:mtext><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&#8901;</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>4.195</mml:mn><mml:mtext>&#160;</mml:mtext><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&#8901;</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, producing a screen contrast ratio of <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>&#8776;</mml:mo><mml:mn>2178</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec4dot2-sensors-25-05368"><title>4.2. Night Vision Simulation Effectiveness</title><p>To further validate the NIR-LED display&#8217;s night vision rendering performance, we conducted experiments using an image quality test chart and an urban night scene. The reference image (<xref rid="sensors-25-05368-f011" ref-type="fig">Figure 11</xref>a), captured by a visible-light camera under night-sky conditions, features line patterns for spatial resolution assessment, text elements for evaluating legibility, and grayscale patches for tonal reproduction evaluation. Simultaneously, NVG-captured imagery of the same test chart under identical conditions (<xref rid="sensors-25-05368-f011" ref-type="fig">Figure 11</xref>d) demonstrates NVGs&#8217; capability to resolve fine details and grayscale gradations. For display validation, <xref rid="sensors-25-05368-f011" ref-type="fig">Figure 11</xref>a content was rendered on-screen at gamma settings <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>2.85</mml:mn><mml:mtext>&#160;</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> and observed through NVGs. Key observations were as follows:<list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> rendering (<xref rid="sensors-25-05368-f011" ref-type="fig">Figure 11</xref>e) exhibits highlight clipping in bright regions (as arrow-indicated), compromising grayscale discrimination.</p></list-item><list-item><p><inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>2.85</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> rendering (<xref rid="sensors-25-05368-f011" ref-type="fig">Figure 11</xref>f) maintains discernible details at reduced luminance, while preserving grayscale separation.</p></list-item></list></p><p>Infrared camera captures confirmed that <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>2.85</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> implementation achieved targeted luminance attenuation. These results corroborate <xref rid="sec3dot2-sensors-25-05368" ref-type="sec">Section 3.2</xref> conclusions on NVG-compatible gamma compensation efficacy.</p><p>We validated the protocol using an urban night scene (<xref rid="sensors-25-05368-f012" ref-type="fig">Figure 12</xref>). Direct NVG observation of the actual scene (<xref rid="sensors-25-05368-f012" ref-type="fig">Figure 12</xref>d) achieved excellent clarity and contrast, resolving fine details in low-illuminance areas. When displaying the scene on the NIR-LED screen under NVG observation:<list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> rendering (<xref rid="sensors-25-05368-f012" ref-type="fig">Figure 12</xref>e) exhibits luminance elevation in shadow regions (foliage, lawns, and building facades/windows) combined with expanded highlight saturation zones, collectively inducing resolution degradation.</p></list-item><list-item><p><inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>2.85</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> rendering (<xref rid="sensors-25-05368-f012" ref-type="fig">Figure 12</xref>f) demonstrates contrast characteristics approaching the reference output (<xref rid="sensors-25-05368-f012" ref-type="fig">Figure 12</xref>d), maintaining highlight detail clarity (vehicles and streetlights) despite slightly reduced overall sharpness.</p></list-item></list></p><p>Thus, <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mn>2.85</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> output achieves superior scene fidelity relative to actual night vision observation.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05368"><title>5. Conclusions</title><p>This study pioneers the application of LED-based direct-view displays for pilot night vision goggle (NVG) simulation training. Through comprehensive investigations of LED spectral optimization, NVG response characteristics to NIR-LED displays, and compensation methodologies for NVG nonlinearity, we developed a 940 nm NIR-LED display prototype featuring 1.25 mm pixel pitch, 1280 &#215; 1024 resolution, 60 Hz refresh rate, 92.12% uniformity, and 2178:1 contrast ratio. Compared to traditional NVG simulation displays, the developed NIR-LED direct-view system achieves a significantly higher contrast ratio of 2178:1, greatly enhancing the realism of simulated night vision scenes. Furthermore, this work presents the first successful implementation of NIR-LEDs as an image excitation source for NVG simulation, offering a novel approach for night vision simulator display systems. The proposed inverse gamma transformation effectively compensates for NVG nonlinear response, with efficacy validated through qualitative analysis. Comparative assessments demonstrate that NVG observation of this NIR-LED display produces visual perceptions closely approximating direct observation of actual night scenes, confirming its applicability for pilot NVG simulation training. Furthermore, this work establishes a technical foundation for integrated visible-light/NIR-LED displays capable of supporting all-weather simulation training systems under both daylight and nocturnal conditions. Collectively, this system resolves critical fidelity challenges in NVG simulation imagery for pilots, thereby enhancing flight safety during NVG-assisted night operations, and provides a basis for developing next-generation LED-based all-weather display systems.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Writing&#8212;original draft, Y.Z.; Writing&#8212;review and editing, Y.Z., B.X. and K.Q. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets generated during this study are available from the corresponding author on reasonable request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05368"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raghunandan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Sekhar</surname><given-names>M.B.</given-names></name><name name-style="western"><surname>Tripathy</surname><given-names>N.K.</given-names></name><name name-style="western"><surname>Joshi</surname><given-names>V.V.</given-names></name></person-group><article-title>Study of performance characteristics of ANVIS MK-III night-vision goggle and comparison with other NVGs for aviation usage</article-title><source>Indian J. Aerosp. Med.</source><year>2022</year><volume>66</volume><fpage>21</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.25259/IJASM_10_2022</pub-id></element-citation></ref><ref id="B2-sensors-25-05368"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jian</surname><given-names>B.L.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>C.C.</given-names></name></person-group><article-title>Development of an automatic testing platform for Aviator&#8217;s night vision goggle honeycomb defect inspection</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>1403</elocation-id><pub-id pub-id-type="doi">10.3390/s17061403</pub-id><pub-id pub-id-type="pmid">28617310</pub-id><pub-id pub-id-type="pmcid">PMC5492003</pub-id></element-citation></ref><ref id="B3-sensors-25-05368"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Podobedov</surname><given-names>V.B.</given-names></name><name name-style="western"><surname>Eppeldauer</surname><given-names>G.P.</given-names></name><name name-style="western"><surname>Larason</surname><given-names>T.C.</given-names></name></person-group><article-title>Calibration of night vision goggles: An SI-units-based gain measurement technique</article-title><source>Appl. Opt.</source><year>2017</year><volume>56</volume><fpage>5830</fpage><lpage>5837</lpage><pub-id pub-id-type="doi">10.1364/AO.56.005830</pub-id><pub-id pub-id-type="pmid">29047897</pub-id><pub-id pub-id-type="pmcid">PMC5679469</pub-id></element-citation></ref><ref id="B4-sensors-25-05368"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>D.</given-names></name><name name-style="western"><surname>Tripathy</surname><given-names>N.K.</given-names></name><name name-style="western"><surname>Raghunandan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Sekhar</surname><given-names>B.M.</given-names></name></person-group><article-title>Visual acuity through Night Vision Goggles (NVGs): A comparative assessment between Gen 2<sup>++</sup> and Gen 3 NVGs under different illumination conditions</article-title><source>Indian J. Aerosp. Med.</source><year>2021</year><volume>65</volume><fpage>17</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.25259/IJASM_15_2021</pub-id></element-citation></ref><ref id="B5-sensors-25-05368"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Estrera</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Ostromek</surname><given-names>T.E.</given-names></name><name name-style="western"><surname>Isbell</surname><given-names>W.</given-names></name><name name-style="western"><surname>Bacarella</surname><given-names>A.V.</given-names></name></person-group><article-title>Modern Night Vision Goggles for Advanced Infantry Applications</article-title><source>Proceedings of the Helmet-and Head-Mounted Displays VIII: Technologies and Applications</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>21&#8211;23 April 2003</conf-date><volume>Volume 5079</volume><fpage>196</fpage><lpage>207</lpage></element-citation></ref><ref id="B6-sensors-25-05368"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Task</surname><given-names>H.L.</given-names></name><name name-style="western"><surname>Pinkus</surname><given-names>A.R.</given-names></name></person-group><article-title>Theoretical and Applied Aspects of Night Vision Goggle Resolution and Visual Acuity Assessment</article-title><source>Proceedings of the Head-and Helmet-Mounted Displays XII: Design and Applications</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>9&#8211;13 April 2007</conf-date><volume>Volume 6557</volume><fpage>183</fpage><lpage>193</lpage></element-citation></ref><ref id="B7-sensors-25-05368"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Stoychev</surname><given-names>I.</given-names></name><name name-style="western"><surname>Hutov</surname><given-names>I.</given-names></name></person-group><article-title>Night Vision Monocular-Basic Elements and Development Trends. In Environment. Technology. Resources</article-title><source>Proceedings of the International Scientific and Practical Conference</source><conf-loc>Rezekene, Latvia</conf-loc><conf-date>27&#8211;28 June 2024</conf-date><volume>Volume 4</volume><fpage>260</fpage><lpage>268</lpage></element-citation></ref><ref id="B8-sensors-25-05368"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Brickner</surname><given-names>M.S.</given-names></name></person-group><source>Helicopter Flights with Night-Vision Goggles: Human Factors Aspects (No. NASA-TM-101039)</source><publisher-name>NASA</publisher-name><publisher-loc>Moffett Field, CA, USA</publisher-loc><year>1989</year></element-citation></ref><ref id="B9-sensors-25-05368"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Klaes</surname><given-names>S.</given-names></name></person-group><article-title>Flying with Night Vision Goggles: The Desire for realistic Flight Training</article-title><source>Proceedings of the AIAA Modeling and Simulation Technologies Conference and Exhibit</source><conf-loc>Honolulu, HA, USA</conf-loc><conf-date>18&#8211;21 August 2008</conf-date><fpage>7031</fpage></element-citation></ref><ref id="B10-sensors-25-05368"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chrzanowski</surname><given-names>K.</given-names></name></person-group><article-title>Review of night vision technology</article-title><source>Opto-Electron. Rev.</source><year>2013</year><volume>21</volume><fpage>153</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.2478/s11772-013-0089-3</pub-id></element-citation></ref><ref id="B11-sensors-25-05368"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Arthur</surname><given-names>K.</given-names></name></person-group><article-title>Effects of Field of View on Task Performance with Head-Mounted Displays</article-title><source>Proceedings of the Conference Companion on Human Factors in Computing Systems</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>13&#8211;18 April 1996</conf-date><fpage>29</fpage><lpage>30</lpage></element-citation></ref><ref id="B12-sensors-25-05368"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Psotka</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lewis</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>King</surname><given-names>D.</given-names></name></person-group><article-title>Effects of field of view on judgments of self-location: Distortions in distance estimations even when the image geometry exactly fits the field of view</article-title><source>Presence</source><year>1998</year><volume>7</volume><fpage>352</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1162/105474698565776</pub-id></element-citation></ref><ref id="B13-sensors-25-05368"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><article-title>Survey on night vision goggles usage in helicopter crew and its enlightenment on curriculum construction of helicopter medical rescue in night battlefield</article-title><source>Acad. J. Chin. Pla. Med. Sch.</source><year>2023</year><volume>44</volume><fpage>1340</fpage><lpage>1343</lpage></element-citation></ref><ref id="B14-sensors-25-05368"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stasiak</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zyskowska</surname><given-names>M.</given-names></name><name name-style="western"><surname>G&#322;owinkowska</surname><given-names>I.</given-names></name><name name-style="western"><surname>Kowalczuk</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lewkowicz</surname><given-names>R.</given-names></name></person-group><article-title>Influence of night vision goggles with white and green phosphor screens on selected parameters of the eye and fatigue</article-title><source>Ergonomics</source><year>2021</year><volume>65</volume><fpage>999</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1080/00140139.2021.2008019</pub-id><pub-id pub-id-type="pmid">34787060</pub-id></element-citation></ref><ref id="B15-sensors-25-05368"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>Night vision anti-halation method based on infrared and visible video fusion</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>7494</elocation-id><pub-id pub-id-type="doi">10.3390/s22197494</pub-id><pub-id pub-id-type="pmid">36236591</pub-id><pub-id pub-id-type="pmcid">PMC9573233</pub-id></element-citation></ref><ref id="B16-sensors-25-05368"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Parush</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gauthier</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Arseneau</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>D.</given-names></name></person-group><article-title>The human factors of night vision goggles: Perceptual, cognitive, and physical factors</article-title><source>Rev. Hum. Factors Ergon.</source><year>2011</year><volume>7</volume><fpage>238</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1177/1557234X11410392</pub-id></element-citation></ref><ref id="B17-sensors-25-05368"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Du</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Depth perception and distance assessment under night vision goggles and their influence factor</article-title><source>Man-Machine-Environment System Engineering</source><person-group person-group-type="editor"><name name-style="western"><surname>Long</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dhillon</surname><given-names>B.S.</given-names></name></person-group><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2023</year><volume>Volume 941</volume><fpage>115</fpage><lpage>125</lpage></element-citation></ref><ref id="B18-sensors-25-05368"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hogervorst</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Kooi</surname><given-names>F.L.</given-names></name></person-group><article-title>NVG-the-Day: Towards Realistic Night-Vision Training</article-title><source>Proceedings of the Electro-Optical and Infrared Systems: Technology and Applications XI</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>22&#8211;25 September 2014</conf-date><volume>Volume 9249</volume><fpage>213</fpage><lpage>222</lpage></element-citation></ref><ref id="B19-sensors-25-05368"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lazic</surname><given-names>D.A.</given-names></name><name name-style="western"><surname>Grujic</surname><given-names>V.</given-names></name><name name-style="western"><surname>Tanaskovic</surname><given-names>M.</given-names></name></person-group><article-title>The role of flight simulation in flight training of pilots for crisis management</article-title><source>South Florida J. Dev.</source><year>2022</year><volume>3</volume><fpage>3624</fpage><lpage>3636</lpage><pub-id pub-id-type="doi">10.46932/sfjdv3n3-046</pub-id></element-citation></ref><ref id="B20-sensors-25-05368"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gruji&#263;</surname><given-names>V.</given-names></name><name name-style="western"><surname>Tanaskovi&#263;</surname><given-names>M.</given-names></name></person-group><article-title>The Role of Flight Simulation in Flight Training of Pilots for Crisis Management</article-title><source>Proceedings of the Sinteza 2020&#8212;International Scientific Conference on Information Technology and Data Related Research</source><conf-loc>Belgrade, Serbia</conf-loc><conf-date>17&#8211;23 October 2020</conf-date><fpage>214</fpage><lpage>221</lpage></element-citation></ref><ref id="B21-sensors-25-05368"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Dyer</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Young</surname><given-names>K.M.</given-names></name></person-group><source>Night Vision Goggle Research and Training Issues for Ground Forces: A Literature Review</source><publisher-name>Army Research Institute for Behavioral and Social Sciences</publisher-name><publisher-loc>Fort Benning, GA, USA</publisher-loc><year>1998</year></element-citation></ref><ref id="B22-sensors-25-05368"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>R.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Si</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>R.</given-names></name><name name-style="western"><surname>Weng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Application and Technology Development of Night Vision Goggle for US Army</article-title><source>Proceedings of the SPIE 12921, Third International Computing Imaging Conference (CITA 2023)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>1&#8211;3 June 2023</conf-date><volume>Volume 12921</volume></element-citation></ref><ref id="B23-sensors-25-05368"><label>23.</label><element-citation publication-type="webpage"><article-title>Aeromedical Aspects of Night Vision Device (NVD) Training&#8212;Aamedp-1.21, Edition A. (No Enabled Versions); Standards Central</article-title><year>2022</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://publishers.standardstech.com/content/military-dod-stanag-7147" ext-link-type="uri">https://publishers.standardstech.com/content/military-dod-stanag-7147</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-09">(accessed on 9 July 2025)</date-in-citation></element-citation></ref><ref id="B24-sensors-25-05368"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kooi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Toet</surname><given-names>A.</given-names></name></person-group><article-title>What&#8217;s Crucial in Night Vision Goggle Simulation?</article-title><source>Proceedings of the SPIE&#8212;The International Society for Optical Engineering</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>28 March&#8211;1 April 2005</conf-date><volume>Volume 5802</volume><fpage>37</fpage><lpage>46</lpage></element-citation></ref><ref id="B25-sensors-25-05368"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Martin</surname><given-names>E.</given-names></name><name name-style="western"><surname>Howard</surname><given-names>C.</given-names></name></person-group><article-title>53.4: Night vision goggles: Issues for simulation and training</article-title><source>SID Symp. Dig. Tech. Pap.</source><year>2001</year><volume>32</volume><fpage>1304</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1889/1.1831802</pub-id></element-citation></ref><ref id="B26-sensors-25-05368"><label>26.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sproge</surname><given-names>S.</given-names></name></person-group><article-title>Night Vision Goggle Simulation in a Mixed Reality Flight Simulator with Seamless Integrated Real World</article-title><source>Master&#8217;s Thesis</source><publisher-name>Link&#246;ping University</publisher-name><publisher-loc>Link&#246;ping, Sweden</publisher-loc><year>2024</year></element-citation></ref><ref id="B27-sensors-25-05368"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Beilstein</surname><given-names>D.L.</given-names></name></person-group><article-title>Visual Simulation of Night Vision Goggles in a Chromakeyed, Augmented, Virtual Environment</article-title><source>Ph.D. Thesis</source><publisher-name>Naval Postgraduate School</publisher-name><publisher-loc>Monterey, CA, USA</publisher-loc><year>2003</year></element-citation></ref><ref id="B28-sensors-25-05368"><label>28.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Curley</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Huggins</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ellis</surname><given-names>S.</given-names></name><name name-style="western"><surname>Meyer</surname><given-names>F.</given-names></name><name name-style="western"><surname>Grayson</surname><given-names>E.</given-names></name><name name-style="western"><surname>Johnston</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lawrence</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Diemunsch</surname><given-names>J.</given-names></name></person-group><article-title>NOCTURNAL: A virtual reality simulation exploring the effects of night vision goggle configurations and luminance conditions on human performance</article-title><source>International Conference on Human-Computer Interaction</source><person-group person-group-type="editor"><name name-style="western"><surname>Stephanidis</surname><given-names>C.</given-names></name><name name-style="western"><surname>Antona</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ntoa</surname><given-names>S.</given-names></name><name name-style="western"><surname>Salvendy</surname><given-names>G.</given-names></name></person-group><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2025</year><fpage>14</fpage><lpage>23</lpage></element-citation></ref><ref id="B29-sensors-25-05368"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ernst</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Laudien</surname><given-names>T.</given-names></name><name name-style="western"><surname>Schmerwitz</surname><given-names>S.</given-names></name></person-group><article-title>Implementation of a Mixed-Reality Flight Simulator: Blending Real and Virtual with a Video-See-Through Headmounted Display</article-title><source>Proceedings of the Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications V</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>30 April&#8211;5 May 2023</conf-date><volume>Volume 12538</volume><fpage>181</fpage><lpage>190</lpage></element-citation></ref><ref id="B30-sensors-25-05368"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baarspul</surname><given-names>M.</given-names></name></person-group><article-title>A review of flight simulation techniques</article-title><source>Prog. Aerosp. Sci.</source><year>1990</year><volume>27</volume><fpage>1</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/0376-0421(90)90006-6</pub-id></element-citation></ref><ref id="B31-sensors-25-05368"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Miljkovi&#263;</surname><given-names>D.</given-names></name></person-group><article-title>Wide-View Visual Systems for Flight Simulation</article-title><source>Proceedings of the MIPRO 2009</source><conf-loc>Opatija, Hrvatska</conf-loc><conf-date>25&#8211;29 May 2009</conf-date><fpage>375</fpage><lpage>380</lpage></element-citation></ref><ref id="B32-sensors-25-05368"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name></person-group><article-title>Ergonomic analysis of pilot&#8217;s night vision goggles in operational use and performance improvement</article-title><source>Infrared Technol.</source><year>2022</year><volume>44</volume><fpage>1287</fpage><lpage>1292</lpage></element-citation></ref><ref id="B33-sensors-25-05368"><label>33.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Hogervorst</surname><given-names>M.</given-names></name></person-group><article-title>Toward Realistic Night-Vision Simulation; SPIE Newsroom</article-title><year>2009</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.spie.org/news/1573-toward-realistic-night-vision-simulation" ext-link-type="uri">https://www.spie.org/news/1573-toward-realistic-night-vision-simulation</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-09">(accessed on 9 July 2025)</date-in-citation></element-citation></ref><ref id="B34-sensors-25-05368"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ali</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Lyon</surname><given-names>P.</given-names></name><name name-style="western"><surname>De Meerleer</surname><given-names>P.</given-names></name></person-group><article-title>Night Vision Goggle Stimulation Using LCoS and DLP Projection Technology, which is Better?</article-title><source>Proceedings of the Display Technologies and Applications for Defense, Security, and Avionics VIII; and Head-and Helmet-Mounted Displays XIX</source><conf-loc>Baltimore, MD, USA</conf-loc><conf-date>5&#8211;9 May 2014</conf-date><volume>Volume 9086</volume><fpage>908602</fpage></element-citation></ref><ref id="B35-sensors-25-05368"><label>35.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Clark</surname><given-names>J.</given-names></name></person-group><article-title>Physics-Based Stimulation for Night Vision Goggle Simulation (No. AFRLHEAZTR20060050)</article-title><year>2006</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://apps.dtic.mil/sti/citations/ADA458393" ext-link-type="uri">https://apps.dtic.mil/sti/citations/ADA458393</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-09">(accessed on 9 July 2025)</date-in-citation></element-citation></ref><ref id="B36-sensors-25-05368"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Martin</surname><given-names>E.</given-names></name><name name-style="western"><surname>Clark</surname><given-names>J.</given-names></name></person-group><article-title>Physics Based Simulation of Night Vision Goggles</article-title><source>Presented at the IMAGE 2000 Conference</source><conf-loc>Scottsdale, AZ, USA</conf-loc><conf-date>10&#8211;14 July 2000</conf-date></element-citation></ref><ref id="B37-sensors-25-05368"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Burggraf</surname><given-names>H.</given-names></name><name name-style="western"><surname>Plass</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lloyd</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Nigus</surname><given-names>S.G.</given-names></name><name name-style="western"><surname>Ford</surname><given-names>B.K.</given-names></name></person-group><article-title>High level Vis IR Stimulated NVG Training</article-title><source>Proceedings of the IMAGE 09 Conference</source><conf-loc>St. Louis, MO, USA</conf-loc><conf-date>10&#8211;15 October 2009</conf-date></element-citation></ref><ref id="B38-sensors-25-05368"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lloyd</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Nigus</surname><given-names>S.G.</given-names></name><name name-style="western"><surname>Ford</surname><given-names>B.K.</given-names></name></person-group><article-title>Towards repeatable, deterministic NVG Stimulation</article-title><source>Proceedings of the IMAGE 08 Conference</source><conf-loc>St. Louis, MO, USA</conf-loc><conf-date>31 May&#8211;4 June 2008</conf-date></element-citation></ref><ref id="B39-sensors-25-05368"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lloyd</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Nigus</surname><given-names>G.S.</given-names></name><name name-style="western"><surname>Ford</surname><given-names>B.K.</given-names></name><name name-style="western"><surname>Linn</surname><given-names>T.</given-names></name></person-group><article-title>Proposed Method of Measuring Display Systems for Training with Stimulated Night Vision Goggles</article-title><source>Proceedings of the 2010 Image Conference</source><conf-loc>Scottsdale, AZ, USA</conf-loc><conf-date>6&#8211;7 July 2010</conf-date></element-citation></ref><ref id="B40-sensors-25-05368"><label>40.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>K.</given-names></name></person-group><article-title>A novel LED spherical visual display system and its image geometric correction method</article-title><source>Artificial Intelligence and Human-Computer Interaction</source><person-group person-group-type="editor"><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Siarry</surname><given-names>P.</given-names></name></person-group><publisher-name>IOS Press</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2024</year><volume>Volume 385</volume><fpage>67</fpage><lpage>75</lpage></element-citation></ref><ref id="B41-sensors-25-05368"><label>41.</label><element-citation publication-type="book"><std>ISO/CIE 11664-1:2019(E)</std><source>Colorimetry&#8212;Part 1: CIE Standard Colorimetric Observers</source><publisher-name>ISO</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2019</year></element-citation></ref><ref id="B42-sensors-25-05368"><label>42.</label><element-citation publication-type="book"><std>MIL-STD-3009</std><source>Lighting, Aircraft, Night Vision Imaging System (NVIS) Compatible</source><publisher-name>Department of Defense</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2001</year></element-citation></ref><ref id="B43-sensors-25-05368"><label>43.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vatsia</surname><given-names>M.L.</given-names></name><name name-style="western"><surname>Stich</surname><given-names>U.K.</given-names></name><name name-style="western"><surname>Dunlap</surname><given-names>D.</given-names></name></person-group><source>Night-Sky Radiant Sterance from 450 to 2000 Nanometers (No. ECOM7022)</source><publisher-name>U.S. Army Electronics Command</publisher-name><publisher-loc>Fort Monmouth, NJ, USA</publisher-loc><year>1972</year></element-citation></ref><ref id="B44-sensors-25-05368"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>S.T.</given-names></name></person-group><article-title>Advances and challenges in microdisplays and imaging optics for virtual reality and mixed reality</article-title><source>Device</source><year>2024</year><volume>2</volume><fpage>100398</fpage><pub-id pub-id-type="doi">10.1016/j.device.2024.100398</pub-id></element-citation></ref><ref id="B45-sensors-25-05368"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Miao</surname><given-names>W.C.</given-names></name><name name-style="western"><surname>Hsiao</surname><given-names>F.H.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>Y.H.</given-names></name><name name-style="western"><surname>Tsai</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Chung</surname><given-names>R.J.</given-names></name><etal/></person-group><article-title>Microdisplays: Mini-LED, micro-OLED, and micro-LED</article-title><source>Adv. Optical Mater.</source><year>2024</year><volume>12</volume><fpage>2300112</fpage><pub-id pub-id-type="doi">10.1002/adom.202300112</pub-id></element-citation></ref><ref id="B46-sensors-25-05368"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hsiang</surname><given-names>E.L.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>S.T.</given-names></name></person-group><article-title>Performance comparison between mini-LED backlit LCD and OLED display for 15.6-inch notebook computers</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>1239</elocation-id><pub-id pub-id-type="doi">10.3390/app12031239</pub-id></element-citation></ref><ref id="B47-sensors-25-05368"><label>47.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bj&#246;rck</surname><given-names>&#197;.</given-names></name></person-group><source>Numerical Methods for Least Squares Problems</source><publisher-name>Society for Industrial and Applied Mathematics</publisher-name><publisher-loc>Philadelphia, PA, USA</publisher-loc><year>2024</year></element-citation></ref><ref id="B48-sensors-25-05368"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mascarenhas</surname><given-names>W.F.</given-names></name></person-group><article-title>The divergence of the BFGS and Gauss Newton methods</article-title><source>Math. Program.</source><year>2014</year><volume>147</volume><fpage>253</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1007/s10107-013-0720-6</pub-id></element-citation></ref><ref id="B49-sensors-25-05368"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>Z.H.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>Q.Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.Y.</given-names></name></person-group><article-title>A machine-learning strategy to detect Mura defects in a low-contrast image by piecewise gamma correction</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>1484</elocation-id><pub-id pub-id-type="doi">10.3390/s24051484</pub-id><pub-id pub-id-type="pmid">38475020</pub-id><pub-id pub-id-type="pmcid">PMC10934610</pub-id></element-citation></ref><ref id="B50-sensors-25-05368"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>X.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>Infrared and visible image fusion based on visual saliency map and image contrast enhancement</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>6390</elocation-id><pub-id pub-id-type="doi">10.3390/s22176390</pub-id><pub-id pub-id-type="pmid">36080849</pub-id><pub-id pub-id-type="pmcid">PMC9460677</pub-id></element-citation></ref><ref id="B51-sensors-25-05368"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Hsiang</surname><given-names>E.L.</given-names></name><name name-style="western"><surname>Akimoto</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>S.T.</given-names></name></person-group><article-title>Enhancing a display&#8217;s sunlight readability with tone mapping</article-title><source>Photonics</source><year>2024</year><volume>11</volume><elocation-id>578</elocation-id><pub-id pub-id-type="doi">10.3390/photonics11060578</pub-id></element-citation></ref><ref id="B52-sensors-25-05368"><label>52.</label><element-citation publication-type="book"><std>IEC 62922:2016/AMD1:2021</std><source>Organic Light Emitting Diode (OLED) Panels for General Lighting&#8212;Performance Requirements</source><publisher-name>International Electrotechnical Commission</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2021</year></element-citation></ref><ref id="B53-sensors-25-05368"><label>53.</label><element-citation publication-type="book"><std>IEC 61947-1:2002</std><source>Electronic Projection&#8212;Measurement and Documentation of Key Performance Criteria&#8212;Part 1: Fixed Resolution Projectors</source><publisher-name>International Electrotechnical Commission</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2002</year></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05368-f001" orientation="portrait"><label>Figure 1</label><caption><p>Spectral response profiles of human vision and Generation III NVGs.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g001.jpg"/></fig><fig position="float" id="sensors-25-05368-f002" orientation="portrait"><label>Figure 2</label><caption><p>Current-voltage characteristics and irradiance-to-current response curves of ultra-low current NIR-LEDs: (<bold>a</bold>) current-voltage characteristics; (<bold>b</bold>) irradiance-to-current response under NVG-coupled measurement.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g002.jpg"/></fig><fig position="float" id="sensors-25-05368-f003" orientation="portrait"><label>Figure 3</label><caption><p>System hardware architecture block diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g003.jpg"/></fig><fig position="float" id="sensors-25-05368-f004" orientation="portrait"><label>Figure 4</label><caption><p>Rendering of the LED direct-view display prototype.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g004.jpg"/></fig><fig position="float" id="sensors-25-05368-f005" orientation="portrait"><label>Figure 5</label><caption><p>Grayscale test patterns of the NIR-LED direct-view display: (<bold>a</bold>) original grayscale test chart; (<bold>b</bold>) infrared camera captured image; (<bold>c</bold>) NVG-captured image.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g005.jpg"/></fig><fig position="float" id="sensors-25-05368-f006" orientation="portrait"><label>Figure 6</label><caption><p>Nonlinear response curve of NVGs.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g006.jpg"/></fig><fig position="float" id="sensors-25-05368-f007" orientation="portrait"><label>Figure 7</label><caption><p>Gamma compensation schematic: (<bold>a</bold>) nonlinear response curve of NVGs versus gamma compensation profiles; (<bold>b</bold>) block diagram of inverse gamma correction implementation. (&#9312; Original test chart; &#9313; rendering after &#947; = 0.35 compensation; &#9314; rendering after &#947; = 2.85 compensation; &#9315; output after applying &#947; = 0.35 to 3).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g007.jpg"/></fig><fig position="float" id="sensors-25-05368-f008" orientation="portrait"><label>Figure 8</label><caption><p>Gamma compensation comparative results: (<bold>a</bold>) original grayscale test chart; (<bold>b</bold>) &#947; = 0.35 compensated rendering; (<bold>c</bold>) &#947; = 2.85 compensated rendering; (<bold>d</bold>) NVG-captured image of original display; (<bold>e</bold>) NVG-captured image after &#947; = 2.85 screen compensation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g008.jpg"/></fig><fig position="float" id="sensors-25-05368-f009" orientation="portrait"><label>Figure 9</label><caption><p>Display uniformity test patterns: (<bold>a</bold>) uniformity test pattern; (<bold>b</bold>) infrared camera-captured display image; (<bold>c</bold>) NVG-captured display image.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g009.jpg"/></fig><fig position="float" id="sensors-25-05368-f010" orientation="portrait"><label>Figure 10</label><caption><p>Display contrast test patterns: (<bold>a</bold>) contrast test pattern; (<bold>b</bold>) infrared-camera-captured display image; (<bold>c</bold>) NVG-captured display image.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g010.jpg"/></fig><fig position="float" id="sensors-25-05368-f011" orientation="portrait"><label>Figure 11</label><caption><p>Night vision scene rendering comparison of NIR-LED display: (<bold>a</bold>) reference night scene 1; (<bold>b</bold>) infrared-camera-captured display output (&#947; = 1); (<bold>c</bold>) infrared-camera-captured display output (&#947; = 2.85); (<bold>d</bold>) NVG-captured reference scene 1; (<bold>e</bold>) NVG-captured display output (&#947; = 1); (<bold>f</bold>) NVG-captured display output (&#947; = 2.85).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g011.jpg"/></fig><fig position="float" id="sensors-25-05368-f012" orientation="portrait"><label>Figure 12</label><caption><p>Night vision scene rendering comparison of NIR-LED display: (<bold>a</bold>) reference night scene 2; (<bold>b</bold>) infrared-camera-captured display output (&#947; = 1); (<bold>c</bold>) infrared-camera-captured display output (&#947; = 2.85); (<bold>d</bold>) NVG-captured reference scene 2; (<bold>e</bold>) NVG-captured display output (&#947; = 1); (<bold>f</bold>) NVG-captured display output (&#947; = 2.85).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05368-g012.jpg"/></fig><table-wrap position="float" id="sensors-25-05368-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05368-t001_Table 1</object-id><label>Table 1</label><caption><p>Threshold currents for &#8220;red glow&#8221; effect in NIR-LEDs.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Wavelength (nm)</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">780</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">810</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">830</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">850</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">880</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">900</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">940</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Threshold currents (&#956;A)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">300</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">--</td></tr></tbody></table><table-wrap-foot><fn><p>Notes: (1) These data may exhibit variations due to differences in observer visual perception. (2) &#8220;--&#8221; indicates not visible or undetected.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-05368-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05368-t002_Table 2</object-id><label>Table 2</label><caption><p>Infrared irradiance test data of NIR-LEDs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Point</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P3</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P4</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P6</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P7</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P8</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">P9</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrared irradiance (<inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&#8901;</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.285</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.741</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.265</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.502</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.843</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.276</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.833</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.318</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.372</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>