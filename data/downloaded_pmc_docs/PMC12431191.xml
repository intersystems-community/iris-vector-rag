<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431191</article-id><article-id pub-id-type="pmcid-ver">PMC12431191.1</article-id><article-id pub-id-type="pmcaid">12431191</article-id><article-id pub-id-type="pmcaiid">12431191</article-id><article-id pub-id-type="doi">10.3390/s25175521</article-id><article-id pub-id-type="publisher-id">sensors-25-05521</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Priori Knowledge Makes Low-Light Image Enhancement More Reasonable</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0269-2760</contrib-id><name name-style="western"><surname>Chen</surname><given-names initials="Z">Zefei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6263-7187</contrib-id><name name-style="western"><surname>Lin</surname><given-names initials="Y">Yongjie</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="c1-sensors-25-05521" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Xu</surname><given-names initials="J">Jianmin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Lu</surname><given-names initials="K">Kai</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Huang</surname><given-names initials="Z">Zihao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Wang</surname><given-names initials="Q">Qiong</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Huang</surname><given-names initials="T">Teng</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Pang</surname><given-names initials="Y">Yan</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Liu</surname><given-names initials="J">Jin</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Li</surname><given-names initials="J">Jianjun</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Wang</surname><given-names initials="J">Jia</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05521">School of Civil Engineering &amp; Transportation, South China University of Technology, Guangzhou 510641, China; <email>zefeichen@126.com</email> (Z.C.); <email>aujmxu@scut.edu.cn</email> (J.X.); <email>kailu@scut.edu.cn</email> (K.L.); <email>202010101551@mail.scut.edu.cn</email> (Z.H.)</aff><author-notes><corresp id="c1-sensors-25-05521"><label>*</label>Correspondence: <email>linyjscut@scut.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>04</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5521</elocation-id><history><date date-type="received"><day>21</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>18</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>19</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05521.pdf"/><abstract><p>This paper presents a priori knowledge-based low-light image enhancement framework, termed Priori DCE (Priori Deep Curve Estimation). The priori knowledge consists of two key aspects: (1) enhancing a low-light image is an ill-posed task, as the brightness of the enhanced image corresponding to a low-light image is uncertain. To resolve this issue, we incorporate priori channels into the model to guide the brightness of the enhanced image; (2) during the enhancement of a low-light image, the brightness of pixels may increase or decrease. This paper explores the probability of a pixel&#8217;s brightness increasing/decreasing as its prior enhancement/suppression probability. Intuitively, pixels with higher brightness should have a higher priori suppression probability, while pixels with lower brightness should have a higher priori enhancement probability. Inspired by this, we propose an enhancement function that adaptively adjusts the priori enhancement probability based on variations in pixel brightness. In addition, we propose the Global-Attention Block (GA Block). The GA Block ensures that, during the low-light image enhancement process, each pixel in the enhanced image is computed based on all the pixels in the low-light image. This approach facilitates interactions between all pixels in the enhanced image, thereby achieving visual balance. The experimental results on the LOLv2-Synthetic dataset demonstrate that Priori DCE has a significant advantage. Specifically, compared to the SOTA Retinexformer, the Priori DCE improves the PSNR index and SSIM index from 25.67 and 92.82 to 29.49 and 93.6, respectively, while the NIQE index decreases from 3.94 to 3.91.</p></abstract><kwd-group><kwd>priori knowledge</kwd><kwd>priori channels</kwd><kwd>priori enhancement/suppression probability</kwd><kwd>GA Block</kwd></kwd-group><funding-group><award-group><funding-source>Natural Science Foundation of Guangdong Province Youth Enhancement Project</funding-source><award-id>2023A1515030120</award-id></award-group><award-group><funding-source>State Key Laboratory of Subtropical Building and Urban Science</funding-source><award-id>2023KA04</award-id></award-group><funding-statement>This paper is supported by Natural Science Foundation of Guangdong Province Youth Enhancement Project (2023A1515030120) and State Key Laboratory of Subtropical Building and Urban Science (2023KA04).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05521"><title>1. Introduction</title><p>Due to irreversible environmental factors and technical constraints, some photographs are often captured under suboptimal conditions, such as underexposure or overexposure [<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>]. This not only challenges human visual perception but also poses significant difficulties for more advanced image processing tasks, such as object detection [<xref rid="B2-sensors-25-05521" ref-type="bibr">2</xref>], multi-object tracking [<xref rid="B3-sensors-25-05521" ref-type="bibr">3</xref>], and instance segmentation [<xref rid="B4-sensors-25-05521" ref-type="bibr">4</xref>]. Therefore, low-light image enhancement has been a prominent research focus. Low-light image enhancement is primarily achieved through two methods: (1) adjusting the camera parameters based on the environmental conditions before capturing, such as increasing the ISO, decreasing the shutter speed, and widening the aperture; (2) applying algorithms to map low-light images to reference ones after capturing, such as histogram equalization [<xref rid="B5-sensors-25-05521" ref-type="bibr">5</xref>], gamma correction, and retinex [<xref rid="B6-sensors-25-05521" ref-type="bibr">6</xref>].</p><p>Although existing methods enhance image brightness, they also introduce noise, blur, and artifacts in the enhanced images [<xref rid="B7-sensors-25-05521" ref-type="bibr">7</xref>]. Plain methods, such as histogram equalization and gamma correction, often produce unnatural artifacts. This occurs because these methods naively apply enhancement functions to map low-light images to reference images, without considering the image as a whole. Land et al. [<xref rid="B6-sensors-25-05521" ref-type="bibr">6</xref>] proposed the retinex hypothesis (a synthesis of the retina and cortex). According to this hypothesis, the observed image can be decomposed into two components: an illumination map, which represents the light intensity from the external environment, and a reflectance map, which corresponds to the intrinsic properties of the objects. Many researchers have applied the retinex theory to model the image decomposition process and recover the reflectance map [<xref rid="B8-sensors-25-05521" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05521" ref-type="bibr">9</xref>]. However, due to limitations such as the capturing device and environmental conditions, the observed image often contains noise and distortions, which are subsequently propagated to the reflectance map during decomposition.</p><p>In the past decade, with the rapid development of computational power in computers, methods based on convolutional neural networks (CNNs) [<xref rid="B10-sensors-25-05521" ref-type="bibr">10</xref>] have emerged as mushrooms after the rain [<xref rid="B11-sensors-25-05521" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05521" ref-type="bibr">12</xref>]. Thanks to the powerful ability of convolutional neural networks, these methods demonstrated remarkable performance across a wide range of tasks. However, deep learning methods are data-driven, which limits their application in low-light image enhancement. With the emergence of paired image (low-light image and reference image) datasets, such as LOLv1 [<xref rid="B13-sensors-25-05521" ref-type="bibr">13</xref>], LOLv2 [<xref rid="B14-sensors-25-05521" ref-type="bibr">14</xref>], LSRW [<xref rid="B15-sensors-25-05521" ref-type="bibr">15</xref>] and SICE [<xref rid="B16-sensors-25-05521" ref-type="bibr">16</xref>], deep learning has started to be widely applied to low-light image enhancement tasks. Many researchers have attempted to use CNNs to approximate the decomposition process of the retinex theory [<xref rid="B7-sensors-25-05521" ref-type="bibr">7</xref>,<xref rid="B13-sensors-25-05521" ref-type="bibr">13</xref>,<xref rid="B17-sensors-25-05521" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05521" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05521" ref-type="bibr">19</xref>]. In addition, GANs [<xref rid="B20-sensors-25-05521" ref-type="bibr">20</xref>] have also been widely used in low-light image enhancement, such as pix2pix [<xref rid="B21-sensors-25-05521" ref-type="bibr">21</xref>], CycleGAN [<xref rid="B22-sensors-25-05521" ref-type="bibr">22</xref>], PD-GAN [<xref rid="B23-sensors-25-05521" ref-type="bibr">23</xref>], and EnlightenGAN [<xref rid="B24-sensors-25-05521" ref-type="bibr">24</xref>]. However, deep learning-based methods still suffer from noise, blur, and artifacts. To address this issue, we explain the reasons from the following three perspectives and propose corresponding improvements.</p><p>Firstly, in the process of low-light image enhancement, we intuitively assume that a pixel with higher brightness is more likely to be suppressed (become darker), while a pixel with lower brightness is more likely to be enhanced (become brighter), so as to achieve visual balance in the enhanced image. Once the structure and hyperparameters of the low-light image enhancement model are fixed, the enhancement/suppression probability of pixels is also fixed. Therefore, we refer to this probability as the pixel&#8217;s priori enhancement/suppression probability. Most existing methods are end-to-end black-box models [<xref rid="B7-sensors-25-05521" ref-type="bibr">7</xref>,<xref rid="B25-sensors-25-05521" ref-type="bibr">25</xref>], and their operational mechanisms are uninterpretable. Unlike end-to-end models, ref. Guo [<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>,<xref rid="B26-sensors-25-05521" ref-type="bibr">26</xref>] propose using parameterized enhancement functions to individually map pixels in a low-light image, with parameters generated by a deep learning model. In [<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>,<xref rid="B26-sensors-25-05521" ref-type="bibr">26</xref>], the priori enhancement probability of any pixel is fixed at 0.5, which is evidently unreasonable. To resolve this issue, we introduce an enhancement function that adaptively adjusts the priori enhancement probability based on pixel brightness. This ensures that, during the low-light image enhancement process, pixels with lower brightness have a higher priori enhancement probability, while those with higher brightness have a lower priori enhancement probability.</p><p>Secondly, in existing low-light image enhancement methods, the receptive field [<xref rid="B27-sensors-25-05521" ref-type="bibr">27</xref>] of each pixel in the enhanced image is limited. This limitation causes the pixels to be inconsistent with the overall image, leading to issues such as noise, blur, and artifacts. Ref. [<xref rid="B25-sensors-25-05521" ref-type="bibr">25</xref>] introduces the Squeeze-and-Excitation [<xref rid="B28-sensors-25-05521" ref-type="bibr">28</xref>] into [<xref rid="B29-sensors-25-05521" ref-type="bibr">29</xref>], proposing SE-Res2Net and using it as the backbone network for low-light image enhancement tasks. This method enhances pixel interaction in the enhanced image by averaging each channel and then multiplying it by the corresponding channel, thereby increasing the interaction between pixels to some extent. Recently, Carion et al. [<xref rid="B30-sensors-25-05521" ref-type="bibr">30</xref>] introduced the Transformer [<xref rid="B31-sensors-25-05521" ref-type="bibr">31</xref>] into computer vision tasks, proposing DETR. However, DETR computes interactions between the features at each position and all positions in the feature map, resulting in a substantial computational burden. Zhu et al. [<xref rid="B32-sensors-25-05521" ref-type="bibr">32</xref>] incorporated the deformable convolutions [<xref rid="B33-sensors-25-05521" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05521" ref-type="bibr">34</xref>] into the Transformer, significantly reducing the computational complexity. Inspired by this, this paper proposes the GA Block (Global-Attention Block). The GA Block first computes four reference points for each position in the feature map. Then, the weights of these reference points are calculated based on cosine similarity. Finally, the weighted sum of these features at the four reference points is computed to obtain the output feature at the corresponding position. Since the GA Block computes each pixel in the enhanced image based on all pixels in the low-light image, it alleviates issues such as noise, blur, and artifacts in the enhanced image.</p><p>Thirdly, since the relationship between abnormal images and reference images in low-light image enhancement tasks is many-to-many, the brightness of the enhanced images should be controllable. The brightness of the enhanced images generated by [<xref rid="B13-sensors-25-05521" ref-type="bibr">13</xref>,<xref rid="B24-sensors-25-05521" ref-type="bibr">24</xref>,<xref rid="B35-sensors-25-05521" ref-type="bibr">35</xref>] is uncertain. Ref. Guo [<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>,<xref rid="B26-sensors-25-05521" ref-type="bibr">26</xref>] maps arbitrary low-light images to enhanced images with an average brightness of 0.5. Refs. [<xref rid="B7-sensors-25-05521" ref-type="bibr">7</xref>,<xref rid="B19-sensors-25-05521" ref-type="bibr">19</xref>] incorporate the ratio of the brightness between low-light and reference images into the illumination adjustment process. To address this issue, we propose a priori channels to indicate the brightness of the enhanced image, which is then cross-concat with the low-light image. During the inference stage, the brightness of the enhanced image can be adjusted by modifying the priori channels. The code and experiments have been open-sourced at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/zefeichen/PrioriDCE">https://github.com/zefeichen/PrioriDCE</uri> (accessed on 18 August 2025).</p><p>The contributions of this paper are as follows:<list list-type="order"><list-item><p>This paper presents a parametric enhancement function in which the priori enhancement probability is adaptively adjusted based on pixel&#8217;s brightness.</p></list-item><list-item><p>This paper proposes GA Block, which allows each pixel in the enhanced image to be computed from all the pixels in the low-light image, making the enhanced image appear more natural.</p></list-item><list-item><p>This paper proposes a priori channels for indicating the brightness of the enhanced image, which allows the enhanced image to be freely adjusted.</p></list-item><list-item><p>This paper presents comprehensive experiments, and the results demonstrate that the proposed Priori DCE significantly enhances the quality of the enhanced images.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05521"><title>2. Related Works</title><sec id="sec2dot1-sensors-25-05521"><title>2.1. Plain Methods</title><p>Histogram equalization transforms the probability density function of a low-light image into a uniform distribution using an enhancement function. Pizer et al. [<xref rid="B36-sensors-25-05521" ref-type="bibr">36</xref>] proposed Adaptive Histogram Equalization (AHE). AHE divides the image into several subimages, such as multiple <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> subimages, and then applies histogram equalization to each subimage. Gamma correction maps a low-light image to a reference image using a gamma function. However, due to its simplicity, gamma correction has noticeable drawbacks, such as loss of detail, inappropriate exposure, and unnatural boundary enhancement. To overcome these drawbacks, Bennett et al. [<xref rid="B37-sensors-25-05521" ref-type="bibr">37</xref>] enhanced each pixel in a low-light image by adjusting its virtual exposure. Yuan et al. [<xref rid="B38-sensors-25-05521" ref-type="bibr">38</xref>] used an <italic toggle="yes">S</italic>-shaped curve with two parameters to enhance low-light images, automatically adjusting these two parameters based on the image&#8217;s brightness.</p></sec><sec id="sec2dot2-sensors-25-05521"><title>2.2. Retinex</title><p>Land et al. [<xref rid="B6-sensors-25-05521" ref-type="bibr">6</xref>] first proposed a low-light image enhancement method based on the imaging principles of the camera, named retinex. The principle of retinex is to decompose the observed image into a reflectance map and an illumination map. Based on this principle, they first introduced SSR (Single-Scale Retinex). SSR estimates the illumination map by applying a Gaussian filter to the observed image and then calculates the reflectance map by deriving it from the observed image and the illumination map. Expanding upon SSR, Rahman et al. [<xref rid="B39-sensors-25-05521" ref-type="bibr">39</xref>] introduced MSR (Multi-Scale Retinex), which involves estimating the illumination map through a Gaussian filter at various scales to enhance image details and textures. Nonetheless, akin to SSR, MSR encounters challenges with pronounced color casts in images. To address this issue, Jobson et al. [<xref rid="B40-sensors-25-05521" ref-type="bibr">40</xref>] improved MSR and proposed MSRCR (Multi-Scale Retinex with Color Restoration). MSRCR builds upon MSR by introducing a color restoration factor <italic toggle="yes">C</italic>, to adjust for the color distortion caused by contrast enhancement in local regions of the image. Wang et al. [<xref rid="B41-sensors-25-05521" ref-type="bibr">41</xref>] proposed using a dual logarithmic transformation function to map pixels, which helps balance detail and natural in the enhanced image.</p></sec><sec id="sec2dot3-sensors-25-05521"><title>2.3. Deep Learning</title><p>In recent years, the increasing computational power has led to the widespread application of deep learning-based methods in low-light image enhancement tasks. Given that the decomposition process in retinex theory remains unknown and deep learning models exhibit powerful fitting capabilities, Wei et al. [<xref rid="B13-sensors-25-05521" ref-type="bibr">13</xref>] utilized a deep learning model to fit the decomposition process in the retinex theory and proposed RetinexNet. Jiang et al. [<xref rid="B24-sensors-25-05521" ref-type="bibr">24</xref>] proposed EnlightenGAN, which employs both a local and a global discriminator to evaluate the local and global features of an image, thereby preserving its details and textures in the enhanced image. Cai et al. [<xref rid="B42-sensors-25-05521" ref-type="bibr">42</xref>] incorporated the transformer [<xref rid="B31-sensors-25-05521" ref-type="bibr">31</xref>] into the retinex model, proposing Retinexformer. As the attention mechanism in transformers is pixel-based, it considerably increases the model&#8217;s computational cost. To address this, Retinexformer modifies the attention mechanism from a pixel-based to a channel-based mechanism. However, whether using retinex theory-based methods or end-to-end methods, the essence of these approaches is to use deep learning to fit the mapping relationship from the low-light image to the reference image. Unlike the aforementioned models, Guo et al. [<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>] employ a series of parametric functions as enhancement functions, with a deep learning model generating the parameters for these functions. These enhancement functions are then applied to map the low-light image to the reference image. Although [<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>] is not fundamentally different from previous deep learning-based methods, this approach makes the low-light image enhancement process interpretable. However, the parametric functions used in the methods of [<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>] assign a priori enhancement probability of 0.5 to all pixels, which is clearly unreasonable.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05521"><title>3. Methodology</title><p>In this section, we first present the framework and data flow of the Priori DCE, as illustrated in <xref rid="sensors-25-05521-f001" ref-type="fig">Figure 1</xref>a. Subsequently, we provide a detailed description of the model&#8217;s key components, including the parameter generation model (<xref rid="sec3dot2-sensors-25-05521" ref-type="sec">Section 3.2</xref>), the enhancement function (<xref rid="sec3dot3-sensors-25-05521" ref-type="sec">Section 3.3</xref>), and the training process (<xref rid="sec3dot4-sensors-25-05521" ref-type="sec">Section 3.4</xref>).</p><sec id="sec3dot1-sensors-25-05521"><title>3.1. Pipeline</title><p>Since the photos taken of the same scene change with variations in lighting intensity, the relationship between an abnormal image and a reference image in the same scene is not one-to-one but rather many-to-many. Consequently, the input to Priori DCE comprises two components: the low-light image and the priori channels, as depicted on the left side of <xref rid="sensors-25-05521-f001" ref-type="fig">Figure 1</xref>a. The low-light image represents the scene information under abnormal lighting conditions, while the priori channels represents the desired brightness of the enhanced image. To achieve this goal, during the training stage, we use the channels information of the reference image as the priori channels. Unlike the training stage, the inference stage does not have a corresponding reference image, so we need to predefine the priori channels. Next, we first introduce the data flow during the training stage.</p><list list-type="order"><list-item><p>The reference image serves as the enhancement target for a low-light image of size <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and the process of deriving the priori channels from it is illustrated in <xref rid="sensors-25-05521-f002" ref-type="fig">Figure 2</xref>. The reference image is first segmented by channel into Chan R, Chan G, and Chan B, denoted as <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, the mean value of each corresponding channel is calculated, and these priori channels (Priori R, Priori G, and Priori B) are generated based on these mean values, denoted as <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This process can be represented by Equation (<xref rid="FD1-sensors-25-05521" ref-type="disp-formula">1</xref>).<disp-formula id="FD1-sensors-25-05521"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Cross-concat takes the low-light image and the priori channels as inputs and merges them in the order shown in <xref rid="sensors-25-05521-f001" ref-type="fig">Figure 1</xref>a. The merged result is denoted as <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and the merging process is illustrated in Equation (<xref rid="FD2-sensors-25-05521" ref-type="disp-formula">2</xref>).<disp-formula id="FD2-sensors-25-05521"><label>(2)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>The parameter generation model processes <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, generating parameters <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> that correspond to a set of enhancement functions for each pixel in the low-light image.</p></list-item><list-item><p>The low-light image is enhanced using the obtained enhancement functions to generate the corresponding enhanced image.</p></list-item><list-item><p>Calculate the loss value between the enhanced image and the reference image, then perform backpropagation and update the parameters in the parameter generation model.</p></list-item></list><p>During the inference stage, in order to enhance the low-light image to the corresponding brightness, the low-light image is first split by channels, and then the average value of each channel is calculated to obtain the corresponding Chan R, Chan G, and Chan B. Finally, the Chan R, Chan G, and Chan B are scaled by a factor of <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> to serve as prior channels, as shown in the inference stage of <xref rid="sensors-25-05521-f001" ref-type="fig">Figure 1</xref>a.</p></sec><sec id="sec3dot2-sensors-25-05521"><title>3.2. Parameter Generation</title><p>The parameter generation model is based on UNet [<xref rid="B43-sensors-25-05521" ref-type="bibr">43</xref>] and comprises DownSample, UpSample, Concat, Parameter Head, and GA <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Block</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. <xref rid="sensors-25-05521-f001" ref-type="fig">Figure 1</xref>b shows the details of DownSample, UpSample, and Parameter Head. DownSample is an average pooling layer with a kernel size of <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, stride of 2, and padding of 0. Its primary role is to perform <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> downsampling on the feature map, thereby increasing the model&#8217;s receptive field. UpSample is a deconvolution layer with input channels of <italic toggle="yes">C</italic>, output channels of <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>, a kernel size of <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, a stride of 2, and padding of 0. Its primary role is to double the height and width of the feature map while reducing the channels to half of the original. The role of Concat is to combine feature maps from multiple scales. The Parameter Head comprises a convolutional layer and an activation layer (Tanh+2). The convolutional layer has input channels of 128, output channels of <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, a kernel size of <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, a stride of 1, and padding of 1, where <italic toggle="yes">k</italic> denotes the number of iterations of the enhancement function. This layer adjusts the channels of the output feature map based on the number of iterations of the enhancement function. The activation layer then scales the output feature map to the range <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. GA <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Block</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> refers to a block where the input is a feature map with <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channels and the output is a feature map with <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channels. <xref rid="sensors-25-05521-f003" ref-type="fig">Figure 3</xref> illustrates the structure of GA <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Block</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> on the left and the core GA on the right. In <xref rid="sensors-25-05521-f003" ref-type="fig">Figure 3</xref>, Conv/<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Linear</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicates that the input to the Conv/Linear layer is a feature map with <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channels, and the output is a feature map with <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channels. A Conv/Linear layer without special notation implies that both the input and output feature maps have the same size and channels. Notably, GA <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Block</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> alters only the channels in the feature map (from <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), without modifying its size.</p><p>GA is the core component of GA <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Block</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, as depicted on the right side of <xref rid="sensors-25-05521-f003" ref-type="fig">Figure 3</xref>. To illustrate the data processing procedure of GA, we use the feature <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> at the red point <italic toggle="yes">p</italic> with coordinates <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as an example. The processing steps are outlined as follows.</p><list list-type="order"><list-item><p>A linear transformation of <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is performed using the fully connected layer Linear (top) to obtain <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in Equation (<xref rid="FD3-sensors-25-05521" ref-type="disp-formula">3</xref>).<disp-formula id="FD3-sensors-25-05521"><label>(3)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>The position of the feature <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is encoded [<xref rid="B32-sensors-25-05521" ref-type="bibr">32</xref>], and then the result of the position encoding is added to <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to obtain <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in Equation (<xref rid="FD4-sensors-25-05521" ref-type="disp-formula">4</xref>).<disp-formula id="FD4-sensors-25-05521"><label>(4)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>E</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>The fully connected layer <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Linear</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (below) is used to linearly transform <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, mapping the <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> with <italic toggle="yes">C</italic> channels to an positional feature <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> with 8 channels, where <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in Equation (<xref rid="FD5-sensors-25-05521" ref-type="disp-formula">5</xref>). <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to four reference points related to point <italic toggle="yes">p</italic>, namely the cyan point <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the green point <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the pink point <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and the yellow point <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD5-sensors-25-05521"><label>(5)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>The weight <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the feature <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated based on the cosine similarity between the features <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the feature <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in Equation (<xref rid="FD6-sensors-25-05521" ref-type="disp-formula">6</xref>).<disp-formula id="FD6-sensors-25-05521"><label>(6)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Calculate the weighted sum of the four reference points to obtain the output feature at the red point <italic toggle="yes">p</italic>, as shown in Equation (<xref rid="FD7-sensors-25-05521" ref-type="disp-formula">7</xref>).<disp-formula id="FD7-sensors-25-05521"><label>(7)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item></list></sec><sec id="sec3dot3-sensors-25-05521"><title>3.3. Enhancement Function</title><sec id="sec3dot3dot1-sensors-25-05521"><title>3.3.1. Priori Probability</title><p>Under conditions of excessively strong or weak lighting, photos may suffer from overexposure or underexposure. The SICE dataset [<xref rid="B16-sensors-25-05521" ref-type="bibr">16</xref>] includes a variety of abnormal images taken under different lighting conditions, along with their corresponding reference images. <xref rid="sensors-25-05521-f004" ref-type="fig">Figure 4</xref> demonstrates the pixel ratios in the grey, R, G, and B channels that require enhancement (brightening) as brightness changes during the transformation from abnormal images to reference images. It is evident that as pixels&#8217; brightness increases, the proportion of pixels requiring enhancement decreases gradually. This suggests that during the enhancement process of abnormal images, a pixel with higher brightness should have a higher probability to be suppressed, while a pixel with lower brightness should have a higher probability to be enhanced.</p><p>The enhancement function is formed by a series of parameterized base functions that undergo multiple iterations. These base functions are denoted as <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>&#945;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is a parameter and satisfies <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For convenience, this paper denotes the enhancement function with <italic toggle="yes">k</italic> iterations as <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the parameter corresponding to the base function at the <italic toggle="yes">i</italic>-th iteration. The enhancement function maps the pixel&#8217;s brightness from <italic toggle="yes">x</italic> to <italic toggle="yes">y</italic>. The enhancement function obtained by iterating the basis function once or twice is shown in Equations (<xref rid="FD8-sensors-25-05521" ref-type="disp-formula">8</xref>) and (<xref rid="FD9-sensors-25-05521" ref-type="disp-formula">9</xref>), respectively, while the enhancement function with three or more iterations of the basis function follow a similar form.<disp-formula id="FD8-sensors-25-05521"><label>(8)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05521"><label>(9)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a <italic toggle="yes">k</italic>-dimensional space spanned over the interval <italic toggle="yes">I</italic>, and the measure <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> represents the size of space <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. For example, <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is a 1-dimensional space, and its corresponding measure <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is the length; <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is a 2-dimensional space, and its corresponding measure <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is the area; <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is a 3-dimensional space, and its corresponding measure <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is the volume. Each point in the space <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents an enhancement function. For a pixel with brightness <italic toggle="yes">x</italic>, the space <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is divided into three subspaces based on whether the pixel is enhanced: (1) the subspace where <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, corresponding to an enhancement function that increases the pixel&#8217;s brightness; this subspace is denoted as <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>; (2) the subspace where <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, corresponding to an enhancement function that leaves the pixel unchanged; this subspace is denoted as <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>; (3) the subspace where <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, corresponding to an enhancement function that decreases the pixel&#8217;s brightness; this subspace is denoted as <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. It is important to note that, whether increasing, leaving unchanged, or decreasing the pixel&#8217;s brightness, the overall goal is to improve the image&#8217;s quality.</p><p>Given that the parameter <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the output of the deep learning model, and the deep learning model is an inherently uninterpretable black-box model. Therefore, we assume that the probability of <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> taking any value in the interval <italic toggle="yes">I</italic> is equal, which means that the parameter <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be considered as a random variable following a uniform distribution on the interval <italic toggle="yes">I</italic>, denoted as <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the joint distribution of <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; thus, the probability density function of <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is given by Equation (<xref rid="FD10-sensors-25-05521" ref-type="disp-formula">10</xref>).<disp-formula id="FD10-sensors-25-05521"><label>(10)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Based on the above analysis, the probability that a pixel with brightness <italic toggle="yes">x</italic> is enhanced, unchanged, or suppressed is shown in Equation (<xref rid="FD11-sensors-25-05521" ref-type="disp-formula">11</xref>).<disp-formula id="FD11-sensors-25-05521"><label>(11)</label><mml:math id="mm89" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>M</mml:mi><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>M</mml:mi><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>M</mml:mi><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo></mml:msubsup></mml:msub><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) are solely dependent on the basis functions, which are determined and immutable. Consequently, these probabilities are termed the prior enhancement probability, priori unchanged probability, and priori suppression probability for a pixel with brightness <italic toggle="yes">x</italic>, respectively. For simplicity, we collectively refer to these probabilities as priori probabilities.</p></sec><sec id="sec3dot3dot2-sensors-25-05521"><title>3.3.2. The Shortcomings of the Current Method</title><p>Zero DCE employs a series of parameterized quadratic functions <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>&#945;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as basis functions, as showed in Equation (<xref rid="FD12-sensors-25-05521" ref-type="disp-formula">12</xref>).<disp-formula id="FD12-sensors-25-05521"><label>(12)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>&#945;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><xref rid="sensors-25-05521-f005" ref-type="fig">Figure 5</xref>a illustrates the curves of three special basis functions <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which also act as the enhancement functions when the basis functions are iterated once. The horizontal axis represents the original pixel&#8217;s brightness, while the vertical axis represents the enhanced brightness. Specifically, the enhancement function <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> coincides with <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. As shown in <xref rid="sensors-25-05521-f005" ref-type="fig">Figure 5</xref>a, for a pixel with an original brightness of <italic toggle="yes">x</italic>, when the parameter <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> gradually changes from <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to 0, the enhanced brightness decreases from <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mspace width="4pt"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>; when the parameter <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> then gradually changes from 0 to 1, the enhanced brightness decreases further from <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The space <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of parameter <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is 1-dimensional, and its corresponding measure <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is a length of 2. At the same time, the measures of its corresponding subspaces <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> are both 1. Furthermore, the priori enhancement probability <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and priori suppression probability <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> satisfy <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8801;</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8801;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The enhancement function <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is derived by iterating the basis function <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>&#945;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> twice. At this point, the parameter space <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is a 2-dimensional plane, and the corresponding measure <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is a square with an area of 4. <xref rid="sensors-25-05521-f005" ref-type="fig">Figure 5</xref>b shows pixels with different brightness and the corresponding planar space <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The vertical axis (<italic toggle="yes">x</italic>-axis) represents the pixel&#8217;s brightness. The <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> space is a parameter space composed of parameters <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. Surfaces of the same color are parallel to the <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> plane, representing the subspace <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> of a pixel with brightness <italic toggle="yes">x</italic>, while different colors represent the area of the corresponding surface. It can be seen that as we move upward along the vertical axis, the pixels&#8217; brightness <italic toggle="yes">x</italic> gradually increases, and the area of the corresponding subspace <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> decreases from 1.61 (red) to 1.34 (blue).</p><p><xref rid="sensors-25-05521-f006" ref-type="fig">Figure 6</xref> illustrates the priori enhancement probabilitiy for the enhancement functions <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. It is observed that the priori enhancement probability of <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> remains constant at 0.5, whereas the priori enhancement probabilities of <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> decrease as brightness increases. Overall, these priori enhancement probabilities remain below 0.5. In other words, the prior suppression probabilities for <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> consistently exceed their prior enhancement probabilities, which is unreasonable.</p></sec><sec id="sec3dot3dot3-sensors-25-05521"><title>3.3.3. Solutions</title><p>To solve the problem raised in <xref rid="sec3dot3dot2-sensors-25-05521" ref-type="sec">Section 3.3.2</xref>, in this section, we propose using the cubic function <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>&#945;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as the basis function, as shown in Equation (<xref rid="FD13-sensors-25-05521" ref-type="disp-formula">13</xref>).<disp-formula id="FD13-sensors-25-05521"><label>(13)</label><mml:math id="mm135" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>&#945;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#183;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><xref rid="sensors-25-05521-f007" ref-type="fig">Figure 7</xref>a shows the curves of these basis functions <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. It can be seen that for a pixel with brightness <italic toggle="yes">x</italic>, when the parameter <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> gradually increases from 1 to <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>, its enhanced brightness decreases from <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. During this process, the pixel&#8217;s brightness is enhanced. When the parameter <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> gradually increases from <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> to 3, the enhanced brightness decreases from <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. During this process, the pixel&#8217;s brightness is suppressed. For a pixel with brightness <italic toggle="yes">x</italic>, its priori enhancement probability is <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1.5</mml:mn></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. In other words, the priori enhancement probability of a pixel with brightness <italic toggle="yes">x</italic> is adaptive.</p><p>When the basis function <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>&#945;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is iterated twice to obtain the enhancement function <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, the corresponding parameter space <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> forms a square with the area of 4. Similar to <xref rid="sensors-25-05521-f005" ref-type="fig">Figure 5</xref>b, <xref rid="sensors-25-05521-f007" ref-type="fig">Figure 7</xref>b shows the enhancement subspace <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> corresponding to pixels with different brightness. It can be observed that as <italic toggle="yes">x</italic> increases, the area of the enhancement subspace <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> gradually decreases from 4 (red) to 0.94 (blue).</p><p><xref rid="sensors-25-05521-f006" ref-type="fig">Figure 6</xref> shows these priori enhancement probabilities of the enhancement functions <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. It can be observed that as the pixel&#8217;s brightness gradually increases, these priori enhancement probabilities decrease from 1 to 0.</p><p>From the above analysis, it can be seen that (<inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>) is more suitable as an enhancement function than (<inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>).</p></sec></sec><sec id="sec3dot4-sensors-25-05521"><title>3.4. Training</title><p>The Structural Similarity Index Measure (SSIM) quantifies the similarity between two different images in terms of brightness, contrast, and structure. When SSIM is used as a loss function, it is defined by Equation (<xref rid="FD14-sensors-25-05521" ref-type="disp-formula">14</xref>):<disp-formula id="FD14-sensors-25-05521"><label>(14)</label><mml:math id="mm163" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>&#956;</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>&#956;</mml:mi><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">j</italic> represents the index of an image, while <italic toggle="yes">J</italic> denotes the total number of images. <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> refers to the low-light image, and <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> corresponds to the enhanced image. <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:msub></mml:mrow></mml:math></inline-formula> are the mean values of <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm169" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, respectively. <inline-formula><mml:math id="mm170" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm171" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:msub></mml:mrow></mml:math></inline-formula> represent the variances of <inline-formula><mml:math id="mm172" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, respectively, while <inline-formula><mml:math id="mm174" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the covariance between <inline-formula><mml:math id="mm175" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm177" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm178" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are constants.</p><p>The Mean Squared Error (MSE) is employed to quantify the brightness difference between the low-light image and the enhanced image. When MSE is used as a loss function, it is defined by Equation (<xref rid="FD15-sensors-25-05521" ref-type="disp-formula">15</xref>).<disp-formula id="FD15-sensors-25-05521"><label>(15)</label><mml:math id="mm179" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> represent the height and width of the image, respectively. <inline-formula><mml:math id="mm180" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the brightness of the pixel at coordinates <inline-formula><mml:math id="mm181" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the image <inline-formula><mml:math id="mm182" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>The total loss function includes <inline-formula><mml:math id="mm183" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm184" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, as shown in the following Equation (<xref rid="FD16-sensors-25-05521" ref-type="disp-formula">16</xref>).<disp-formula id="FD16-sensors-25-05521"><label>(16)</label><mml:math id="mm185" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm186" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm187" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent the weights of the loss functions <inline-formula><mml:math id="mm188" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm189" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which are set to 20 and 5, respectively, in this paper.</p></sec></sec><sec id="sec4-sensors-25-05521"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05521"><title>4.1. Experimental Details</title><sec id="sec4dot1dot1-sensors-25-05521"><title>4.1.1. Implementation Details</title><p>All experiments in this paper were conducted on a platform with the Ubuntu 22.04 operating system, and the hardware environment consists of Intel i7-13700K CPU, NVIDIA RTX 4090 24 GB GPU, 32 GB RAM. The Priori DCE is implemented based on the PyTorch framework. To accelerate the convergence of the model training process, the optimizer is set to AdamW [<xref rid="B44-sensors-25-05521" ref-type="bibr">44</xref>] with a weight decay of <inline-formula><mml:math id="mm190" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>e</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and the batchsize, numworkers, epochs, and initial learning rate are set to 1, 4, 120, and <inline-formula><mml:math id="mm191" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>e</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. The learning rate decay strategy is that reducing the learning rate to <inline-formula><mml:math id="mm192" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> of its previous value every 10 epochs.</p></sec><sec id="sec4dot1dot2-sensors-25-05521"><title>4.1.2. Datasets</title><p>In recent years, several low-light image datasets have been proposed, which are mainly classified into two categories: paired datasets (with reference) and unpaired datasets (without reference).</p><p>The paired datasets used in the experiments include LOLv1 [<xref rid="B13-sensors-25-05521" ref-type="bibr">13</xref>], LOLv2 [<xref rid="B14-sensors-25-05521" ref-type="bibr">14</xref>], and LSRW [<xref rid="B15-sensors-25-05521" ref-type="bibr">15</xref>]. LOLv1 is the first dataset that includes images from real-world scenarios and contains 485 training pairs and 15 validation pairs. The LOLv2 dataset comprises two subsets: LOLv2-Real and LOLv2-Synthetic. LOLv2-Real contains 689 training pairs and 100 validation pairs, while LOLv2-Synthetic includes 900 training pairs and 100 validation pairs. The LSRW dataset also includes two subsets: LSRW-Huawei and LSRW-Nikon. LSRW-Huawei, collected with a Huawei phone, consists of 2450 training pairs and 30 validation pairs. LSRW-Nikon, collected with a Nikon camera, contains 3150 training pairs and 20 validation pairs. The unpaired datasets used in the experiments are DICM [<xref rid="B45-sensors-25-05521" ref-type="bibr">45</xref>], LIME [<xref rid="B8-sensors-25-05521" ref-type="bibr">8</xref>], MEF [<xref rid="B46-sensors-25-05521" ref-type="bibr">46</xref>], and NPE [<xref rid="B41-sensors-25-05521" ref-type="bibr">41</xref>], which contain 69, 10, 17, and 85 low-light images, respectively.</p></sec><sec id="sec4dot1dot3-sensors-25-05521"><title>4.1.3. Metrics</title><p>The evaluation on paired datasets primarily relies on two metrics: PSNR (Peak Signal-to-Noise Ratio; higher is better) and SSIM (Structural Similarity; higher is better). PSNR quantifies the ratio of signal to noise in an image, while SSIM measures the similarity between the enhanced image and the reference image in terms of luminance, contrast, and structure. Notably, the unit of SSIM is expressed as a percentage (%) in the experiments. For unpaired datasets, the evaluation primarily utilizes NIQE [<xref rid="B47-sensors-25-05521" ref-type="bibr">47</xref>] (Natural Image Quality Evaluator; lower is better). On the one hand, the metrics for each image in the dataset are computed and averaged to assess the model&#8217;s performance, denoted as <inline-formula><mml:math id="mm193" overflow="scroll"><mml:mrow><mml:msub><mml:mi>PSNR</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm194" overflow="scroll"><mml:mrow><mml:msub><mml:mi>SSIM</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm195" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. On the other hand, to evaluate the stability of enhancement performance, we use the standard deviation instead of the mean, represented as <inline-formula><mml:math id="mm196" overflow="scroll"><mml:mrow><mml:msub><mml:mi>PSNR</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm197" overflow="scroll"><mml:mrow><mml:msub><mml:mi>SSIM</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm198" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively.</p></sec></sec><sec id="sec4dot2-sensors-25-05521"><title>4.2. Ablation Study</title><sec id="sec4dot2dot1-sensors-25-05521"><title>4.2.1. The Impact of Loss Function Strategies</title><p>In order to enhance the model&#8217;s capability for low-light image enhancement, the loss function used during training is composed of two components: <inline-formula><mml:math id="mm199" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm200" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. To evaluate the impact of <inline-formula><mml:math id="mm201" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm202" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the model is first trained using <inline-formula><mml:math id="mm203" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as the sole loss function. Subsequently, both <inline-formula><mml:math id="mm204" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm205" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are employed together as the loss function for training.</p><p><xref rid="sensors-25-05521-t001" ref-type="table">Table 1</xref> shows the quantization performance of Priori DCE trained with different loss function strategies on the datasets LOLv1, LOLv2-Real, and LOLv2-Synthetic. As can be seen from <xref rid="sensors-25-05521-t001" ref-type="table">Table 1</xref>, compared to using <inline-formula><mml:math id="mm206" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> alone as the loss function, the inclusion of <inline-formula><mml:math id="mm207" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> improves the Priori DCE&#8217;s enhancement capability to some extent.</p><p><xref rid="sensors-25-05521-f008" ref-type="fig">Figure 8</xref> visually presents the enhancement results using different loss function strategies. As shown, when <inline-formula><mml:math id="mm208" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is used alone, both the brightness and visual quality of the enhanced image improve significantly. However, some blurring is observed on the surfaces of the wooden floor and the chair. Incorporating <inline-formula><mml:math id="mm209" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> into the loss function alleviates this shortcoming, leading to more natural texture transitions. Notably, when both <inline-formula><mml:math id="mm210" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm211" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are combined, the enhanced image outperforms the reference image in terms of the NIQE metric. In the reference image, the texture transition on the wooden floor is relatively sharp, whereas in the enhanced image, the texture becomes noticeably softer, improving the overall visual quality.</p></sec><sec id="sec4dot2dot2-sensors-25-05521"><title>4.2.2. The Impact of Hyperparameter <italic toggle="yes">k</italic></title><p>As the hyperparameter <italic toggle="yes">k</italic> (the number of iterations of the basis function) increases, the range of pixel&#8217;s brightness <italic toggle="yes">x</italic> that is mapped becomes broader, allowing the enhanced pixel to either become brighter or darker. <xref rid="sensors-25-05521-f009" ref-type="fig">Figure 9</xref> demonstrates the enhancement performance of the Priori DCE with different <italic toggle="yes">k</italic> on the LOLv1 dataset. The <italic toggle="yes">x</italic>-axis represents the hyperparameter <italic toggle="yes">k</italic>, and the <italic toggle="yes">y</italic>-axis shows the corresponding evaluation metrics (<inline-formula><mml:math id="mm212" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>PSNR</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm213" overflow="scroll"><mml:mrow><mml:msub><mml:mi>SSIM</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm214" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). It is evident that as <italic toggle="yes">k</italic> increases from 1 to 5, the Priori DCE&#8217;s enhancement performance gradually improves, after which it stabilizes. <xref rid="sensors-25-05521-f010" ref-type="fig">Figure 10</xref> visually presents the enhancement results of the Priori DCE on two low-light images (Bowling Alley and Kitchen Cabinet) with different <italic toggle="yes">k</italic>. The numbers <inline-formula><mml:math id="mm215" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>b</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at the top of the images indicate the corresponding PSNR, SSIM, and NIQE, respectively. It can be observed that when <italic toggle="yes">k</italic> is 1, although the brightness of the enhanced image improves relative to the original low-light image, the issue of underexposure remains. This is due to the limited range of enhanced pixels mapped by the corresponding enhancement function. As <italic toggle="yes">k</italic> increases from 1 to 5, this issue is alleviated. The corresponding enhanced images gradually brighten, and the image quality significantly improves, with the NIQE decreasing from 5.497 and 6.044 to 2.726 and 3.769, respectively. However, the increase in <italic toggle="yes">k</italic> does not result in indefinite improvement in enhancement performance. Once <italic toggle="yes">k</italic> exceeds 5, the brightness and quality of the enhanced images stabilize, with the NIQE changing from 2.726 and 3.769 to 2.823 and 4.166, respectively.</p></sec><sec id="sec4dot2dot3-sensors-25-05521"><title>4.2.3. Model Structure</title><p>To verify the role of different modules in the Priori DCE proposed in this paper, we use a plain model without any special modules as the baseline and progressively incorporate various modules. Training and testing are performed on the LOLv1, LOLv2-Real, and LOLv2-Synthetic datasets, with quantitative results presented in <xref rid="sensors-25-05521-t002" ref-type="table">Table 2</xref>. As shown in <xref rid="sensors-25-05521-t002" ref-type="table">Table 2</xref>, compared to the baseline, the inclusion of the priori channels improves model performance across all three datasets. The priori channels specifies the direction of enhancement for the low-light image, so the enhanced image is more similar to the reference image. Subsequently, incorporating GA further boosts performance, as the enhancement of each pixel in the low-light image is no longer confined to local regions but can be inferred globally. Finally, the addition of the priori probability further improves enhancement performance by providing adaptive priori enhancement probability that varies based on the brightness of pixels.</p></sec><sec id="sec4dot2dot4-sensors-25-05521"><title>4.2.4. The Impact of Scale Factor <inline-formula><mml:math id="mm216" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula></title><p>To investigate the effect of the scale factor <inline-formula><mml:math id="mm217" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> on enhancement performance, experiments were conducted on the datasets DICM, LIME, MEF, and NPE, with the results presented in <xref rid="sensors-25-05521-f011" ref-type="fig">Figure 11</xref> (top). As shown, optimal performance is achieved on the datasets DICM, LIME, MEF, and NPE when <inline-formula><mml:math id="mm218" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> is set to <inline-formula><mml:math id="mm219" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm220" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm221" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and 1, respectively. This is due to the variability in the brightness of different low-light images, as well as the non-uniqueness of the reference images corresponding to them. Therefore, the corresponding priori channels are also different. <xref rid="sensors-25-05521-f011" ref-type="fig">Figure 11</xref> (bottom) shows the proportion of pixels with different brightness in these four datasets. It can be seen that the brightness in the LIME and MEF datasets are mainly concentrated in the low-light regions. Therefore, to achieve a good enhancement effect, a larger <inline-formula><mml:math id="mm222" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> is required. In contrast, the brightness distribution in the DICM and NPE datasets is more balanced, so a smaller <inline-formula><mml:math id="mm223" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> value is sufficient.</p><p>A <inline-formula><mml:math id="mm224" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> value of 0.5, 1, 2, and 4 indicates that the brightness of the enhanced image is 0.5 times, 1 times, 2 times, and 4 times that of the original low-light image, and <xref rid="sensors-25-05521-f012" ref-type="fig">Figure 12</xref> visualizes the enhancement results corresponding to these <inline-formula><mml:math id="mm225" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula>, with different colored numbers representing the average brightness on the corresponding channels. As shown in <xref rid="sensors-25-05521-f012" ref-type="fig">Figure 12</xref>, the average brightness of the three channels of the original low-light image is 0.27, 0.32, and 0.33, respectively. The three channels of the enhanced image corresponding to a <inline-formula><mml:math id="mm226" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> of 0.5 are 0.18, 0.22, and 0.21, respectively, which is significantly lower than that of the original low-light image. The enhanced image when the <inline-formula><mml:math id="mm227" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> is 1 is almost the same brightness as the original low-light image. When the <inline-formula><mml:math id="mm228" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> is 2, the brightness of the enhanced image is close to 0.5, and the quality of the enhanced image is also the best. When the <inline-formula><mml:math id="mm229" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> is 4, the corresponding enhanced image is significantly overexposed. As can be seen from the above, the output of the model can be adjusted by adjusting the value of <inline-formula><mml:math id="mm230" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula>, so that for different low-light images, enhanced images with different brightness can be generated.</p></sec></sec><sec id="sec4dot3-sensors-25-05521"><title>4.3. Performance Evaluation</title><sec id="sec4dot3dot1-sensors-25-05521"><title>4.3.1. Reference Evaluation</title><p>The LOLv1 [<xref rid="B13-sensors-25-05521" ref-type="bibr">13</xref>], LOLv2 [<xref rid="B14-sensors-25-05521" ref-type="bibr">14</xref>], and LSRW [<xref rid="B15-sensors-25-05521" ref-type="bibr">15</xref>] datasets are currently popular paired datasets. In this section, we evaluate several recent SOTA methods along with Priori DCE on these paired datasets, with the results presented in <xref rid="sensors-25-05521-t003" ref-type="table">Table 3</xref>. As shown, EnlightenGAN, MELLEN-IC, Retinexformer, LLFlow, and Priori DCE achieve the best performance, with Priori DCE demonstrating the strongest enhancement performance. Notably, when comparing Priori DCE with the second-best performing Retinexformer on the LOLv2-Synthetic dataset, the <inline-formula><mml:math id="mm231" overflow="scroll"><mml:mrow><mml:msub><mml:mi>PSNR</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm232" overflow="scroll"><mml:mrow><mml:msub><mml:mi>SSIM</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> improve from 25.67 and 92.82 to 29.49 and 93.6, respectively, while the <inline-formula><mml:math id="mm233" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for image quality decreases slightly from 3.94 to 3.91. Additionally, <xref rid="sensors-25-05521-t003" ref-type="table">Table 3</xref> reports the <inline-formula><mml:math id="mm234" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for both the low-light image (Low) and reference image (Reference). It is evident that some methods, such as MELLEN-IC, Retinexformer, LLFlow, and Priori DCE, produce enhanced images with <inline-formula><mml:math id="mm235" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> lower than those of the reference images. This can be attributed to the fact that low-light image enhancement models are trained on large datasets, most of which contain high-quality reference images (lower NIQE). Consequently, these models not only learn how to enhance brightness but also improve the overall image quality.</p><p><xref rid="sensors-25-05521-t003" ref-type="table">Table 3</xref> also shows the complexity and running speed of different models. It can be seen that DeepUPE has a very small number of parameters, and its running speed even reaches 2426, and in exchange, its enhanced quality is terrible. Since the Priori DCE model introduced the attention mechanism GA, its running speed was reduced from 24 to 11.67 compared to Zero DCE, and in exchange, the enhancement quality of Zero DCE was also weaker than that of Priori DCE. It can be seen that although the enhanced quality of Priori DCE has been enhanced, how to improve the operation efficiency is an important research direction in the future.</p><p><xref rid="sensors-25-05521-f013" ref-type="fig">Figure 13</xref>, <xref rid="sensors-25-05521-f014" ref-type="fig">Figure 14</xref>, <xref rid="sensors-25-05521-f015" ref-type="fig">Figure 15</xref>, <xref rid="sensors-25-05521-f016" ref-type="fig">Figure 16</xref> and <xref rid="sensors-25-05521-f017" ref-type="fig">Figure 17</xref> visually display the enhancement results on different datasets. The numbers <inline-formula><mml:math id="mm236" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>b</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at the top of these images correspond to the PSNR, SSIM, and NIQE values of the respective images. The histograms at the bottom represent the frequency distribution of brightness in the R, G, and B channels for the corresponding images.</p><p><xref rid="sensors-25-05521-f013" ref-type="fig">Figure 13</xref> shows the low-light image of a swimming pool scene from the LOLv1 dataset and these corresponding enhanced images generated by several SOTA methods. As shown in the figure, the brightness of BL, DeepUPE, and Zero DCE is relatively low. Correspondingly, the brightness distribution histograms are concentrated primarily in the lower brightness range on the left. The enhancement quality of these three methods is even worse than that of the original low-light image. In contrast, EnlightenGAN, GLADNet, LIME, RetinexNet, and Zero DCE++ exhibit significant improvements in brightness. However, while the brightness was increased, different levels of noise were also introduced, with the noise in the trees and the surrounding areas of the enhanced image being particularly noticeable. RetinxeNet even encountered serious stylization issues. This led to a decrease in the enhancement quality. The performance of KinD, KinD++, MELLEN-IC, Retinexformer, LLFlow, and Priori DCE is relatively better. In the reference image, the text on the clock is brighter. In the enhanced images from KinD, KinD++, MELLEN-IC, Retinexformer, and LLFlow, there is a noticeable color shift in the clock text. Meanwhile, thanks to the inclusion of priori channels, The enhancement effect of Priori DCE on these texts in the clock is closest to that of reference.</p><p><xref rid="sensors-25-05521-f014" ref-type="fig">Figure 14</xref> shows the low-light image of a grassland scene from the LOLv2-Real dataset and the corresponding enhanced images generated by several SOTA methods. BL and DeepUPE still have weak brightness, and their corresponding frequency distribution histograms are mainly concentrated in the low-brightness region. EnlightenGAN is notably yellowish. In fact, the brightness of the reference image is also low, but the brightness of these enhanced images generated by GLADNet, KinD, KinD++, RetinexNet, and Zero DCE++, is significantly increased, even surpassing the brightness of the reference. The enhancement quality of these methods is therefore not satisfactory, as the NIQE is greater than the reference and even exceeds 7. In comparison, Retinexformer, LLFlow, and Priori DCE are closer to the reference. However, based on the enhancement results of the grass area within the cyan and green boxes in the image, Retinexformer images are relatively blurry, while LLFlow and Priori DCE images are more satisfactory.</p><p><xref rid="sensors-25-05521-f015" ref-type="fig">Figure 15</xref> shows the low-light image of building from the dataset LOLv2-Synthetic and these corresponding enhanced images generated by several SOTA methods. It can be observed that the brightness of the reference is high, and the frequency distribution histogram is more evenly distributed. From the rooftop area within the cyan box, the colors in the reference image are predominantly composed of gray and warm yellow tones. Among these methods, only MELLEN-IC, Retinexformer, and Priori DCE are closest to the reference, while the colors of other methods show significant deviations. In BL, LIME, and LLFlow, the transition between the roof and the sky even shows a noticeable white area.</p><p><xref rid="sensors-25-05521-f016" ref-type="fig">Figure 16</xref> shows the low-light image of the rest room scene in the LSRW-Huawei dataset and these corresponding enhanced images generated by several SOTA methods. The brightness of BL and DeepUPE is relatively weak. EnlightenGAN, GALDNet, KinD++, and LIME exhibit significant color deviation and generate excessive noise. RetinexNet still suffers from stylization issues. MELLEN-IC and Zero DCE++ exhibit higher brightness compared to the reference, which results in lower PSNR and SSIM values for these methods. This occurs because these methods focus solely on enhancing the low-light image without accounting for the reference. Among these methods, Priori DCE most closely matches the reference image, successfully improving both image quality and brightness.</p><p><xref rid="sensors-25-05521-f017" ref-type="fig">Figure 17</xref> shows the low-light image of multi-floor building from the LSRW-Nikon dataset and the corresponding enhanced images generated by several SOTA methods. It is evident that the building areas in the reference image are relatively bright. Among these methods, only LLFlow and Priori DCE yield satisfactory results in both brightness and enhancement quality. Other methods either produce insufficient brightness or exhibit noticeable color distortions. In the window highlighted by the cyan box, Priori DCE introduces more noise, resulting in lower enhancement quality compared to LLFlow.</p><p>The enhancement results above indicate that Retinexformer, LLFlow, and Priori DCE achieve good enhancement quality. However, the performance of Retinexformer and LLFlow is unstable. For instance, Retinexformer produces darker enhanced image on the LSRW-Nikon dataset, while LLFlow introduces white transitions in the image from the LOLv2-Synthetic dataset. In contrast, Priori DCE demonstrates a clear advantage in both enhancement quality and stability across these datasets.</p></sec><sec id="sec4dot3dot2-sensors-25-05521"><title>4.3.2. Unreference Evaluation</title><p>DICM, LIME, MEF, and NPE are currently popular datasets without reference. Similar to <xref rid="sec4dot3dot1-sensors-25-05521" ref-type="sec">Section 4.3.1</xref>, we conduct experiments on these datasets using the same SOTA methods, with the results presented in <xref rid="sensors-25-05521-t004" ref-type="table">Table 4</xref>. Since these datasets do not have reference images, we use <inline-formula><mml:math id="mm237" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm238" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as evaluation metrics. Additionally, we calculate the average results across different datasets, as shown in the <inline-formula><mml:math id="mm239" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> column of <xref rid="sensors-25-05521-t004" ref-type="table">Table 4</xref>. As shown in <xref rid="sensors-25-05521-t004" ref-type="table">Table 4</xref>, the top three models are EnlightenGAN, MELLEN-IC, and Priori DCE. Among these, Priori DCE shows a clear advantage in both enhancement quality and stability. Compared to MELLEN-IC, which ranks second in the <inline-formula><mml:math id="mm240" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> metric, Priori DCE reduces the <inline-formula><mml:math id="mm241" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm242" overflow="scroll"><mml:mrow><mml:msub><mml:mi>NIQE</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> scores from 3.15 and 0.914 to 3.031 and 0.839, respectively. This demonstrates the effectiveness and stability advantages of Priori DCE.</p><p><xref rid="sensors-25-05521-f018" ref-type="fig">Figure 18</xref> shows the low-light image of the lunar landing scene from the DICM dataset and the corresponding enhanced images generated by several SOTA methods. It can be observed that RetinexNet still exhibits obvious stylization, while the astronaut&#8217;s color in LLFlow and GLADNet has turned white. The top three methods with the best enhancement quality are Priori DCE, MELLEN-IC, and EnlightenGAN, with corresponding NIQE values of 3.05, 2.778, and 2.34, respectively. From the zoomed-in views of the radar and wheels, it can be seen that EnlightenGAN provides the highest enhancement quality, preserving rich details while increasing brightness. Similar to EnlightenGAN, MELLEN-IC increases brightness further but sacrifices some details. Priori DCE, while retaining details, has insufficient brightness.</p><p><xref rid="sensors-25-05521-f019" ref-type="fig">Figure 19</xref> shows the low-light image of a streetlight scene from the LIME dataset and the corresponding enhanced images generated by several SOTA methods. From the zoomed-in views of the buildings and streetlight, it can be seen that GLADNet, KinD++, LIME, RetinexNet, and Zero DCE exhibit the most significant noise. LLFlow produces the brightest image with only a small amount of noise. Priori DCE introduces the least noise around the streetlight and provides the most natural transition between the streetlight and its surrounding environment.</p><p><xref rid="sensors-25-05521-f020" ref-type="fig">Figure 20</xref> shows the low-light image of the venice from the MEF dataset and these corresponding enhanced images generated by several SOTA methods. Notably, Priori DCE achieves the highest brightness while maintaining strong enhancement quality. Furthermore, the enhancement quality of EnlightenGAN, LIME, and Retinexformer is also relatively high, primarily due to their lower brightness in these enhanced images.</p><p><xref rid="sensors-25-05521-f021" ref-type="fig">Figure 21</xref> shows the low-light image of the forest from the NPE dataset and the corresponding enhanced images generated by several SOTA methods. The zoomed-in view of the sky reveals that BL, DeepUPE, GLADNet, Zero DCE++, and LLFlow largely overlook its texture, whereas RetinexNet and Priori DCE provide the most effective enhancement of the sky&#8217;s texture. In the zoomed-in view of the forest, the brightness of DeepUPE, GLADNet, Retinexformer, LLFlow, and Priori DCE are relatively low, while other methods are noticeably overexposed. KinD appears somewhat blurred, neglecting the texture. RetinexNet appears highly unnatural.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05521"><title>5. Conclusions</title><p>This paper incorporates priori knowledge into low-light image enhancement method through the use of priori channels and priori enhancement probability and names the method Priori DCE. The priori channels enable the adjustable brightness of the enhanced image, while the priori enhancement probabilities are adaptively adjusted according to the brightness of individual pixels. Additionally, the GA module is integrated to facilitate interactions between pixels in the enhanced image, promoting visual balance in the enhanced image. Experimental results demonstrate the superior performance of the proposed Priori DCE. However, due to the introduction of GA modules, on the one hand, it improves the enhanced quality of the model, and on the other hand, it also reduces the operational efficiency. How to solve this problem is one of the main research directions in the future.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Z.C.: Data analysis, writing original draft; Y.L.: Supervision, review and editing; J.X.: Supervision; K.L.: Supervision; Z.H.: Review. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets used in this work are openly available.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05521"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kwong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cong</surname><given-names>R.</given-names></name></person-group><article-title>Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>1777</fpage><lpage>1786</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00185</pub-id></element-citation></ref><ref id="B2-sensors-25-05521"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>YOLOv3: An Incremental Improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id><pub-id pub-id-type="arxiv">1804.02767</pub-id></element-citation></ref><ref id="B3-sensors-25-05521"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Meinhardt</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Leal-Taix&#233;</surname><given-names>L.</given-names></name><name name-style="western"><surname>Feichtenhofer</surname><given-names>C.</given-names></name></person-group><article-title>TrackFormer: Multi-Object Tracking with Transformers</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>8834</fpage><lpage>8844</lpage><pub-id pub-id-type="doi">10.1109/CVPR52688.2022.00864</pub-id></element-citation></ref><ref id="B4-sensors-25-05521"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gkioxari</surname><given-names>G.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><article-title>Mask R-CNN</article-title><source>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2980</fpage><lpage>2988</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.322</pub-id></element-citation></ref><ref id="B5-sensors-25-05521"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>X.</given-names></name></person-group><article-title>A simple and effective histogram equalization approach to image enhancement</article-title><source>Digit. Signal Process.</source><year>2004</year><volume>14</volume><fpage>158</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.dsp.2003.07.002</pub-id></element-citation></ref><ref id="B6-sensors-25-05521"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Land</surname><given-names>E.H.</given-names></name><name name-style="western"><surname>McCann</surname><given-names>J.J.</given-names></name></person-group><article-title>Lightness and Retinex Theory</article-title><source>J. Opt. Soc. Am.</source><year>1971</year><volume>61</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1364/JOSA.61.000001</pub-id><pub-id pub-id-type="pmid">5541571</pub-id></element-citation></ref><ref id="B7-sensors-25-05521"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Beyond Brightening Low-light Images</article-title><source>Int. J. Comput. Vis.</source><year>2021</year><volume>129</volume><fpage>1013</fpage><lpage>1037</lpage><pub-id pub-id-type="doi">10.1007/s11263-020-01407-x</pub-id></element-citation></ref><ref id="B8-sensors-25-05521"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ling</surname><given-names>H.</given-names></name></person-group><article-title>LIME: Low-Light Image Enhancement via Illumination Map Estimation</article-title><source>IEEE Trans. Image Process.</source><year>2017</year><volume>26</volume><fpage>982</fpage><lpage>993</lpage><pub-id pub-id-type="doi">10.1109/TIP.2016.2639450</pub-id><pub-id pub-id-type="pmid">28113318</pub-id></element-citation></ref><ref id="B9-sensors-25-05521"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.P.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name></person-group><article-title>A Weighted Variational Model for Simultaneous Reflectance and Illumination Estimation</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>2782</fpage><lpage>2790</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.304</pub-id></element-citation></ref><ref id="B10-sensors-25-05521"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lecun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bottou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Haffner</surname><given-names>P.</given-names></name></person-group><article-title>Gradient-based learning applied to document recognition</article-title><source>Proc. IEEE</source><year>1998</year><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="B11-sensors-25-05521"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>Proceedings of the 3rd International Conference on Learning Representations, ICLR 2015</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>7&#8211;9 May 2015</conf-date></element-citation></ref><ref id="B12-sensors-25-05521"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="B13-sensors-25-05521"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>Deep Retinex Decomposition for Low-Light Enhancement</article-title><source>Proceedings of the British Machine Vision Conference</source><conf-loc>Newcastle, UK</conf-loc><conf-date>3&#8211;6 September 2018</conf-date><publisher-name>British Machine Vision Association</publisher-name><publisher-loc>Durham, UK</publisher-loc><year>2018</year></element-citation></ref><ref id="B14-sensors-25-05521"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>Sparse Gradient Regularized Deep Retinex Network for Robust Low-Light Image Enhancement</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>2072</fpage><lpage>2086</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3050850</pub-id><pub-id pub-id-type="pmid">33460379</pub-id></element-citation></ref><ref id="B15-sensors-25-05521"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xuan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>F.</given-names></name><name name-style="western"><surname>Han</surname><given-names>S.</given-names></name></person-group><article-title>R2RNet: Low-light image enhancement via Real-low to Real-normal Network</article-title><source>J. Vis. Commun. Image Represent.</source><year>2023</year><volume>90</volume><fpage>103712</fpage><pub-id pub-id-type="doi">10.1016/j.jvcir.2022.103712</pub-id></element-citation></ref><ref id="B16-sensors-25-05521"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Learning a Deep Single Image Contrast Enhancer from Multi-Exposure Images</article-title><source>IEEE Trans. Image Process.</source><year>2018</year><volume>27</volume><fpage>2049</fpage><lpage>2062</lpage><pub-id pub-id-type="doi">10.1109/TIP.2018.2794218</pub-id><pub-id pub-id-type="pmid">29994747</pub-id></element-citation></ref><ref id="B17-sensors-25-05521"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gharbi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Barron</surname><given-names>J.T.</given-names></name><name name-style="western"><surname>Hasinoff</surname><given-names>S.W.</given-names></name><name name-style="western"><surname>Durand</surname><given-names>F.</given-names></name></person-group><article-title>Deep bilateral learning for real-time image enhancement</article-title><source>ACM Trans. Graph.</source><year>2017</year><volume>36</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/3072959.3073592</pub-id></element-citation></ref><ref id="B18-sensors-25-05521"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>W.S.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Underexposed Photo Enhancement Using Deep Illumination Estimation</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>6842</fpage><lpage>6850</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.00701</pub-id></element-citation></ref><ref id="B19-sensors-25-05521"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name></person-group><article-title>Kindling the Darkness: A Practical Low-light Image Enhancer</article-title><source>Proceedings of the 27th ACM International Conference on Multimedia, MM &#8217;19</source><conf-loc>Nice, France</conf-loc><conf-date>21&#8211;25 October 2019</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2019</year><fpage>1632</fpage><lpage>1640</lpage><pub-id pub-id-type="doi">10.1145/3343031.3350926</pub-id></element-citation></ref><ref id="B20-sensors-25-05521"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goodfellow</surname><given-names>I.</given-names></name><name name-style="western"><surname>Pouget-Abadie</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mirza</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Warde-Farley</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ozair</surname><given-names>S.</given-names></name><name name-style="western"><surname>Courville</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Generative adversarial networks</article-title><source>Commun. ACM</source><year>2020</year><volume>63</volume><fpage>139</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1145/3422622</pub-id></element-citation></ref><ref id="B21-sensors-25-05521"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Isola</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>T.</given-names></name><name name-style="western"><surname>Efros</surname><given-names>A.A.</given-names></name></person-group><article-title>Image-to-Image Translation with Conditional Adversarial Networks</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>5967</fpage><lpage>5976</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.632</pub-id></element-citation></ref><ref id="B22-sensors-25-05521"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Park</surname><given-names>T.</given-names></name><name name-style="western"><surname>Isola</surname><given-names>P.</given-names></name><name name-style="western"><surname>Efros</surname><given-names>A.A.</given-names></name></person-group><article-title>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</article-title><source>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2242</fpage><lpage>2251</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.244</pub-id></element-citation></ref><ref id="B23-sensors-25-05521"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>D.</given-names></name></person-group><article-title>PD-GAN: Perceptual-Details GAN for Extremely Noisy Low Light Image Enhancement</article-title><source>Proceedings of the ICASSP 2021&#8212;2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>6&#8211;11 June 2021</conf-date><fpage>1840</fpage><lpage>1844</lpage><pub-id pub-id-type="doi">10.1109/ICASSP39728.2021.9413433</pub-id></element-citation></ref><ref id="B24-sensors-25-05521"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>EnlightenGAN: Deep Light Enhancement Without Paired Supervision</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>2340</fpage><lpage>2349</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3051462</pub-id><pub-id pub-id-type="pmid">33481709</pub-id></element-citation></ref><ref id="B25-sensors-25-05521"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>G.D.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.L.P.</given-names></name></person-group><article-title>Multiscale Low-Light Image Enhancement Network with Illumination Constraint</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2022</year><volume>32</volume><fpage>7403</fpage><lpage>7417</lpage><pub-id pub-id-type="doi">10.1109/tcsvt.2022.3186880</pub-id></element-citation></ref><ref id="B26-sensors-25-05521"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name></person-group><article-title>Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>44</volume><fpage>4225</fpage><lpage>4238</lpage><pub-id pub-id-type="doi">10.1109/TITS.2020.3042973</pub-id><pub-id pub-id-type="pmid">33656989</pub-id></element-citation></ref><ref id="B27-sensors-25-05521"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zemel</surname><given-names>R.</given-names></name></person-group><article-title>Understanding the effective receptive field in deep convolutional neural networks</article-title><source>Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&#8217;16</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>5&#8211;10 December 2016</conf-date><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2016</year><fpage>4905</fpage><lpage>4913</lpage></element-citation></ref><ref id="B28-sensors-25-05521"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-Excitation Networks</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00745</pub-id></element-citation></ref><ref id="B29-sensors-25-05521"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Torr</surname><given-names>P.</given-names></name></person-group><article-title>Res2Net: A New Multi-Scale Backbone Architecture</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>43</volume><fpage>652</fpage><lpage>662</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2938758</pub-id><pub-id pub-id-type="pmid">31484108</pub-id></element-citation></ref><ref id="B30-sensors-25-05521"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Carion</surname><given-names>N.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Synnaeve</surname><given-names>G.</given-names></name><name name-style="western"><surname>Usunier</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zagoruyko</surname><given-names>S.</given-names></name></person-group><article-title>End-to-End Object Detection with Transformers</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Vedaldi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bischof</surname><given-names>H.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name><name name-style="western"><surname>Frahm</surname><given-names>J.M.</given-names></name></person-group><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>213</fpage><lpage>229</lpage></element-citation></ref><ref id="B31-sensors-25-05521"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>L.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&#8217;17</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#8211;9 December 2017</conf-date><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2017</year><fpage>6000</fpage><lpage>6010</lpage></element-citation></ref><ref id="B32-sensors-25-05521"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Su</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name></person-group><article-title>Deformable DETR: Deformable Transformers for End-to-End Object Detection</article-title><source>Proceedings of the 9th International Conference on Learning Representations, ICLR 2021</source><conf-loc>Virtual Event, Vienna, Austria</conf-loc><conf-date>3&#8211;7 May 2021</conf-date></element-citation></ref><ref id="B33-sensors-25-05521"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name></person-group><article-title>Deformable Convolutional Networks</article-title><source>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>764</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.89</pub-id></element-citation></ref><ref id="B34-sensors-25-05521"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name></person-group><article-title>Deformable ConvNets V2: More Deformable, Better Results</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>9300</fpage><lpage>9308</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.00953</pub-id></element-citation></ref><ref id="B35-sensors-25-05521"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>GLADNet: Low-Light Enhancement Network with Global Awareness</article-title><source>Proceedings of the 2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date> 15&#8211;19 May 2018</conf-date><fpage>751</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1109/FG.2018.00118</pub-id></element-citation></ref><ref id="B36-sensors-25-05521"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pizer</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Amburn</surname><given-names>E.P.</given-names></name><name name-style="western"><surname>Austin</surname><given-names>J.D.</given-names></name><name name-style="western"><surname>Cromartie</surname><given-names>R.</given-names></name><name name-style="western"><surname>Geselowitz</surname><given-names>A.</given-names></name><name name-style="western"><surname>Greer</surname><given-names>T.</given-names></name><name name-style="western"><surname>ter Haar Romeny</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zimmerman</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>Zuiderveld</surname><given-names>K.</given-names></name></person-group><article-title>Adaptive histogram equalization and its variations</article-title><source>Comput. Vision Graph. Image Process.</source><year>1987</year><volume>39</volume><fpage>355</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/S0734-189X(87)80186-X</pub-id></element-citation></ref><ref id="B37-sensors-25-05521"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bennett</surname><given-names>E.P.</given-names></name><name name-style="western"><surname>McMillan</surname><given-names>L.</given-names></name></person-group><article-title>Video enhancement using per-pixel virtual exposures</article-title><source>Proceedings of the ACM SIGGRAPH 2005 Papers, SIGGRAPH &#8217;05</source><conf-loc>Los Angeles, CA, USA</conf-loc><conf-date>31 July&#8211;4 August 2005</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2005</year><fpage>845</fpage><lpage>852</lpage><pub-id pub-id-type="doi">10.1145/1186822.1073272</pub-id></element-citation></ref><ref id="B38-sensors-25-05521"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Automatic Exposure Correction of Consumer Photographs</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2012</source><conf-loc>Florence, Italy</conf-loc><conf-date>7&#8211;13 October 2012</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Fitzgibbon</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lazebnik</surname><given-names>S.</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sato</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Schmid</surname><given-names>C.</given-names></name></person-group><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2012</year><fpage>771</fpage><lpage>785</lpage></element-citation></ref><ref id="B39-sensors-25-05521"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jobson</surname><given-names>D.</given-names></name><name name-style="western"><surname>Woodell</surname><given-names>G.</given-names></name></person-group><article-title>Multi-scale retinex for color image enhancement</article-title><source>Proceedings of the 3rd IEEE International Conference on Image Processing</source><conf-loc>Lausanne, Switzerland</conf-loc><conf-date>19 September 1996</conf-date><volume>Volume 3</volume><fpage>1003</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1109/ICIP.1996.560995</pub-id></element-citation></ref><ref id="B40-sensors-25-05521"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jobson</surname><given-names>D.</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Woodell</surname><given-names>G.</given-names></name></person-group><article-title>A multiscale retinex for bridging the gap between color images and the human observation of scenes</article-title><source>IEEE Trans. Image Process.</source><year>1997</year><volume>6</volume><fpage>965</fpage><lpage>976</lpage><pub-id pub-id-type="doi">10.1109/83.597272</pub-id><pub-id pub-id-type="pmid">18282987</pub-id></element-citation></ref><ref id="B41-sensors-25-05521"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title>Naturalness Preserved Enhancement Algorithm for Non-Uniform Illumination Images</article-title><source>IEEE Trans. Image Process.</source><year>2013</year><volume>22</volume><fpage>3538</fpage><lpage>3548</lpage><pub-id pub-id-type="doi">10.1109/TIP.2013.2261309</pub-id><pub-id pub-id-type="pmid">23661319</pub-id></element-citation></ref><ref id="B42-sensors-25-05521"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bian</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Timofte</surname><given-names>R.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>12504</fpage><lpage>12513</lpage></element-citation></ref><ref id="B43-sensors-25-05521"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>Proceedings of the Medical Image Computing and Computer-Assisted Intervention&#8212;MICCAI 2015</source><conf-loc>Munich, Germany</conf-loc><conf-date>5&#8211;9 October 2015</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Navab</surname><given-names>N.</given-names></name><name name-style="western"><surname>Hornegger</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wells</surname><given-names>W.M.</given-names></name><name name-style="western"><surname>Frangi</surname><given-names>A.F.</given-names></name></person-group><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="B44-sensors-25-05521"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Loshchilov</surname><given-names>I.</given-names></name><name name-style="western"><surname>Hutter</surname><given-names>F.</given-names></name></person-group><article-title>Decoupled Weight Decay Regularization</article-title><source>Proceedings of the International Conference on Learning Representations</source><conf-loc>Toulon, France</conf-loc><conf-date>24&#8211;26 April 2017</conf-date></element-citation></ref><ref id="B45-sensors-25-05521"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>C.S.</given-names></name></person-group><article-title>Contrast Enhancement Based on Layered Difference Representation of 2D Histograms</article-title><source>IEEE Trans. Image Process.</source><year>2013</year><volume>22</volume><fpage>5372</fpage><lpage>5384</lpage><pub-id pub-id-type="doi">10.1109/TIP.2013.2284059</pub-id><pub-id pub-id-type="pmid">24108715</pub-id></element-citation></ref><ref id="B46-sensors-25-05521"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Perceptual Quality Assessment for Multi-Exposure Image Fusion</article-title><source>IEEE Trans. Image Process.</source><year>2015</year><volume>24</volume><fpage>3345</fpage><lpage>3356</lpage><pub-id pub-id-type="doi">10.1109/tip.2015.2442920</pub-id><pub-id pub-id-type="pmid">26068317</pub-id></element-citation></ref><ref id="B47-sensors-25-05521"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mittal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Soundararajan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Bovik</surname><given-names>A.C.</given-names></name></person-group><article-title>Making a &#8220;Completely Blind&#8221; Image Quality Analyzer</article-title><source>IEEE Signal Process. Lett.</source><year>2013</year><volume>20</volume><fpage>209</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1109/LSP.2012.2227726</pub-id></element-citation></ref><ref id="B48-sensors-25-05521"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>D.</given-names></name><name name-style="western"><surname>An</surname><given-names>N.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name></person-group><article-title>Bilevel Fast Scene Adaptation for Low-Light Image Enhancement</article-title><source>Int. J. Comput. Vis.</source><year>2023</year><pub-id pub-id-type="doi">10.1007/s11263-023-01900-z</pub-id></element-citation></ref><ref id="B49-sensors-25-05521"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chau</surname><given-names>L.P.</given-names></name><name name-style="western"><surname>Kot</surname><given-names>A.</given-names></name></person-group><article-title>Low-Light Image Enhancement with Normalizing Flow</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2022</year><volume>36</volume><fpage>2604</fpage><lpage>2612</lpage><pub-id pub-id-type="doi">10.1609/aaai.v36i3.20162</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05521-f001" orientation="portrait"><label>Figure 1</label><caption><p>(<bold>a</bold>) The framework of Priori DCE. (<bold>b</bold>) Implementation details in (<bold>a</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g001.jpg"/></fig><fig position="float" id="sensors-25-05521-f002" orientation="portrait"><label>Figure 2</label><caption><p>The process of deriving the priori channels during the training stage.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g002.jpg"/></fig><fig position="float" id="sensors-25-05521-f003" orientation="portrait"><label>Figure 3</label><caption><p>Global-Attention Block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g003.jpg"/></fig><fig position="float" id="sensors-25-05521-f004" orientation="portrait"><label>Figure 4</label><caption><p>The ratio of pixels with different brightness that are enhanced during the process of transforming abnormal images to reference images in the SICE dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g004.jpg"/></fig><fig position="float" id="sensors-25-05521-f005" orientation="portrait"><label>Figure 5</label><caption><p>(<bold>a</bold>,<bold>b</bold>) illustrate the enhancement spaces of the enhancement functions <inline-formula><mml:math id="mm243" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm244" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g005.jpg"/></fig><fig position="float" id="sensors-25-05521-f006" orientation="portrait"><label>Figure 6</label><caption><p>The prior enhancement probability of different enhancement functions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g006.jpg"/></fig><fig position="float" id="sensors-25-05521-f007" orientation="portrait"><label>Figure 7</label><caption><p>(<bold>a</bold>,<bold>b</bold>) illustrate the enhancement spaces of the enhancement functions <inline-formula><mml:math id="mm245" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm246" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g007.jpg"/></fig><fig position="float" id="sensors-25-05521-f008" orientation="portrait"><label>Figure 8</label><caption><p>The visualization of the enhancement results of Priori DCE on the same low-light image under different loss function strategies. * indicates that the value does not exist.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g008.jpg"/></fig><fig position="float" id="sensors-25-05521-f009" orientation="portrait"><label>Figure 9</label><caption><p>As <italic toggle="yes">k</italic> gradually increases from 1 to 8, the enhancement performance of the corresponding Priori DCE on the LOLv1 dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g009.jpg"/></fig><fig position="float" id="sensors-25-05521-f010" orientation="portrait"><label>Figure 10</label><caption><p>As <italic toggle="yes">k</italic> gradually increases from 1 to 8, the enhanced images generated by corresponding Priori DCE.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g010.jpg"/></fig><fig position="float" id="sensors-25-05521-f011" orientation="portrait"><label>Figure 11</label><caption><p>(<bold>Top</bold>): the enhancement performance of Priori DCE with different scale factor <inline-formula><mml:math id="mm247" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> on datasets DICM, LIME, MEF and NPE, respectively. (<bold>Bottom</bold>): the ratio of pixels with different brightness in these four datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g011.jpg"/></fig><fig position="float" id="sensors-25-05521-f012" orientation="portrait"><label>Figure 12</label><caption><p>The enhanced result of priori DCE when the <inline-formula><mml:math id="mm248" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> was 0.5, 1, 2, and 4, respectively. The numbers in red, green, and blue represent the average on the corresponding channel.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g012.jpg"/></fig><fig position="float" id="sensors-25-05521-f013" orientation="portrait"><label>Figure 13</label><caption><p>The enhancement visualization on the LOLv1 dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g013.jpg"/></fig><fig position="float" id="sensors-25-05521-f014" orientation="portrait"><label>Figure 14</label><caption><p>The enhancement visualization on the LOLv2-Real dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g014.jpg"/></fig><fig position="float" id="sensors-25-05521-f015" orientation="portrait"><label>Figure 15</label><caption><p>The enhancement visualization on the LOLv2-Synthetic dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g015.jpg"/></fig><fig position="float" id="sensors-25-05521-f016" orientation="portrait"><label>Figure 16</label><caption><p>The enhancement visualization on the LSRW-Huawei dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g016.jpg"/></fig><fig position="float" id="sensors-25-05521-f017" orientation="portrait"><label>Figure 17</label><caption><p>The enhancement visualization on the LSRW-Nikon dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g017.jpg"/></fig><fig position="float" id="sensors-25-05521-f018" orientation="portrait"><label>Figure 18</label><caption><p>The enhancement visualization on the DICM dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g018.jpg"/></fig><fig position="float" id="sensors-25-05521-f019" orientation="portrait"><label>Figure 19</label><caption><p>The enhancement visualization on the LIME dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g019.jpg"/></fig><fig position="float" id="sensors-25-05521-f020" orientation="portrait"><label>Figure 20</label><caption><p>The enhancement visualization on the MEF dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g020.jpg"/></fig><fig position="float" id="sensors-25-05521-f021" orientation="portrait"><label>Figure 21</label><caption><p>The enhancement visualization on the NPE dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05521-g021.jpg"/></fig><table-wrap position="float" id="sensors-25-05521-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05521-t001_Table 1</object-id><label>Table 1</label><caption><p>The enhancement performance of Priori DCE under different loss function strategies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Loss</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LOLv1</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LOLv2-Real</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LOLv2-Synthetic</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm249" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm250" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm251" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm252" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm253" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm254" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm255" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm256" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm257" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm258" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">25.015</td><td align="center" valign="middle" rowspan="1" colspan="1">75.102</td><td align="center" valign="middle" rowspan="1" colspan="1">4.444</td><td align="center" valign="middle" rowspan="1" colspan="1">25.825</td><td align="center" valign="middle" rowspan="1" colspan="1">73.812</td><td align="center" valign="middle" rowspan="1" colspan="1">5.357</td><td align="center" valign="middle" rowspan="1" colspan="1">28.326</td><td align="center" valign="middle" rowspan="1" colspan="1">91.380</td><td align="center" valign="middle" rowspan="1" colspan="1">3.869</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm259" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> + <inline-formula><mml:math id="mm260" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.775</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.222</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.572</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.828</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.791</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.381</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.488</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.598</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.911</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05521-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05521-t002_Table 2</object-id><label>Table 2</label><caption><p>The enhancement performance of the models after adding different modules to the baseline. Red font indicates that the item is optimal for the column.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Priori
<break/>
Channels</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">GA</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Priori<break/>
Probability</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin" rowspan="1">LOLv1</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin" rowspan="1">LOLv2-Real</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin" rowspan="1">LOLv2-Synthetic</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm261" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm262" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm263" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm264" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm265" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm266" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm267" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm268" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm269" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td colspan="3" align="center" valign="middle" rowspan="1">baseline</td><td align="center" valign="middle" rowspan="1" colspan="1">21.002</td><td align="center" valign="middle" rowspan="1" colspan="1">77.737</td><td align="center" valign="middle" rowspan="1" colspan="1">3.675</td><td align="center" valign="middle" rowspan="1" colspan="1">22.028</td><td align="center" valign="middle" rowspan="1" colspan="1">76.907</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">4.290</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">21.495</td><td align="center" valign="middle" rowspan="1" colspan="1">89.793</td><td align="center" valign="middle" rowspan="1" colspan="1">3.879</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">22.408</td><td align="center" valign="middle" rowspan="1" colspan="1">76.945</td><td align="center" valign="middle" rowspan="1" colspan="1">3.823</td><td align="center" valign="middle" rowspan="1" colspan="1">23.877</td><td align="center" valign="middle" rowspan="1" colspan="1">76.815</td><td align="center" valign="middle" rowspan="1" colspan="1">4.382</td><td align="center" valign="middle" rowspan="1" colspan="1">22.453</td><td align="center" valign="middle" rowspan="1" colspan="1">90.580</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">3.818</named-content>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">24.376</td><td align="center" valign="middle" rowspan="1" colspan="1">79.162</td><td align="center" valign="middle" rowspan="1" colspan="1">3.819</td><td align="center" valign="middle" rowspan="1" colspan="1">25.074</td><td align="center" valign="middle" rowspan="1" colspan="1">78.529</td><td align="center" valign="middle" rowspan="1" colspan="1">4.394</td><td align="center" valign="middle" rowspan="1" colspan="1">29.115</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">93.661</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">3.895</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">25.775</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">81.222</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">3.572</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">26.828</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">80.791</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.381</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">29.488</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.598</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.911</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05521-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05521-t003_Table 3</object-id><label>Table 3</label><caption><p>The enhancement performance of different SOTA models on the LOLv1, LOLv2, and LSRW datasets. The best and second-best performances are represented in red and blue, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Complexity</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LOLv1</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LOLv2_Real</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LOLv2_Synthetic</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LSRW_Huawei</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">LSRW_Nikon</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
MACs&#160;(G)
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Params&#160;(M)
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
FPS
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm270" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm271" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm272" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm273" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm274" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm275" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm276" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm277" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm278" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm279" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm280" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm281" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm282" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">PSNR</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm283" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">SSIM</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm284" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Low</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">5.72</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">6.01</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">4.09</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">3.16</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">3.45</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Reference</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">4.25</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">4.73</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">4.19</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">3.44</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">4.24</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BL [<xref rid="B48-sensors-25-05521" ref-type="bibr">48</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">150.799</td><td align="left" valign="middle" rowspan="1" colspan="1">1.606</td><td align="left" valign="middle" rowspan="1" colspan="1">146.895</td><td align="left" valign="middle" rowspan="1" colspan="1">10.31</td><td align="left" valign="middle" rowspan="1" colspan="1">40.13</td><td align="left" valign="middle" rowspan="1" colspan="1">7.31</td><td align="left" valign="middle" rowspan="1" colspan="1">12.89</td><td align="left" valign="middle" rowspan="1" colspan="1">43.53</td><td align="left" valign="middle" rowspan="1" colspan="1">7.73</td><td align="left" valign="middle" rowspan="1" colspan="1">13.58</td><td align="left" valign="middle" rowspan="1" colspan="1">61.44</td><td align="left" valign="middle" rowspan="1" colspan="1">4.74</td><td align="left" valign="middle" rowspan="1" colspan="1">11.78</td><td align="left" valign="middle" rowspan="1" colspan="1">31.24</td><td align="left" valign="middle" rowspan="1" colspan="1">3.06</td><td align="left" valign="middle" rowspan="1" colspan="1">13.43</td><td align="left" valign="middle" rowspan="1" colspan="1">36.19</td><td align="left" valign="middle" rowspan="1" colspan="1">3.85</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DeepUPE [<xref rid="B18-sensors-25-05521" ref-type="bibr">18</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">45.935</td><td align="left" valign="middle" rowspan="1" colspan="1">0.079</td><td align="left" valign="middle" rowspan="1" colspan="1">2426.394</td><td align="left" valign="middle" rowspan="1" colspan="1">12.71</td><td align="left" valign="middle" rowspan="1" colspan="1">45.04</td><td align="left" valign="middle" rowspan="1" colspan="1">7.79</td><td align="left" valign="middle" rowspan="1" colspan="1">14.60</td><td align="left" valign="middle" rowspan="1" colspan="1">47.02</td><td align="left" valign="middle" rowspan="1" colspan="1">8.23</td><td align="left" valign="middle" rowspan="1" colspan="1">13.82</td><td align="left" valign="middle" rowspan="1" colspan="1">60.50</td><td align="left" valign="middle" rowspan="1" colspan="1">4.37</td><td align="left" valign="middle" rowspan="1" colspan="1">13.63</td><td align="left" valign="middle" rowspan="1" colspan="1">36.25</td><td align="left" valign="middle" rowspan="1" colspan="1">3.00</td><td align="left" valign="middle" rowspan="1" colspan="1">13.36</td><td align="left" valign="middle" rowspan="1" colspan="1">35.97</td><td align="left" valign="middle" rowspan="1" colspan="1">3.64</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EnlightenGAN&#160;[<xref rid="B24-sensors-25-05521" ref-type="bibr">24</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">42.983</td><td align="left" valign="middle" rowspan="1" colspan="1">17.48</td><td align="left" valign="middle" rowspan="1" colspan="1">65.15</td><td align="left" valign="middle" rowspan="1" colspan="1">4.89</td><td align="left" valign="middle" rowspan="1" colspan="1">18.64</td><td align="left" valign="middle" rowspan="1" colspan="1">67.67</td><td align="left" valign="middle" rowspan="1" colspan="1">5.50</td><td align="left" valign="middle" rowspan="1" colspan="1">16.57</td><td align="left" valign="middle" rowspan="1" colspan="1">77.15</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">3.83</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">17.85</td><td align="left" valign="middle" rowspan="1" colspan="1">48.92</td><td align="left" valign="middle" rowspan="1" colspan="1">2.94</td><td align="left" valign="middle" rowspan="1" colspan="1">15.92</td><td align="left" valign="middle" rowspan="1" colspan="1">42.09</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">3.18</named-content>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GLADNet&#160;[<xref rid="B35-sensors-25-05521" ref-type="bibr">35</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">200.6</td><td align="left" valign="middle" rowspan="1" colspan="1">12.15</td><td align="left" valign="middle" rowspan="1" colspan="1">79.839</td><td align="left" valign="middle" rowspan="1" colspan="1">19.72</td><td align="left" valign="middle" rowspan="1" colspan="1">68.20</td><td align="left" valign="middle" rowspan="1" colspan="1">6.80</td><td align="left" valign="middle" rowspan="1" colspan="1">19.82</td><td align="left" valign="middle" rowspan="1" colspan="1">68.47</td><td align="left" valign="middle" rowspan="1" colspan="1">7.73</td><td align="left" valign="middle" rowspan="1" colspan="1">18.11</td><td align="left" valign="middle" rowspan="1" colspan="1">82.59</td><td align="left" valign="middle" rowspan="1" colspan="1">3.99</td><td align="left" valign="middle" rowspan="1" colspan="1">19.00</td><td align="left" valign="middle" rowspan="1" colspan="1">49.45</td><td align="left" valign="middle" rowspan="1" colspan="1">2.96</td><td align="left" valign="middle" rowspan="1" colspan="1">16.63</td><td align="left" valign="middle" rowspan="1" colspan="1">44.07</td><td align="left" valign="middle" rowspan="1" colspan="1">3.36</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">KinD&#160;[<xref rid="B19-sensors-25-05521" ref-type="bibr">19</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">61.01</td><td align="left" valign="middle" rowspan="1" colspan="1">114.35</td><td align="left" valign="middle" rowspan="1" colspan="1">24.385</td><td align="left" valign="middle" rowspan="1" colspan="1">17.64</td><td align="left" valign="middle" rowspan="1" colspan="1">77.13</td><td align="left" valign="middle" rowspan="1" colspan="1">3.90</td><td align="left" valign="middle" rowspan="1" colspan="1">20.58</td><td align="left" valign="middle" rowspan="1" colspan="1">81.78</td><td align="left" valign="middle" rowspan="1" colspan="1">4.14</td><td align="left" valign="middle" rowspan="1" colspan="1">17.27</td><td align="left" valign="middle" rowspan="1" colspan="1">75.78</td><td align="left" valign="middle" rowspan="1" colspan="1">4.25</td><td align="left" valign="middle" rowspan="1" colspan="1">17.03</td><td align="left" valign="middle" rowspan="1" colspan="1">49.88</td><td align="left" valign="middle" rowspan="1" colspan="1">2.64</td><td align="left" valign="middle" rowspan="1" colspan="1">15.47</td><td align="left" valign="middle" rowspan="1" colspan="1">44.04</td><td align="left" valign="middle" rowspan="1" colspan="1">3.46</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">KinD++ [<xref rid="B7-sensors-25-05521" ref-type="bibr">7</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">1050</td><td align="left" valign="middle" rowspan="1" colspan="1">17.42</td><td align="left" valign="middle" rowspan="1" colspan="1">14.844</td><td align="left" valign="middle" rowspan="1" colspan="1">17.75</td><td align="left" valign="middle" rowspan="1" colspan="1">75.82</td><td align="left" valign="middle" rowspan="1" colspan="1">4.01</td><td align="left" valign="middle" rowspan="1" colspan="1">17.66</td><td align="left" valign="middle" rowspan="1" colspan="1">76.09</td><td align="left" valign="middle" rowspan="1" colspan="1">4.20</td><td align="left" valign="middle" rowspan="1" colspan="1">17.48</td><td align="left" valign="middle" rowspan="1" colspan="1">78.57</td><td align="left" valign="middle" rowspan="1" colspan="1">4.76</td><td align="left" valign="middle" rowspan="1" colspan="1">16.97</td><td align="left" valign="middle" rowspan="1" colspan="1">41.15</td><td align="left" valign="middle" rowspan="1" colspan="1">3.02</td><td align="left" valign="middle" rowspan="1" colspan="1">14.74</td><td align="left" valign="middle" rowspan="1" colspan="1">36.80</td><td align="left" valign="middle" rowspan="1" colspan="1">3.72</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LIME&#160;[<xref rid="B8-sensors-25-05521" ref-type="bibr">8</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">3.463</td><td align="left" valign="middle" rowspan="1" colspan="1">16.05</td><td align="left" valign="middle" rowspan="1" colspan="1">48.60</td><td align="left" valign="middle" rowspan="1" colspan="1">8.79</td><td align="left" valign="middle" rowspan="1" colspan="1">17.16</td><td align="left" valign="middle" rowspan="1" colspan="1">48.02</td><td align="left" valign="middle" rowspan="1" colspan="1">9.31</td><td align="left" valign="middle" rowspan="1" colspan="1">16.37</td><td align="left" valign="middle" rowspan="1" colspan="1">73.74</td><td align="left" valign="middle" rowspan="1" colspan="1">4.76</td><td align="left" valign="middle" rowspan="1" colspan="1">17.13</td><td align="left" valign="middle" rowspan="1" colspan="1">39.31</td><td align="left" valign="middle" rowspan="1" colspan="1">3.44</td><td align="left" valign="middle" rowspan="1" colspan="1">14.64</td><td align="left" valign="middle" rowspan="1" colspan="1">34.99</td><td align="left" valign="middle" rowspan="1" colspan="1">3.61</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MELLEN-IC&#160;[<xref rid="B25-sensors-25-05521" ref-type="bibr">25</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">2532</td><td align="left" valign="middle" rowspan="1" colspan="1">8.275</td><td align="left" valign="middle" rowspan="1" colspan="1">1.432</td><td align="left" valign="middle" rowspan="1" colspan="1">17.23</td><td align="left" valign="middle" rowspan="1" colspan="1">75.44</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">3.31</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">20.75</td><td align="left" valign="middle" rowspan="1" colspan="1">78.98</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">3.32</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">21.57</td><td align="left" valign="middle" rowspan="1" colspan="1">88.08</td><td align="left" valign="middle" rowspan="1" colspan="1">3.98</td><td align="left" valign="middle" rowspan="1" colspan="1">18.22</td><td align="left" valign="middle" rowspan="1" colspan="1">53.48</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">2.64</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">16.71</td><td align="left" valign="middle" rowspan="1" colspan="1">45.08</td><td align="left" valign="middle" rowspan="1" colspan="1">3.41</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Retinexformer&#160;[<xref rid="B42-sensors-25-05521" ref-type="bibr">42</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">9.293</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">25.15</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">84.34</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">2.97</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">22.79</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">83.86</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">3.59</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">25.67</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">92.82</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">3.94</td><td align="left" valign="middle" rowspan="1" colspan="1">16.25</td><td align="left" valign="middle" rowspan="1" colspan="1">49.48</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">2.60</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">15.56</td><td align="left" valign="middle" rowspan="1" colspan="1">42.38</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">3.27</named-content>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RetinexNet&#160;[<xref rid="B13-sensors-25-05521" ref-type="bibr">13</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">9.342</td><td align="left" valign="middle" rowspan="1" colspan="1">16.77</td><td align="left" valign="middle" rowspan="1" colspan="1">42.50</td><td align="left" valign="middle" rowspan="1" colspan="1">9.73</td><td align="left" valign="middle" rowspan="1" colspan="1">16.10</td><td align="left" valign="middle" rowspan="1" colspan="1">40.71</td><td align="left" valign="middle" rowspan="1" colspan="1">10.56</td><td align="left" valign="middle" rowspan="1" colspan="1">17.14</td><td align="left" valign="middle" rowspan="1" colspan="1">75.64</td><td align="left" valign="middle" rowspan="1" colspan="1">5.69</td><td align="left" valign="middle" rowspan="1" colspan="1">16.82</td><td align="left" valign="middle" rowspan="1" colspan="1">38.50</td><td align="left" valign="middle" rowspan="1" colspan="1">4.33</td><td align="left" valign="middle" rowspan="1" colspan="1">13.49</td><td align="left" valign="middle" rowspan="1" colspan="1">28.94</td><td align="left" valign="middle" rowspan="1" colspan="1">4.27</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Zero DCE&#160;[<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">517.129</td><td align="left" valign="middle" rowspan="1" colspan="1">28.539</td><td align="left" valign="middle" rowspan="1" colspan="1">24.091</td><td align="left" valign="middle" rowspan="1" colspan="1">14.86</td><td align="left" valign="middle" rowspan="1" colspan="1">56.24</td><td align="left" valign="middle" rowspan="1" colspan="1">8.22</td><td align="left" valign="middle" rowspan="1" colspan="1">18.06</td><td align="left" valign="middle" rowspan="1" colspan="1">57.95</td><td align="left" valign="middle" rowspan="1" colspan="1">8.77</td><td align="left" valign="middle" rowspan="1" colspan="1">17.76</td><td align="left" valign="middle" rowspan="1" colspan="1">81.40</td><td align="left" valign="middle" rowspan="1" colspan="1">4.36</td><td align="left" valign="middle" rowspan="1" colspan="1">16.41</td><td align="left" valign="middle" rowspan="1" colspan="1">46.19</td><td align="left" valign="middle" rowspan="1" colspan="1">3.15</td><td align="left" valign="middle" rowspan="1" colspan="1">15.05</td><td align="left" valign="middle" rowspan="1" colspan="1">41.37</td><td align="left" valign="middle" rowspan="1" colspan="1">3.40</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Zero DCE++ [<xref rid="B26-sensors-25-05521" ref-type="bibr">26</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">0.109</td><td align="left" valign="middle" rowspan="1" colspan="1">0.594</td><td align="left" valign="middle" rowspan="1" colspan="1">123.976</td><td align="left" valign="middle" rowspan="1" colspan="1">17.04</td><td align="left" valign="middle" rowspan="1" colspan="1">56.25</td><td align="left" valign="middle" rowspan="1" colspan="1">8.46</td><td align="left" valign="middle" rowspan="1" colspan="1">18.14</td><td align="left" valign="middle" rowspan="1" colspan="1">55.18</td><td align="left" valign="middle" rowspan="1" colspan="1">9.06</td><td align="left" valign="middle" rowspan="1" colspan="1">18.64</td><td align="left" valign="middle" rowspan="1" colspan="1">83.52</td><td align="left" valign="middle" rowspan="1" colspan="1">4.55</td><td align="left" valign="middle" rowspan="1" colspan="1">18.12</td><td align="left" valign="middle" rowspan="1" colspan="1">45.55</td><td align="left" valign="middle" rowspan="1" colspan="1">3.27</td><td align="left" valign="middle" rowspan="1" colspan="1">15.10</td><td align="left" valign="middle" rowspan="1" colspan="1">39.20</td><td align="left" valign="middle" rowspan="1" colspan="1">3.56</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LLFlow [<xref rid="B49-sensors-25-05521" ref-type="bibr">49</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">0.761</td><td align="left" valign="middle" rowspan="1" colspan="1">24.06</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">86.02</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">4.07</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">26.43</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">90.26</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">4.53</td><td align="left" valign="middle" rowspan="1" colspan="1">19.22</td><td align="left" valign="middle" rowspan="1" colspan="1">82.41</td><td align="left" valign="middle" rowspan="1" colspan="1">4.66</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">20.09</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">55.07</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">2.88</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">16.88</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">45.66</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">3.73</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Priori DCE</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">834.2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.21</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.673</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">25.77</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.22</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.57</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">26.83</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.79</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.38</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">29.49</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">93.60</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">3.91</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">21.39</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">56.76</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.13</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">18.33</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">48.52</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.40</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05521-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05521-t004_Table 4</object-id><label>Table 4</label><caption><p>The enhancement performance of different SOTA models on the DICM, LIME, MEF, and NPE datasets. The best and second-best performances are represented in red and blue, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="5" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">
<inline-formula>
<mml:math id="mm285" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi>NIQE</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th colspan="5" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">
<inline-formula>
<mml:math id="mm286" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi>NIQE</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
DICM
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
LIME
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
MEF
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
NPE
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm287" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">avg</mml:mi></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
DICM
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
LIME
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
MEF
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
NPE
</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm288" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">avg</mml:mi></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Low</td><td align="left" valign="middle" rowspan="1" colspan="1">3.317</td><td align="left" valign="middle" rowspan="1" colspan="1">3.566</td><td align="left" valign="middle" rowspan="1" colspan="1">3.256</td><td align="left" valign="middle" rowspan="1" colspan="1">3.187</td><td align="left" valign="middle" rowspan="1" colspan="1">3.332</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BL&#160;[<xref rid="B48-sensors-25-05521" ref-type="bibr">48</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">4.078</td><td align="left" valign="middle" rowspan="1" colspan="1">4.216</td><td align="left" valign="middle" rowspan="1" colspan="1">3.383</td><td align="left" valign="middle" rowspan="1" colspan="1">4.424</td><td align="left" valign="middle" rowspan="1" colspan="1">4.025</td><td align="left" valign="middle" rowspan="1" colspan="1">1.534</td><td align="left" valign="middle" rowspan="1" colspan="1">1.933</td><td align="left" valign="middle" rowspan="1" colspan="1">0.665</td><td align="left" valign="middle" rowspan="1" colspan="1">1.877</td><td align="left" valign="middle" rowspan="1" colspan="1">1.502</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DeepUPE&#160;[<xref rid="B18-sensors-25-05521" ref-type="bibr">18</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.542</td><td align="left" valign="middle" rowspan="1" colspan="1">3.793</td><td align="left" valign="middle" rowspan="1" colspan="1">3.199</td><td align="left" valign="middle" rowspan="1" colspan="1">3.591</td><td align="left" valign="middle" rowspan="1" colspan="1">3.531</td><td align="left" valign="middle" rowspan="1" colspan="1">0.985</td><td align="left" valign="middle" rowspan="1" colspan="1">2.094</td><td align="left" valign="middle" rowspan="1" colspan="1">0.546</td><td align="left" valign="middle" rowspan="1" colspan="1">1.258</td><td align="left" valign="middle" rowspan="1" colspan="1">1.221</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EnlightenGAN&#160;[<xref rid="B24-sensors-25-05521" ref-type="bibr">24</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">3.056</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">3.380</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">2.895</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">3.368</td><td align="left" valign="middle" rowspan="1" colspan="1">3.175</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">0.823</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">1.514</td><td align="left" valign="middle" rowspan="1" colspan="1">0.466</td><td align="left" valign="middle" rowspan="1" colspan="1">1.241</td><td align="left" valign="middle" rowspan="1" colspan="1">1.011</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GLADNet&#160;[<xref rid="B35-sensors-25-05521" ref-type="bibr">35</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.276</td><td align="left" valign="middle" rowspan="1" colspan="1">3.902</td><td align="left" valign="middle" rowspan="1" colspan="1">3.179</td><td align="left" valign="middle" rowspan="1" colspan="1">3.271</td><td align="left" valign="middle" rowspan="1" colspan="1">3.407</td><td align="left" valign="middle" rowspan="1" colspan="1">0.955</td><td align="left" valign="middle" rowspan="1" colspan="1">2.608</td><td align="left" valign="middle" rowspan="1" colspan="1">0.625</td><td align="left" valign="middle" rowspan="1" colspan="1">0.986</td><td align="left" valign="middle" rowspan="1" colspan="1">1.293</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">KinD&#160;[<xref rid="B19-sensors-25-05521" ref-type="bibr">19</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.351</td><td align="left" valign="middle" rowspan="1" colspan="1">4.357</td><td align="left" valign="middle" rowspan="1" colspan="1">3.378</td><td align="left" valign="middle" rowspan="1" colspan="1">3.269</td><td align="left" valign="middle" rowspan="1" colspan="1">3.589</td><td align="left" valign="middle" rowspan="1" colspan="1">1.017</td><td align="left" valign="middle" rowspan="1" colspan="1">3.794</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">0.464</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">0.934</td><td align="left" valign="middle" rowspan="1" colspan="1">1.552</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">KinD++&#160;[<xref rid="B7-sensors-25-05521" ref-type="bibr">7</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.280</td><td align="left" valign="middle" rowspan="1" colspan="1">4.853</td><td align="left" valign="middle" rowspan="1" colspan="1">3.471</td><td align="left" valign="middle" rowspan="1" colspan="1">3.636</td><td align="left" valign="middle" rowspan="1" colspan="1">3.810</td><td align="left" valign="middle" rowspan="1" colspan="1">1.000</td><td align="left" valign="middle" rowspan="1" colspan="1">4.466</td><td align="left" valign="middle" rowspan="1" colspan="1">0.473</td><td align="left" valign="middle" rowspan="1" colspan="1">1.220</td><td align="left" valign="middle" rowspan="1" colspan="1">1.790</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LIME&#160;[<xref rid="B8-sensors-25-05521" ref-type="bibr">8</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.471</td><td align="left" valign="middle" rowspan="1" colspan="1">3.835</td><td align="left" valign="middle" rowspan="1" colspan="1">3.488</td><td align="left" valign="middle" rowspan="1" colspan="1">3.470</td><td align="left" valign="middle" rowspan="1" colspan="1">3.566</td><td align="left" valign="middle" rowspan="1" colspan="1">1.179</td><td align="left" valign="middle" rowspan="1" colspan="1">2.364</td><td align="left" valign="middle" rowspan="1" colspan="1">0.804</td><td align="left" valign="middle" rowspan="1" colspan="1">1.258</td><td align="left" valign="middle" rowspan="1" colspan="1">1.401</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MELLEN-IC&#160;[<xref rid="B25-sensors-25-05521" ref-type="bibr">25</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">2.911</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">3.503</td><td align="left" valign="middle" rowspan="1" colspan="1">3.097</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">3.087</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">3.150</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">0.784</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">1.673</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">0.415</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">0.783</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">0.914</named-content>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Retinexformer&#160;[<xref rid="B42-sensors-25-05521" ref-type="bibr">42</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.353</td><td align="left" valign="middle" rowspan="1" colspan="1">3.705</td><td align="left" valign="middle" rowspan="1" colspan="1">3.139</td><td align="left" valign="middle" rowspan="1" colspan="1">3.174</td><td align="left" valign="middle" rowspan="1" colspan="1">3.343</td><td align="left" valign="middle" rowspan="1" colspan="1">0.972</td><td align="left" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">1.468</named-content>
</td><td align="left" valign="middle" rowspan="1" colspan="1">0.753</td><td align="left" valign="middle" rowspan="1" colspan="1">1.014</td><td align="left" valign="middle" rowspan="1" colspan="1">1.052</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RetinexNet&#160;[<xref rid="B13-sensors-25-05521" ref-type="bibr">13</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">4.315</td><td align="left" valign="middle" rowspan="1" colspan="1">4.916</td><td align="left" valign="middle" rowspan="1" colspan="1">4.904</td><td align="left" valign="middle" rowspan="1" colspan="1">4.388</td><td align="left" valign="middle" rowspan="1" colspan="1">4.631</td><td align="left" valign="middle" rowspan="1" colspan="1">1.715</td><td align="left" valign="middle" rowspan="1" colspan="1">3.557</td><td align="left" valign="middle" rowspan="1" colspan="1">1.475</td><td align="left" valign="middle" rowspan="1" colspan="1">1.496</td><td align="left" valign="middle" rowspan="1" colspan="1">2.061</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Zero DCE&#160;[<xref rid="B1-sensors-25-05521" ref-type="bibr">1</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.430</td><td align="left" valign="middle" rowspan="1" colspan="1">3.786</td><td align="left" valign="middle" rowspan="1" colspan="1">3.309</td><td align="left" valign="middle" rowspan="1" colspan="1">3.433</td><td align="left" valign="middle" rowspan="1" colspan="1">3.489</td><td align="left" valign="middle" rowspan="1" colspan="1">1.195</td><td align="left" valign="middle" rowspan="1" colspan="1">2.093</td><td align="left" valign="middle" rowspan="1" colspan="1">0.861</td><td align="left" valign="middle" rowspan="1" colspan="1">1.223</td><td align="left" valign="middle" rowspan="1" colspan="1">1.343</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Zero DCE++&#160;[<xref rid="B26-sensors-25-05521" ref-type="bibr">26</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.543</td><td align="left" valign="middle" rowspan="1" colspan="1">4.092</td><td align="left" valign="middle" rowspan="1" colspan="1">3.568</td><td align="left" valign="middle" rowspan="1" colspan="1">3.603</td><td align="left" valign="middle" rowspan="1" colspan="1">3.701</td><td align="left" valign="middle" rowspan="1" colspan="1">1.259</td><td align="left" valign="middle" rowspan="1" colspan="1">2.489</td><td align="left" valign="middle" rowspan="1" colspan="1">0.953</td><td align="left" valign="middle" rowspan="1" colspan="1">1.249</td><td align="left" valign="middle" rowspan="1" colspan="1">1.488</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LLFlow&#160;[<xref rid="B49-sensors-25-05521" ref-type="bibr">49</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">3.368</td><td align="left" valign="middle" rowspan="1" colspan="1">3.891</td><td align="left" valign="middle" rowspan="1" colspan="1">3.515</td><td align="left" valign="middle" rowspan="1" colspan="1">3.556</td><td align="left" valign="middle" rowspan="1" colspan="1">3.583</td><td align="left" valign="middle" rowspan="1" colspan="1">0.885</td><td align="left" valign="middle" rowspan="1" colspan="1">2.007</td><td align="left" valign="middle" rowspan="1" colspan="1">0.542</td><td align="left" valign="middle" rowspan="1" colspan="1">0.877</td><td align="left" valign="middle" rowspan="1" colspan="1">1.078</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Priori DCE (Ours)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.155</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">3.153</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">2.848</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">2.968</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">3.031</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.872</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">1.174</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.603</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">0.708</named-content>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">0.839</named-content>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>