<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431334</article-id><article-id pub-id-type="pmcid-ver">PMC12431334.1</article-id><article-id pub-id-type="pmcaid">12431334</article-id><article-id pub-id-type="pmcaiid">12431334</article-id><article-id pub-id-type="doi">10.3390/s25175412</article-id><article-id pub-id-type="publisher-id">sensors-25-05412</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title><monospace>LARS:</monospace> A Light-Augmented Reality System for Collective Robotic Interaction <xref rid="fn1-sensors-25-05412" ref-type="author-notes">&#8224;</xref></article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8637-7436</contrib-id><name name-style="western"><surname>Raoufi</surname><given-names initials="M">Mohsen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05412" ref-type="aff">1</xref><xref rid="af2-sensors-25-05412" ref-type="aff">2</xref><xref rid="c1-sensors-25-05412" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4733-998X</contrib-id><name name-style="western"><surname>Romanczuk</surname><given-names initials="P">Pawel</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05412" ref-type="aff">1</xref><xref rid="af3-sensors-25-05412" ref-type="aff">3</xref><xref rid="fn2-sensors-25-05412" ref-type="author-notes">&#8225;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2458-8289</contrib-id><name name-style="western"><surname>Hamann</surname><given-names initials="H">Heiko</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05412" ref-type="aff">1</xref><xref rid="af4-sensors-25-05412" ref-type="aff">4</xref><xref rid="fn2-sensors-25-05412" ref-type="author-notes">&#8225;</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Neamtu</surname><given-names initials="CGD">Calin Gheorghe Dan</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Comes</surname><given-names initials="R">Radu</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Fang</surname><given-names initials="JJ">Jing-Jing</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Popovici</surname><given-names initials="DM">Dorin-Mircea</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05412"><label>1</label>Science of Intelligence, Research Cluster of Excellence, 10587 Berlin, Germany; <email>pawel.romanczuk@hu-berlin.de</email> (P.R.); <email>heiko.hamann@uni-konstanz.de</email> (H.H.)</aff><aff id="af2-sensors-25-05412"><label>2</label>Department of Electrical Engineering and Computer Science, Technical University of Berlin, 10587 Berlin, Germany</aff><aff id="af3-sensors-25-05412"><label>3</label>Department of Biology, Humboldt University of Berlin, 10115 Berlin, Germany</aff><aff id="af4-sensors-25-05412"><label>4</label>Department of Computer and Information Science, University of Konstanz, 78457 Konstanz, Germany</aff><author-notes><corresp id="c1-sensors-25-05412"><label>*</label>Correspondence: <email>mohsenraoufi@icloud.com</email></corresp><fn id="fn1-sensors-25-05412"><label>&#8224;</label><p>This article is a revised and expanded version of a paper entitled Raoufi, M.; Romanczuk, P.; Hamann, H. LARS: Light Augmented Reality System for Swarm. In Proceedings of the 14th International Conference, ANTS 2024, Konstanz, Germany, 9&#8211;11 October 2024.</p></fn><fn id="fn2-sensors-25-05412"><label>&#8225;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>02</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5412</elocation-id><history><date date-type="received"><day>03</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>22</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>26</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05412.pdf"/><abstract><p>Collective robotics systems hold great potential for future education and public engagement; however, only a few are utilized in these contexts. One reason is the lack of accessible tools to convey their complex, embodied interactions. In this work, we introduce the Light-Augmented Reality System (<monospace>LARS</monospace>), an open-source, marker-free, cross-platform tool designed to support experimentation, education, and outreach in collective robotics. <monospace>LARS</monospace> employs Extended Reality (XR) to project dynamic visual objects into the physical environment. This enables indirect robot&#8211;robot communication through stigmergy while preserving the physical and sensing constraints of the real robots, and enhances robot&#8211;human interaction by making otherwise hidden information visible. The system is low-cost, easy to deploy, and platform-independent without requiring hardware modifications. By projecting visible information in real time, <monospace>LARS</monospace> facilitates reproducible experiments and bridges the gap between abstract collective dynamics and observable behavior. We demonstrate that <monospace>LARS</monospace> can serve both as a research tool and as a means to motivate students and the broader public to engage with collective robotics. Its accessibility and flexibility make it an effective platform for illustrating complex multi-robot interactions, promoting hands-on learning, and expanding public understanding of collective, embodied intelligence.</p></abstract><kwd-group><kwd>collective robotics</kwd><kwd>swarm robotics</kwd><kwd>multi-robot systems</kwd><kwd>Extended Reality (XR)</kwd><kwd>Augmented Reality (AR)</kwd><kwd>Mixed Reality (MR)</kwd><kwd>open-source robotics</kwd><kwd>human&#8211;robot interaction</kwd><kwd>educational robotics</kwd></kwd-group><funding-group><award-group><funding-source>Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)</funding-source><award-id>390523135</award-id></award-group><funding-statement>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany&#8217;s Excellence Strategy&#8212;EXC 2002/1 &#8220;Science of Intelligence&#8221;&#8212;project number 390523135.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05412"><title>1. Introduction</title><p>Since the first appearance of Extended Reality (XR) technologies, they have been adopted across a wide range of applications&#160;[<xref rid="B1-sensors-25-05412" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05412" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05412" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05412" ref-type="bibr">4</xref>]. Among these, Augmented Reality (AR) and Mixed Reality (MR), in particular, are increasingly used in educational technology and robotics&#160;[<xref rid="B5-sensors-25-05412" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05412" ref-type="bibr">6</xref>]. These tools enrich learning experiences and broaden research possibilities. In education, XR facilitates understanding of abstract or complex topics through immersive and interactive experiences&#160;[<xref rid="B2-sensors-25-05412" ref-type="bibr">2</xref>]. In robotics, and especially in multi-robot systems, XR offers significant potential for education, providing opportunities to showcase collective behaviors and engage a general audience in an instructive and interactive manner. In these systems, XR tools create dynamic virtual environments that are observable to humans and serve as a medium for interaction with multi-robot systems. By making intangible and complex concepts in collective systems more accessible to human users, XR technologies support better understanding, analysis, and communication of how these complex systems function.</p><p>The application of XR technology in robotics research goes further than simple visualization; it becomes a basis for developing and testing advanced multi-robot scenarios&#160;[<xref rid="B5-sensors-25-05412" ref-type="bibr">5</xref>,<xref rid="B7-sensors-25-05412" ref-type="bibr">7</xref>]. They open up new possibilities for researchers to study their system in augmented environments that go beyond the real-world limitations of lab settings. At the core, these systems are equipped with detection and tracking components which are necessary tools to study physical collective systems, whether artificial or natural&#160;[<xref rid="B8-sensors-25-05412" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05412" ref-type="bibr">9</xref>]. As an intermediate step between simulation and real-world deployment, XR platforms can split the sim-to-real gap into a sim-to-lab and a lab-to-real step, making bridging the gap toward realization more manageable.</p><p>We present the Light-Augmented Reality System (<monospace>LARS</monospace>) as an open-source, and cost-effective AR tool for collective robotics. <monospace>LARS</monospace> uses projection-based augmentation to embed virtual elements directly into the robots&#8217; shared physical environment, enabling indirect robot&#8211;robot and human&#8211;robot interaction without modifying the physical robots. It is compatible with a range of mobile robots, from miniature platforms (e.g., Kilobot&#160;[<xref rid="B10-sensors-25-05412" ref-type="bibr">10</xref>]) to mid-sized robots (e.g., Thymio&#160;[<xref rid="B11-sensors-25-05412" ref-type="bibr">11</xref>]).</p><p><monospace>LARS</monospace> has been developed and refined through its use in multiple collective robotics projects&#160;[<xref rid="B12-sensors-25-05412" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05412" ref-type="bibr">13</xref>], evolving to meet the needs of research in this field. Beyond research, <monospace>LARS</monospace> has also been used in public demonstrations to make complex concepts interpretable to human observers through visual augmentation. These experiences shaped a set of design requirements for creating a robust, versatile tool for collective robotics research, education, and outreach.</p><p>Based on these experiences, we listed a set of design requirements that guided the development of <monospace>LARS</monospace>. The key requirements are as follows:<list list-type="bullet"><list-item><p>R1: Closed-loop interaction. The system must support experiments where the virtual environment, and hence visual feedback, is continuously updated based on the tracked state of robots, running as a standalone setup without requiring external infrastructure (See <xref rid="sensors-25-05412-f001" ref-type="fig">Figure 1</xref>).</p></list-item><list-item><p>R2: Platform adaptability. The system must support marker-free tracking across diverse robotic platforms, enabling integration with different robot types and simplifying experimental setup.</p></list-item><list-item><p>R3: Low-latency response. The system should maintain low latency and perceptually smooth visual updates to ensure accurate timing in interactive tasks.</p></list-item><list-item><p>R4: Flexible environment augmentation. Users should be able to project dynamic, versatile elements, such as gradients, or visual noise, that both robots and human observers can interpret.</p></list-item><list-item><p>R5: Runtime configurability. Parameters (e.g., detection thresholds) must be adjustable at runtime via the GUI, without restarting the software or modifying source code.</p></list-item><list-item><p>R6: Integrated data logging. The system should provide built-in data logging and video recording for post-experiment analysis.</p></list-item><list-item><p>R7: Scalability and efficiency. The architecture should remain computationally efficient while supporting large numbers of agents.</p></list-item><list-item><p>R8: Open accessibility. <monospace>LARS</monospace> is released as a free and open-source tool, explicitly building on prior open-source software and extending this collaborative practice to support transparency, reproducibility, and community-driven development&#160;[<xref rid="B14-sensors-25-05412" ref-type="bibr">14</xref>].</p></list-item></list></p></sec><sec id="sec2-sensors-25-05412"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05412"><title>2.1. Augmented and Extended Reality Systems</title><p>Extending reality by augmenting virtual objects into the physical world has been widely studied both as an industrial technology and as a research tool&#160;[<xref rid="B2-sensors-25-05412" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05412" ref-type="bibr">3</xref>,<xref rid="B15-sensors-25-05412" ref-type="bibr">15</xref>]. Extended Reality refers broadly to technologies that blend real and virtual content. This includes Augmented Reality, where virtual elements are overlaid onto the real world; Mixed Reality, where real and virtual elements coexist and interact; and Virtual Reality, which immerses the user entirely in a synthetic environment&#160;[<xref rid="B16-sensors-25-05412" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05412" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05412" ref-type="bibr">18</xref>]. While these terms are sometimes used inconsistently&#160;[<xref rid="B2-sensors-25-05412" ref-type="bibr">2</xref>], they span a continuum of immersion, ranging from minimal augmentation of the real world to fully virtual experiences. Most XR systems are developed for human users, accessed through head-mounted displays, tablets, or smartphones. However, robots have also been considered as target users in several studies&#160;[<xref rid="B7-sensors-25-05412" ref-type="bibr">7</xref>,<xref rid="B19-sensors-25-05412" ref-type="bibr">19</xref>]. The applications of XR systems in robotics cover a wide range of topics, mainly with the aim of enhancing HRI with the focus on the projected information observable only for the human user&#160;[<xref rid="B7-sensors-25-05412" ref-type="bibr">7</xref>,<xref rid="B20-sensors-25-05412" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05412" ref-type="bibr">21</xref>]. Unlike humans, robots do not rely on visual interfaces but instead perceive their environment through sensors. Consequently, XR systems designed for interaction with humans <italic toggle="yes">and robots</italic> require a spatial, environment-level approach to augmentation, rather than device-centric rendering.</p></sec><sec id="sec2dot2-sensors-25-05412"><title>2.2. Spatial Augmented Reality for Multi-Robot Systems</title><p>Spatial Augmented Reality (SAR) shifts the augmentation into the environment, typically via projectors or shared screens&#160;[<xref rid="B22-sensors-25-05412" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05412" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05412" ref-type="bibr">24</xref>]. Instead of delivering content through wearable devices, SAR embeds virtual elements into the physical workspace, making them visible to both robots and humans without individual displays. In industrial robotics, SAR has been used to project instructions, task boundaries, or robot trajectories onto workspaces, enhancing human understanding and streamlining collaborative workflows without the need for screens or head-mounted displays&#160;[<xref rid="B5-sensors-25-05412" ref-type="bibr">5</xref>,<xref rid="B25-sensors-25-05412" ref-type="bibr">25</xref>].</p><p>However, the use of SAR in multi-robot systems, particularly in collective robotics, remains limited. One of the earliest examples&#160;[<xref rid="B26-sensors-25-05412" ref-type="bibr">26</xref>] visualized pheromone trails projected into the environment to help interpret foraging behaviors. While effective for observation, this system was tailored to a specific experiment and lacked general-purpose capabilities or support for robot&#8211;environment interaction. In contrast to the SAR approach, a few systems have adopted a Virtual Reality approach for multi-robot systems, in which the physical limitations of the robots are modified or extended virtually. For example, in Kilogrid&#160;[<xref rid="B27-sensors-25-05412" ref-type="bibr">27</xref>] and ARK&#160;[<xref rid="B28-sensors-25-05412" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05412" ref-type="bibr">29</xref>], robots operate in an environment that is digitally controlled, with virtual cues such as gradients or pheromone fields projected into the arena. These systems enable precise control of swarm behavior through environmental modulation, but they are often designed for a single platform and are not easily transferable to other robots or experimental designs. In essence, they shift the system toward the VR end of the XR spectrum, focusing more on digitally defining the robot&#8217;s world than on making physical environments more interpretable or interactive.</p><p>Compared to prior XR-based systems for multi-robot platforms, <monospace>LARS</monospace> aims to generalize beyond single-purpose tools by integrating multiple functions into a unified, flexible architecture. Previous systems have often focused on one or two specific objectives: some provided real-time logging or experiment replay capabilities&#160;[<xref rid="B30-sensors-25-05412" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05412" ref-type="bibr">31</xref>]; others visualized robot states or internal variables on a GUI to support analysis&#160;[<xref rid="B5-sensors-25-05412" ref-type="bibr">5</xref>,<xref rid="B32-sensors-25-05412" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05412" ref-type="bibr">33</xref>] or through an AR headset for human users&#160;[<xref rid="B34-sensors-25-05412" ref-type="bibr">34</xref>]; some enabled limited virtual environmental features to test behaviors under specific conditions&#160;[<xref rid="B26-sensors-25-05412" ref-type="bibr">26</xref>,<xref rid="B35-sensors-25-05412" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-05412" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05412" ref-type="bibr">37</xref>]; and a few extended robot capabilities using platform-specific augmentation&#160;[<xref rid="B27-sensors-25-05412" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05412" ref-type="bibr">28</xref>,<xref rid="B38-sensors-25-05412" ref-type="bibr">38</xref>].</p><p>Unlike these, <monospace>LARS</monospace> is designed as a general-purpose SAR system that combines these objectives in a closed-loop setup by projecting virtual elements directly into the physical arena, creating a shared real&#8211;virtual environment visible to both robots and human observers. It supports marker-free tracking across various robot types, enables spatial interaction through dynamic visual cues, allows control over virtual environmental properties, and embeds visualization directly into the shared physical space. These features make it suitable for scalable, interactive, and observational experiments in collective robotics. A detailed comparison with prior systems is provided in <xref rid="sensors-25-05412-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec2dot3-sensors-25-05412"><title>2.3. Tracking and Localization Systems</title><p>Many tracking systems have been developed to study collective behavior in biological and artificial systems. In biology, tracking enables quantitative analysis of group-level patterns, while in robotics, it supports experiment documentation and performance evaluation. Most existing systems rely on physical markers or tags to identify individuals&#160;[<xref rid="B39-sensors-25-05412" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05412" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05412" ref-type="bibr">41</xref>,<xref rid="B42-sensors-25-05412" ref-type="bibr">42</xref>], but marker-free approaches offer greater flexibility&#160;[<xref rid="B43-sensors-25-05412" ref-type="bibr">43</xref>], especially for offline post-processing of recorded videos.</p><p>For online tracking, marker-free systems face added challenges, as they require prior knowledge of object properties (e.g., shape or appearance). In tracking multiple objects, they must also maintain accuracy and identity consistency across scales. Choosing the appropriate tracking approach involves navigating trade-offs, such as generalization versus specialization. In the design of our tracking system, we take advantage of the geometric regularity of the robots: from a downward-facing camera, their bodies appear approximately circular. This allows us to adopt an algorithm originally developed specifically for Kilobots, ARK&#160;[<xref rid="B28-sensors-25-05412" ref-type="bibr">28</xref>], which detects robots using simple circle detection. When combined with more sophisticated filtering and tracking algorithms, this minimal detection approach proves robust in maintaining individual identities, even during close interactions and collisions. For non-circular robots, approximate roundness is sufficient; alternatively, a simple printed ring can also be added to improve detection accuracy. This design enables us to robustly track a variety of mobile robots, as we demonstrate later in the paper.</p></sec></sec><sec id="sec3-sensors-25-05412"><title>3. Materials and Methods</title><p>The core material of this work is the <monospace>LARS</monospace> system itself, which consists of a software&#8211;hardware pipeline for tracking, visualization, and interaction in collective robotics experiments. In this section, we first describe the software architecture that coordinates these functions, followed by details of its implementation.</p><sec id="sec3dot1-sensors-25-05412"><title>3.1. Software Architecture</title><p>The goal of the software is to provide an all-in-one application, running on a local computer connected to the camera and projector, that orchestrates the coordination between these components along with user inputs received either through a GUI or directly from a controller device. The application uses a predetermined physical model to generate virtual, visual scenes that interact with robots. A projector displays the scene directly onto the surface where the robots are placed. This interface serves as the primary means to merge virtual and real-world elements into a Mixed Reality world. <monospace>LARS</monospace> also includes interfaces that use central information to transmits messages directly to the robots. Although the use of a central transmitter, such as the overhead controller for Kilobots, departs from the decentralized assumptions and bypasses the physical limitation of robots, we keep this feature because it allows users to program the robots and send control commands, such as starting or stopping experiments.</p><p>By incorporating insights from established software design paradigms&#160;[<xref rid="B30-sensors-25-05412" ref-type="bibr">30</xref>], <monospace>LARS</monospace> adopts and extends the Model&#8211;View&#8211;Controller (MVC) design pattern&#160;[<xref rid="B44-sensors-25-05412" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05412" ref-type="bibr">45</xref>] to address the design requirements outlined in <xref rid="sec1-sensors-25-05412" ref-type="sec">Section 1</xref> (R1&#8211;R7). This separation between the world model, control logic, and visualization allows each layer to be modified or replaced independently, simplifying adaptation to new platforms (R2) and experimental needs (R3&#8211;R7). The MVC pattern is thus well-suited to facilitating immersive interaction between users (human and robot) and the virtual and physical elements in the arena (see <xref rid="sensors-25-05412-f002" ref-type="fig">Figure 2</xref>).</p><sec id="sec3dot1dot1-sensors-25-05412"><title>3.1.1. View Layer</title><p>The view layer manages the system&#8217;s visual representation and user interaction. It consists of two main components:<list list-type="bullet"><list-item><p><italic toggle="yes">Presentation Block</italic> is responsible for displaying the XR environment and presenting information to the user. It includes the GUI, which provides an enhanced view of the environment from the bird&#8217;s-eye camera with augmented overlays tailored to the experiment&#8217;s objectives (R4).</p></list-item><list-item><p><italic toggle="yes">Interaction Block</italic> manages both human and robot inputs. Users interact with the system either indirectly via GUI functions or directly by sending messages to the robots (R5). This block also acquires visual data from the camera and forwards it to the tracking module in the control layer.</p></list-item></list></p></sec><sec id="sec3dot1dot2-sensors-25-05412"><title>3.1.2. Control Layer</title><p>The control layer handles the operational logic of the application. It consists of the following:<list list-type="bullet"><list-item><p><italic toggle="yes">Application Controller Module</italic> governs execution flow, coordinates between modules, processes user and system events, and updates the world model based on scene changes (R1).</p></list-item><list-item><p><italic toggle="yes">Tracking Block</italic> detects and tracks objects in the environment to ensure consistent identification across frames (R2).</p></list-item></list></p></sec><sec id="sec3dot1dot3-sensors-25-05412"><title>3.1.3. Model Layer</title><p>The model layer contains the internal representation of the XR environment. It includes the world model, which stores the current state of the environment, and the Physics of Objects module, which defines the physical rules for virtual entities (R1, R4).</p></sec></sec><sec id="sec3dot2-sensors-25-05412"><title>3.2. Implementation Details</title><p>We implemented the software in C++ language and based the application in the Qt framework&#160;[<xref rid="B46-sensors-25-05412" ref-type="bibr">46</xref>], using the OpenCV library&#160;[<xref rid="B47-sensors-25-05412" ref-type="bibr">47</xref>] for image processing and real-time video handling. These choices are motivated by their open-source nature, strong community support, and suitability for real-time performance. The current version of <monospace>LARS</monospace> is tested on Ubuntu 20.04 and 22.04 (R8).</p><p>The view layer&#8217;s Presentation Block is implemented using Qt Painter for rendering virtual elements such as shapes, gradients, noise textures, and overlays (e.g., Voronoi diagrams, interaction networks). Rendering is updated every 15 ms (60 FPS) via a timer-driven loop, with the output sent to the projector as a secondary display. Spatial alignment between the virtual scene and the physical arena is achieved through the virtual&#8211;real coordinate mapping described in <xref rid="sec3dot4-sensors-25-05412" ref-type="sec">Section 3.4</xref>.</p><p>In the control layer, synchronization between tracking, rendering, and projection is managed within Qt&#8217;s main event loop, ensuring that each projected frame corresponds to the most recent tracking data (R3). Because LARS is a standalone, closed-loop system that does not rely on external tracking data, or direct, real-time state exchange with robots, strict hardware-level synchronization is not strictly necessary. Frame-to-frame consistency is sufficient for both robot sensing and human observation, allowing the system to remain lightweight while maintaining responsive interaction.</p></sec><sec id="sec3dot3-sensors-25-05412"><title>3.3. Detection and Tracking Infrastructure</title><p>The detection and tracking module in <monospace>LARS</monospace> builds upon the tracker developed for <monospace>ARK</monospace> [<xref rid="B28-sensors-25-05412" ref-type="bibr">28</xref>], which has been tested effectively for Kilobot experiments. Leveraging the generality of the detection based on the Hough circle method implemented by CUDA libraries&#160;[<xref rid="B48-sensors-25-05412" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-05412" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-05412" ref-type="bibr">50</xref>], and harnessing the regularity of circular shape of swarm robotic platforms, <monospace>LARS</monospace> extends the application to any circular-shaped robot, beyond Kilobot (R2). While full circularity is not required, detection remains effective for platforms like Thymio&#160;II with semi-rounded shapes. For non-circular platforms, a simple printed ring can be added to improve detection robustness. By optimizing some parts of the original <monospace>ARK</monospace> code, we boosted the tracking performance from 9 to 38 FPS in the current version of <monospace>LARS</monospace> (R3, R7). For more technical details, please see the repository (original implementation: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/DiODeProject/KilobotArena">https://github.com/DiODeProject/KilobotArena</uri>, accessed on 22 August 2025 [<xref rid="B28-sensors-25-05412" ref-type="bibr">28</xref>]; our modified version: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/mohsen-raoufi/LARS">https://github.com/mohsen-raoufi/LARS</uri>, accessed on 22 August 2025 [<xref rid="B51-sensors-25-05412" ref-type="bibr">51</xref>]).</p><p>In addition to position, the system detects robot colors, which can represent internal states (e.g., LED indicators on Kilobots). To improve reliability, color detection is currently limited to red and blue, as demonstrated in synchronization&#160;[<xref rid="B13-sensors-25-05412" ref-type="bibr">13</xref>] and heterogeneous phototaxis scenarios&#160;[<xref rid="B12-sensors-25-05412" ref-type="bibr">12</xref>].</p></sec><sec id="sec3dot4-sensors-25-05412"><title>3.4. Mapping Virtual to Real Coordinates</title><p>To connect the virtual world to the real environment, we need to find the relation between three coordinates: projector (P), real-world (R), and camera (C) (as shown in <xref rid="sensors-25-05412-f003" ref-type="fig">Figure 3</xref>a). This process also makes self-calibration possible and ensures precise projection of virtual objects into the real world with every execution of the application. We use four Fiducial (ArUco&#160;[<xref rid="B52-sensors-25-05412" ref-type="bibr">52</xref>]) markers, with predetermined positions in the projector coordinates (e.g., <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>). The user determines the locations based on the desired location of the arena. The arena specifies the boundary within which the experiment is conducted (see <xref rid="sensors-25-05412-f001" ref-type="fig">Figure 1</xref>). Once all four markers are detected on the camera frame (<inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>), we apply a <monospace>getPerspectiveTransform</monospace> function&#160;[<xref rid="B47-sensors-25-05412" ref-type="bibr">47</xref>] to calculate the transformation matrix from the P to C coordinates (<inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mi>PC</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>). The inverse of this matrix provides the transformation in the opposite direction (<inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mi>CP</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mi>PC</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). This method allows us to project virtual objects into the physical world without needing their explicit position in the real-world coordinates. The transformation matrix <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mi>CP</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the key to mapping virtual objects onto the real world. Given the position of objects (e.g., robots) in the camera frame and the transformation matrix, the program ensures seamless interaction between the entities of the two worlds.</p></sec><sec id="sec3dot5-sensors-25-05412"><title>3.5. Augmenting Reality with Projected Visual Objects</title><p>Modifying the environment based on the state of robots enables the possibility of implementing indirect communication among robots, between robots and the environment, and between robots and human users. To achieve this, we augment the environment by creating virtual scenes. Creating virtual scenes for XR environments for human users typically involves very intricate processes to make the integration of the virtual entities into the real world seamless, and enhance the user&#8217;s interactive experience. Our objective in determining the appropriate level of complexity for virtual object design was to eliminate excessive nuances that do not enhance the quality of experience for either human or robot users. This simplification reduces computational load and enhances overall system performance.</p><p>At the base of creating projected visual objects, we rely on the WM which encompasses the fundamental principles of physics governing the interaction between objects in the XR environment, as well as the parameters and states of the world. The user defines the rules according to the requirements of the experiments in the WM. Consider a setting in which the trace of robots&#8217; movement resembles pheromone trails. The aggregation of pheromones is modeled via the principle physical rules of the WM. Additionally, environmental parameters including the evaporation rate and the width or intensity of the pheromone are described in the WM.</p></sec><sec id="sec3dot6-sensors-25-05412"><title>3.6. Direct Communication to Robots</title><p>Compared to indirect augmented communication, which presumes the realistic limitation of robotic platforms, a direct and centralized communication approach allows researchers to explore system behavior in configurations that exceed its physical restrictions. This freedom in experimentation has contributed to the adoption of this approach in various previous studies. To preserve this direct interaction capacity, we implemented a channel for sending commands to the robots via output devices. However, it is important to note that this aspect is not the primary focus of this paper. In our experiments with two platforms, we utilized the infrared (IR) messaging protocol via the overhead controller (OHC) for Kilobots, and WiFi communication for other more advanced robots (e.g., Thymios with Raspberry Pi board, Raspberry Pi Foundation, Cambridge, UK). Combining this centralized communication with the projection of the network (e.g., a Voronoi-based network) the user can effectively demonstrate an abstract notion in a tangible way. The benefit of integrating an output device controller specifically for Kilobots is to program the robots via the OHC. In addition, it enables immersing the reality with more virtual information, as demonstrated in&#160;[<xref rid="B28-sensors-25-05412" ref-type="bibr">28</xref>]. This enables robots to interact with a virtual environment, where messages are sent to the robots directly.</p></sec></sec><sec sec-type="results" id="sec4-sensors-25-05412"><title>4. Results</title><p>To assess the capabilities of <monospace>LARS</monospace>, we conducted a series of benchmarking experiments that measure the system&#8217;s performance under different conditions. These include tests of scalability, tracking precision, responsiveness under processing load, and robustness to environmental noise and lighting variations. With these benchmarks, we establish that <monospace>LARS</monospace> is both reliable and efficient, even at collective scales and under various environmental conditions, such as brightness or visual noise.</p><sec id="sec4dot1-sensors-25-05412"><title>4.1. Tracking Precision with 100 Stationary Real Robots</title><p>To assess our system&#8217;s real-world tracking performance, we placed 100 real Kilobots in a known grid pattern. Then we tracked and logged their positions for two minutes. In this experiment, we measured tracking precision and frame rate performance of the system under two processing conditions: (1)&#160;logging only, where positions were recorded to a text file without saving any video; and (2)&#160;logging plus video recording, where both the raw camera input and the processed GUI frames were saved as video files in real time (R6).</p><p>To evaluate tracking precision, we measured the standard deviation of each robot&#8217;s detected position over time while the robots remained stationary. The resulting distribution of position noise was low across all robots, confirming that LARS maintains accurate tracking even under moderate processing loads (see <xref rid="sensors-25-05412-f004" ref-type="fig">Figure 4</xref>a). We also did not observe any spatial correlation for the precision error. Frame rate (captured by FPS) was computed both from software-reported values and from timestamp differences in the log files, with negligible discrepancy between the two, indicating consistent and reliable timing (see <xref rid="sensors-25-05412-f004" ref-type="fig">Figure 4</xref>b). Considering 30&#160;FPS as a baseline for smooth tracking and visualization performance, the performance is higher than this threshold for the log-only case but falls slightly below it under higher processing loads. Next, we characterize performance scalability with respect to the number of tracked objects.</p></sec><sec id="sec4dot2-sensors-25-05412"><title>4.2. System Scalability</title><p>In this experiment, we evaluated how tracking scales with an increasing number of robots (R7). To be able to go beyond the limited number of physical robots in our lab, we rendered projections of up to 399 virtual robots. Each robot was rendered in the scene and tracked using the standard pipeline. The robots were arranged in grid formations of varying dimensions, and we measured the system&#8217;s performance under two processing conditions as before.</p><p>The system maintained stable performance with up to 399 robots, with FPS decreasing gradually as the number of tracked objects increased. Even under heavier processing load (videos recording plus logging), it remained usable in most scenarios, confirming scalability beyond typical lab-scale experiments. It is notable, in the log-only condition, that the system could track 200 robots with a rate of 20&#160;FPS which, to the best of our knowledge, exceeds the scale of current lab experiments with real robots. In the extreme case of 399 virtual robots, the system performed at around 10 FPS representing an improvement compared to the closest related work&#160;[<xref rid="B28-sensors-25-05412" ref-type="bibr">28</xref>].</p></sec><sec id="sec4dot3-sensors-25-05412"><title>4.3. Robustness to Environmental Conditions</title><p>To evaluate the robustness of LARS under varying environmental conditions, we conducted two systematic experiments using 100 real Kilobots arranged on a stationary grid. The goal was to quantify how detection and tracking performance respond to changes in (i) lighting and (ii) spatial visual noise.</p><sec id="sec4dot3dot1-sensors-25-05412"><title>4.3.1. Lighting Conditions</title><p>In the first experiment, we varied the brightness of the projected white background and observed its effect on the detection module. Since precise light measurements (e.g., in lux) were unavailable, we used the value (V) component in the HSV color space as a proxy for brightness. Starting from maximum brightness (V = 255), we gradually reduced the value to zero. At each level, the detection procedure was repeated 50 times using an automated routine implemented in LARS, without involving the tracking component. As shown in <xref rid="sensors-25-05412-f005" ref-type="fig">Figure 5</xref>, LARS maintained perfect detection (100 true positives) above a brightness threshold of approximately 30, which is a dark setting (see the <xref rid="app1-sensors-25-05412" ref-type="app">Supplementary Video S1</xref>&#160;[<xref rid="B53-sensors-25-05412" ref-type="bibr">53</xref>]). Below this range, performance degraded sharply, with increased variance across repetitions. This drop-off highlights the importance of adequate contrast between the robots and the background in enabling reliable marker-free detection.</p></sec><sec id="sec4dot3dot2-sensors-25-05412"><title>4.3.2. Environmental Visual Noise</title><p>In the second experiment, we tested the effect of structured visual noise on detection and tracking performance. We generated a dynamic Gaussian noise field and projected on top of the robot arena, and its spatial scale was systematically adjusted by varying the number of tiles used to discretize the noise field. We started from large tiles (2 &#215; 2) with high spatial correlation to finer resolutions. Here, we evaluated both detection-only and tracking modes.</p><p>As shown in <xref rid="sensors-25-05412-f005" ref-type="fig">Figure 5</xref>, the number of false positives in detection increases significantly at fine noise scales, particularly when the spatial correlation of the background decreases. In contrast, the tracking module maintains a fixed count of 100 robots by construction and shows no false positives, but the positional accuracy degrades under higher visual noise. We quantified this degradation by computing the mean positional error of the tracked robots compared to their positions in noiseless condition. While the error increases at smaller noise scales, the absolute values remain low, confirming that tracking remains stable even under degraded visual conditions. The <xref rid="app1-sensors-25-05412" ref-type="app">Supplementary Video S1</xref> demonstrates this robustness qualitatively&#160;[<xref rid="B53-sensors-25-05412" ref-type="bibr">53</xref>].</p><p>Together, these experiments demonstrate that while the detection pipeline is sensitive to lighting and visual noise, the tracking module is comparatively more robust in both count and spatial accuracy. These findings underscore the value of initializing tracking under clean conditions.</p></sec></sec><sec id="sec4dot4-sensors-25-05412"><title>4.4. Closed-Loop Latency</title><p>In systems like <monospace>LARS</monospace>, where visual tracking is tightly integrated with real-time projection, evaluating closed-loop latency is essential. Unlike conventional tracking-only benchmarks, the end-to-end latency includes delays introduced by image acquisition, processing, decision-making, and rendering to the physical environment. Therefore, we conducted an experiment to characterize the full latency of the system from image acquisition to detection and tracking to projection, as experienced through the projected visual feedback (R3).</p><p>We designed an experiment using a virtual robot moving in a straight-line trajectory. The virtual robot was rendered and updated continuously within the system, moving back and forth across the arena between its boundaries. Its speed was varied systematically across trials. Simultaneously, a yellow ring was projected at the robot&#8217;s tracked position to visualize the delay between the computed location and the projected output. Due to processing and rendering latency, a spatial offset emerged between the robot&#8217;s actual position and the center of the projected ring (see <xref rid="sensors-25-05412-f006" ref-type="fig">Figure 6</xref>a). This offset represented the delay in the closed-loop pipeline and increased with robot speed. To quantify this delay, we manually adjusted the radius of the ring throughout the experiment such that the virtual robot&#8217;s center approximately coincided with the edge of the ring. Although this process introduced some measurement uncertainty, it provided a practical way to estimate the spatial gap between robot and ring under different motion speeds. A plot of the recorded ring radius versus the robot&#8217;s speed shows a clear linear relationship (<xref rid="sensors-25-05412-f006" ref-type="fig">Figure 6</xref>c), with a regression slope of 0.08 s, which we interpret as the effective end-to-end latency of the system.</p><p>As an additional check, we analyzed a video recording of the experiment (see the <xref rid="app1-sensors-25-05412" ref-type="app">Supplementary Video S1</xref>&#160;[<xref rid="B53-sensors-25-05412" ref-type="bibr">53</xref>]) and compared the temporal period of the robot&#8217;s oscillatory motion with that of the projected ring. The time difference between these two signals was consistent with the spatial measurement and yielded a latency estimate in the range of 0.05&#8211;0.06 s.</p><p>It is important to interpret latency in the context of the experiment being performed. For instance, in slow-moving systems such as Kilobots, even relatively high latencies may still be acceptable, as the robots&#8217; low speeds make them more tolerant to visual feedback delays. Furthermore, in scenarios with higher processing demands, such as complex visual overlays, real-time logging, or larger collectives, we expect an increase in latency, similar to what we observed for the reduced FPS. Nonetheless, the latency observed in our tests remains within a practical range for most collective robotics experiments. For example, in a demonstration with nine Thymio robots (<xref rid="sensors-25-05412-f007" ref-type="fig">Figure 7</xref>f), the projected traces remain closely aligned with the robots&#8217; positions, with no visible lag between the robot bodies and the endpoint of their motion paths. This visual coherence provides a practical indication of the system&#8217;s responsiveness and confirms that the latency remains negligible relative to the temporal and spatial resolution required by the experiment.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05412"><title>5. Discussion</title><p>The effectiveness of an interactive system such as <monospace>LARS</monospace> can not be fully evaluated by benchmarks like tracking accuracy, latency, or calibration precision alone. While these metrics are essential for baseline validation, the broader utility of <monospace>LARS</monospace> lies in its ability to enable and support a broader range of experiments in collective robotics. By combining tracking, interaction, and visualization into a closed-loop single spatial Augmented Reality system, <monospace>LARS</monospace> facilitates experimental setups that would otherwise be difficult to implement in a typical laboratory setting. To show how these capabilities manifest in practice, we tested a set of use-case scenarios where LARS was deployed in varied experimental contexts. These scenarios demonstrate how the system meets the design goals of enabling light-based interaction, supporting marker-free tracking across robot platforms, and improving observability of complex collective behaviors. The main features enabling these capabilities are summarized below.</p><list list-type="bullet"><list-item><p><bold>Light-Based Visualization and Interaction: </bold><monospace>LARS</monospace> uses projected light as a medium for indirect communication, enabling stigmergic coordination among robots and improving the observability of the system for human users (R1). It supports a range of static and dynamic visual elements, including programmable fields and noise patterns (<xref rid="sensors-25-05412-f007" ref-type="fig">Figure 7</xref>b), and overlays representing abstract properties such as interaction networks and centroids (<xref rid="sensors-25-05412-f007" ref-type="fig">Figure 7</xref>f).</p></list-item><list-item><p><bold>Marker-Free Cross-Platform Tracking: </bold> The system supports marker-free tracking for a variety of ground robot platforms without requiring hardware modifications (R2). We demonstrated this capability with Kilobots (small), and Thymios (medium), as well as passive moving objects such as colored balls (see <xref rid="sensors-25-05412-f007" ref-type="fig">Figure 7</xref>d and the <xref rid="app1-sensors-25-05412" ref-type="app">Supplementary Video S2</xref>&#160;[<xref rid="B55-sensors-25-05412" ref-type="bibr">55</xref>]). To adapt the detection to a new platform, the user adjusts only a few detection parameters (four sliders) via the GUI, without restarting the software or altering the code. This flexibility simplifies integration and reduces setup time across different experimental platforms.</p></list-item><list-item><p><bold>Responsive Performance: </bold><monospace>LARS</monospace> maintains high frame rates and low latency, crucial for perceptually smooth performance for dynamic and responsive interactions in AR applications (R3). Our system on a desktop PC (housing a 16-core CPU, and 32&#160;GB RAM) achieves a frame rate of around 38&#160;frames per second (see <xref rid="sensors-25-05412-f004" ref-type="fig">Figure 4</xref>c for more details). This number exceeds the requirement for most laboratory scenarios involving ground mobile robots. The low-latency performance is evident from the projected traces closely following the robots&#8217; movements without visible lag or mismatch (see <xref rid="sensors-25-05412-f007" ref-type="fig">Figure 7</xref>f, and the <xref rid="app1-sensors-25-05412" ref-type="app">Supplementary Video S3</xref>&#160;[<xref rid="B56-sensors-25-05412" ref-type="bibr">56</xref>]).</p></list-item><list-item><p><bold>Scalability to Robot Numbers: </bold><monospace>LARS</monospace> supports experiments at scale, with tested capacity reaching up to 109&#160;real Kilobots (<xref rid="sensors-25-05412-f007" ref-type="fig">Figure 7</xref>c), and 399 (<xref rid="sensors-25-05412-f004" ref-type="fig">Figure 4</xref>c) simultaneously tracked and rendered virtual robots (R7). Additionally, it recorded a video stream on local storage and logged the position of each robot in a text file without a significant drop in the performance. The actual frame rate varied depending on concurrent processes, particularly file-writing operations, as explained in the previous section (see <xref rid="sensors-25-05412-f004" ref-type="fig">Figure 4</xref>b). This demonstrates the system&#8217;s ability to satisfy the requirement for lab experiment hardware with various collective sizes.</p></list-item><list-item><p><bold>Integrated Logging and Capturing System: </bold> Besides all the online processes during an experiment, logging the data and recording the video frames are necessary features to enable post-processing and analysis of the experiment offline (R6). We implemented a logging system that records the state of robots, including their position, velocity, color, and IDs for every frame. In addition, two threads run to record the unprocessed frame data from the camera, and the processed frame data after tracking. The processed frame is similar to what is displayed to the user on the GUI.</p></list-item><list-item><p><bold>Ease of Setup and Robustness: </bold> The self-calibration feature of the system enabled us to easily adjust the size of the arena from Thymios (<inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3.7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m<sup>2</sup>, in <xref rid="sensors-25-05412-f007" ref-type="fig">Figure 7</xref>f) to Kilobots (<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m<sup>2</sup>, e.g., in <xref rid="sensors-25-05412-f007" ref-type="fig">Figure 7</xref>b) by simply modifying four inputs (R2). This user-friendly setup process eliminates complex configuration steps. The system&#8217;s ease of setup is combined with its robustness, which has been validated through continuous operation under demanding conditions during public demonstrations and various events, where <monospace>LARS</monospace> was used to showcase collective robotic scenarios.</p></list-item><list-item><p><bold>Controllable Environment Parameters: </bold> Users have complete control over the XR environment, either prior to the execution, for example by modifying the physics of interaction codes, or during its runtime by modifying the properties of the environment via the GUI (R5).</p></list-item><list-item><p><bold>Cost-Effectiveness: </bold> One of the main goals of releasing <monospace>LARS</monospace> is to provide accessibility to research tools by creating an affordable system that facilitates the use of XR technologies in developing laboratories, non-commercial users in academia, and educational institutions.</p></list-item></list><sec><title>Current Limitations and Future Directions</title><p>In the development of the system, our objective was to satisfy the requirements that apply to most general collective robotic experiments. While we have pushed the limits of what <monospace>LARS</monospace> can do, recognizing its constraints will clarify the necessary next steps for further development of the system. We list several <monospace>LARS</monospace> features that may require further development and customization to meet specific individual requirements of different users in the future.</p><list list-type="bullet"><list-item><p>Explicit orientation detection: We use circle detection for the localization of robots in the environment. While an implicit orientation is calculated based on the direction of the velocity vector of robots, it is necessary to explicitly track the orientation of robots in some experiments.</p></list-item><list-item><p>Reading tracking data from external trackers: Despite being designed for affordability, users might prefer to integrate marker-based tracker systems, like ViCon&#160;[<xref rid="B57-sensors-25-05412" ref-type="bibr">57</xref>] or OptiTrack&#160;[<xref rid="B58-sensors-25-05412" ref-type="bibr">58</xref>]. Such advanced tracking systems offer precise and high-frequency tracking data, including orientation and position in the three-dimensional space. Integrating this data into <monospace>LARS</monospace> will improve the performance of the tracking component.</p></list-item><list-item><p>Interact with external programs for possible extensions: <monospace>LARS</monospace> is designed as a modular, standalone software that contains all the necessary functionalities. Nevertheless, incorporating the use of other programs will enhance its adaptability to meet more sophisticated requirements. In our evaluation experiments, we implemented an offline version of this approach by calling a Python (v3.9, 3.x series) function to create images or animations of environments containing robots with specific behaviors or parameters. An online approach, where the external program receives the state of the environment and generates the corresponding visual elements in real time for projection, remains a potential future extension.</p></list-item><list-item><p>Offloading heavy environment generation to external programs: Stress testing revealed that extremely high-frequency, high-resolution environment generation (e.g., dense noise patterns) can slow down the system. In such special cases, offloading the rendering process to an external program and importing the resulting environment file into <monospace>LARS</monospace> could reduce computational load and maintain real-time performance.</p></list-item><list-item><p>Flexibility to varying number of robots during an experiment: The current tracking algorithm assumes a fixed number of robots to track during the course of an experiment. However, some experiments may require adaptive tracking as the population size changes over time, although this introduces a trade-off in tracking accuracy. One potential solution is to define an upper limit on the number of robots and allow newly detected robots to be added dynamically if they pass the filtering process.</p></list-item><list-item><p>More complex environments: Rendering more realistic virtual scenes is a primary goal in the development of AR for humans. Gamifying Mixed Reality with more complex and realistic objects will assist human user engagement, particularly for instructional applications.</p></list-item><list-item><p>Extensive end-to-end benchmarking: While our evaluation includes scalability, latency, and robustness, a full benchmarking campaign would require assessing the system under a wider range of conditions, including moving real and virtual robots, occlusion scenarios, and long-term ID consistency. Compared to tracking-only evaluations, closed-loop SAR systems require well-defined end-to-end tests that are currently lacking in collective robotics research, and developing such benchmarks would advance both reproducibility and cross-platform comparability.</p></list-item></list></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05412"><title>6. Conclusions</title><p><monospace>LARS</monospace> is a testbed system designed to support multi-robot experimentation by combining tracking, logging, and visualization in a single, integrated platform. More importantly, it serves as a medium through which the environment is augmented with virtual objects, enabling new forms of interaction without altering the physical limitations of the robots. By projecting arbitrary visual elements into the arena, <monospace>LARS</monospace> enriches experimental conditions and expands the range of behaviors that can be studied. The augmented environment is a step to bridge the gap between simulation and reality.</p><p>Our benchmark evaluation demonstrates that LARS can scale to large numbers of robots, maintain tracking precision and responsiveness under different processing loads, and operate reliably under varying lighting and environmental conditions. Although not originally designed as a benchmarking tool, we showed that the system can be used to generate systematic performance data. We further illustrated the practical value of LARS through a range of example scenarios, involving both educational demonstrations and collective behavior experiments with Kilobots, Thymios, and passive objects. These examples highlight how LARS supports the design and communication of complex collective systems through light-based interaction and visual feedback. The videos of several demonstrations are available as <xref rid="app1-sensors-25-05412" ref-type="app">Supplementary Material</xref> online (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://figshare.com/collections/LARS_videos/7995766/2">https://figshare.com/collections/LARS_videos/7995766/2</uri>, accessed on 22 August 2025).</p><p>The smooth projection of virtual observable elements alongside physical robots shows a powerful means to make collective robotics more accessible and interpretable. AR systems like <monospace>LARS</monospace> can be used to showcase complex collective behaviors in an intuitive way, making them particularly suitable for educational activities and public demonstrations. By expanding the range of possible experimental conditions without altering the physical limitations of robots, such tools provide a flexible interface for researchers to study these multi-robot system in controlled, repeatable, and visually enriched environments. As an open-source software (licensed under the GNU General Public License v3.0), <monospace>LARS</monospace> is available on this&#160;repository&#160;[<xref rid="B51-sensors-25-05412" ref-type="bibr">51</xref>].</p></sec></body><back><ack><title>Acknowledgments</title><p>For the preparation of this manuscript, the authors used ChatGPT (OpenAI, GPT-4, accessed April 2025) to assist with grammar checking and language polishing. The authors have reviewed and edited all output and take full responsibility for the content of this publication.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-sensors-25-05412"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mdpi.com/article/10.3390/s25175412/s1">https://www.mdpi.com/article/10.3390/s25175412/s1</uri>: Video S1: Benchmarking tests; Video S2: Platform Adaptability Test; Video S3: Introducing LARS with Example Behaviors.</p><supplementary-material id="sensors-25-05412-s001" position="float" content-type="local-data" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sensors-25-05412-s001.zip" position="float" orientation="portrait"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceptualization, M.R.; methodology, M.R.; software, M.R.; validation, M.R.; resources, P.R.; data curation, M.R.; writing&#8212;original draft preparation, M.R.; writing&#8212;review and editing, M.R., P.R. and H.H.; visualization, M.R.; supervision, H.H.; project administration, M.R. and H.H.; funding acquisition, P.R. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original data presented in the study are openly available in DepositOnce at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://depositonce.tu-berlin.de/handle/11303/25207">https://depositonce.tu-berlin.de/handle/11303/25207</uri>, accessed on 22 August 2025. The updated version of the software can be accessed online via the GitHub repository at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/mohsen-raoufi/LARS">https://github.com/mohsen-raoufi/LARS</uri>, accessed on 22 August 2025.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">AR</td><td align="left" valign="middle" rowspan="1" colspan="1">Augmented Reality</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ARK</td><td align="left" valign="middle" rowspan="1" colspan="1">Augmented Reality for Kilobots</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AV</td><td align="left" valign="middle" rowspan="1" colspan="1">Augmented Virtuality</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Graphics Processing Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GUI</td><td align="left" valign="middle" rowspan="1" colspan="1">Graphical User Interface</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HRI</td><td align="left" valign="middle" rowspan="1" colspan="1">Human&#8211;Robot Interaction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HSI</td><td align="left" valign="middle" rowspan="1" colspan="1">Human&#8211;Swarm Interaction</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IR</td><td align="left" valign="middle" rowspan="1" colspan="1">Infrared</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LARS</td><td align="left" valign="middle" rowspan="1" colspan="1">Light-Augmented Reality System</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MR</td><td align="left" valign="middle" rowspan="1" colspan="1">Mixed Reality</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MVC</td><td align="left" valign="middle" rowspan="1" colspan="1">Model&#8211;View&#8211;Controller</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OHC</td><td align="left" valign="middle" rowspan="1" colspan="1">Overhead Controller</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WM</td><td align="left" valign="middle" rowspan="1" colspan="1">World Model</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">XR</td><td align="left" valign="middle" rowspan="1" colspan="1">Extended Reality</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05412"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Billinghurst</surname><given-names>M.</given-names></name><name name-style="western"><surname>Clark</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>G.</given-names></name></person-group><article-title>A survey of augmented reality</article-title><source>Found. Trends<sup>&#174;</sup> Hum.&#8211;Comput. Interact.</source><year>2015</year><volume>8</volume><fpage>73</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1561/1100000049</pub-id></element-citation></ref><ref id="B2-sensors-25-05412"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>&#199;&#246;ltekin</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lochhead</surname><given-names>I.</given-names></name><name name-style="western"><surname>Madden</surname><given-names>M.</given-names></name><name name-style="western"><surname>Christophe</surname><given-names>S.</given-names></name><name name-style="western"><surname>Devaux</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pettit</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lock</surname><given-names>O.</given-names></name><name name-style="western"><surname>Shukla</surname><given-names>S.</given-names></name><name name-style="western"><surname>Herman</surname><given-names>L.</given-names></name><name name-style="western"><surname>Stacho&#328;</surname><given-names>Z.</given-names></name><etal/></person-group><article-title>Extended reality in spatial sciences: A review of research challenges and future directions</article-title><source>ISPRS Int. J. Geo-Inf.</source><year>2020</year><volume>9</volume><elocation-id>439</elocation-id><pub-id pub-id-type="doi">10.3390/ijgi9070439</pub-id></element-citation></ref><ref id="B3-sensors-25-05412"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vasarainen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Paavola</surname><given-names>S.</given-names></name><name name-style="western"><surname>Vetoshkina</surname><given-names>L.</given-names></name></person-group><article-title>A systematic literature review on extended reality: Virtual, augmented and mixed reality in working life</article-title><source>Int. J. Virtual Real.</source><year>2021</year><volume>21</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.20870/IJVR.2021.21.2.4620</pub-id></element-citation></ref><ref id="B4-sensors-25-05412"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>C&#225;rdenas-Robledo</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Hern&#225;ndez-Uribe</surname><given-names>&#211;.</given-names></name><name name-style="western"><surname>Reta</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cantoral-Ceballos</surname><given-names>J.A.</given-names></name></person-group><article-title>Extended reality applications in industry 4.0.&#8211;A systematic literature review</article-title><source>Telemat. Inform.</source><year>2022</year><volume>73</volume><fpage>101863</fpage><pub-id pub-id-type="doi">10.1016/j.tele.2022.101863</pub-id></element-citation></ref><ref id="B5-sensors-25-05412"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Makhataeva</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Varol</surname><given-names>H.A.</given-names></name></person-group><article-title>Augmented reality for robotics: A review</article-title><source>Robotics</source><year>2020</year><volume>9</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.3390/robotics9020021</pub-id></element-citation></ref><ref id="B6-sensors-25-05412"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>Y.P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.Q.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>C.</given-names></name><name name-style="western"><surname>Pearson</surname><given-names>L.H.</given-names></name><name name-style="western"><surname>Pretty</surname><given-names>C.G.</given-names></name><name name-style="western"><surname>Chase</surname><given-names>J.G.</given-names></name></person-group><article-title>Integrating Virtual, Mixed, and Augmented Reality into Remote Robotic Applications: A Brief Review of Extended Reality-Enhanced Robotic Systems for Intuitive Telemanipulation and Telemanufacturing Tasks in Hazardous Conditions</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>12129</elocation-id><pub-id pub-id-type="doi">10.3390/app132212129</pub-id></element-citation></ref><ref id="B7-sensors-25-05412"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Suzuki</surname><given-names>R.</given-names></name><name name-style="western"><surname>Karim</surname><given-names>A.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hedayati</surname><given-names>H.</given-names></name><name name-style="western"><surname>Marquardt</surname><given-names>N.</given-names></name></person-group><article-title>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</article-title><source>Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>30 April&#8211;5 May 2022</conf-date><fpage>1</fpage><lpage>32</lpage></element-citation></ref><ref id="B8-sensors-25-05412"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Walter</surname><given-names>T.</given-names></name><name name-style="western"><surname>Couzin</surname><given-names>I.D.</given-names></name></person-group><article-title>TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields</article-title><source>Elife</source><year>2021</year><volume>10</volume><fpage>e64000</fpage><pub-id pub-id-type="doi">10.7554/eLife.64000</pub-id><pub-id pub-id-type="pmid">33634789</pub-id><pub-id pub-id-type="pmcid">PMC8096434</pub-id></element-citation></ref><ref id="B9-sensors-25-05412"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Plum</surname><given-names>F.</given-names></name></person-group><article-title>OmniTrax: A deep learning-driven multi-animal tracking and pose-estimation add-on for Blender</article-title><source>J. Open Source Softw.</source><year>2024</year><volume>9</volume><fpage>5549</fpage><pub-id pub-id-type="doi">10.21105/joss.05549</pub-id></element-citation></ref><ref id="B10-sensors-25-05412"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rubenstein</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ahler</surname><given-names>C.</given-names></name><name name-style="western"><surname>Nagpal</surname><given-names>R.</given-names></name></person-group><article-title>Kilobot: A low cost scalable robot system for collective behaviors</article-title><source>Proceedings of the 2012 IEEE International Conference on Robotics and Automation</source><conf-loc>St. Paul, MN, USA</conf-loc><conf-date>14&#8211;18 May 2012</conf-date><fpage>3293</fpage><lpage>3298</lpage></element-citation></ref><ref id="B11-sensors-25-05412"><label>11.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Riedo</surname><given-names>F.</given-names></name></person-group><article-title>Thymio: A Holistic Approach to Designing Accessible Educational Robots</article-title><source>Ph.D. Thesis</source><publisher-name>&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL)</publisher-name><publisher-loc>Lausanne, Switzerland</publisher-loc><year>2015</year></element-citation></ref><ref id="B12-sensors-25-05412"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Romanczuk</surname><given-names>P.</given-names></name><name name-style="western"><surname>Hamann</surname><given-names>H.</given-names></name></person-group><article-title>Estimation of continuous environments by robot swarms: Correlated networks and decision-making</article-title><source>Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>London, UK</conf-loc><conf-date>29 May&#8211;2 June 2023</conf-date><fpage>5486</fpage><lpage>5492</lpage></element-citation></ref><ref id="B13-sensors-25-05412"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Romanczuk</surname><given-names>P.</given-names></name><name name-style="western"><surname>Hamann</surname><given-names>H.</given-names></name></person-group><article-title>Individuality in Swarm Robots with the Case Study of Kilobots: Noise, Bug, or Feature?</article-title><source>Proceedings of the ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference</source><conf-loc>Sapporo, Japan</conf-loc><conf-date>24&#8211;28 July 2023</conf-date><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2023</year></element-citation></ref><ref id="B14-sensors-25-05412"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name></person-group><source>LARS: Light Augmented Reality System for Collective Robotics Interaction</source><publisher-name>Technische Universit&#228;t Berlin</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2025</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://depositonce.tu-berlin.de/handle/11303/25207" ext-link-type="uri">https://depositonce.tu-berlin.de/handle/11303/25207</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-22">(accessed on 22 August 2025)</date-in-citation></element-citation></ref><ref id="B15-sensors-25-05412"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>T.C.</given-names></name><name name-style="western"><surname>Tseng</surname><given-names>H.P.</given-names></name></person-group><article-title>Extended reality in applied sciences education: A systematic review</article-title><source>Appl. Sci.</source><year>2025</year><volume>15</volume><elocation-id>4038</elocation-id><pub-id pub-id-type="doi">10.3390/app15074038</pub-id></element-citation></ref><ref id="B16-sensors-25-05412"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Milgram</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kishino</surname><given-names>F.</given-names></name></person-group><article-title>A taxonomy of mixed reality visual displays</article-title><source>IEICE Trans. Inf. Syst.</source><year>1994</year><volume>77</volume><fpage>1321</fpage><lpage>1329</lpage></element-citation></ref><ref id="B17-sensors-25-05412"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Azuma</surname><given-names>R.T.</given-names></name></person-group><article-title>A survey of augmented reality</article-title><source>Presence Teleoper. Virtual Environ.</source><year>1997</year><volume>6</volume><fpage>355</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1162/pres.1997.6.4.355</pub-id></element-citation></ref><ref id="B18-sensors-25-05412"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.K.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.W.Y.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>H.Y.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>J.C.</given-names></name></person-group><article-title>Current status, opportunities and challenges of augmented reality in education</article-title><source>Comput. Educ.</source><year>2013</year><volume>62</volume><fpage>41</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.compedu.2012.10.024</pub-id></element-citation></ref><ref id="B19-sensors-25-05412"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hoenig</surname><given-names>W.</given-names></name><name name-style="western"><surname>Milanes</surname><given-names>C.</given-names></name><name name-style="western"><surname>Scaria</surname><given-names>L.</given-names></name><name name-style="western"><surname>Phan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bolas</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ayanian</surname><given-names>N.</given-names></name></person-group><article-title>Mixed reality for robotics</article-title><source>Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Hamburg, Germany</conf-loc><conf-date>28 September&#8211;2 October 2015</conf-date><fpage>5382</fpage><lpage>5387</lpage></element-citation></ref><ref id="B20-sensors-25-05412"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Schroepfer</surname><given-names>P.</given-names></name><name name-style="western"><surname>Gr&#252;ndling</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Schauffel</surname><given-names>N.</given-names></name><name name-style="western"><surname>Oehrl</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pape</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kuhlen</surname><given-names>T.W.</given-names></name><name name-style="western"><surname>Weyers</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ellwart</surname><given-names>T.</given-names></name><name name-style="western"><surname>Pradalier</surname><given-names>C.</given-names></name></person-group><article-title>Navigating Real-World Complexity: A Multi-Medium System for Heterogeneous Robot Teams and Multi-Stakeholder Human-Robot Interaction</article-title><source>Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</source><conf-loc>Boulder, CO, USA</conf-loc><conf-date>11&#8211;15 March 2024</conf-date><fpage>630</fpage><lpage>638</lpage></element-citation></ref><ref id="B21-sensors-25-05412"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tooher</surname><given-names>K.</given-names></name><name name-style="western"><surname>Bingham</surname><given-names>L.</given-names></name><name name-style="western"><surname>Garcia</surname><given-names>A.D.</given-names></name></person-group><article-title>Immersive Technologies for Human-in-the-Loop Lunar Surface Simulations</article-title><source>Proceedings of the 45th International IEEE Aerospace Conference</source><conf-loc>Big Sky, MT, USA</conf-loc><conf-date>2&#8211;9 March 2024</conf-date></element-citation></ref><ref id="B22-sensors-25-05412"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Raskar</surname><given-names>R.</given-names></name><name name-style="western"><surname>Welch</surname><given-names>G.</given-names></name><name name-style="western"><surname>Cutts</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lake</surname><given-names>A.</given-names></name><name name-style="western"><surname>Stesin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Fuchs</surname><given-names>H.</given-names></name></person-group><article-title>The office of the future: A unified approach to image-based modeling and spatially immersive displays</article-title><source>Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>19&#8211;24 July 1998</conf-date><fpage>179</fpage><lpage>188</lpage></element-citation></ref><ref id="B23-sensors-25-05412"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Raskar</surname><given-names>R.</given-names></name><name name-style="western"><surname>Welch</surname><given-names>G.</given-names></name><name name-style="western"><surname>Low</surname><given-names>K.L.</given-names></name><name name-style="western"><surname>Bandyopadhyay</surname><given-names>D.</given-names></name></person-group><article-title>Shader lamps: Animating real objects with image-based illumination</article-title><source>Proceedings of the Eurographics Workshop on Rendering Techniques</source><conf-loc>London, UK</conf-loc><conf-date>25&#8211;27 June 2001</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2001</year><fpage>89</fpage><lpage>102</lpage></element-citation></ref><ref id="B24-sensors-25-05412"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bimber</surname><given-names>O.</given-names></name><name name-style="western"><surname>Raskar</surname><given-names>R.</given-names></name></person-group><source>Spatial Augmented Reality: Merging Real and Virtual Worlds</source><publisher-name>CRC Press</publisher-name><publisher-loc>Boca Raton, FL, USA</publisher-loc><year>2005</year></element-citation></ref><ref id="B25-sensors-25-05412"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nee</surname><given-names>A.</given-names></name></person-group><article-title>Projection-based augmented reality interface for robot grasping tasks</article-title><source>Proceedings of the 2019 4th International Conference on Robotics, Control and Automation</source><conf-loc>Shenzhen, China</conf-loc><conf-date>19&#8211;21 July 2019</conf-date><fpage>100</fpage><lpage>104</lpage></element-citation></ref><ref id="B26-sensors-25-05412"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garnier</surname><given-names>S.</given-names></name><name name-style="western"><surname>Combe</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jost</surname><given-names>C.</given-names></name><name name-style="western"><surname>Theraulaz</surname><given-names>G.</given-names></name></person-group><article-title>Do ants need to estimate the geometrical properties of trail bifurcations to find an efficient route? A swarm robotics test bed</article-title><source>PLoS Comput. Biol.</source><year>2013</year><volume>9</volume><elocation-id>e1002903</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002903</pub-id><pub-id pub-id-type="pmid">23555202</pub-id><pub-id pub-id-type="pmcid">PMC3610605</pub-id></element-citation></ref><ref id="B27-sensors-25-05412"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Valentini</surname><given-names>G.</given-names></name><name name-style="western"><surname>Antoun</surname><given-names>A.</given-names></name><name name-style="western"><surname>Trabattoni</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wiandt</surname><given-names>B.</given-names></name><name name-style="western"><surname>Tamura</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hocquard</surname><given-names>E.</given-names></name><name name-style="western"><surname>Trianni</surname><given-names>V.</given-names></name><name name-style="western"><surname>Dorigo</surname><given-names>M.</given-names></name></person-group><article-title>Kilogrid: A novel experimental environment for the Kilobot robot</article-title><source>Swarm Intell.</source><year>2018</year><volume>12</volume><fpage>245</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1007/s11721-018-0155-z</pub-id></element-citation></ref><ref id="B28-sensors-25-05412"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reina</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cope</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Nikolaidis</surname><given-names>E.</given-names></name><name name-style="western"><surname>Marshall</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Sabo</surname><given-names>C.</given-names></name></person-group><article-title>ARK: Augmented reality for Kilobots</article-title><source>IEEE Robot. Autom. Lett.</source><year>2017</year><volume>2</volume><fpage>1755</fpage><lpage>1761</lpage><pub-id pub-id-type="doi">10.1109/LRA.2017.2700059</pub-id></element-citation></ref><ref id="B29-sensors-25-05412"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feola</surname><given-names>L.</given-names></name><name name-style="western"><surname>Reina</surname><given-names>A.</given-names></name><name name-style="western"><surname>Talamali</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Trianni</surname><given-names>V.</given-names></name></person-group><article-title>Multi-swarm interaction through augmented reality for Kilobots</article-title><source>IEEE Robot. Autom. Lett.</source><year>2023</year><volume>8</volume><fpage>6907</fpage><lpage>6914</lpage><pub-id pub-id-type="doi">10.1109/LRA.2023.3312031</pub-id></element-citation></ref><ref id="B30-sensors-25-05412"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Millard</surname><given-names>A.G.</given-names></name><name name-style="western"><surname>Redpath</surname><given-names>R.</given-names></name><name name-style="western"><surname>Jewers</surname><given-names>A.M.</given-names></name><name name-style="western"><surname>Arndt</surname><given-names>C.</given-names></name><name name-style="western"><surname>Joyce</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hilder</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>McDaid</surname><given-names>L.J.</given-names></name><name name-style="western"><surname>Halliday</surname><given-names>D.M.</given-names></name></person-group><article-title>ARDebug: An augmented reality tool for analysing and debugging swarm robotic systems</article-title><source>Front. Robot. AI</source><year>2018</year><volume>5</volume><elocation-id>87</elocation-id><pub-id pub-id-type="doi">10.3389/frobt.2018.00087</pub-id><pub-id pub-id-type="pmid">33500966</pub-id><pub-id pub-id-type="pmcid">PMC7805932</pub-id></element-citation></ref><ref id="B31-sensors-25-05412"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ghiringhelli</surname><given-names>F.</given-names></name><name name-style="western"><surname>Guzzi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Di Caro</surname><given-names>G.A.</given-names></name><name name-style="western"><surname>Caglioti</surname><given-names>V.</given-names></name><name name-style="western"><surname>Gambardella</surname><given-names>L.M.</given-names></name><name name-style="western"><surname>Giusti</surname><given-names>A.</given-names></name></person-group><article-title>Interactive augmented reality for understanding and analyzing multi-robot systems</article-title><source>Proceedings of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>Chicago, IL, USA</conf-loc><conf-date>14&#8211;18 September 2014</conf-date><fpage>1195</fpage><lpage>1201</lpage></element-citation></ref><ref id="B32-sensors-25-05412"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>K&#228;stner</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lambrecht</surname><given-names>J.</given-names></name></person-group><article-title>Augmented-reality-based visualization of navigation data of mobile robots on the microsoft hololens-possibilities and limitations</article-title><source>Proceedings of the 2019 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)</source><conf-loc>Bangkok, Thailand</conf-loc><conf-date>18&#8211;20 November 2019</conf-date><fpage>344</fpage><lpage>349</lpage></element-citation></ref><ref id="B33-sensors-25-05412"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Batra</surname><given-names>S.</given-names></name><name name-style="western"><surname>Klingner</surname><given-names>J.</given-names></name><name name-style="western"><surname>Correll</surname><given-names>N.</given-names></name></person-group><article-title>Augmented reality for human&#8211;swarm interaction in a swarm-robotic chemistry simulation</article-title><source>Artif. Life Robot.</source><year>2022</year><volume>27</volume><fpage>407</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1007/s10015-022-00763-w</pub-id></element-citation></ref><ref id="B34-sensors-25-05412"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>A multichannel human-swarm robot interaction system in augmented reality</article-title><source>Virtual Real. Intell. Hardw.</source><year>2020</year><volume>2</volume><fpage>518</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1016/j.vrih.2020.05.006</pub-id></element-citation></ref><ref id="B35-sensors-25-05412"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sugawara</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kazama</surname><given-names>T.</given-names></name><name name-style="western"><surname>Watanabe</surname><given-names>T.</given-names></name></person-group><article-title>Foraging behavior of interacting robots with virtual pheromone</article-title><source>Proceedings of the 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566)</source><conf-loc>Sendai, Japan</conf-loc><conf-date>28 September&#8211;2 October 2004</conf-date><volume>Volume 3</volume><fpage>3074</fpage><lpage>3079</lpage></element-citation></ref><ref id="B36-sensors-25-05412"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Arvin</surname><given-names>F.</given-names></name><name name-style="western"><surname>Krajn&#237;k</surname><given-names>T.</given-names></name><name name-style="western"><surname>Turgut</surname><given-names>A.E.</given-names></name><name name-style="western"><surname>Yue</surname><given-names>S.</given-names></name></person-group><article-title>COS<italic toggle="yes">&#934;</italic>: Artificial pheromone system for robotic swarms research</article-title><source>Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Hamburg, Germany</conf-loc><conf-date>28 September&#8211;2 October 2015</conf-date><fpage>407</fpage><lpage>412</lpage></element-citation></ref><ref id="B37-sensors-25-05412"><label>37.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Na</surname><given-names>S.</given-names></name><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Turgut</surname><given-names>A.E.</given-names></name><name name-style="western"><surname>Krajn&#237;k</surname><given-names>T.</given-names></name><name name-style="western"><surname>Arvin</surname><given-names>F.</given-names></name></person-group><article-title>Extended artificial pheromone system for swarm robotic applications</article-title><source>Proceedings of the Artificial Life Conference</source><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2019</year><fpage>608</fpage><lpage>615</lpage></element-citation></ref><ref id="B38-sensors-25-05412"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Antoun</surname><given-names>A.</given-names></name><name name-style="western"><surname>Valentini</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hocquard</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wiandt</surname><given-names>B.</given-names></name><name name-style="western"><surname>Trianni</surname><given-names>V.</given-names></name><name name-style="western"><surname>Dorigo</surname><given-names>M.</given-names></name></person-group><article-title>Kilogrid: A modular virtualization environment for the Kilobot robot</article-title><source>Proceedings of the 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Daejeon, Republic of Korea</conf-loc><conf-date>9&#8211;14 October 2016</conf-date><fpage>3809</fpage><lpage>3814</lpage></element-citation></ref><ref id="B39-sensors-25-05412"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fiala</surname><given-names>M.</given-names></name></person-group><article-title>Artag, an improved marker system based on artoolkit</article-title><source>Natl. Res. Counc. Can. Publ. Number NRC</source><year>2004</year><volume>47419</volume><fpage>2004</fpage></element-citation></ref><ref id="B40-sensors-25-05412"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Field</surname><given-names>M.</given-names></name><name name-style="western"><surname>Stirling</surname><given-names>D.</given-names></name><name name-style="western"><surname>Naghdy</surname><given-names>F.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>Z.</given-names></name></person-group><article-title>Motion capture in robotics review</article-title><source>Proceedings of the 2009 IEEE International Conference on Control and Automation</source><conf-loc>Christchurch, New Zealand</conf-loc><conf-date>9&#8211;11 December 2009</conf-date><fpage>1697</fpage><lpage>1702</lpage></element-citation></ref><ref id="B41-sensors-25-05412"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krajn&#237;k</surname><given-names>T.</given-names></name><name name-style="western"><surname>Nitsche</surname><given-names>M.</given-names></name><name name-style="western"><surname>Faigl</surname><given-names>J.</given-names></name><name name-style="western"><surname>Van&#283;k</surname><given-names>P.</given-names></name><name name-style="western"><surname>Saska</surname><given-names>M.</given-names></name><name name-style="western"><surname>P&#345;eu&#269;il</surname><given-names>L.</given-names></name><name name-style="western"><surname>Duckett</surname><given-names>T.</given-names></name><name name-style="western"><surname>Mejail</surname><given-names>M.</given-names></name></person-group><article-title>A practical multirobot localization system</article-title><source>J. Intell. Robot. Syst.</source><year>2014</year><volume>76</volume><fpage>539</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1007/s10846-014-0041-x</pub-id></element-citation></ref><ref id="B42-sensors-25-05412"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Reina</surname><given-names>A.</given-names></name><name name-style="western"><surname>Salvaro</surname><given-names>M.</given-names></name><name name-style="western"><surname>Francesca</surname><given-names>G.</given-names></name><name name-style="western"><surname>Garattoni</surname><given-names>L.</given-names></name><name name-style="western"><surname>Pinciroli</surname><given-names>C.</given-names></name><name name-style="western"><surname>Dorigo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Birattari</surname><given-names>M.</given-names></name></person-group><article-title>Augmented reality for robots: Virtual sensing technology applied to a swarm of e-pucks</article-title><source>Proceedings of the 2015 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>15&#8211;18 June 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2015</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B43-sensors-25-05412"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mathis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mamidanna</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cury</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Abe</surname><given-names>T.</given-names></name><name name-style="western"><surname>Murthy</surname><given-names>V.N.</given-names></name><name name-style="western"><surname>Mathis</surname><given-names>M.W.</given-names></name><name name-style="western"><surname>Bethge</surname><given-names>M.</given-names></name></person-group><article-title>DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nat. Neurosci.</source><year>2018</year><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="B44-sensors-25-05412"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krasner</surname><given-names>G.E.</given-names></name><name name-style="western"><surname>Pope</surname><given-names>S.T.</given-names></name></person-group><article-title>A description of the model-view-controller user interface paradigm in the smalltalk-80 system</article-title><source>J. Object Oriented Program.</source><year>1988</year><volume>1</volume><fpage>26</fpage><lpage>49</lpage></element-citation></ref><ref id="B45-sensors-25-05412"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>MacWilliams</surname><given-names>A.</given-names></name><name name-style="western"><surname>Reicher</surname><given-names>T.</given-names></name><name name-style="western"><surname>Klinker</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bruegge</surname><given-names>B.</given-names></name></person-group><article-title>Design patterns for augmented reality systems</article-title><source>Proceedings of the International Workshop exploring the Design and Engineering of Mixed Reality Systems (MIXER)</source><conf-loc>Funchal, Portugal</conf-loc><conf-date>13 January 2004</conf-date><comment>CEUR Workshop Proceedings</comment></element-citation></ref><ref id="B46-sensors-25-05412"><label>46.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>The Qt Company</collab></person-group><source>Qt Application Framework</source><comment>Version 6.0</comment><publisher-name>The Qt Company</publisher-name><publisher-loc>Espoo, Finland</publisher-loc><year>2020</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.qt.io/" ext-link-type="uri">https://www.qt.io/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-22">(accessed on 22 August 2025)</date-in-citation></element-citation></ref><ref id="B47-sensors-25-05412"><label>47.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Bradski</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kaehler</surname><given-names>A.</given-names></name></person-group><article-title>OpenCV Library Version 4.5.5</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://opencv.org/" ext-link-type="uri">https://opencv.org/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-05">(accessed on 5 April 2024)</date-in-citation></element-citation></ref><ref id="B48-sensors-25-05412"><label>48.</label><element-citation publication-type="patent"><person-group person-group-type="author"><name name-style="western"><surname>Hough</surname><given-names>P.V.</given-names></name></person-group><article-title>Method and Means for Recognizing Complex Patterns</article-title><source>U.S. Patent</source><patent>3,069,654</patent><day>18</day><month>December</month><year>1962</year></element-citation></ref><ref id="B49-sensors-25-05412"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Duda</surname><given-names>R.O.</given-names></name><name name-style="western"><surname>Hart</surname><given-names>P.E.</given-names></name></person-group><article-title>Use of the Hough transformation to detect lines and curves in pictures</article-title><source>Commun. ACM</source><year>1972</year><volume>15</volume><fpage>11</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1145/361237.361242</pub-id></element-citation></ref><ref id="B50-sensors-25-05412"><label>50.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>NVIDIA Corporation</collab></person-group><article-title>CUDA Toolkit</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://developer.nvidia.com/cuda-toolkit" ext-link-type="uri">https://developer.nvidia.com/cuda-toolkit</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-05">(accessed on 5 April 2024)</date-in-citation></element-citation></ref><ref id="B51-sensors-25-05412"><label>51.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name></person-group><article-title>LARS: Light Augmented Reality System for Interaction in Multi-Robot Scenarios</article-title><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/mohsen-raoufi/LARS" ext-link-type="uri">https://github.com/mohsen-raoufi/LARS</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-05">(accessed on 5 April 2024)</date-in-citation></element-citation></ref><ref id="B52-sensors-25-05412"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garrido-Jurado</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mu&#241;oz-Salinas</surname><given-names>R.</given-names></name><name name-style="western"><surname>Madrid-Cuevas</surname><given-names>F.J.</given-names></name><name name-style="western"><surname>Mar&#237;n-Jim&#233;nez</surname><given-names>M.J.</given-names></name></person-group><article-title>Automatic generation and detection of highly reliable fiducial markers under occlusion</article-title><source>Pattern Recognit.</source><year>2014</year><volume>47</volume><fpage>2280</fpage><lpage>2292</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2014.01.005</pub-id></element-citation></ref><ref id="B53-sensors-25-05412"><label>53.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name></person-group><source>LARS: Light Augmented Reality System&#8212;Benchmarking Tests Video</source><publisher-name>Figshare</publisher-name><publisher-loc>London, UK</publisher-loc><year>2025</year><pub-id pub-id-type="doi">10.6084/m9.figshare.29967694.v1</pub-id></element-citation></ref><ref id="B54-sensors-25-05412"><label>54.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Romanczuk</surname><given-names>P.</given-names></name><name name-style="western"><surname>Hamann</surname><given-names>H.</given-names></name></person-group><article-title>LARS: Light Augmented Reality System for Swarm</article-title><source>Proceedings of the Swarm Intelligence: 14th International Conference, ANTS 2024</source><conf-loc>Konstanz, Germany</conf-loc><conf-date>9&#8211;11 October 2024</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><volume>Volume 14987</volume><fpage>246</fpage></element-citation></ref><ref id="B55-sensors-25-05412"><label>55.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name></person-group><source>LARS: Light Augmented Reality System&#8212;Platform Adaptability Test Video</source><publisher-name>Figshare</publisher-name><publisher-loc>London, UK</publisher-loc><year>2025</year><pub-id pub-id-type="doi">10.6084/m9.figshare.29967193.v1</pub-id></element-citation></ref><ref id="B56-sensors-25-05412"><label>56.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Raoufi</surname><given-names>M.</given-names></name></person-group><source>LARS: Light Augmented Reality System&#8212;Introducing LARS</source><publisher-name>Figshare</publisher-name><publisher-loc>London, UK</publisher-loc><year>2025</year><pub-id pub-id-type="doi">10.6084/m9.figshare.30005467.v1</pub-id><pub-id pub-id-type="pmid">40942839</pub-id></element-citation></ref><ref id="B57-sensors-25-05412"><label>57.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>Vicon</collab></person-group><source>Vicon Motion Systems</source><publisher-name>Vicon</publisher-name><publisher-loc>Amherst, MA, USA</publisher-loc><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.vicon.com/" ext-link-type="uri">https://www.vicon.com/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-22">(accessed on 22 August 2025)</date-in-citation></element-citation></ref><ref id="B58-sensors-25-05412"><label>58.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>OptiTrack</collab></person-group><source>OptiTrack&#8212;Motion Capture Systems</source><publisher-name>OptiTrack</publisher-name><publisher-loc>Corvallis, OR, USA</publisher-loc><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://optitrack.com" ext-link-type="uri">https://optitrack.com</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-08-22">(accessed on 22 August 2025)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05412-f001" orientation="portrait"><label>Figure 1</label><caption><p>The view layer presents two main interfaces: the GUI (the left window) which contains the bird&#8217;s-eye view of the camera, with overlaid rendered information (such as FPS and robot IDs), and the virtual scene (the middle picture), which is projected as the second display on the environment. The right image shows the real&#8211;virtual domain, containing both the robots and the virtual visual objects such as the Voronoi tessellations of the space.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05412-g001.jpg"/></fig><fig position="float" id="sensors-25-05412-f002" orientation="portrait"><label>Figure 2</label><caption><p>(<bold>a</bold>) <monospace>LARS</monospace> software and hardware architecture structured into model, control, and view layers. The pipeline forms a closed loop from sensing (camera) to computation (detection, tracking, world model, physics) to actuation (projection), with user interaction via the GUI and optional robot controller, enabling interaction between users, robots, and the augmented environment. (<bold>b</bold>) Schematic runtime workflow for one update cycle: (i) frame acquisition; (ii) circle detection (raw detections, including noise and false positives); (iii) tracking with filtering and data association to maintain consistent robot IDs; (iv) world model update and environment dynamics, including interaction networks, communication fields, and other virtual cues; (v) rendering via Qt Painter and projection mapping; (vi) updated real&#8211;virtual environment as presented in the next camera frame to robots and users. Logging and video recording run in parallel, while the GUI allows users to monitor system state and adjust parameters at runtime.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05412-g002.jpg"/></fig><fig position="float" id="sensors-25-05412-f003" orientation="portrait"><label>Figure 3</label><caption><p>(<bold>a</bold>) Mapping and translation between virtual and real coordinates by using ArUco markers. (<bold>b</bold>) Real Kilobot demonstrator using <monospace>LARS</monospace> showing nine Kilobots detected in an environment, with light projection at their detected locations, and the visualization of Voronoi tessellation. (<bold>c</bold>) The 3D model of the full demonstrator including the hardware structure with scaffolding, camera, projector, and mirror. The mirror increases the effective projection distance, compensating for the projector&#8217;s limited projection angle and enabling full coverage of the robot arena.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05412-g003.jpg"/></fig><fig position="float" id="sensors-25-05412-f004" orientation="portrait"><label>Figure 4</label><caption><p>Evaluation of tracking precision, system performance, and scalability. (<bold>a</bold>) Tracked position of 100 real Kilobots, with different color for each robot. The lower inset shows the distribution of the tracked position for the robot with highest tracking error (i.e., highest variance). The red plus sign marks the average tracked position (gray dots). The upper inset shows the detected points overlaid on an image of the actual Kilobot. (<bold>b</bold>) Comparison of recorded and calculated FPS under two processing conditions. Video recording reduces performance, while both FPS measures remain consistent across loads. (<bold>c</bold>) Scalability test showing FPS as a function of the number of robots. FPS decreases with increasing swarm size, but the system remains responsive even with 399 robots, maintaining approximately 10 FPS under the heavy processing load. Each data point is averaged over multiple trials, with error bars indicating standard deviation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05412-g004.jpg"/></fig><fig position="float" id="sensors-25-05412-f005" orientation="portrait"><label>Figure 5</label><caption><p>Evaluation of detection and tracking robustness under brightness and visual noise conditions. (<bold>a</bold>) Number of correctly detected robots (true positives) as a function of the brightness variable (V) of the environment in HSV space. The actual number of robots in the environment is 100 (red dotted line); values below this indicate missed detections. Detection performance declines sharply below a brightness threshold (<inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>&#8776;</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). The inset shows a zoomed view of the low-brightness regime (green rectangle). (<bold>b</bold>) Robustness to visual noise projected into the environment. Left axis: number of false positives in detection and tracking across varying spatial scales of Gaussian noise. Right axis: mean positional error in tracking. While detection accuracy decreases at finer noise scales, tracking maintains correct count with only a moderate increase in spatial error. For both plots, each data point is averaged over multiple trials, with error bars indicating standard deviation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05412-g005.jpg"/></fig><fig position="float" id="sensors-25-05412-f006" orientation="portrait"><label>Figure 6</label><caption><p>Evaluation of closed-loop latency in the perception-to-rendering pipeline. (<bold>a</bold>) Experimental setup showing a virtual robot (with an image of a Kilobot) moving leftward while a projected ring (yellow) lags behind due to system latency. The ring&#8217;s radius was manually adjusted so that the robot&#8217;s center aligned with the edge of the ring. (<bold>b</bold>) Position and speed of the virtual robot over time during the latency evaluation experiment. The blue curve shows the oscillatory x-position of the robot as it moves back and forth across the arena, while the red steps indicate the corresponding speed values manually configured during the experiment. (<bold>c</bold>) Measured radius of the ring (i.e., spatial lag) as a function of the robot&#8217;s speed. The slope of the linear regression indicates an approximate latency of 0.08 s. Each point corresponds to a manual measurement at a given speed.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05412-g006.jpg"/></fig><fig position="float" id="sensors-25-05412-f007" orientation="portrait"><label>Figure 7</label><caption><p>Example scenarios with different robots and environment settings. (<bold>a</bold>) GUI snapshot of 42 Kilobots synchronizing on a grid with their internal binary state being detected by the color of their LED in blue or red; (<bold>b</bold>) user view of 63 Kilobots making a collective decision on a tiled environment with projected dynamic salt-and-pepper noise&#160;[<xref rid="B54-sensors-25-05412" ref-type="bibr">54</xref>]; (<bold>c</bold>) GUI snapshot of 109 Kilobots with the trace of their random movement decaying over time; (<bold>d</bold>) GUI snapshot of two active balls randomly moving in the bounded arena, being tracked by <monospace>LARS</monospace> without the need for any markers; (<bold>e</bold>) GUI snapshot of two Thymios with different colors locating the center of the light distribution (projected by <monospace>LARS</monospace>). The trace of each robot shows the consistency of the color detection of each robot over time, even after a collision&#160;[<xref rid="B54-sensors-25-05412" ref-type="bibr">54</xref>]. (<bold>f</bold>) User view of Thymios moving randomly, with their centroid, the projection of their trajectory (light blue trails), their Voronoi tessellation (black lines) and the corresponding network (green lines).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05412-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05412-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05412-t001_Table 1</object-id><label>Table 1</label><caption><p>Related Extended Reality systems for multi-robots.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS <sup>1</sup></th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Detection Method</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Supported Robots [Tested on]</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Immersion Level</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Direct Commu-<break/>Nication</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Max. Arena Size (m<sup>2</sup>)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cost (K&#8364;)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<monospace>
<bold>LARS</bold>
</monospace>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hough circle</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Circular robots [Kilobot, Thymio, E-puck]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AR-MR with interactive visual objects</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WiFi, IR for kilobot</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2.3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3.7</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5&#8211;4.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARK [<xref rid="B28-sensors-25-05412" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hough circle</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Circular robots <sup>2</sup> [Kilobots]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VR (only virtual)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IR for Kilobot</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2.2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2.2</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0&#8211;3.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KiloGrid [<xref rid="B27-sensors-25-05412" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5 *</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kilobots</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VR (only virtual)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8776;15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARDebug [<xref rid="B30-sensors-25-05412" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10 *</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ArUco tags</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-miniature robots</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VR (only virtual)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WiFi, Bluetooth</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2.5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0&#8211;3.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B26-sensors-25-05412" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5 *</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hough circle</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Circular robots <sup>2</sup> [Alice]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AR with visual trails</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1.05</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5&#8211;4.0</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> (*) stands for the numbers reported by the reference. <sup>2</sup> The tracking algorithm can potentially be used on circular robots.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>