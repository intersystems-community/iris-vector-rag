<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431335</article-id><article-id pub-id-type="pmcid-ver">PMC12431335.1</article-id><article-id pub-id-type="pmcaid">12431335</article-id><article-id pub-id-type="pmcaiid">12431335</article-id><article-id pub-id-type="doi">10.3390/s25175486</article-id><article-id pub-id-type="publisher-id">sensors-25-05486</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MineVisual: A Battery-Free Visual Perception Scheme in Coal Mine</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8121-3264</contrib-id><name name-style="western"><surname>Li</surname><given-names initials="M">Ming</given-names></name></contrib><contrib contrib-type="author"><name name-style="western"><surname>Bao</surname><given-names initials="Z">Zhongxu</given-names></name></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="S">Shuting</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2651-3432</contrib-id><name name-style="western"><surname>Yang</surname><given-names initials="X">Xu</given-names></name><xref rid="c1-sensors-25-05486" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Niu</surname><given-names initials="Q">Qiang</given-names></name></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yang</surname><given-names initials="M">Muyu</given-names></name></contrib><contrib contrib-type="author"><name name-style="western"><surname>Chen</surname><given-names initials="S">Shaolong</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Korzeniewska</surname><given-names initials="E">Ewa</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05486">School of Computer Science and Technology, China University of Mining and Technology, Xuzhou 221116, China; <email>lmgyw@cumt.edu.cn</email> (M.L.); <email>baozx@cumt.edu.cn</email> (Z.B.); <email>niuq@cumt.edu.cn</email> (Q.N.); <email>ts24170137p31@cumt.edu.cn</email> (M.Y.); <email>ts24170047a31@cumt.edu.cn</email> (S.C.)</aff><author-notes><corresp id="c1-sensors-25-05486"><label>*</label>Correspondence: <email>yang_xu@cumt.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>03</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5486</elocation-id><history><date date-type="received"><day>26</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>26</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>03</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 12:25:22.357"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05486.pdf"/><abstract><p>The demand for robust safety monitoring in underground coal mines is increasing, yet traditional methods face limitations in long-term stability due to inadequate energy supply and high maintenance requirements. To address the critical challenges of high computational demand and energy constraints in this resource-limited environment, this paper proposes MineVisual, a battery-free visual sensing scheme specifically designed for underground coal mines. The core of MineVisual is an optimized lightweight deep neural network employing depthwise separable convolution modules to enhance computational efficiency and reduce energy consumption. Crucially, we introduce an energy-aware dynamic pruning network (EADP-Net) ensuring a sustained inference accuracy and energy efficiency across fluctuating power conditions. The system integrates supercapacitor buffering and voltage regulation for stable operation under wind intermittency. Experimental validation demonstrates that MineVisual achieves high accuracy (e.g., 91.5% Top-1 on mine-specific tasks under high power) while significantly enhancing the energy efficiency (reducing inference energy to 6.89 mJ under low power) and robustness under varying wind speeds. This work provides an effective technical pathway for intelligent safety monitoring in complex underground environments and conclusively proves the feasibility of battery-free deep learning inference in extreme settings like coal mines.</p></abstract><kwd-group><kwd>battery-free visual sensing</kwd><kwd>energy harvesting</kwd><kwd>energy-aware adaptive pruning</kwd><kwd>deep learning inference</kwd><kwd>underground coal mine safety</kwd></kwd-group><funding-group><award-group><funding-source>Deep Earth Probe and Mineral Resources Exploration-National Science and Technology Major Project</funding-source><award-id>2024ZD1003905</award-id></award-group><award-group><funding-source>Natural Science Foundation of Jiangsu Province</funding-source><award-id>BK20221109</award-id></award-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>51874302</award-id></award-group><funding-statement>This work was supported by the Deep Earth Probe and Mineral Resources Exploration-National Science and Technology Major Project (Grant Number 2024ZD1003905), the Natural Science Foundation of Jiangsu Province (Grant Number BK20221109), and the National Natural Science Foundation of China (Grant Number 51874302).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05486"><title>1. Introduction</title><p>In recent years, significant breakthroughs in battery-free sensing technology have fundamentally transformed industrial monitoring paradigms. By leveraging diverse energy harvesting mechanisms [<xref rid="B1-sensors-25-05486" ref-type="bibr">1</xref>]&#8212;including triboelectric nanogenerators (TENGs) [<xref rid="B2-sensors-25-05486" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05486" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05486" ref-type="bibr">4</xref>], near-field communication (NFC) [<xref rid="B5-sensors-25-05486" ref-type="bibr">5</xref>], and RF backscattering [<xref rid="B6-sensors-25-05486" ref-type="bibr">6</xref>]&#8212;this technology achieves truly self-powered operation [<xref rid="B7-sensors-25-05486" ref-type="bibr">7</xref>]. This paradigm eliminates the recurring costs and environmental burdens of battery replacement, enabling sustainable long-term deployment in challenging environments including forests, oceans, and underground mines.</p><p>While significant progress has been made in battery-free sensing for terrestrial and subaquatic environments [<xref rid="B8-sensors-25-05486" ref-type="bibr">8</xref>], research specifically addressing the unique challenges of underground coal mines remains nascent. Within this domain, battery-free technology predominantly leverages the mine ventilation system as the primary source for energy harvesting [<xref rid="B9-sensors-25-05486" ref-type="bibr">9</xref>]. As a core component ensuring coal mine safety, the ventilation system offers a technically compatible source of wind energy due to the shaft&#8217;s stable airflow environment (average wind speed 4 m/s, uniformly distributed, max 10 m/s) [<xref rid="B10-sensors-25-05486" ref-type="bibr">10</xref>]. While prior works have demonstrated the feasibility of powering basic sensor nodes via shaft wind energy [<xref rid="B11-sensors-25-05486" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05486" ref-type="bibr">12</xref>], the harsh underground environment&#8212;characterized by dust, humidity, and fluctuating airflow&#8212;often compromises stability and reduces energy harvesting efficiency [<xref rid="B13-sensors-25-05486" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05486" ref-type="bibr">14</xref>].</p><p>In recent years, battery-free technology in mine safety monitoring has primarily focused on basic energy harvesting and communication mechanisms, and it has yet to advance into complex applications such as machine learning (ML) model inference on devices. For safety monitoring, battery-free sensors in coal mines must not only collect data but also process it locally to reduce transmission costs and conserve scarce energy. Despite advances in lightweight techniques like model compression [<xref rid="B15-sensors-25-05486" ref-type="bibr">15</xref>] and layer-wise quantization [<xref rid="B16-sensors-25-05486" ref-type="bibr">16</xref>] that reduce computational energy, real-time ML inference on underground battery-free devices remains a pressing challenge due to stringent power constraints in dynamic environments.</p><p>This paper addresses the feasibility of battery-free ML in underground coal mines. We propose MineVisual, a novel battery-free visual perception scheme. It harvests energy from mine ventilation airflow via a custom low-starting-torque wind turbine, eliminating wiring and batteries for reduced maintenance and sustainability. Microcontroller nodes and cameras capture imagery, processed by a lightweight ML model on-device. This enables real-time detection of safety hazards&#8212;personnel location, helmet compliance, and tunnel deformation&#8212;while minimizing data transmission and conserving energy. However, realizing this scheme confronts two primary challenges:<list list-type="bullet"><list-item><p>How to achieve stable and efficient energy harvesting from the mine ventilation system to power computation-intensive visual sensing under fluctuating wind conditions;</p></list-item><list-item><p>How to design a lightweight ML model capable of dynamically adapting its computational complexity in real-time to maintain accuracy under constrained and variable power availability.</p></list-item></list></p><p>To tackle the first challenge, we designed a wind turbine with low-starting-torque DC motors, optimized for low-speed mine airflow, and combined with real-time power monitoring for dynamic regulation and stable operation. To address the second, we introduce <bold>EADP-Net</bold> (Energy-Aware Dynamic Pruning Network), a depthwise separable convolution-based model for energy efficiency. Unlike Meta-Pruning [<xref rid="B17-sensors-25-05486" ref-type="bibr">17</xref>] or single-shot pruning [<xref rid="B18-sensors-25-05486" ref-type="bibr">18</xref>], it features (1) an energy-state factor &#945; (Equation (4)) adjusting pruning via a power surplus, thereby preventing accuracy drops; and (2) hierarchical-layer neuron optimization. This reduces FLOPs by 26.3&#8211;70.3% while maintaining 68.6&#8211;91.5% accuracy across energy states.</p><p>Accordingly, we rigorously evaluate two critical dimensions of MineVisual:<list list-type="bullet"><list-item><p>The performance of lightweight deep neural network (DNN) models (including accuracy, inference latency, and energy efficiency) when executing critical safety monitoring tasks in the resource-constrained underground coal mine environment;</p></list-item><list-item><p>The feasibility and effectiveness of the energy-aware adaptive pruning mechanism on ultra-low-power microcontroller platforms under real fluctuating power.</p></list-item></list></p><p>By designing an end-to-end workflow from energy harvesting to on-device inference, we tested accuracy and power consumption dynamically across dynamically varying energy conditions. The results show that optimized wind harvesting combined with the EADP-Net model enables complex visual DNN inference on battery-less devices. This establishes a pathway for sustainable, intelligent mine monitoring and proves adaptive low-power deep learning in extreme environments.</p><p>Compared to other battery-free sensing methods [<xref rid="B9-sensors-25-05486" ref-type="bibr">9</xref>,<xref rid="B19-sensors-25-05486" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05486" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05486" ref-type="bibr">21</xref>], as shown in <xref rid="sensors-25-05486-t001" ref-type="table">Table 1</xref>, MineVisual is more capable of performing complex calculations and long-term deployment in coal mining environments. It is powered by wind turbines and combines an energy-aware dynamic pruning model (EADP-Net) to maintain high visual inference accuracy and energy efficiency under fluctuating energy conditions, enabling true battery-free edge intelligence for complex visual tasks such as personnel safety compliance monitoring.</p><p>The paper is organized as follows: <xref rid="sec2-sensors-25-05486" ref-type="sec">Section 2</xref> details scheme design; <xref rid="sec3-sensors-25-05486" ref-type="sec">Section 3</xref> presents energy harvesting and EADP-Net; <xref rid="sec4-sensors-25-05486" ref-type="sec">Section 4</xref> validates experiments; <xref rid="sec5-sensors-25-05486" ref-type="sec">Section 5</xref> concludes.</p></sec><sec id="sec2-sensors-25-05486"><title>2. Scheme Overview</title><p>In this section, we outline the design of the proposed battery-free visual perception scheme, MineVisual, specifically engineered for the demanding environment of underground coal mines. As depicted in <xref rid="sensors-25-05486-f001" ref-type="fig">Figure 1</xref>, MineVisual integrates two core components: (1) a custom wind energy harvesting module utilizing a low-starting-torque DC generator to efficiently capture ventilation airflow and convert it into electrical power, and (2) the EADP-Net, a lightweight deep neural network optimized for visual monitoring tasks under severe power constraints. The EADP-Net model incorporates a novel dynamic energy-aware pruning mechanism. This mechanism continuously monitors the real-time output power from the energy harvester and dynamically adjusts the network&#8217;s computational complexity in response to the current energy state. By pruning the network to reduce complexity and selectively disabling specific layers or neurons, the model adapts to fluctuating power availability while maintaining high inference accuracy.</p><p>The deployment of the MineVisual scheme within the operational underground coal mine environment follows a structured two-phase approach: offline training and underground inference.</p><p><bold>Offline Training Phase:</bold> In this preparatory stage, video footage and images depicting personnel, equipment, and mine scenes are captured underground using the designated camera sensors. This raw visual data undergoes preprocessing steps, including image reshaping and normalization, to standardize the input format for model training. Subsequently, the EADP-Net model is subjected to optimization techniques, specifically network-layer pruning and neuron pruning, to reduce its structural complexity. This is followed by model quantization and compression, significantly diminishing the model&#8217;s storage footprint and computational demands. The outcome of this phase is a highly optimized, lightweight model specifically tailored for deployment on resource-constrained, low-power microcontrollers.</p><p><bold>Underground Inference Phase:</bold> Upon deployment, the scheme harnesses energy from the custom wind turbine module and potentially other ambient sources to power the sensing and computation nodes. Concurrently, camera-equipped sensor nodes capture real-time visual data from the mine environment. Acquired images are immediately preprocessed (reshaping and normalization) to match the model&#8217;s input requirements. These preprocessed images are then fed into the lightweight EADP-Net model for on-device inference. Crucially, leveraging the integrated dynamic energy-aware pruning mechanism, the model autonomously adjusts its computational complexity (i.e., the extent of active layers and neurons) in real-time based on the instantaneous power output from the energy harvester. This dynamic adaptation ensures sustained inference accuracy and maximizes energy efficiency across the fluctuating power conditions endemic to mine ventilation systems. Finally, the model outputs its prediction results (e.g., classifications or detections), enabling real-time monitoring and the generation of critical alerts concerning personnel safety (location, PPE compliance) and infrastructural integrity (tunnel deformation).</p></sec><sec id="sec3-sensors-25-05486"><title>3. Scheme Design</title><sec id="sec3dot1-sensors-25-05486"><title>3.1. Energy Harvesting in Coal Mine</title><p>To achieve sustainable battery-free operation in underground coal mines, we propose a turbine-based rotational energy harvesting module that converts the kinetic energy of the mandatory ventilation airflow into electrical power. Although ventilation airflow is generated by powered fans, direct electrical coupling to industrial power lines is prohibited by mine safety standards (e.g., Safety requirements for mining main ventilation fan system [<xref rid="B22-sensors-25-05486" ref-type="bibr">22</xref>]). Wind energy harvesting provides intrinsic isolation, eliminating risks of high-voltage exposure and simplifying deployment. A miniature wind turbine, installed within the mine&#8217;s ventilation ducts, directly drives a low-starting-torque DC motor, generating electrical energy without intermediate AC&#8211;DC conversion, thereby minimizing energy losses. The harvested energy is continuously monitored by an integrated power management unit, which dynamically regulates and allocates the available power to ensure a sufficient supply under varying operating conditions. The overall scheme architecture comprises three functional modules: energy harvesting, data collection, and processing. Electrical energy from the harvesting module is first delivered to the central processor. Once the processor detects that the capacitor voltage exceeds a predefined threshold, it activates the imaging subsystem to capture real-time underground imagery. The acquired images are then transmitted to the processor, where a lightweight, on-device model performs inference and analysis. Physical separation between the imaging module and the processor enables independent transmission paths for power and data via short-range cabling, improving both power stability and system responsiveness. Among those, the DC output is stabilized via a switched-capacitor voltage regulator (3.3 V output, 85% efficiency) and buffered by a 10F supercapacitor. This ensures sustained operation during wind lulls (&lt;2.7 m/s), enabling cold-start at 3.6 V. The scheme activates only when the capacitor voltage exceeds 3.0 V, preventing undervoltage lockout. Furthermore, the scheme design leverages the periodic fluctuation characteristics of mine ventilation airflow to establish a flexible harvest&#8211;wake&#8211;sense&#8211;infer operational loop, thereby achieving intermittent yet autonomous functionality in the absence of a continuous power supply.</p></sec><sec id="sec3dot2-sensors-25-05486"><title>3.2. Energy-Aware Model Pruning Mechanism</title><p>Building upon the energy harvesting foundation, we now focus on designing an efficient, lightweight, and low-power deep learning model tailored for underground monitoring tasks. To address the stringent low-power inference requirements in coal mines, we propose the EADP-Net (Energy-Aware Dynamic Pruning Network). The overall architecture of EADP-Net is depicted in <xref rid="sensors-25-05486-f002" ref-type="fig">Figure 2</xref>. EADP-Net incorporates the design principles of established lightweight neural networks [<xref rid="B23-sensors-25-05486" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05486" ref-type="bibr">24</xref>], utilizing depthwise separable convolution (DSC) as its fundamental building block [<xref rid="B15-sensors-25-05486" ref-type="bibr">15</xref>]. By decomposing standard convolution into depthwise and pointwise convolutions, DSC significantly reduces computational complexity, model parameters, and memory footprint, thereby enhancing computational and energy efficiency. Crucially, we augment the DSC module with a novel energy-aware mechanism. This mechanism dynamically adjusts the processing complexity within the depthwise convolution stage based on the scheme&#8217;s real-time energy state. Specifically, it modulates the feature channel scaling factors (&#945; in <xref rid="sensors-25-05486-f003" ref-type="fig">Figure 3</xref>) in response to the instantaneous power availability, enabling fine-grained control over computational resource allocation. This real-time adaptation optimizes energy consumption while maintaining task performance, ensuring efficient operation on resource-constrained underground hardware platforms. The structure of this enhanced energy-aware DSC module is illustrated in <xref rid="sensors-25-05486-f003" ref-type="fig">Figure 3</xref>. This design allows the network to adaptively adjust its computational load under varying energy conditions, enhancing scheme stability and efficiency while achieving the core objective of low-energy deep learning inference. Furthermore, to address the significant challenge of unstable power generation caused by wind speed fluctuations and to achieve further reductions in computational demand and energy consumption, we introduce a complementary energy-aware model pruning mechanism. This mechanism optimizes computational resource utilization by dynamically adjusting the number of active neural network layers and neurons based on the current energy state. The energy state is derived in real-time from the wind turbine module&#8217;s power output (<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), monitored continuously during scheme operation. The instantaneous wind power <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated as<disp-formula id="FD1-sensors-25-05486"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>&#961;</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the air density, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the swept area of the wind turbine blades <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>&#960;</mml:mi><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is related only to the area of the blades <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the wind speed, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the energy conversion efficiency of the fan blades, and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the conversion efficiency of the generator motor.</p><sec id="sec3dot2dot1-sensors-25-05486"><title>3.2.1. Network-Layer Pruning</title><p>The goal of network-layer pruning is to decide whether to remove certain network layers based on the current energy state. Assuming that the total number of layers in the network is <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, we can measure the importance of layer <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> by calculating its <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> norm for the weight matrix <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which contains all the connectivity parameters of the layer. The <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> paradigm number represents the overall size of the weights, and a larger <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> norm usually means that the weights of that layer contribute more to the network performance.<disp-formula id="FD2-sensors-25-05486"><label>(2)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the input and output neuron counts of layer <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the connection weight between the <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th input neuron and the <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th output neuron of the layer <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> norm measures the Euclidean magnitude of the weight tensor, with higher values indicating stronger contribution to feature representation and thus greater importance to network performance. Based on the ratio of the calculated norm to the sum of the <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> norms of all layers (<inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we calculate the pruning ratio <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for each layer:<disp-formula id="FD3-sensors-25-05486"><label>(3)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8214;</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the importance ratio of the weight of layer <italic toggle="yes">l</italic> to the total weight. To determine whether to retain a layer under different energy states, we introduce the energy state factor <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which is related to the current wind power generation intensity <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the average power generation capacity <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD4-sensors-25-05486"><label>(4)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>&#955;</mml:mi><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent="false"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">&#955;</italic> is a hyperparameter used to control the influence of power on pruning intensity. Then, we dynamically adjust whether to retain each layer based on the pruning ratio <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the current energy state factor <italic toggle="yes">&#945;</italic>. Set a threshold <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. When <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is less than this threshold, prune the layer; otherwise, retain the layer.<disp-formula id="FD5-sensors-25-05486"><label>(5)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8901;</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot2dot2-sensors-25-05486"><title>3.2.2. Neuron Pruning</title><p>The goal of neuron pruning is to selectively remove unimportant neurons based on their importance, thereby reducing computational load and energy consumption. Suppose that layer <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> has <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> neurons. For the <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th neuron in layer <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, its activation value <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> in the <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th forward propagation can be obtained using the activation function <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD6-sensors-25-05486"><label>(6)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the input signal of neuron i during the <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th forward propagation, which is usually a weighted sum:<disp-formula id="FD7-sensors-25-05486"><label>(7)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight connecting the <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th neuron in layer <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to the <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th neuron in layer <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the bias term of the <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th neuron in layer <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the activation value of the <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th neuron in layer <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>during the <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th forward propagation. We use the mean squared activation value (MSA) of the activation value of the <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th neuron in the layer <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as a measure of its importance. This metric represents the average activation energy of the neuron across the training set. Neurons with consistently low activations (MSA close to 0) contribute minimally to downstream feature representations, indicating their potential redundancy and suitability for pruning.<disp-formula id="FD8-sensors-25-05486"><label>(8)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the activation value of the <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th neuron in the layer <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> during the <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th forward propagation, and <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of time steps in the forward propagation, which is typically equal to the number of training samples or the number of batches of data. Based on MSA values of neurons (defined in Equation (8)) and the energy state factor <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> introduced above, we establish a dynamic threshold <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to prune neurons exhibiting lower importance.<disp-formula id="FD9-sensors-25-05486"><label>(9)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#8901;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot2dot3-sensors-25-05486"><title>3.2.3. Pruning Strategy</title><p>We design a hierarchical pruning strategy that dynamically adapts neural network complexity to real-time energy availability, governed by the energy state factor <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>:</p><p><bold>High-Energy State (<italic toggle="yes">p</italic> &#8805; 90 mW):</bold> Under sufficient wind power, the scheme deploys the full network architecture without pruning. This maximizes inference accuracy for critical tasks (e.g., personnel detection, helmet compliance) by retaining all layers and neurons.</p><p><bold>Medium energy state (40 mW &#8804; <italic toggle="yes">p</italic> &lt; 90 mW):</bold> When the wind speed is moderate and the harvested energy is usable but not abundant enough to fully support all computations, the scheme automatically prunes some neural network layers and neurons dynamically. Specifically, we prioritize pruning neurons and layers that contribute less to task performance. This reduces the computational burden and power consumption without significantly compromising model accuracy. This dynamic pruning process is based on the importance of the weights within the layers and neurons. Furthermore, the pruning ratio is dynamically adjusted by an energy-aware mechanism that monitors computational demand in real-time, aiming to achieve a reasonable balance between performance and energy efficiency.</p><p><bold>Low-energy state (<italic toggle="yes">p</italic> &lt; 40 mW):</bold> Under severe energy constraints, aggressive pruning retains only core computational pathways. To ensure that basic task execution can still be maintained under ultra-low-power conditions, we increase the pruning intensity, reducing the number of layers and neurons in the model. At this stage, the pruning strategy prioritizes retaining core computational capacity, ensuring the model can still accomplish the most critical tasks when energy resources are severely limited.</p><p>This energy-state-driven pruning strategy optimally allocates limited power resources to computational demands. By reducing redundant operations during low-energy intervals and maximizing accuracy during high-energy windows, the scheme achieves sustained task performance (85.2% Top-1 accuracy at 6.89 mJ) while enhancing operational sustainability in volatile mine environments.</p><p>The complete energy-aware pruning workflow is formalized in Algorithm 1, which dynamically adjusts model complexity based on real-time power <italic toggle="yes">P</italic>, average power <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, and energy state factor &#945;.
<array orientation="portrait"><tbody><tr><td colspan="2" align="left" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1"><bold>Algorithm 1.</bold> Low-Energy Consumption Model Based on Energy Awareness</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">1:</td><td align="left" valign="top" rowspan="1" colspan="1"><bold>Input:</bold>&#160;<italic toggle="yes">P</italic> (real-time wind power), <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;(average wind power over a time window), <italic toggle="yes">&#955;</italic> (scaling factor for energy state sensitivity), <italic toggle="yes">N</italic> (total network layers), <italic toggle="yes">M</italic> (total neurons per layer), trainSet, valSet</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2:</td><td align="left" valign="top" rowspan="1" colspan="1"><bold>Output:</bold> Optimized neural network model</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">3:</td><td align="left" valign="top" rowspan="1" colspan="1"><italic toggle="yes">M &#8592;</italic> initial-DNN(<italic toggle="yes">N</italic>, <italic toggle="yes">M</italic>)//<italic toggle="yes">Construct lightweight DNN with N layers and M neurons per layer</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">4:</td><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#945;</mml:mi><mml:mo>&#8592;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>&#955;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent="false"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>//<italic toggle="yes">Define energy state factor</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">5:</td><td align="left" valign="top" rowspan="1" colspan="1"><italic toggle="yes">&#946; &#8592;</italic> 1//<italic toggle="yes">Set initial pruning factor</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">6:</td><td align="left" valign="top" rowspan="1" colspan="1"><bold>for</bold> each training iteration <bold>do</bold></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">7:</td><td align="left" valign="top" rowspan="1" colspan="1">&#8195;<italic toggle="yes">&#946; &#8592; &#945;/</italic>/<italic toggle="yes">Set pruning factor based on energy state</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">8:</td><td align="left" valign="top" rowspan="1" colspan="1">&#8195;Prune <italic toggle="yes">&#946;&#183;N</italic> layers with least importance//<italic toggle="yes">Perform layer pruning based on importance ranking</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">9:</td><td align="left" valign="top" rowspan="1" colspan="1">&#8195;Prune <italic toggle="yes">&#946;&#183;M</italic> neurons per layer with least contribution//<italic toggle="yes">Perform neuron pruning</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">10:</td><td align="left" valign="top" rowspan="1" colspan="1">&#8195;Fine-tune the pruned model <italic toggle="yes">M</italic> on trainSet//<italic toggle="yes">Fine-tune the model</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">11:</td><td align="left" valign="top" rowspan="1" colspan="1"><bold>end for</bold></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">12:</td><td align="left" valign="top" rowspan="1" colspan="1">Evaluate the pruned model <italic toggle="yes">M</italic> on valSet//<italic toggle="yes">Evaluate the model on the validation set</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">13:</td><td align="left" valign="top" rowspan="1" colspan="1"><bold>if</bold> performance degradation exceeds threshold <bold>then</bold></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">14:</td><td align="left" valign="top" rowspan="1" colspan="1">&#8195;Roll back the last pruning operation//<italic toggle="yes">Restore the previous state</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">15:</td><td align="left" valign="top" rowspan="1" colspan="1"><bold>end if</bold></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">16:</td><td align="left" valign="top" rowspan="1" colspan="1">Adjust energy state factor dynamically if <italic toggle="yes">P</italic> changes significantly//<italic toggle="yes">Recalculate pruning factor if P fluctuates</italic></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">17:</td><td align="left" valign="top" rowspan="1" colspan="1"><bold>repeat</bold></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">18:</td><td align="left" valign="top" rowspan="1" colspan="1">&#8195;Repeat steps 3&#8211;9//<italic toggle="yes">Continue pruning and adjusting</italic></td></tr><tr><td align="center" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">19:</td><td align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>until</bold> convergence or significant energy state change <bold>return</bold> optimized model <italic toggle="yes">M/</italic>/<italic toggle="yes">Output the final optimized model</italic></td></tr></tbody></array></p></sec></sec><sec id="sec3dot3-sensors-25-05486"><title>3.3. Battery-Free Inference in Mine</title><p>To address the critical need for real-time hazard detection in high-risk underground mines, we implement on-device inference to eliminate cloud dependency and transmission delays. This approach leverages local computational resources to achieve ultra-low-latency analysis (&lt;330 ms total runtime) essential for safety-critical responses. Furthermore, the network environment in underground coal mines is often unstable or even non-existent in certain areas. Relying on cloud-based inference cannot guarantee reliable scheme operation, whereas on-board inference functions independently without requiring network connectivity. Additionally, on-board inference reduces the energy consumption associated with data upload and leverages local computational resources to achieve efficient, low-power inference. To achieve this objective, we first conduct offline training using efficient hardware resources. Subsequently, to accommodate the computational and storage constraints of low-power hardware platforms, the model undergoes compression and quantization. Finally, the optimized model is deployed on a microcontroller, enabling battery-free, low-power inference underground. The overview of offline training and on-board inference is shown in <xref rid="sensors-25-05486-f004" ref-type="fig">Figure 4</xref>.</p><p><bold>Offline Training:</bold> To ensure robust deployment on low-power underground devices, we adopted a multi-stage training approach. Initial training leveraged Mini-ImageNet [<xref rid="B25-sensors-25-05486" ref-type="bibr">25</xref>] and Tiny-ImageNet [<xref rid="B26-sensors-25-05486" ref-type="bibr">26</xref>] to establish baseline accuracy under controlled conditions. Subsequently, to maintain accuracy in the complex underground environments characterized by insufficient lighting and dust interference, the CUMT-CMUID coal mine image dataset [<xref rid="B27-sensors-25-05486" ref-type="bibr">27</xref>] and CUMT-HelmeT safety helmet dataset [<xref rid="B28-sensors-25-05486" ref-type="bibr">28</xref>] were used. These datasets were specifically collected and constructed for coal mine operational environments to enhance the robustness of critical visual tasks: personnel detection, safety compliance verification, and tunnel deformation monitoring. To mitigate challenges from low-light conditions and airborne particulates, we implemented targeted preprocessing. First, we augmented the dataset diversity through techniques such as rotation, flipping, cropping, and scaling to simulate image variations under different viewpoints and illumination conditions. We also adjusted image brightness and contrast to accommodate potential underground lighting fluctuations. Furthermore, we introduced noise to improve the model&#8217;s resilience to environmental interference, thereby enhancing its generalization capability. Second, to accelerate training and improve model stability, we normalized the images by scaling pixel values to the range [0, 1] and standardizing each channel to have zero mean and unit variance. This effectively mitigates the negative impact of inter-image lighting variations during training, promoting faster convergence. Finally, considering the constrained computational capabilities of low-power underground devices, we resized the images to a smaller fixed resolution (128 &#215; 128 pixels). This reduction in dimensionality significantly decreases the computational load and memory footprint, thereby improving the inference speed to meet the resource constraints of the target hardware. Model training employed cross-validated workflows with tuned loss functions and optimizers, ensuring generalization across mine topographies while preventing overfitting. This pipeline balances accuracy with the stringent resource constraints of target hardware.</p><p><bold>Quantization:</bold> Post-training quantization is applied to the optimized deep neural network (DNN) model to enhance deployability on ultra-low-power microcontrollers. This process converts the model&#8217;s floating-point weights and activation values into low-precision integers, significantly reducing memory footprint and computational latency with minimal precision loss. After converting the trained model to TensorFlow Lite format, we employ post-training quantization via STM32&#8217;s X-CUBE-AI toolchain to perform the quantization [<xref rid="B29-sensors-25-05486" ref-type="bibr">29</xref>]. The tool automatically analyzes the distributions of weights and activations, adjusts quantization parameters, and calculates scaling factors to minimize precision loss. Finally, it generates an optimized quantized model suitable for the microcontroller, prepared for subsequent deployment and inference.</p><p><bold>Deployment:</bold> Following model optimization, the quantized deep learning model is deployed onto the microcontroller to enable real-time inference for underground safety tasks. To support dynamic model pruning under varying energy states, we integrated a power monitoring subsystem that directly interfaces with the wind turbine&#8217;s output. This subsystem employs a precision power sensor (INA226AIDGSR manufactured by Texas Instruments from Dallas, TX, USA) coupled with an analog-to-digital converter (ADC) to continuously sample current and voltage data. The acquired measurements are converted into real-time power values (<italic toggle="yes">p</italic>) within the microcontroller, creating a closed-loop feedback mechanism. These instantaneous power readings directly trigger the EADP-Net&#8217;s adaptive pruning module, which dynamically configures the network&#8217;s active layers and neurons based on the current energy state. Inference results are transmitted via LoRa (Semtech SX1276 manufactured by Semtech from Camarillo, CA, USA) at 10 dBm output power (as shown in <xref rid="sensors-25-05486-f001" ref-type="fig">Figure 1</xref>). Each transmission consumes 14.2 mJ (20-byte payload), scheduled only when there is a supercapacitor charge &gt; 80% to avoid scheme shutdown. This design demonstrates less than 1% data loss during 500 m tunnel testing [<xref rid="B30-sensors-25-05486" ref-type="bibr">30</xref>]. This integrated approach ensures stable, low-power inference operations in underground coal mines&#8212;even during significant wind energy fluctuations&#8212;while maintaining critical safety monitoring capabilities.</p></sec></sec><sec id="sec4-sensors-25-05486"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05486"><title>4.1. Wind Power Generation Device</title><p>The wind energy harvesting module employs a custom turbine with 50 mm radius blades coupled to a DC generator (12 mm radius, 34 mm height, 13 mm output-shaft height; see <xref rid="sensors-25-05486-f005" ref-type="fig">Figure 5</xref>a). Wind conditions were generated by a 122 W industrial ventilation fan (rated speed 2000 r/min, airflow 2650 m<sup>3</sup>/h, maximum wind speed 20 m/s). For the turbine, the measured no-load voltage shows a cut-in wind speed of 2.7 m/s (3.6 V), and rises approximately linearly from 5.0 V to 11.7 V over 3&#8211;8 m/s. To further characterize the wind energy harvesting module&#8217;s power output, we measured its delivered power at different wind speeds. The results (see <xref rid="sensors-25-05486-f005" ref-type="fig">Figure 5</xref>b) indicate that in the 2.7&#8211;4.0 m/s range, the output power increases slowly with wind speed, reaching at most 15.2 mW; in the 5&#8211;6 m/s range, the output power rises rapidly from 31.4 mW to 60.3 mW, approaching the power consumption level of the sensing node; and in the 7&#8211;8 m/s range, the growth accelerates further, with a maximum measured output of 165.2 mW. In the wind speed range of 1.5&#8211;3.0 m/s, the scheme can meet the minimum operating voltage required by the camera and the microcontroller. The system integrates the wind power generation module with the power management unit (BQ25570 manufactured by Your Cee from Shenzhen, China) in a single package, incorporating a 10F supercapacitor (BCAP0010 P300 X11 manufactured by Maxwell Technologies from San Diego, CA, USA) for temporary energy storage, ensuring a stable power supply even under fluctuating wind speeds. Based on this power curve, the voltage threshold control module startup logic can be set to ensure that the system only enters operational mode under safe energy conditions, thereby preventing abnormal interruptions or false detections caused by low voltage.</p></sec><sec id="sec4dot2-sensors-25-05486"><title>4.2. Model Performance Evaluation</title><sec id="sec4dot2dot1-sensors-25-05486"><title>4.2.1. Benchmark Performance and Energy Grading</title><p>To establish a performance benchmark, we first evaluated the offline-trained model&#8217;s accuracy under stable high-power conditions (simulating optimal wind speeds). This ensures that the trained model can run properly on the STM32 development board under these conditions and achieve high accuracy as a benchmark performance. Next, prior to the model pruning experiments, it was necessary to define the wind speed ranges corresponding to different energy states because the power output of the wind power generation scheme is directly impacted by wind speed, an unavoidable factor in practical applications. Fluctuations in wind speed led to variations in power output, requiring real-time monitoring of wind speed and power generation combined with the actual power supply requirements of the STM32 development board to appropriately adjust the neural network complexity and model operating state. We established distinct energy states to reflect different operational conditions of the wind power scheme: high wind speeds enable the scheme to provide ample power, suitable for running compute-intensive tasks; conversely, low wind speeds result in reduced power output, necessitating decreased model complexity to lower computational consumption, thereby extending scheme runtime while maintaining basic task execution capability. We defined three discrete energy states based on real-time generator output (<italic toggle="yes">p</italic>), as illustrated in <xref rid="sensors-25-05486-t002" ref-type="table">Table 2</xref>.</p></sec><sec id="sec4dot2dot2-sensors-25-05486"><title>4.2.2. Dynamic Pruning Strategy Validation</title><p>To rigorously validate the efficacy of the energy-aware pruning mechanism, we evaluated network-layer pruning, neuron pruning, and joint-layer neuron pruning across three distinct energy states (high: <italic toggle="yes">p</italic> &#8805; 90 mW; medium: 40 mW &#8804; <italic toggle="yes">p</italic> &lt;90 mW; low: <italic toggle="yes">p</italic> &lt; 40 mW) using the Mini-ImageNet dataset [<xref rid="B25-sensors-25-05486" ref-type="bibr">25</xref>]. The experiments aimed to optimize computational performance by dynamically adjusting model complexity under varying energy conditions while maintaining inference accuracy and efficiency. As shown in <xref rid="sensors-25-05486-t003" ref-type="table">Table 3</xref>, under high-energy conditions, we first benchmarked the performance of the original model, recording its parameter count, floating-point operations (FLOPs), memory footprint (for SRAM and Flash), and inference accuracy (Top-1 and Top-5), and thereby establishing baselines for subsequent pruning. The baseline model achieved 79.7% Top-1 accuracy with 39.59 K parameters and 1610.56 KB Flash usage. For medium-energy states, we applied a moderate pruning strategy, removing selected network layers and neurons to reduce computational load while minimizing the impact on inference accuracy. And computational load was reduced by 26.3% (to 1402.1 MFLOPs) while retaining 78.6% Top-1 accuracy. Under low-energy constraints, we implemented more aggressive pruning, cutting additional network layers and neurons to adapt to the constrained energy supply. Aggressive joint pruning further minimized resource consumption (979.56 KB Flash, 68.6% Top-1 accuracy) while reducing inference energy by 70.3% compared to high-power operation. Critically, experimental results demonstrate a tunable accuracy&#8211;efficiency tradeoff where energy savings scaled proportionally to pruning intensity without catastrophic accuracy collapse. These results confirm that our energy-adaptive approach dynamically optimizes computational resource allocation in response to power fluctuations&#8212;a critical capability for sustainable operation in volatile mine environments.</p><p><xref rid="sensors-25-05486-f006" ref-type="fig">Figure 6</xref> shows the performance of several pruning strategies on the Mini-ImageNet dataset [<xref rid="B25-sensors-25-05486" ref-type="bibr">25</xref>], measured by Top-1 and Top-5 accuracy. Under high-energy conditions, EADP-Net achieves a Top-1 accuracy of 79.7% and a Top-5 accuracy of 86.7%, outperforming Meta-Pruning [<xref rid="B17-sensors-25-05486" ref-type="bibr">17</xref>] (70.8%/73.3%) and SNIP-FSL [<xref rid="B18-sensors-25-05486" ref-type="bibr">18</xref>] (73.5%/74.8%). Specifically, EADP-Net also demonstrates good performance when the energy budget is reduced. In the medium-energy states, its Top-1 accuracy reaches 73.6% and Top-5 accuracy reaches 78.2%, matching or surpassing the comparison methods in terms of Top-5 accuracy and maintaining effective competitiveness in terms of Top-1 accuracy. Even under the low-energy conditions, EADP-Net maintains a reasonable Top-1 accuracy of 68.6% and Top-5 accuracy of 75.3%, demonstrating its robust performance under strict resource constraints. These results indicate that EADP-Net offers a favorable tradeoff between accuracy and efficiency across different operating points, making it suitable for deployment scenarios with varying energy budgets.</p></sec><sec id="sec4dot2dot3-sensors-25-05486"><title>4.2.3. Robustness Evaluation Across Heterogeneous Datasets</title><p>To assess the model&#8217;s generalization capabilities under domain shift, we evaluated the model&#8217;s performance across different datasets: Mini-ImageNet [<xref rid="B25-sensors-25-05486" ref-type="bibr">25</xref>] (with 60,000 images divided into 100 classes), Tiny-ImageNet [<xref rid="B26-sensors-25-05486" ref-type="bibr">26</xref>] (with 100,000 images divided into 200 classes), and our purpose-built coal mine dataset. The coal mine dataset is constructed based on the CUMT-CMUID [<xref rid="B27-sensors-25-05486" ref-type="bibr">27</xref>] underground coal mine image dataset and the CUMT-HelmeT [<xref rid="B28-sensors-25-05486" ref-type="bibr">28</xref>] underground safety helmet detection dataset, utilizing high-resolution images captured by KBA12(B) intrinsically safe alarm cameras in multiple underground coal mines. It encompasses diverse scenarios such as underground tunnels, workshops, and coal mine conveyor belts. The high-resolution images typically contain substantial background information irrelevant to the task, imposing significant computational burdens on low-power devices. Consequently, to accommodate the resource constraints of the STM32 development board, we cropped the images to retain task-relevant regions, such as personnel, equipment, and tunnels. The resulting cropped coal mine dataset comprises 1000 images, each with a resolution of 128 &#215; 128 pixels, categorized into four classes: Person, Helmet-person, Tunnel, and Belt-foreign matter. This approach reduces computational load while enhancing image processing efficiency, enabling the dataset to better suit the processing requirements of low-power devices.</p><p>In order to prove the inference accuracy of MineVisual under different energy modes, the coal mine dataset was evaluated together with Mini-ImageNet and Tiny-ImageNet. As quantified in <xref rid="sensors-25-05486-t004" ref-type="table">Table 4</xref> and <xref rid="sensors-25-05486-f007" ref-type="fig">Figure 7</xref>, under high-energy conditions, the inference accuracy of MineVisual achieved a Top-1 accuracy of 91.5% on the coal mine dataset, outperforming Mini-ImageNet (79.7%) and Tiny-ImageNet (71.2%), and thus indicating the strong adaptability and learning capability of our scheme for this specific condition. As the energy state decreased, the model&#8217;s accuracy gradually declined. However, even under low-energy conditions, the inference accuracy of MineVisual maintained a Top-1 accuracy of 85.2% on the coal mine dataset, demonstrating a robust performance for underground coal mine tasks.</p></sec><sec id="sec4dot2dot4-sensors-25-05486"><title>4.2.4. Quantization Impact on Deployment Efficiency</title><p>To facilitate effective deployment of the trained model on an STM32 development board, we employed the X-CUBE-AI tool to apply quantization at FP32 (32-bit floating-point), FP16 (16-bit floating-point), and Int8 (8-bit integer) precision levels. We then evaluated the model&#8217;s accuracy, memory usage, and inference latency on the Mini-ImageNet dataset under different power states.</p><p>As shown in <xref rid="sensors-25-05486-t005" ref-type="table">Table 5</xref>, the experimental results demonstrate that reducing the quantization precision significantly decreases both the model&#8217;s memory footprint and inference latency. For instance, under the high-power state, the FP32 quantized model exhibits the highest memory consumption (SRAM: 483.12 KB, Flash: 1610.56 KB), an inference latency of 357 ms in Mini-ImageNet (400 ms in the coal mine dataset), and a Top-1 accuracy of 79.7% in Mini-ImageNet (91.5% in coal mine dataset). Progressively lowering the quantization precision to FP16 and Int8 gradually reduces the memory usage and inference latency, while the accuracy experiences a slight decrease, yet remains reasonably good. Under the low-power state, the Int8 quantized model requires 250.17 KB of SRAM and 816.15 KB of Flash, achieves an inference latency of 193 ms in Mini-ImageNet (216 ms in coal mine dataset), and maintains a Top-1 accuracy of 66.2% in Mini-ImageNet (79.9% in coal mine dataset). These experiments validate that quantization effectively reduces memory requirements and inference latency while preserving acceptable performance levels under low-power conditions, thus demonstrating its efficacy for deploying deep learning models in resource-constrained environments.</p></sec><sec id="sec4dot2dot5-sensors-25-05486"><title>4.2.5. Ablation Study on Energy-Aware Pruning</title><p>To isolate the independent contribution of energy-aware pruning (EAP) from other optimizations such as quantization and depthwise separable convolutions (DSCs), we conducted controlled ablation experiments. Four configurations were considered: (A) DSC + FP32 without pruning, (B) DSC + FP32 with EAP, (C) DSC + INT8 without pruning, (D) DSC + INT8 with EAP. These experiments reveal the unique effect of pruning on accuracy, memory footprint, inference latency, and energy consumption.</p><p>The results in <xref rid="sensors-25-05486-t006" ref-type="table">Table 6</xref> demonstrate that quantization alone (A &#8594; C) reduces latency by 36.7% and Flash by 40.2% with a 5.1-point Top-1 accuracy drop. Pruning alone (A &#8594; B) reduces Flash by 34.4% (mid energy) with a 6.1-point Top-1 drop; aggressive pruning at low energy achieves a 39.2% Flash reduction with an 11.1-point drop. System-level measurements further show that, when combined with quantization, EAP reduces inference energy from 23.23 mJ (high) to 13.76 mJ (medium) and 6.89 mJ (low), while keeping end-to-end latency within 329&#8211;359 ms. These results confirm that EAP provides independent and tunable efficiency gains beyond quantization and DSC.</p></sec></sec><sec id="sec4dot3-sensors-25-05486"><title>4.3. System-Level Energy Efficiency Assessment</title><p><xref rid="sensors-25-05486-f008" ref-type="fig">Figure 8</xref> depicts the hardware equipment for low-power reasoning in coal mines. To evaluate the end-to-end energy efficiency of MineVisual under operational conditions, we measured the scheme&#8217;s power consumption during model inference using an STM32F746G-DISCO (manufactured by STMicroelectronics, made in Shenzhen, China) development board (320 KB SRAM/1 MB Flash) integrated with an Arducam Mini 2 MP Plus camera module (manufactured by Arducam, from Nanjing, China); an EFM32 Zero Gecko module (manufactured by Silicon Labs, from Austin, TX, USA) was employed for high-resolution power profiling. In order to balance energy and computational requirements, the low-power mine inference platform used an edge processor with neural network acceleration as the main controller, complemented by an infrared camera optimized for low-illumination imaging and a low-power LoRa radio for intermittent long-range telemetry, so that the overall scheme&#8217;s power budget is constrained below 2 W. For field deployment underground, all components are packaged within an explosion-proof, dustproof enclosure and installed using magnetic and snap-fit mechanisms for quick installation and removal on mine supports. Optical and mechanical front-end protections (a wind-driven dust shield and optical filter) are employed to mitigate coal dust contamination and improve image quality under weak lighting. The power subsystem is designed for plug-and-play autonomous operation. When the wind-driven generator produces sufficient voltage, the integrated power management unit permits self-excitation and the platform executes the complete cycle of data acquisition, on-device inference, and subsequent data transmission. This integrated hardware and measurement approach provides a realistic assessment of MineVisual&#8217;s energy performance and demonstrates feasibility for passive, low-power visual sensing in constrained underground environments. The model was quantized to INT8 to accommodate hardware constraints. Power metrics were evaluated across three energy modes, with key parameters defined as follows:<list list-type="simple"><list-item><label>(1)</label><p><bold>Inference Power</bold>: Active power during model execution;</p></list-item><list-item><label>(2)</label><p><bold>Inference Time</bold>: Duration per inference pass;</p></list-item><list-item><label>(3)</label><p><bold>Inference Energy</bold>: Total energy consumed during inference (E<sub>inf</sub> = P<sub>inf</sub> &#215; t<sub>inf</sub>);</p></list-item><list-item><label>(4)</label><p><bold>Total Runtime</bold>: End-to-end latency (image capture, preprocessing, inference);</p></list-item><list-item><label>(5)</label><p><bold>Total Energy</bold>: System-wide consumption (E<sub>total</sub> = P<sub>avg</sub> &#215; t<sub>total</sub>).</p></list-item></list></p><fig position="anchor" id="sensors-25-05486-f008" orientation="portrait"><label>Figure 8</label><caption><p>Hardware equipment for low-power reasoning in coal mines.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g008.jpg"/></fig><p>As illustrated in <xref rid="sensors-25-05486-t007" ref-type="table">Table 7</xref>, the high-power mode (<italic toggle="yes">p</italic> &#8805; 90 mW) yields 102.8 mW inference power, 226 ms inference time, and 28.95 mJ total energy consumption at 329 ms runtime. Under medium-power (40 mW &#8804; <italic toggle="yes">p</italic> &lt; 90 mW), these reduce to 68.1 mW and 21.24 mJ (202 ms). Critically, the low-power mode (<italic toggle="yes">p</italic> &lt; 40 mW) achieves 6.89 mJ inference energy (70.4% reduction vs. high power) and 13.49 mJ total energy despite an 8.1% runtime increase (359 ms) from non-inference overheads. Although the inference time decreases with lower-power modes, the time required for other stages (such as data sampling and preprocessing) increases, resulting in a slight overall increase in total runtime. This demonstrates MineVisual&#8217;s ability to maintain functionality during energy scarcity while reducing inference power by 65.3% versus the high-power mode. Despite a modest increase in inference time, overall energy efficiency improves, validating the effectiveness of quantization and power-aware mechanisms, and confirming that the energy generated by the wind power node meets the application requirements of our low-power device.</p><p>To provide a holistic view of the proposed system, we further summarize its system-level performance beyond the inference results. <xref rid="sensors-25-05486-t008" ref-type="table">Table 8</xref> highlights the end-to-end energy budget, energy harvesting range, communication protocol, functionality, payload characteristics, and detailed energy breakdown. This overview enables readers to better understand the practical feasibility and deployment conditions of the proposed self-powered LoRa sensing-inference system.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05486"><title>5. Conclusions</title><p>This paper introduces MineVisual, a novel battery-free visual perception system designed for underground coal mines, which enables sustainable deep learning inference under extreme energy constraints. By combining a customized wind energy harvester optimized for low-speed ventilation airflow and EADP-Net&#8212;a lightweight deep neural network employing depthwise separable convolutions&#8212;the system achieves substantial computational efficiency. An energy-aware adaptive pruning mechanism dynamically adjusts model complexity through layer-wise and neuron-wise pruning in response to real-time power variations, preserving high inference accuracy (e.g., 91.5% Top-1 under sufficient power) while significantly reducing energy consumption (as low as 6.89 mJ per inference under low power). The experimental results demonstrate robust performance across variable wind speeds (2.7&#8211;10 m/s) in critical safety applications such as personnel detection, helmet compliance monitoring, and tunnel deformation analysis. Although a slight decline in accuracy occurs under ultra-low-power conditions (e.g., 85.2% Top-1 at 35.7 mW), the system exhibits remarkable energy efficiency and operational resilience in harsh mining environments. This work establishes a foundational framework for self-adaptive, battery-free deep learning in resource-limited industrial settings, opening up new avenues for intelligent safety monitoring under extreme conditions. Practical challenges such as dust accumulation, mechanical wear, and intermittent wind availability remain to be addressed in future efforts, which will focus on enhanced energy harvester reliability, improved power management, supercapacitor integration, dust-proof designs, and further algorithm-level optimizations for better energy&#8211;accuracy tradeoffs.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.L.; methodology, M.L. and X.Y.; software, M.L.; validation, M.L., S.L. and Z.B.; investigation, Q.N. and S.C.; data curation, M.Y.; writing&#8212;original draft preparation, X.Y.; writing&#8212;review and editing, all authors; visualization, X.Y. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are available in a publicly accessible repository. Mini-ImageNet datasets: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/yaoyao-liu/mini-imagenet-tools">https://github.com/yaoyao-liu/mini-imagenet-tools</uri>, accessed on 2 May 2025; Tiny-ImageNet datasets: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/seshuad/IMagenet">https://github.com/seshuad/IMagenet</uri>, accessed on 7 May 2025; CUMT-CMUID datasets and CUMT-HelmeT datasets: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/CUMT-AIPR-Lab">https://github.com/CUMT-AIPR-Lab</uri>, accessed on 14 May 2025.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">TENGs</td><td align="left" valign="middle" rowspan="1" colspan="1">triboelectric nanogenerators</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NFC</td><td align="left" valign="middle" rowspan="1" colspan="1">near-field communication</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ML</td><td align="left" valign="middle" rowspan="1" colspan="1">machine learning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DNN</td><td align="left" valign="middle" rowspan="1" colspan="1">deep neural network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSC</td><td align="left" valign="middle" rowspan="1" colspan="1">depthwise separable convolution</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MSA</td><td align="left" valign="middle" rowspan="1" colspan="1">mean squared activation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ADC</td><td align="left" valign="middle" rowspan="1" colspan="1">analog-to-digital converter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FLOPs</td><td align="left" valign="middle" rowspan="1" colspan="1">floating-point operations</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05486"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Avila</surname><given-names>B.Y.L.</given-names></name><name name-style="western"><surname>Vazquez</surname><given-names>C.A.G.</given-names></name><name name-style="western"><surname>Baluja</surname><given-names>O.P.</given-names></name><name name-style="western"><surname>Cotfas</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>Cotfas</surname><given-names>P.A.</given-names></name></person-group><article-title>Energy harvesting techniques for wireless sensor networks: A systematic literature review</article-title><source>Energy Strategy Rev.</source><year>2025</year><volume>57</volume><fpage>101617</fpage><pub-id pub-id-type="doi">10.1016/j.esr.2024.101617</pub-id></element-citation></ref><ref id="B2-sensors-25-05486"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>Efficient electrical energy conversion strategies from triboelectric nanogenerators to practical applications: A review</article-title><source>Nano Energy</source><year>2024</year><volume>132</volume><fpage>110383</fpage><pub-id pub-id-type="doi">10.1016/j.nanoen.2024.110383</pub-id></element-citation></ref><ref id="B3-sensors-25-05486"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>J.</given-names></name></person-group><article-title>Hybrid harvesting of wind and wave energy based on triboelectric-piezoelectric nanogenerators</article-title><source>Sustain. Energy Technol. Assess.</source><year>2023</year><volume>60</volume><fpage>103466</fpage><pub-id pub-id-type="doi">10.1016/j.seta.2023.103466</pub-id></element-citation></ref><ref id="B4-sensors-25-05486"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>X.R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.H.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Various energy harvesting strategies and innovative applications of triboelectric-electromagnetic hybrid nanogenerators</article-title><source>J. Alloys Compd.</source><year>2024</year><volume>1009</volume><fpage>176941</fpage><pub-id pub-id-type="doi">10.1016/j.jallcom.2024.176941</pub-id></element-citation></ref><ref id="B5-sensors-25-05486"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lazaro</surname><given-names>A.</given-names></name><name name-style="western"><surname>Villarino</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lazaro</surname><given-names>M.</given-names></name><name name-style="western"><surname>Canellas</surname><given-names>N.</given-names></name><name name-style="western"><surname>Prieto-Simon</surname><given-names>B.</given-names></name><name name-style="western"><surname>Girbau</surname><given-names>D.</given-names></name></person-group><article-title>Recent advances in batteryless NFC sensors for chemical sensing and biosensing</article-title><source>Biosensors</source><year>2023</year><volume>13</volume><elocation-id>775</elocation-id><pub-id pub-id-type="doi">10.3390/bios13080775</pub-id><pub-id pub-id-type="pmid">37622861</pub-id><pub-id pub-id-type="pmcid">PMC10452174</pub-id></element-citation></ref><ref id="B6-sensors-25-05486"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mirza</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>W.U.</given-names></name><name name-style="western"><surname>Ihsan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Han</surname><given-names>Z.</given-names></name></person-group><article-title>The state of AI-empowered backscatter communications: A comprehensive survey</article-title><source>IEEE Internet Things J.</source><year>2023</year><volume>10</volume><fpage>21763</fpage><lpage>21786</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2023.3299210</pub-id></element-citation></ref><ref id="B7-sensors-25-05486"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cui</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>X.</given-names></name></person-group><article-title>Bio-inspired structures for energy harvesting self-powered sensing and smart monitoring</article-title><source>Mech. Syst. Signal Process.</source><year>2025</year><volume>228</volume><fpage>112459</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2025.112459</pub-id></element-citation></ref><ref id="B8-sensors-25-05486"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cai</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Battery-free wireless sensor networks: A comprehensive survey</article-title><source>IEEE Internet Things J.</source><year>2022</year><volume>10</volume><fpage>5543</fpage><lpage>5570</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2022.3222386</pub-id></element-citation></ref><ref id="B9-sensors-25-05486"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Berbille</surname><given-names>A.</given-names></name><name name-style="western"><surname>Willatzen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><etal/></person-group><article-title>Self-powered sensing platform based on triboelectric nanogenerators towards intelligent mining industry</article-title><source>Nat. Commun.</source><year>2025</year><volume>16</volume><fpage>5141</fpage><pub-id pub-id-type="doi">10.1038/s41467-025-60418-9</pub-id><pub-id pub-id-type="pmid">40461480</pub-id><pub-id pub-id-type="pmcid">PMC12134231</pub-id></element-citation></ref><ref id="B10-sensors-25-05486"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Belle</surname><given-names>B.</given-names></name></person-group><article-title>Real-time air velocity monitoring in mines a quintessential design parameter for managing major mine health and safety hazards</article-title><source>Proceedings of the 13th Coal Operators&#8217; Conference</source><conf-loc>Warsaw, Poland</conf-loc><conf-date>14&#8211;15 February 2013</conf-date><fpage>184</fpage><lpage>198</lpage></element-citation></ref><ref id="B11-sensors-25-05486"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yeboah</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ackor</surname><given-names>N.</given-names></name><name name-style="western"><surname>Abrowah</surname><given-names>E.</given-names></name></person-group><article-title>Evaluation of wind energy recovery from an underground mine exhaust ventilation system</article-title><source>J. Eng.</source><year>2023</year><volume>2023</volume><fpage>8822475</fpage><pub-id pub-id-type="doi">10.1155/2023/8822475</pub-id></element-citation></ref><ref id="B12-sensors-25-05486"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fazlizan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chong</surname><given-names>W.T.</given-names></name><name name-style="western"><surname>Yip</surname><given-names>S.Y.</given-names></name><name name-style="western"><surname>Hew</surname><given-names>W.P.</given-names></name><name name-style="western"><surname>Poh</surname><given-names>S.C.</given-names></name></person-group><article-title>Design and experimental analysis of an exhaust air energy recovery wind turbine generator</article-title><source>Energies</source><year>2015</year><volume>8</volume><fpage>6566</fpage><lpage>6584</lpage><pub-id pub-id-type="doi">10.3390/en8076566</pub-id></element-citation></ref><ref id="B13-sensors-25-05486"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>L.C.</given-names></name><name name-style="western"><surname>Zou</surname><given-names>H.X.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>K.X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>S.X.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.M.</given-names></name></person-group><article-title>Mechanical intelligent energy harvesting: From methodology to applications</article-title><source>Adv. Energy Mater.</source><year>2023</year><volume>13</volume><fpage>2300557</fpage><pub-id pub-id-type="doi">10.1002/aenm.202300557</pub-id></element-citation></ref><ref id="B14-sensors-25-05486"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zishan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Molla</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Rashid</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>K.H.</given-names></name><name name-style="western"><surname>Fazlizan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lipu</surname><given-names>M.S.H.</given-names></name><name name-style="western"><surname>Tariq</surname><given-names>M.</given-names></name><name name-style="western"><surname>Alsalami</surname><given-names>O.M.</given-names></name><name name-style="western"><surname>Sarker</surname><given-names>M.R.</given-names></name></person-group><article-title>Comprehensive analysis of kinetic energy recovery systems for efficient energy harnessing from unnaturally generated wind sources</article-title><source>Sustainability</source><year>2023</year><volume>15</volume><elocation-id>15345</elocation-id><pub-id pub-id-type="doi">10.3390/su152115345</pub-id></element-citation></ref><ref id="B15-sensors-25-05486"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chollet</surname><given-names>F.</given-names></name></person-group><article-title>Xception: Deep learning with depthwise separable convolutions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>1251</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.195</pub-id></element-citation></ref><ref id="B16-sensors-25-05486"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nazari</surname><given-names>N.</given-names></name><name name-style="western"><surname>Salehi</surname><given-names>M.E.</given-names></name></person-group><article-title>Inter-layer hybrid quantization scheme for hardware friendly implementation of embedded deep neural networks</article-title><source>Proceedings of the Great Lakes Symposium on VLSI 2023 (GLSVLSI)</source><conf-loc>Knoxville, TN, USA</conf-loc><conf-date>5&#8211;7 June 2023</conf-date><fpage>193</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1145/3583781.3590316</pub-id></element-citation></ref><ref id="B17-sensors-25-05486"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Meta-pruning: Learning to Prune on Few-Shot Learning</article-title><source>Proceedings of the 17th International Conference on Knowledge Science, Engineering and Management (KSEM)</source><conf-loc>Birmingham, UK</conf-loc><conf-date>16&#8211;18 August 2024</conf-date><fpage>74</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1007/978-981-97-5492-2_6</pub-id></element-citation></ref><ref id="B18-sensors-25-05486"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>H.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>Y.</given-names></name></person-group><article-title>SNIP-FSL: Finding task-specific lottery jackpots for few-shot learning</article-title><source>Knowl. Based Syst.</source><year>2022</year><volume>247</volume><fpage>108427</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2022.108427</pub-id></element-citation></ref><ref id="B19-sensors-25-05486"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Afzal</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Akbar</surname><given-names>W.</given-names></name><name name-style="western"><surname>Rodriguez</surname><given-names>O.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Boyle</surname><given-names>D.</given-names></name><name name-style="western"><surname>Adib</surname><given-names>F.</given-names></name><name name-style="western"><surname>Haddadi</surname><given-names>H.</given-names></name></person-group><article-title>Towards battery-free machine learning and inference in underwater environments</article-title><source>Proceedings of the 23rd Annual International Workshop on Mobile Computing Systems and Applications (HotMobile &#8217;22)</source><conf-loc>Tempe, AZ, USA</conf-loc><conf-date>9&#8211;10 March 2022</conf-date><fpage>29</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1145/3508396.3512877</pub-id></element-citation></ref><ref id="B20-sensors-25-05486"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Afzal</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Akbar</surname><given-names>W.</given-names></name><name name-style="western"><surname>Rodriguez</surname><given-names>O.</given-names></name><name name-style="western"><surname>Doumet</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ha</surname><given-names>U.</given-names></name><name name-style="western"><surname>Ghaffarivardavagh</surname><given-names>R.</given-names></name><name name-style="western"><surname>Adib</surname><given-names>F.</given-names></name></person-group><article-title>Battery-free wireless imaging of underwater environments</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>5546</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-33223-x</pub-id><pub-id pub-id-type="pmid">36163186</pub-id><pub-id pub-id-type="pmcid">PMC9512789</pub-id></element-citation></ref><ref id="B21-sensors-25-05486"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>K.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>X.</given-names></name></person-group><article-title>Stretchable self-powered TENG sensor array for human robot interaction based on conductive ionic gels and LSTM neural network</article-title><source>IEEE Sens. J</source><year>2024</year><volume>24</volume><fpage>37962</fpage><lpage>37969</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3464633</pub-id></element-citation></ref><ref id="B22-sensors-25-05486"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>State Administration of Coal Mine Safety</collab></person-group><source>Coal Mine Safety Regulation</source><publisher-name>State Administration of Coal Mine Safety</publisher-name><publisher-loc>Beijing, China</publisher-loc><year>2022</year><fpage>67</fpage></element-citation></ref><ref id="B23-sensors-25-05486"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name></person-group><article-title>Review of Lightweight Deep Convolutional Neural Networks</article-title><source>Arch. Comput. Method Eng.</source><year>2024</year><volume>31</volume><fpage>1915</fpage><lpage>1937</lpage><pub-id pub-id-type="doi">10.1007/s11831-023-10032-z</pub-id></element-citation></ref><ref id="B24-sensors-25-05486"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.I.</given-names></name><name name-style="western"><surname>Galindo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wong</surname><given-names>L.K.</given-names></name><name name-style="western"><surname>Shuai</surname><given-names>H.H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.H.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>W.H.</given-names></name></person-group><article-title>Lightweight deep learning for resource-constrained environments: A survey</article-title><source>ACM Comput. Surv.</source><year>2024</year><volume>56</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.1145/3657282</pub-id></element-citation></ref><ref id="B25-sensors-25-05486"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vinyals</surname><given-names>O.</given-names></name><name name-style="western"><surname>Blundell</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lillicrap</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kavukcuoglu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wierstra</surname><given-names>D.</given-names></name></person-group><article-title>Matching networks for one shot learning</article-title><source>Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS)</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>5&#8211;10 December 2016</conf-date></element-citation></ref><ref id="B26-sensors-25-05486"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Le</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>Tiny imagenet visual recognition challenge</article-title><source>CS 231N</source><year>2015</year><volume>7</volume><fpage>3</fpage></element-citation></ref><ref id="B27-sensors-25-05486"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>D.Q.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Kou</surname><given-names>Q.Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>C.G.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>J.S.</given-names></name></person-group><article-title>Lightweight network based on residual information for foreign body classification on coal conveyor belt</article-title><source>J. China Coal Soc.</source><year>2022</year><volume>47</volume><fpage>1361</fpage><lpage>1369</lpage><pub-id pub-id-type="doi">10.13225/j.cnki.jccs.xr21.1736</pub-id></element-citation></ref><ref id="B28-sensors-25-05486"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Lightweight super resolution method based on blueprint separable convolution for mine image</article-title><source>J. China Coal Soc.</source><year>2024</year><volume>49</volume><fpage>4038</fpage><lpage>4050</lpage><pub-id pub-id-type="doi">10.13225/j.cnki.jccs.2023.1101</pub-id></element-citation></ref><ref id="B29-sensors-25-05486"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.Q.</given-names></name></person-group><article-title>A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>10558</fpage><lpage>10578</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2024.3447085</pub-id><pub-id pub-id-type="pmid">39167504</pub-id></element-citation></ref><ref id="B30-sensors-25-05486"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Augustin</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Clausen</surname><given-names>T.H.</given-names></name><name name-style="western"><surname>Townsley</surname><given-names>W.M.</given-names></name></person-group><article-title>A Study of LoRa: Long Range &amp; Low Power Networks for the Internet of Things</article-title><source>Sensors</source><year>2016</year><volume>16</volume><elocation-id>1466</elocation-id><pub-id pub-id-type="doi">10.3390/s16091466</pub-id><pub-id pub-id-type="pmid">27618064</pub-id><pub-id pub-id-type="pmcid">PMC5038744</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05486-f001" orientation="portrait"><label>Figure 1</label><caption><p>Working process of the battery-free visual perception scheme in coal mines.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g001.jpg"/></fig><fig position="float" id="sensors-25-05486-f002" orientation="portrait"><label>Figure 2</label><caption><p>Energy-aware dynamic pruning network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g002.jpg"/></fig><fig position="float" id="sensors-25-05486-f003" orientation="portrait"><label>Figure 3</label><caption><p>Energy-aware optimized depthwise separable convolution block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g003.jpg"/></fig><fig position="float" id="sensors-25-05486-f004" orientation="portrait"><label>Figure 4</label><caption><p>Offline training and on-board inference. (<bold>a</bold>): offline training: dataset preprocessing &#8594; model pruning &#8594; quantization/compression; (<bold>b</bold>): on-board inference: wind energy harvesting &#8594; image capture &#8594; EADP-Net adaptive inference &#8594; safety alerts.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g004.jpg"/></fig><fig position="float" id="sensors-25-05486-f005" orientation="portrait"><label>Figure 5</label><caption><p>Wind power generation device and its power curve with wind speed. (<bold>a</bold>): The fan blade and generator of the wind power generation; (<bold>b</bold>): voltage&#8211;power curve with wind speed and supercapacitor charging profile.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g005a.jpg"/><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g005b.jpg"/></fig><fig position="float" id="sensors-25-05486-f006" orientation="portrait"><label>Figure 6</label><caption><p>Comparison experiments of different pruning strategies on the Mini-ImageNet dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g006.jpg"/></fig><fig position="float" id="sensors-25-05486-f007" orientation="portrait"><label>Figure 7</label><caption><p>Inference accuracy of MineVisual on three different datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05486-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05486-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05486-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison with other battery-free sensing methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Underwater Backscatter Imaging<break/>[<xref rid="B19-sensors-25-05486" ref-type="bibr">19</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Underwater Battery-Free ML<break/>[<xref rid="B20-sensors-25-05486" ref-type="bibr">20</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TESS [<xref rid="B9-sensors-25-05486" ref-type="bibr">9</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">TENG-SA [<xref rid="B21-sensors-25-05486" ref-type="bibr">21</xref>]</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MineVisual<break/>(Ours)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Primary Feature</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wireless underwater imaging</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Marine/underwater environmental monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Air quality monitoring in gold mines</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Human motion monitoring and human&#8211;robot interaction (HRI)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Safety monitoring in coal mines</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Energy Source</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Underwater sound </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Underwater sound </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wind energy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mechanical energy </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wind energy</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Communication Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic backscatter </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustic backscatter </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LoRa</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bluetooth</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LoRa</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Processing and Intelligence</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">On-board processing unit (FPGA) controls image capture, data packetization, and communication</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Small CNNs trained offline, Keras2C implementation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A data acquisition module aggregates and compiles data from multiple sensors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSTM-based network with adjustable complexity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">On-device deep learning inference with a lightweight model (EADP-Net) </td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Key Performance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average power: ~276 &#181;W (active imaging) <break/> data rate: ~1 kbps</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total energy/inference: 5.40 mJ <break/> accuracy: ~63% (small sample dataset)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Starting windspeed: 0.32 m/s <break/> power density: 16.36 mW/m<sup>2</sup>
<break/> lifetime: &gt;3 months</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Output voltage range: 1.8&#8211;10.5 V; <break/> recognition accuracy: ~90%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inference energy: 6.89 mJ (low power) <break/> accuracy: 68.6&#8211;91.5% (top-1)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Target Environment</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Underwater environments</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Underwater environments</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Underground gold mines</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wearable, on-body applications</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Underground coal mines</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05486-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05486-t002_Table 2</object-id><label>Table 2</label><caption><p>Different energy states based on real-time generator output.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Energy State</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Real-Time Power Output</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">p</italic> &gt; 90 mW</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">40 mW &lt; <italic toggle="yes">p</italic> &lt; 90 mW</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><italic toggle="yes">p</italic> &lt; 40 mW</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05486-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05486-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparative experiments conducted on network-layer pruning, neuron pruning, and joint network-layer and neuron pruning in three different energy states.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Energy Status</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SRAM</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Flash</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Top-1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Top-5</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(M)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(M)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(KB)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(KB)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Original</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">483.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1610.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.7</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Pruning<break/>(Layer)</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38</td><td align="center" valign="middle" rowspan="1" colspan="1">29.86</td><td align="center" valign="middle" rowspan="1" colspan="1">457.23</td><td align="center" valign="middle" rowspan="1" colspan="1">1402.1</td><td align="center" valign="middle" rowspan="1" colspan="1">78.6</td><td align="center" valign="middle" rowspan="1" colspan="1">83.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">387.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1186.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.1</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Pruning<break/>(Neurons)</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">31.47</td><td align="center" valign="middle" rowspan="1" colspan="1">468.53</td><td align="center" valign="middle" rowspan="1" colspan="1">1215.36</td><td align="center" valign="middle" rowspan="1" colspan="1">78.1</td><td align="center" valign="middle" rowspan="1" colspan="1">83.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.82</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">363.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1076.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.7</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Pruning<break/>(Layer-Neurons)</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">19.27</td><td align="center" valign="middle" rowspan="1" colspan="1">359.56</td><td align="center" valign="middle" rowspan="1" colspan="1">1056.35</td><td align="center" valign="middle" rowspan="1" colspan="1">73.6</td><td align="center" valign="middle" rowspan="1" colspan="1">78.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">312.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">979.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.3</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05486-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05486-t004_Table 4</object-id><label>Table 4</label><caption><p>Inference accuracy of MineVisual on three different datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Energy Status</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Mini-ImageNet</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Tiny-ImageNet</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Coal Mine Dataset</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Top-1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Top-5 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Top-1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Top-5 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Top-1 (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">79.7</td><td align="center" valign="middle" rowspan="1" colspan="1">86.7</td><td align="center" valign="middle" rowspan="1" colspan="1">71.2</td><td align="center" valign="middle" rowspan="1" colspan="1">76.5</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">73.6</td><td align="center" valign="middle" rowspan="1" colspan="1">78.2</td><td align="center" valign="middle" rowspan="1" colspan="1">67.2</td><td align="center" valign="middle" rowspan="1" colspan="1">71.4</td><td align="center" valign="middle" rowspan="1" colspan="1">87.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05486-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05486-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison experiments with different quantization accuracies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Quant</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Energy Status</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">SRAM (KB)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Flash (KB)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Inference Latency (ms)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Top-1 (%)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MINI-ImageNet</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Coal Mine</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MINI-ImageNet</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Coal Mine</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">FP32</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">483.12</td><td align="center" valign="middle" rowspan="1" colspan="1">1610.56</td><td align="center" valign="middle" rowspan="1" colspan="1">357</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">79.7</td><td align="center" valign="middle" rowspan="1" colspan="1">91.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">359.56</td><td align="center" valign="middle" rowspan="1" colspan="1">1056.35</td><td align="center" valign="middle" rowspan="1" colspan="1">289</td><td align="center" valign="middle" rowspan="1" colspan="1">324</td><td align="center" valign="middle" rowspan="1" colspan="1">73.6</td><td align="center" valign="middle" rowspan="1" colspan="1">87.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">312.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">979.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">248</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">278</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.2</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">FP16</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">356.17</td><td align="center" valign="middle" rowspan="1" colspan="1">1214.32</td><td align="center" valign="middle" rowspan="1" colspan="1">276</td><td align="center" valign="middle" rowspan="1" colspan="1">309</td><td align="center" valign="middle" rowspan="1" colspan="1">77.3</td><td align="center" valign="middle" rowspan="1" colspan="1">88.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">327.71</td><td align="center" valign="middle" rowspan="1" colspan="1">968.13</td><td align="center" valign="middle" rowspan="1" colspan="1">266</td><td align="center" valign="middle" rowspan="1" colspan="1">298</td><td align="center" valign="middle" rowspan="1" colspan="1">76.5</td><td align="center" valign="middle" rowspan="1" colspan="1">85.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">294.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">912.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">215</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">241</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.7</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Int8</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">287.5</td><td align="center" valign="middle" rowspan="1" colspan="1">963.12</td><td align="center" valign="middle" rowspan="1" colspan="1">226</td><td align="center" valign="middle" rowspan="1" colspan="1">253</td><td align="center" valign="middle" rowspan="1" colspan="1">74.6</td><td align="center" valign="middle" rowspan="1" colspan="1">86</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">274.36</td><td align="center" valign="middle" rowspan="1" colspan="1">888.32</td><td align="center" valign="middle" rowspan="1" colspan="1">202</td><td align="center" valign="middle" rowspan="1" colspan="1">226</td><td align="center" valign="middle" rowspan="1" colspan="1">73.2</td><td align="center" valign="middle" rowspan="1" colspan="1">82.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">250.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">816.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">193</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">216</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.9</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05486-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05486-t006_Table 6</object-id><label>Table 6</label><caption><p>Ablation experiment on energy-aware pruning.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Config</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Quant.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pruning</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Top-1 (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Top-5 (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Flash (KB)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SRAM (KB)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Latency (ms)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notes</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">A</td><td align="center" valign="middle" rowspan="1" colspan="1">FP32</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">79.7</td><td align="center" valign="middle" rowspan="1" colspan="1">86.7</td><td align="center" valign="middle" rowspan="1" colspan="1">1610.56</td><td align="center" valign="middle" rowspan="1" colspan="1">483.12</td><td align="center" valign="middle" rowspan="1" colspan="1">357</td><td align="center" valign="middle" rowspan="1" colspan="1">Baseline-FP32 (High)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">B-M</td><td align="center" valign="middle" rowspan="1" colspan="1">FP32</td><td align="center" valign="middle" rowspan="1" colspan="1">EAP (mid)</td><td align="center" valign="middle" rowspan="1" colspan="1">73.6</td><td align="center" valign="middle" rowspan="1" colspan="1">78.2</td><td align="center" valign="middle" rowspan="1" colspan="1">1056.35</td><td align="center" valign="middle" rowspan="1" colspan="1">359.56</td><td align="center" valign="middle" rowspan="1" colspan="1">289</td><td align="center" valign="middle" rowspan="1" colspan="1">Pure pruning (Medium)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">B-L</td><td align="center" valign="middle" rowspan="1" colspan="1">FP32</td><td align="center" valign="middle" rowspan="1" colspan="1">EAP (low)</td><td align="center" valign="middle" rowspan="1" colspan="1">68.6</td><td align="center" valign="middle" rowspan="1" colspan="1">75.3</td><td align="center" valign="middle" rowspan="1" colspan="1">979.56</td><td align="center" valign="middle" rowspan="1" colspan="1">312.78</td><td align="center" valign="middle" rowspan="1" colspan="1">280</td><td align="center" valign="middle" rowspan="1" colspan="1">Pure pruning (Low)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C</td><td align="center" valign="middle" rowspan="1" colspan="1">INT8</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10007;</td><td align="center" valign="middle" rowspan="1" colspan="1">74.6</td><td align="center" valign="middle" rowspan="1" colspan="1">79.3</td><td align="center" valign="middle" rowspan="1" colspan="1">963.12</td><td align="center" valign="middle" rowspan="1" colspan="1">287.50</td><td align="center" valign="middle" rowspan="1" colspan="1">226</td><td align="center" valign="middle" rowspan="1" colspan="1">Quantization only (High)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D-M</td><td align="center" valign="middle" rowspan="1" colspan="1">INT8</td><td align="center" valign="middle" rowspan="1" colspan="1">EAP (mid)</td><td align="center" valign="middle" rowspan="1" colspan="1">69.5</td><td align="center" valign="middle" rowspan="1" colspan="1">72.3</td><td align="center" valign="middle" rowspan="1" colspan="1">631.81</td><td align="center" valign="middle" rowspan="1" colspan="1">213.97</td><td align="center" valign="middle" rowspan="1" colspan="1">183</td><td align="center" valign="middle" rowspan="1" colspan="1">(INT8 + Pruning, Medium)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D-L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">INT8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EAP (low)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">585.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">186.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">177</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(INT8 + Pruning, Low)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05486-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05486-t007_Table 7</object-id><label>Table 7</label><caption><p>Energy consumption analysis for three different energy states.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Energy Status</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Inference Power</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Inference Time</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Inference Energy</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Total Runtime</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Total Energy</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(mW)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(ms)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(mJ)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(ms)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(mJ)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">102.8</td><td align="center" valign="middle" rowspan="1" colspan="1">226</td><td align="center" valign="middle" rowspan="1" colspan="1">23.23</td><td align="center" valign="middle" rowspan="1" colspan="1">329</td><td align="center" valign="middle" rowspan="1" colspan="1">28.95</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">68.1</td><td align="center" valign="middle" rowspan="1" colspan="1">202</td><td align="center" valign="middle" rowspan="1" colspan="1">13.76</td><td align="center" valign="middle" rowspan="1" colspan="1">332</td><td align="center" valign="middle" rowspan="1" colspan="1">21.24</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">193</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">359</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.49</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05486-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05486-t008_Table 8</object-id><label>Table 8</label><caption><p>System-level summary of the proposed self-powered LoRa sensing-inference system.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value/Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">End-to-end energy per cycle</td><td align="center" valign="middle" rowspan="1" colspan="1">~30&#8211;35 mJ (including sensing, inference, and LoRa transmission)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Harvesting range</td><td align="center" valign="middle" rowspan="1" colspan="1">Self-sustainable at wind speeds &#8805; 3.7 m/s (tested with micro-turbine energy harvester)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wireless protocol</td><td align="center" valign="middle" rowspan="1" colspan="1">LoRa (868/915 MHz band), max payload 51 bytes per packet</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Functionality</td><td align="center" valign="middle" rowspan="1" colspan="1">Image sensing (low-resolution grayscale), on-device EAP-Net inference, LoRa transmission of results</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Payload size</td><td align="center" valign="middle" rowspan="1" colspan="1">~32 KB image before compression; inference results compressed to &lt;1 KB for LoRa</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Energy breakdown</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensing: ~3&#8211;5 mJ; inference: ~6.9&#8211;23.2 mJ (depending on pruning level); transmission: ~5&#8211;7 mJ</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>