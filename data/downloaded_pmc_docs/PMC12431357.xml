<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431357</article-id><article-id pub-id-type="pmcid-ver">PMC12431357.1</article-id><article-id pub-id-type="pmcaid">12431357</article-id><article-id pub-id-type="pmcaiid">12431357</article-id><article-id pub-id-type="doi">10.3390/s25175232</article-id><article-id pub-id-type="publisher-id">sensors-25-05232</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Advancing Remote Life Sensing for Search and Rescue: A Novel Framework for Precise Vital Signs Detection via Airborne UWB Radar</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Jing</surname><given-names initials="Y">Yu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="fn1-sensors-25-05232" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yan</surname><given-names initials="Y">Yili</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="fn1-sensors-25-05232" ref-type="author-notes">&#8224;</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="Z">Zhao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Qi</surname><given-names initials="F">Fugui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Lei</surname><given-names initials="T">Tao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="J">Jianqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6421-0232</contrib-id><name name-style="western"><surname>Lu</surname><given-names initials="G">Guohua</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-05232" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>L&#225;zaro</surname><given-names initials="A">Antonio</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Chen</surname><given-names initials="C">Chao</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Chen</surname><given-names initials="J">Jinchao</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Zhang</surname><given-names initials="Y">Yingjie</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05232">Department of Military Biomedical Engineering, Air Force Medical University, Xi&#8217;an 710032, China; <email>jingyu1998@fmmu.edu.cn</email> (Y.J.); <email>richard1207@163.com</email> (Y.Y.); <email>lizhaofmmu@fmmu.edu.cn</email> (Z.L.); <email>qifg1992@fmmu.edu.cn</email> (F.Q.); <email>leitaoxman@fmmu.edu.cn</email> (T.L.); <email>wangjq@fmmu.edu.cn</email> (J.W.)</aff><author-notes><corresp id="c1-sensors-25-05232"><label>*</label>Correspondence: <email>lugh1976@fmmu.edu.cn</email>; Tel.: +86-29-84711471</corresp><fn id="fn1-sensors-25-05232"><label>&#8224;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>22</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5232</elocation-id><history><date date-type="received"><day>17</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>25</day><month>7</month><year>2025</year></date><date date-type="accepted"><day>20</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>22</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 13:25:28.783"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05232.pdf"/><abstract><sec sec-type="highlights"><title>Highlights</title><p>
<bold>What are the main findings?</bold>
<list list-type="bullet"><list-item><p>An airborne bio-radar system to remotely sense vital signs of survivors for post-disaster search and rescue was developed.</p></list-item><list-item><p>Theoretical analysis of the impact of interference coming from the motion of the UAV platform and echoes from the background environment on radar detection performance.</p></list-item></list>
</p><p>
<bold>What is the implication of the main finding?</bold>
<list list-type="bullet"><list-item><p>A signal processing framework based on blind source separation was proposed to precisely extract the respiration and heartbeat, which combines the high-order analytical tool and the feedback notch filter.</p></list-item><list-item><p>The remote high-resolution vital signs detection approach is suitable for real-world applications such as search and rescue.</p></list-item></list>
</p></sec><sec><title>Abstract</title><p>Non-contact vital signs detection of the survivors based on bio-radar to identify their life states is significant for field search and rescue. However, when transportation is interrupted, rescue workers and equipment are unable to arrive at the disaster area promptly. In this paper, we report a hovering airborne radar for non-contact vital signs detection to overcome this challenge. The airborne radar system supports a wireless data link, enabling remote control and communication over distances of up to 3 km. In addition, a novel framework based on blind source separation is proposed for vital signals extraction. First, range migration caused by the platform motion is compensated for by the envelope alignment. Then, the respiratory waveform of the human target is extracted by the joint approximative diagonalization of eigenmatrices algorithm. Finally, the heartbeat signal is recovered by respiratory harmonic suppression through a feedback notch filter. The field experiment results demonstrate that the proposed method is capable of precisely extracting vital signals with outstanding robustness and adaptation in more cluttered environments. The work provides a technical basis for remote high-resolution vital signs detection to meet the increasing demands of actual rescue applications.</p></sec></abstract><kwd-group><kwd>airborne radar</kwd><kwd>vital signs detection</kwd><kwd>remote sensing</kwd><kwd>search and rescue</kwd></kwd-group><funding-group><award-group><funding-source>Chinese Comprehensive Research Foundation</funding-source><award-id>KJ-2022-A000308</award-id></award-group><award-group><funding-source>Interdisciplinary Research Fund of Fourth Military Medical University</funding-source><award-id>XJ-2023-000201</award-id></award-group><funding-statement>This work was supported by the Chinese Comprehensive Research Foundation (KJ-2022-A000308), and the Interdisciplinary Research Fund of Fourth Military Medical University (XJ-2023-000201).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05232"><title>1. Introduction</title><p>Wild search and rescue (SAR) is one of the emphases and difficulties of current emergency rescue medicine [<xref rid="B1-sensors-25-05232" ref-type="bibr">1</xref>]. In harsh post-disaster environments, the road is damaged, the traffic is interrupted, the disaster area is vast, and the distribution of the survivors is unknown, which will pose challenges to the deployment of SAR work [<xref rid="B2-sensors-25-05232" ref-type="bibr">2</xref>]. In recent years, unmanned aerial vehicles (UAV) have been gradually used to assist rescue of the trapped hikers and missing persons, and some incidents have shown the specific capability and superiority of the UAVs in large-scale disaster risk reduction due to its flexibility, mobility, and no casualties [<xref rid="B3-sensors-25-05232" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05232" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05232" ref-type="bibr">5</xref>].</p><p>At present, sensors equipped by a UAV-based SAR system mainly include optical sensors (e.g., high-resolution camera [<xref rid="B6-sensors-25-05232" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05232" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05232" ref-type="bibr">8</xref>], thermal imaging camera [<xref rid="B9-sensors-25-05232" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05232" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05232" ref-type="bibr">11</xref>], and multi-spectral camera [<xref rid="B12-sensors-25-05232" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05232" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05232" ref-type="bibr">14</xref>]) and radio frequency sensors (e.g., bio-radar [<xref rid="B15-sensors-25-05232" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05232" ref-type="bibr">16</xref>]). The principles, advantages, and applicable scenarios of these sensors are different. The optical sensors identify targets based on the morphological features, the optical features, and the infrared radiation energy of the human body. However, optical sensors are susceptible to interference from environmental conditions, such as temperature, smoke, fog, and obstruction. Bio-radar senses the presence and physiological activities of humans by transmitting electromagnetic waves and analyzing the received echoes, which is the most effective way to detect vital signs under the above conditions.</p><p>Based on this, our previous study proposed an unmanned SAR scheme based on multi-source sensors [<xref rid="B17-sensors-25-05232" ref-type="bibr">17</xref>], which involves two stages. First, the three optical camera-based UAV systems were released to detect human targets and acquire their position information through global positioning system (GPS) by scanning the disaster-affected area. Second, the radar-based UAV system proceeds to the corresponding positions and hovers to sense physiological information of human targets. In addition, the radar-based UAV system could also find the unconscious victim under dense smoke or under the ruins as shown in <xref rid="sensors-25-05232-f001" ref-type="fig">Figure 1</xref>, which would provide the scientific basis for the formulation of SAR strategies and the allocation of rescue resources.</p><p>However, two challenges make the vital signs detection problem of the airborne radar system difficult. First, the UAV platform will inevitably generate relative movement, and human physiological signals are comparatively weak, which will be overwhelmed by the platform motion [<xref rid="B18-sensors-25-05232" ref-type="bibr">18</xref>]. Second, the background clutter from the environment where the human target is located will further interfere with the radar echoes. It means that in order to detect vital signs with this technology, we need to precisely separate the wanted signals from the aforementioned multiple interferences.</p><p>In recent years, several methods have been proposed to address the platform motion compensation problem. In [<xref rid="B18-sensors-25-05232" ref-type="bibr">18</xref>], Cardillo et al. first extracted the radar self-motion from the signals reflected by stationary objects through identifying the clutter range without any additional sensor. In [<xref rid="B19-sensors-25-05232" ref-type="bibr">19</xref>], Rong et al. proposed a method based on phase residual, which compensates the platform motion by calculating the residual phase between the human subject and the static background, and then decomposing the residual for vital signals extraction. In [<xref rid="B20-sensors-25-05232" ref-type="bibr">20</xref>], Zhang et al. proposed an algorithm to extract respiratory information of the victims by segmenting the UAV motion into multiple time intervals and computing the average energy ratio of the UAV motion signal and human echo signal in the selected frequency band to estimate the compensation coefficient. In [<xref rid="B21-sensors-25-05232" ref-type="bibr">21</xref>], a rotating radar UAV system was designed to locate the trapped individuals in collapsed buildings, and a new unwrapping algorithm was proposed to estimate and compensate for the UAV motion based on the reflected echoes. However, the methods mentioned above are mostly for indoor location and based on signals reflected by stationary objects, and the performance may degrade when there is no stationary object. Furthermore, the impact of the background clutter where the human target is located on the detection performance of the airborne radar system has not been thoroughly investigated.</p><p>In this paper, a portable airborne radar vital signs sensing system is developed for remote data collection, which adopts impulse&#8211;radio ultra-wide band (IR-UWB) radar and Mesh network technology. To address the aforementioned problems, the effect of the platform motion and the background noise on radar detection capacity are theoretically analyzed. Then, a signal processing method based on blind source separation is proposed to separate out vital signals from multiple interferences. First, range migration caused by the platform motion is coarsely compensated by the adjacent envelope alignment method based on cross correlation. Second, we analyze the statistical characteristics of the measured dynamic background clutter from the grassland scenario and find that it may obey the Gaussian distribution, or the mixed Gaussian distribution. In other words, it can be regarded as a Gaussian noise. Then, the respiratory waveform of the human target is extracted by the joint approximative diagonalization of eigenmatrices (JADE) algorithm, which is based on higher-order statistical analysis to remove Gaussian components. Finally, the heartbeat signal is recovered by respiratory harmonic suppression through a feedback notch filter. The performance of this method has been evaluated and tested by field experiments in different scenarios, and these results confirm that the proposed method improves the performance of airborne radar system without additional sensors.</p><p>The remainder of this paper is organized as follows. The airborne radar system composition is introduced in <xref rid="sec2-sensors-25-05232" ref-type="sec">Section 2</xref>. The signal model is described in <xref rid="sec3-sensors-25-05232" ref-type="sec">Section 3</xref>. The proposed method is deliberated in <xref rid="sec4-sensors-25-05232" ref-type="sec">Section 4</xref>. The experimental setup and results are presented in <xref rid="sec5-sensors-25-05232" ref-type="sec">Section 5</xref>. Finally, the conclusions and future works are given in <xref rid="sec6-sensors-25-05232" ref-type="sec">Section 6</xref>.</p></sec><sec id="sec2-sensors-25-05232"><title>2. Airborne Bio-Radar System Design</title><p>This section generally introduces the hardware and software design of the portable airborne radar system. <xref rid="sensors-25-05232-f002" ref-type="fig">Figure 2</xref> presents the hardware structure block diagram of the system, which consists of onboard end and ground receiving end. The hardware components of the onboard end include the sensor module, data processing center, and data transmission module. The ground receiving end is utilized to visualize sensor data and information of the casualty on the map.</p><p>The quadcopter UAV is equipped with three types of sensors: X4M200 IR-UWB radar, high-definition camera, and GPS. The HZHY-AI313 UAV onboard computer drives the three sensors for data acquisition, and it can process data simultaneously. Then, the collected data are transmitted to the ground receiving end through a Mesh network constructed by image transmission radio stations [<xref rid="B22-sensors-25-05232" ref-type="bibr">22</xref>].</p><p>The software control program of the system is developed based on the topic and service architecture of ROS2 (Robot Operating System). First, the ground control station operates the UAV to take off and runs the MAVROS program to obtain real-time position and attitude information of the UAV. After the UAV arrives at the search area, it hovers above the designated location, and then the sensor-driven node is operated to activate the UWB radar and visible light camera to recognize and perceive the injured target. Finally, the data transmission node transmits the position and attitude information of the UAV, the physiological information of the target, and the image information to the ground control station for further processing.</p><p>The pictures of the airborne radar system are shown in <xref rid="sensors-25-05232-f003" ref-type="fig">Figure 3</xref>. Its weight is about 3 kg, and its endurance of flight is about 25 min. Its communication distance is up to 3 km in an unobstructed environment without any data package loss.</p></sec><sec id="sec3-sensors-25-05232"><title>3. Sensing Model</title><sec id="sec3dot1-sensors-25-05232"><title>3.1. Principle of UWB Radar for Vital Signs Detection</title><p><xref rid="sensors-25-05232-f004" ref-type="fig">Figure 4</xref> shows the block diagram and detection principle of a typical UWB radar, which measures the physiological activities of a human target by demodulating the received signal phase [<xref rid="B23-sensors-25-05232" ref-type="bibr">23</xref>]. The response of radar can be expressed as:<disp-formula id="FD1-sensors-25-05232"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi>&#948;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&#948;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#964;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi>&#948;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#964;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the response of the human target, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>&#948;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the response of other static clutter.</p><p>Assuming that there is a stationary human target at a distance <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, when radar is mounted on a mobile platform to detect the human target, the instantaneous distance from the antenna to the surface of the human chest cavity can be expressed as [<xref rid="B16-sensors-25-05232" ref-type="bibr">16</xref>]:<disp-formula id="FD2-sensors-25-05232"><label>(2)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are amplitude and frequency of respiratory signal, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are amplitude and frequency of heartbeat signal. <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the platform motion. In this scenario, the delay of a static object will be dynamic; the response of radar can be further expressed as [<xref rid="B24-sensors-25-05232" ref-type="bibr">24</xref>]:<disp-formula id="FD3-sensors-25-05232"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi>&#948;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>&#948;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Time delay <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> of human target is equal to:<disp-formula id="FD4-sensors-25-05232"><label>(4)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Time delay <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of a static object is equal to:<disp-formula id="FD5-sensors-25-05232"><label>(5)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the speed of transmitted signal. The received signal can be expressed as:<disp-formula id="FD6-sensors-25-05232"><label>(6)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8727;</mml:mo><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the radar transmitted pulse.</p><p>The expression function of radar echo signal of human target is:<disp-formula id="FD7-sensors-25-05232"><label>(7)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>&#952;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#966;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> equals to 7.29 GHz and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#966;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the initial phase, the phase signal can be calculated as [<xref rid="B25-sensors-25-05232" ref-type="bibr">25</xref>]:<disp-formula id="FD8-sensors-25-05232"><label>(8)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>X</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is wavelength of transmitted signal, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The above analysis indicates that for a moving radar system, the phase echo of the human is composed of the radar platform phase shift and the displacement of physiological activity; the phase echo of the stationary target only includes the radar platform phase shift. The aim of the proposed method is to recover the vital signals from the mixed observed signals.</p></sec><sec id="sec3dot2-sensors-25-05232"><title>3.2. Background Clutter</title><p>In the airborne radar detection problem, the interference mainly comes from two aspects: (1) The motion of the UAV platform; (2) Echoes from the background environment. When the environment of the human target is the exposed ground, the background clutter is mainly from other detected objects (stationary objects and ground). In this study, we summarize it as the static background environment. When the environment of the human target is the vegetation covered ground, the propeller rotation and wind will cause the movement of the plant blades. In this paper, we summarize it as the dynamic background environment.</p><sec id="sec3dot2dot1-sensors-25-05232"><title>3.2.1. Static Background Environment</title><p><xref rid="sensors-25-05232-f005" ref-type="fig">Figure 5</xref>a is the sketch of the static background environment scenario. Previous studies have observed that echo signals from these static objects are only modulated by the platform motion. It means that the separate measurement of radar platform motion can be inferred from the static clutters in the range profile [<xref rid="B20-sensors-25-05232" ref-type="bibr">20</xref>].</p><p>Same as Formula (8), the phase signal of a stationary object can be expressed as:<disp-formula id="FD9-sensors-25-05232"><label>(9)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Assuming that there are several stationary objects in the environment, and the raw echo data can be represented as:<disp-formula id="FD10-sensors-25-05232"><label>(10)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is noise signal. Representing the above equation in the following matrix forms:<disp-formula id="FD11-sensors-25-05232"><label>(11)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8943;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8943;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">S</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, is the multi-range channel echo signal matrix, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight matrix, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">S</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the source signal matrix, and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is a vital signal.</p><p>Based on the above echo model, the problem of detecting human vital signals with the airborne radar system can be described as a source signals estimation problem, which estimates unknown source signals (vital signals and platform motion signals) from the mixed signals.</p></sec><sec id="sec3dot2dot2-sensors-25-05232"><title>3.2.2. Dynamic Background Environment</title><p>In this study, we take grassland as a typical dynamic background scenario, as shown in <xref rid="sensors-25-05232-f005" ref-type="fig">Figure 5</xref>b. The grassland scenario has a layered structure. When electromagnetic waves irradiate this rough surface, its scattered waves are no longer plane waves [<xref rid="B26-sensors-25-05232" ref-type="bibr">26</xref>]. Assuming that the scattered echo of a target point in the grass is e, the scattered echo of a range bin can be expressed as:<disp-formula id="FD13-sensors-25-05232"><label>(12)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of scatter points.</p><p>In addition, the propeller rotation will cause the movement of the grass blades, then the grass movement signal of <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> range bin is given as:<disp-formula id="FD14-sensors-25-05232"><label>(13)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the motion signal of a blade, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of the blades.</p><p>When the human target is also in this range bin, the radar echo signal phase shift of this range bin can be represented as:<disp-formula id="FD15-sensors-25-05232"><label>(14)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this scenario, to accurately extract target information, it is necessary to address two issues: platform motion interference and background clutter interference.</p></sec><sec id="sec3dot2dot3-sensors-25-05232"><title>3.2.3. Statistical Characteristic Analysis of Measured Grass-Surface Clutter</title><p>It has been observed that the influence of surface conditions in the form of vegetation and roughness will significantly degrade the radar detection performance. Hence, the comprehensive understanding of the clutter statistical characterization is essential for the design of signal processing techniques.</p><p>In this section, we have taken observations and analysis of grass clutter by the experimental data processing method, which has been the main way to analyze the statistical characteristics of surface clutter [<xref rid="B27-sensors-25-05232" ref-type="bibr">27</xref>]. The time domain waveform and spectrum of grassland clutter signal measured by airborne radar are shown in <xref rid="sensors-25-05232-f006" ref-type="fig">Figure 6</xref>, which has the same frequency range as respiration and heartbeat. The frequency distribution histogram of clutter is shown in <xref rid="sensors-25-05232-f007" ref-type="fig">Figure 7</xref>a. It can be observed that its distribution exhibits the unimodal characteristic, which can be attempted to fit a normal distribution curve.</p><p>First, the Anderson-Darling test is used to test the normality of the overall distribution of clutter data, and the obtained <italic toggle="yes">p</italic>-value is equal to 0.1655. It indicates that there is no significant difference between the distribution of clutter data and the normal distribution. Then, the analysis looking for the best fit of the clutter signal amplitude probability density function (PDF) is performed, and the regression equation is calculated according to the nonlinear least squares method. The determination coefficient of the regression model is equal to 0.9873, ensuring the effect and accuracy of the fitting result.</p><p>The results indicate that the clutter signal may obey the Gaussian distribution or the Gaussian mixture distribution, which could not be filtered by preprocessing. According to the result, the use of high-order cumulants as analytical tools theoretically can completely suppress the influence of Gaussian noise because the higher-order cumulants of Gaussian processes are always equal to zero.</p></sec></sec></sec><sec id="sec4-sensors-25-05232"><title>4. Proposed Vital Signals Extraction Method</title><p>Human vital signals include respiration and heartbeat, which implicate the life state of the human body. The overall process of the proposed vital signals extraction method is shown in <xref rid="sensors-25-05232-f008" ref-type="fig">Figure 8</xref>, which mainly includes pre-processing, respiratory signal extraction, and heartbeat signal extraction.</p><sec id="sec4dot1-sensors-25-05232"><title>4.1. Pre-Processing</title><sec id="sec4dot1dot1-sensors-25-05232"><title>4.1.1. Range Migration Compensation</title><p>The drone hovers above the human target to be detected, and the radar continuously transmits electromagnetic waves to extract vital signs from the radar echo sequences. During this process, the drone will generate relative movements, with distance variation between the array antenna and the target. The position of the target point in the echo will be changed, resulting in range migration between radar echo sequences. The aim of range migration compensation is to correct this offset, which includes envelope alignment and phase compensation. <xref rid="sensors-25-05232-f009" ref-type="fig">Figure 9</xref> shows the schematic diagram of range migration compensation.</p><p>Envelope alignment is the process of translating the echo data to eliminate distance misalignment, also known as coarse compensation. For two adjacent echo sequences, the distance between the target and the radar changes very little, and their envelopes have a strong correlation. Based on this, the adjacent envelope correlation method calculates the corresponding delay through peak values of their cross-correlation function, which can realize alignment on range bins [<xref rid="B28-sensors-25-05232" ref-type="bibr">28</xref>]. Assuming that <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are signal sequences of adjacent range with strong correlation, their cross-correlation function is:<disp-formula id="FD16-sensors-25-05232"><label>(15)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>&#8710;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">&#8747;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#8710;</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mo>&#8710;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The displacement amount can be estimated from the maximum position of the cross-correlation function:<disp-formula id="FD17-sensors-25-05232"><label>(16)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, shifting the corresponding signal according to the value, the range alignment of these two signals can be completed. The above operation should then be repeated until all echo signals are aligned in range.</p><p>After completing envelope alignment, the next step is the phase compensation, also known as fine compensation. Taking the weighted average of the first <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> envelope-aligned echoes as the benchmark, the average value of the difference between the phase of the echo signal and the benchmark is the phase compensation value of this echo signal. <xref rid="sensors-25-05232-f010" ref-type="fig">Figure 10</xref> illustrates the example after range migration compensation processing.</p></sec><sec id="sec4dot1dot2-sensors-25-05232"><title>4.1.2. Background Clutter Removal</title><p>The raw radar data contains direct-current (DC) components caused by static objects and baseline drift caused by environmental factors. These two types of noise are known as background clutter, which will cause strong interference to the wanted signal. The adopted 100 order DC drift removal method is represented by the following equation:<disp-formula id="FD18-sensors-25-05232"><label>(17)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the radar data after processing and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the data after range migration compensation. The 2D pseudo-color image of the raw UWB radar data and the data after preprocessing are displayed in <xref rid="sensors-25-05232-f011" ref-type="fig">Figure 11</xref>. The measurement was conducted indoors with the radar fixed at 2 m above a human target lying on the ground. The respiratory and heartbeat signals can be obtained by extracting the echo signals of the corresponding range bin and performing band-pass filtering, and the waveforms are displayed in <xref rid="sensors-25-05232-f012" ref-type="fig">Figure 12</xref>.</p></sec><sec id="sec4dot1dot3-sensors-25-05232"><title>4.1.3. Human Target Localization</title><p>After the above processing, background interference has been removed, and it is necessary to locate and select the optimal range unit, which is the location of the human target. Computing the square sum of the slow time, the range unit with the maximum sum is the position of the target.<disp-formula id="FD19-sensors-25-05232"><label>(18)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mi>S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the slow time signal of one range, <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the accumulation of <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the range bin with the biggest accumulation.</p></sec></sec><sec id="sec4dot2-sensors-25-05232"><title>4.2. Respiratory Signal Extraction</title><sec id="sec4dot2dot1-sensors-25-05232"><title>4.2.1. Observed Signals Extraction</title><p>Pulse radar divides the entire detection range into equidistant bins, each of which is also known as a range gate. The distributed scatter points on the thorax of a human target can extend over several centimeters of range and appear on the multiple range gates around the range gate with the biggest accumulation [<xref rid="B24-sensors-25-05232" ref-type="bibr">24</xref>]. The probable solution of this problem is to extract the upper and lower two range gates centered on the energy maximum range gate, and five channel signals are extracted in total. Then, the BEADS algorithm is adopted to remove baseline drift [<xref rid="B29-sensors-25-05232" ref-type="bibr">29</xref>].</p></sec><sec id="sec4dot2dot2-sensors-25-05232"><title>4.2.2. Joint Approximate Diagonalization of Eigenmatrices Algorithm</title><p>According to <xref rid="sec3-sensors-25-05232" ref-type="sec">Section 3</xref>, the airborne radar vital signals detection problem can be described as a source signal separation problem. The source signals are respiratory signal, heartbeat signal, platform motion, and interference from the environment, and the observed signals are instantaneous linear mixture of the source signals. From Equation (11), it can be concluded that the observed signals can be written as:<disp-formula id="FD21-sensors-25-05232"><label>(19)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:mo>&#8942;</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The observed signals can be modeled in the following way [<xref rid="B30-sensors-25-05232" ref-type="bibr">30</xref>]:<disp-formula id="FD25-sensors-25-05232"><label>(20)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the data matrix, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the mixing matrix, and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the source component matrix.</p><p>Blind source separation JADE algorithm first whitens the observed signals, the whitening matrix <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> satisfies [<xref rid="B31-sensors-25-05232" ref-type="bibr">31</xref>]:<disp-formula id="FD26-sensors-25-05232"><label>(21)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">U</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the unitary matrix to be calculated. The covariance matrix of the observed data is:<disp-formula id="FD27-sensors-25-05232"><label>(22)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo>[</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Assuming there is no noise, then <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is not full rank, and it has <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> non-zero eigenvalues <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are corresponding feature vectors, the whitening matrix can be expressed as:<disp-formula id="FD28-sensors-25-05232"><label>(23)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>When there is additive Gaussian white noise, <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is generally full rank, arranging <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> feature vectors in descending order: <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>&#8805;</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>&#8805;</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the variance of noise can be written as:<disp-formula id="FD29-sensors-25-05232"><label>(24)</label><mml:math id="mm70" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD30-sensors-25-05232"><label>(25)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The whited signal can be represented as:<disp-formula id="FD31-sensors-25-05232"><label>(26)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">R</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="bold">N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="bold">U</mml:mi><mml:mi mathvariant="bold">S</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">N</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The fact that the higher-order cumulants of any Gaussian process are equal to 0 makes it theoretically possible to completely suppress the impact of Gaussian noise. To separate the wanted signal through the subsequent blind source separation algorithm, it is necessary to construct a fourth-order cumulant matrix of the whitened signal and perform eigenvalue decomposition to obtain the unitary matrix.</p><p>Denoting the <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> column of the unitary matrix is <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">U</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Define <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD32-sensors-25-05232"><label>(27)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where the <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th element of <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The fourth-order cumulant matrix is defined as [<xref rid="B32-sensors-25-05232" ref-type="bibr">32</xref>]:<disp-formula id="FD33-sensors-25-05232"><label>(28)</label><mml:math id="mm82" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Performing eigenvalue decomposition on the fourth-order cumulant matrix <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, the estimated matrix <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> of the unitary matrix <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained:<disp-formula id="FD34-sensors-25-05232"><label>(29)</label><mml:math id="mm86" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mi mathvariant="bold-sans-serif">&#923;</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD35-sensors-25-05232"><label>(30)</label><mml:math id="mm87" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">&#923;</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, the estimated signals can be obtained by the estimated matrix:<disp-formula id="FD36-sensors-25-05232"><label>(31)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The specific process of JADE algorithm is as follows:<list list-type="simple"><list-item><label>(1)</label><p>Decentralizing and whitening the observed signals matrix;</p></list-item><list-item><label>(2)</label><p>Constructing the high-order cumulant matrix <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the whited matrix;</p></list-item><list-item><label>(3)</label><p>Performing joint approximate diagonalization on the matrix <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to obtain the estimated matrix <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> of the unitary matrix <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><label>(4)</label><p>Estimating the source signal according to Equation (31).</p></list-item></list></p></sec></sec><sec id="sec4dot3-sensors-25-05232"><title>4.3. Heartbeat Signal Extraction</title><sec id="sec4dot3dot1-sensors-25-05232"><title>4.3.1. Bandpass Filter</title><p>After the above analysis, interference is mainly concentrated in low frequency range, and the heart rate range of a normal person is 0.85&#8211;3.3 Hz. Hence, the lower cutoff frequency of the bandpass filter is 0.85 Hz, and the upper cutoff frequency of the bandpass filter is 3.3 Hz. The processed signal <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> can be represented as:<disp-formula id="FD37-sensors-25-05232"><label>(32)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8727;</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the impulse response function of the bandpass filter.</p></sec><sec id="sec4dot3dot2-sensors-25-05232"><title>4.3.2. Respiratory Harmonic Localization</title><p>After obtaining the fundamental frequency of the respiratory signal, locate its third, fourth, and fifth harmonics:<disp-formula id="FD38-sensors-25-05232"><label>(33)</label><mml:math id="mm96" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8747;</mml:mo><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>&#8734;</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mo>&#8734;</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>j</mml:mi><mml:mi>&#969;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mo>&#160;</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the extracted respiratory signal in <xref rid="sec4dot2dot2-sensors-25-05232" ref-type="sec">Section 4.2.2</xref>, <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the respiratory fundamental frequency, and <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is amplitude of <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec4dot3dot3-sensors-25-05232"><title>4.3.3. Feedback Notch Filter</title><p>The notch filter is given by the following equation:<disp-formula id="FD41-sensors-25-05232"><label>(34)</label><mml:math id="mm102" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="italic">cos</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn><mml:mi>&#961;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="italic">cos</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the notch frequency, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the radius of the poles of <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Introducing the feedback structure on traditional notch filter [<xref rid="B33-sensors-25-05232" ref-type="bibr">33</xref>], and the transfer function can be expressed as:<disp-formula id="FD42-sensors-25-05232"><label>(35)</label><mml:math id="mm106" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the feedback coefficient. As shown in <xref rid="sensors-25-05232-f013" ref-type="fig">Figure 13</xref>, the addition of the feedback structure makes the overshoot and bandwidth of the notch filter achieve the ideal degree, improving the performance of the notch filter. The feedback notch filter is utilized to remove the third, fourth, and fifth respiratory harmonics.</p></sec></sec></sec><sec id="sec5-sensors-25-05232"><title>5. Experiment and Results</title><sec id="sec5dot1-sensors-25-05232"><title>5.1. Experimental Setup</title><p>Two field experiments in different scenarios are carried out to test and verify the proposed method, and the experimental setups are shown in <xref rid="sensors-25-05232-f014" ref-type="fig">Figure 14</xref>a. Scenario 1 is grassland, and scenario 2 is wall penetration. The human subject lies in the detection area with normal breathing as the test target, facing the radar transceiver antenna. The measurement time is 30 s.</p><p>The parameter setup of X4M200 UWB radar is listed in <xref rid="sensors-25-05232-t001" ref-type="table">Table 1</xref>. To evaluate experimental result, the subject also wears ErgoLab contact bandage sensor, which could wirelessly collect respiratory and electrocardiogram (ECG) signals. The physiological signals collected by the contact sensor are regarded as the ground truth. Finally, we adopt the existing radar self-motion cancellation method in [<xref rid="B19-sensors-25-05232" ref-type="bibr">19</xref>] for comparison. The reference method compensates for the platform motion by calculating residual phase between the human subject and the static clutter, and then it decomposes the residual for vital signals extraction. In our experiments, we place a metal reflector plate in the environment as the stationary object.</p><p>In addition, the signal-to-noise ratio (SNR) is used to evaluate whether the proposed method can accurately estimate the respiratory rate (RR) and the heartbeat rate (HR). The definition of SNR is:<disp-formula id="FD43-sensors-25-05232"><label>(36)</label><mml:math id="mm108" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the signal spectrum, <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of sampling points of the spectrum, and <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the peak coordinate of the spectrum. Considering that the respiratory and heartbeat spectrum are generally between 0 Hz and 2 Hz, we only examine the frequency information within this frequency band.</p></sec><sec id="sec5dot2-sensors-25-05232"><title>5.2. Performance in Realistic Grassland Scenario</title><p>The experimental scenario setup is shown in <xref rid="sensors-25-05232-f014" ref-type="fig">Figure 14</xref>a, the unprocessed radar echo data in this scenario are shown in <xref rid="sensors-25-05232-f014" ref-type="fig">Figure 14</xref>b, and the target localization result is shown in <xref rid="sensors-25-05232-f015" ref-type="fig">Figure 15</xref>. The echo of the human target is clear and visible, while the echo of the reflector is fuzzy, which is due to range ambiguities. The observed signals of extracted five range gates are shown in <xref rid="sensors-25-05232-f016" ref-type="fig">Figure 16</xref>, which are similar in waveform but differ in amplitude, and the signal of the middle range unit has the largest energy. The observed signals for each range bin can be considered as the linear combination of cardiac signal, chest motion, platform motion, and noise. In this experiment, the 46<sup>th</sup>-to-50<sup>th</sup>-range bins are identified and selected for extracting the vital signals.</p><p>After acquisition of raw radar data, signal processing method mentioned in <xref rid="sec4-sensors-25-05232" ref-type="sec">Section 4</xref> is implemented. The respiratory signal extraction results are expressed in <xref rid="sensors-25-05232-f017" ref-type="fig">Figure 17</xref>. According to the corresponding spectrum, it can be inferred that the RR of the bandage sensor is 0.3113 Hz, the RR of the proposed method is 0.3154 Hz, and the RR of the reference method is also 0.3154 Hz. The accuracy of UWB radar is 98.68%, and the difference is caused by time interval error and systematic error, which can be ignored generally. <xref rid="sensors-25-05232-f017" ref-type="fig">Figure 17</xref>b,c clearly shows that the respiratory signal extracted by the proposed method is closer to the reference waveform and has higher SNR compared with the signal extracted by the reference method. The effectiveness of the reference method depends on the quality of the echo signal of the stationary object. In addition, this method requires one or more stationary objects as the reference signal to recover the vital signals, while such restrictive conditions may result in performance degradation in a more cluttered environment.</p><p>The results of heartbeat signal extraction are shown in <xref rid="sensors-25-05232-f018" ref-type="fig">Figure 18</xref>. <xref rid="sensors-25-05232-f018" ref-type="fig">Figure 18</xref>a is the reference ECG data, and <xref rid="sensors-25-05232-f018" ref-type="fig">Figure 18</xref>c is the extracted heartbeat signal after removing the fifth-order respiratory harmonic. The acquired reference HR according to the peak detection method is 1.25 Hz, the estimated HR of Radar is 1.295 Hz, and the accuracy is 96.4%. Comparison of detection results indicates that the proposed method is able to detect the overall movement information of the heart, while the attenuation of morphological amplitude is significant, and this is in line with our expected estimation.</p><p>The results in the first scenario suggest that the proposed method can effectively suppress background clutter and noise interference from the environment, recover the respiratory and heartbeat signals, and improve the SNR.</p></sec><sec id="sec5dot3-sensors-25-05232"><title>5.3. Performance in Through-the-Wall Scenario</title><p>For comprehensive verification of the proposed method, we also explore the feasibility of utilizing the airborne radar system for monitoring the vital signals of a single subject through a brick wall. <xref rid="sensors-25-05232-f019" ref-type="fig">Figure 19</xref> illustrates the experimental scenario description and the collected radar data. The extracted respiratory signal is shown in <xref rid="sensors-25-05232-f020" ref-type="fig">Figure 20</xref>. It has been observed that the captured RR of the bandage sensor is 0.3125 Hz, the RR of the proposed method is 0.332 Hz, and the RR of the reference method is also 0.332 Hz. The accuracy of UWB radar is 93.76%, and the system&#8217;s performance experiences slight degradation due to the inevitable attenuation effect of obstacles on electromagnetic waves. <xref rid="sensors-25-05232-t002" ref-type="table">Table 2</xref> presents a quantitative comparison between the proposed method and the reference method in two scenarios, and the results demonstrate the superior performance and robustness of our proposed method in respiratory waveform recovery.</p><p>Under wall-penetration detection conditions, the bio-radar echo of human physiological motion is weak, and attenuation will reduce the power of the echo, causing distortion of the echo. In addition, multipath effects will seriously affect effective micro-Doppler feature detection and separation. Reducing the center frequency of the radar can enhance its penetration capability, but the sensitivity of the radar will also decrease accordingly. Hence, through-the-wall independent heartbeat signal isolation from the mixture is also challenging for ideal environments [<xref rid="B34-sensors-25-05232" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05232" ref-type="bibr">35</xref>].</p></sec><sec id="sec5dot4-sensors-25-05232"><title>5.4. Impact of Distance Between the System and the Victim</title><p>Considering that the distance between the victim and the airborne radar system is not fixed in practical applications, we also investigate whether the distance would impact the performance of the system in scenario 1. The detection distance ranges from 2 to 5 m. <xref rid="sensors-25-05232-t003" ref-type="table">Table 3</xref> shows the results of the estimation performance of RR and HR for different distances. It illustrates that the estimation performance degrades as the detection distance increases due to the energy attenuation of radar echo signal, which can be addressed by increasing transmission power.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05232"><title>6. Conclusions</title><p>In this paper, an airborne IR-UWB radar system for vital signs detection of the unconscious victim has been developed. The communication distance of the developed system is up to 10 km with low time delay and low packet loss rate, which satisfies the real-time requirement of remote vital signs monitor. In addition, a novel framework based on blind source separation for precise respiration and heartbeat extraction has been proposed. To resolve the signal distortion raised by the background environments, we analyze the statistical characteristics of measured grass clutter and note that the radar echo of clutter obeys the Gaussian distribution. Then, a signal processing framework based on the JADE algorithm is proposed to estimate the respiratory and heartbeat signals from the mixed signals. Extensive results in field trials demonstrate the effectiveness and accuracy of the proposed method. This work may provide a new solution for intelligent SAR in harsh environments.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, G.L. and J.W.; methodology, Y.J. and Y.Y.; software, Y.J. and Z.L.; investigation, F.Q. and T.L.; data curation, Y.Y.; writing&#8212;original draft preparation, Y.J.; writing&#8212;review and editing, F.Q.; visualization, Y.J.; funding acquisition, G.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. Further inquiries can be directed to the corresponding authors.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05232"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wex</surname><given-names>F.</given-names></name><name name-style="western"><surname>Schryen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Feuerriegel</surname><given-names>S.</given-names></name><name name-style="western"><surname>Neumann</surname><given-names>D.</given-names></name></person-group><article-title>Emergency Response in Natural Disaster Management: Allocation and Scheduling of Rescue Units</article-title><source>Eur. J. Oper. Res.</source><year>2014</year><volume>235</volume><fpage>697</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1016/j.ejor.2013.10.029</pub-id></element-citation></ref><ref id="B2-sensors-25-05232"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>P.</given-names></name></person-group><article-title>HMA-SAR: Multi-Agent Search and Rescue for Unknown Located Dynamic Targets in Completely Unknown Environments</article-title><source>IEEE Robot. Autom. Lett.</source><year>2024</year><volume>9</volume><fpage>5567</fpage><lpage>5574</lpage><pub-id pub-id-type="doi">10.1109/LRA.2024.3396097</pub-id></element-citation></ref><ref id="B3-sensors-25-05232"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nefros</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kitsara</surname><given-names>G.</given-names></name><name name-style="western"><surname>Loupasakis</surname><given-names>C.</given-names></name></person-group><article-title>Geographical Information Systems and Remote Sensing Techniques to Reduce the Impact of Natural Disasters in Smart Cities</article-title><source>IFAC-Pap.</source><year>2022</year><volume>55</volume><fpage>72</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.ifacol.2022.08.051</pub-id></element-citation></ref><ref id="B4-sensors-25-05232"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ram&#237;rez-Ayala</surname><given-names>O.</given-names></name><name name-style="western"><surname>Gonz&#225;lez-Hern&#225;ndez</surname><given-names>I.</given-names></name><name name-style="western"><surname>Salazar</surname><given-names>S.</given-names></name><name name-style="western"><surname>Flores</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lozano</surname><given-names>R.</given-names></name></person-group><article-title>Real-Time Person Detection in Wooded Areas Using Thermal Images from an Aerial Perspective</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>9216</elocation-id><pub-id pub-id-type="doi">10.3390/s23229216</pub-id><pub-id pub-id-type="pmid">38005600</pub-id><pub-id pub-id-type="pmcid">PMC10675173</pub-id></element-citation></ref><ref id="B5-sensors-25-05232"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schedl</surname><given-names>D.C.</given-names></name><name name-style="western"><surname>Kurmi</surname><given-names>I.</given-names></name><name name-style="western"><surname>Bimber</surname><given-names>O.</given-names></name></person-group><article-title>Search and Rescue with Airborne Optical Sectioning</article-title><source>Nat. Mach. Intell.</source><year>2020</year><volume>2</volume><fpage>783</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-00261-3</pub-id></element-citation></ref><ref id="B6-sensors-25-05232"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kwak</surname><given-names>K.</given-names></name></person-group><article-title>Collaborative Human Recognition with Lightweight Models in Drone-based Search and Rescue Operations</article-title><source>IEEE Trans. Veh. Technol.</source><year>2023</year><volume>73</volume><fpage>1765</fpage><lpage>1776</lpage><pub-id pub-id-type="doi">10.1109/TVT.2023.3319483</pub-id></element-citation></ref><ref id="B7-sensors-25-05232"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kucukayan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Karacan</surname><given-names>H.</given-names></name></person-group><article-title>YOLO-IHD: Improved Real-Time Human Detection System for Indoor Drones</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>922</elocation-id><pub-id pub-id-type="doi">10.3390/s24030922</pub-id><pub-id pub-id-type="pmid">38339638</pub-id><pub-id pub-id-type="pmcid">PMC10857234</pub-id></element-citation></ref><ref id="B8-sensors-25-05232"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Martinez-Alpiste</surname><given-names>I.</given-names></name><name name-style="western"><surname>Golcarenarenji</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Alcaraz-Calero</surname><given-names>J.M.</given-names></name></person-group><article-title>Search and Rescue Operation using UAVs: A Case Study</article-title><source>Expert Syst. Appl.</source><year>2021</year><volume>178</volume><fpage>114937</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2021.114937</pub-id></element-citation></ref><ref id="B9-sensors-25-05232"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Nan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huo</surname><given-names>H.</given-names></name></person-group><article-title>Object Detection from UAV Thermal Infrared Images and Videos using YOLO Models</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2022</year><volume>112</volume><fpage>102912</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2022.102912</pub-id></element-citation></ref><ref id="B10-sensors-25-05232"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>W.</given-names></name></person-group><article-title>HIT-UAV: A High-altitude Infrared Thermal Dataset for Unmanned Aerial Vehicle-based Object Detection</article-title><source>Sci. Data</source><year>2023</year><volume>10</volume><fpage>227</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-02066-6</pub-id><pub-id pub-id-type="pmid">37080987</pub-id><pub-id pub-id-type="pmcid">PMC10119175</pub-id></element-citation></ref><ref id="B11-sensors-25-05232"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>J.</given-names></name><etal/></person-group><article-title>An infrared dataset for partially occluded person detection in complex environment for search and rescue</article-title><source>Sci. Data</source><year>2025</year><volume>12</volume><fpage>300</fpage><pub-id pub-id-type="doi">10.1038/s41597-025-04600-0</pub-id><pub-id pub-id-type="pmid">39971943</pub-id><pub-id pub-id-type="pmcid">PMC11840078</pub-id></element-citation></ref><ref id="B12-sensors-25-05232"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.Y.</given-names></name></person-group><article-title>Locality Guided Cross-modal Feature Aggregation and Pixel-level Fusion for Multispectral Pedestrian Detection</article-title><source>Inf. Fusion</source><year>2022</year><volume>88</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2022.06.008</pub-id></element-citation></ref><ref id="B13-sensors-25-05232"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>G.</given-names></name></person-group><article-title>Automatic Air-to-ground Recognition of Outdoor Injured Human Targets based on UAV Bimodal Information: The explore study</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>3457</elocation-id><pub-id pub-id-type="doi">10.3390/app12073457</pub-id></element-citation></ref><ref id="B14-sensors-25-05232"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>G.</given-names></name></person-group><article-title>UAV Multispectral Multi-domain Feature Optimization for the Air-to-ground Recognition of Outdoor Injured Human Targets under Cross-scene Environment</article-title><source>Front. Public Health</source><year>2023</year><volume>11</volume><elocation-id>999378</elocation-id><pub-id pub-id-type="doi">10.3389/fpubh.2023.999378</pub-id><pub-id pub-id-type="pmid">36844835</pub-id><pub-id pub-id-type="pmcid">PMC9947796</pub-id></element-citation></ref><ref id="B15-sensors-25-05232"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xue</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>D.</given-names></name></person-group><article-title>Accurate Multi-target Vital Signs Detection Method for FMCW Radar</article-title><source>Measurement</source><year>2023</year><volume>223</volume><fpage>113715</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.113715</pub-id></element-citation></ref><ref id="B16-sensors-25-05232"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aflalo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zalevsky</surname><given-names>Z.</given-names></name></person-group><article-title>Penetrating Barriers: Noncontact Measurement of Vital Bio Signs Using Radio Frequency Technology</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5784</elocation-id><pub-id pub-id-type="doi">10.3390/s24175784</pub-id><pub-id pub-id-type="pmid">39275695</pub-id><pub-id pub-id-type="pmcid">PMC11397866</pub-id></element-citation></ref><ref id="B17-sensors-25-05232"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>G.</given-names></name></person-group><article-title>Mission Chain Driven Unmanned Aerial Vehicle Swarms Cooperation for the Search and Rescue of Outdoor Injured Human Targets</article-title><source>Drones</source><year>2022</year><volume>6</volume><elocation-id>138</elocation-id><pub-id pub-id-type="doi">10.3390/drones6060138</pub-id></element-citation></ref><ref id="B18-sensors-25-05232"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cardillo</surname><given-names>E.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Caddemi</surname><given-names>A.</given-names></name></person-group><article-title>Vital Sign Detection and Radar Self-motion Cancellation Through Clutter Identification</article-title><source>IEEE Trans. Microw. Theory Tech.</source><year>2021</year><volume>69</volume><fpage>1932</fpage><lpage>1942</lpage><pub-id pub-id-type="doi">10.1109/TMTT.2021.3049514</pub-id></element-citation></ref><ref id="B19-sensors-25-05232"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Herschfelt</surname><given-names>A.</given-names></name><name name-style="western"><surname>Holtom</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bliss</surname><given-names>D.W.</given-names></name></person-group><article-title>Cardiac and Respiratory Sensing from a Hovering UAV Radar Platform</article-title><source>Proceedings of the 2021 IEEE Statistical Signal Processing Workshop (SSP)</source><conf-loc>Rio de Janeiro, Brazil</conf-loc><conf-date>11&#8211;14 July 2021</conf-date><fpage>541</fpage><lpage>545</lpage></element-citation></ref><ref id="B20-sensors-25-05232"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>B.-B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Song</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name></person-group><article-title>RF-Search: Searching Unconscious Victim in Smoke Scenes with RF-enabled Drone</article-title><source>Proceedings of the 29th Annual International Conference on Mobile Computing and Networking, Association for Computing Machinery</source><conf-loc>Madrid, Spain</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>1</fpage><lpage>15</lpage></element-citation></ref><ref id="B21-sensors-25-05232"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stockel</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wallrath</surname><given-names>P.</given-names></name><name name-style="western"><surname>Herschel</surname><given-names>R.</given-names></name><name name-style="western"><surname>Pohl</surname><given-names>N.</given-names></name></person-group><article-title>Detection and Monitoring of People in Collapsed Buildings Using a Rotating Radar on a UAV</article-title><source>IEEE Trans. Radar Syst.</source><year>2024</year><volume>2</volume><fpage>13</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1109/TRS.2023.3342368</pub-id></element-citation></ref><ref id="B22-sensors-25-05232"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name></person-group><article-title>Deep Reinforcement Learning for Communication Flow Control in Wireless Mesh Networks</article-title><source>IEEE Netw.</source><year>2021</year><volume>35</volume><fpage>112</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1109/MNET.011.2000303</pub-id></element-citation></ref><ref id="B23-sensors-25-05232"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qiao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>B.</given-names></name><name name-style="western"><surname>He</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name></person-group><article-title>Learning-Refined Integral Null Space Pursuit Algorithm for Noncontact Multisubjects Vital Signs Measurements Using SFCW-UWB and IR-UWB Radar</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2022</year><volume>71</volume><fpage>8506013</fpage><pub-id pub-id-type="doi">10.1109/TIM.2022.3218031</pub-id></element-citation></ref><ref id="B24-sensors-25-05232"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Antide</surname><given-names>E.</given-names></name><name name-style="western"><surname>Zarudniev</surname><given-names>M.</given-names></name><name name-style="western"><surname>Michel</surname><given-names>O.</given-names></name><name name-style="western"><surname>Pelissier</surname><given-names>M.</given-names></name></person-group><article-title>Comparative Study of Radar Architectures for Human Vital Signs Measurement</article-title><source>Proceedings of the 2020 IEEE Radar Conference (RadarConf20)</source><conf-loc>Florence, Italy</conf-loc><conf-date>21&#8211;25 September 2020</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B25-sensors-25-05232"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cardillo</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ferro</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sapienza</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Reliable eye-blinking detection with millimeter-wave radar glasses</article-title><source>IEEE Trans. Microw. Theory Tech.</source><year>2023</year><volume>72</volume><fpage>771</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1109/TMTT.2023.3329707</pub-id></element-citation></ref><ref id="B26-sensors-25-05232"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Rong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>T.</given-names></name></person-group><article-title>Classification Method of Uniform Circular Array Radar Ground Clutter Data Based on Chaotic Genetic Algorithm</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>4596</elocation-id><pub-id pub-id-type="doi">10.3390/s21134596</pub-id><pub-id pub-id-type="pmid">34283130</pub-id><pub-id pub-id-type="pmcid">PMC8271529</pub-id></element-citation></ref><ref id="B27-sensors-25-05232"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rosenberg</surname><given-names>L.</given-names></name><name name-style="western"><surname>Duk</surname><given-names>V.</given-names></name></person-group><article-title>Land Clutter Statistics from an Airborne Passive Bistatic Radar</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>5104009</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2021.3087589</pub-id></element-citation></ref><ref id="B28-sensors-25-05232"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cardillo</surname><given-names>E.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Caddemi</surname><given-names>A.</given-names></name></person-group><article-title>Empowering Blind People Mobility: A Millimeter-Wave Radar Cane</article-title><source>Proceedings of the 2020 IEEE International Workshop on Metrology for Industry 4.0 &amp; IoT</source><conf-loc>Roma, Italy</conf-loc><conf-date>21&#8211;25 September 2020</conf-date><fpage>213</fpage><lpage>217</lpage></element-citation></ref><ref id="B29-sensors-25-05232"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ning</surname><given-names>X.</given-names></name><name name-style="western"><surname>Selesnick</surname><given-names>I.W.</given-names></name><name name-style="western"><surname>Duval</surname><given-names>L.</given-names></name></person-group><article-title>Chromatogram baseline estimation and denoising using sparsity (BEADS)</article-title><source>Chemom. Intell. Lab. Syst.</source><year>2014</year><volume>139</volume><fpage>156</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1016/j.chemolab.2014.09.014</pub-id></element-citation></ref><ref id="B30-sensors-25-05232"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hyv&#228;rinen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Oja</surname><given-names>E.</given-names></name></person-group><article-title>Independent component analysis: Algorithms and applications</article-title><source>Neural Netw.</source><year>2000</year><volume>13</volume><fpage>411</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(00)00026-5</pub-id><pub-id pub-id-type="pmid">10946390</pub-id></element-citation></ref><ref id="B31-sensors-25-05232"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>Enhanced blind source separation algorithm for partial discharge signals using Joint Approximate diagonalization of Eigenmatrices</article-title><source>Measurement</source><year>2025</year><volume>244</volume><fpage>116552</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2024.116552</pub-id></element-citation></ref><ref id="B32-sensors-25-05232"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Adal&#305;</surname><given-names>T.</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>M.</given-names></name></person-group><article-title>Joint Blind Source Separation by Generalized Joint Diagonalization of Cumulant Matrices</article-title><source>Signal Process.</source><year>2011</year><volume>91</volume><fpage>2314</fpage><lpage>2322</lpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2011.04.016</pub-id></element-citation></ref><ref id="B33-sensors-25-05232"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pei</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>B.Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>W.Y.</given-names></name></person-group><article-title>Narrowband Notch Filter Using Feedback Structure Tips &amp; Tricks</article-title><source>IEEE Signal Process. Mag.</source><year>2016</year><volume>33</volume><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1109/msp.2016.2531578</pub-id></element-citation></ref><ref id="B34-sensors-25-05232"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>X.</given-names></name><name name-style="western"><surname>An</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Through-the-wall high-dimensional imaging of human vital signs by combining multiple enhancement algorithms using portable LFMCW-MIMO radar</article-title><source>Measurement</source><year>2022</year><volume>195</volume><fpage>111074</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2022.111074</pub-id></element-citation></ref><ref id="B35-sensors-25-05232"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pramudita</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>D.B.</given-names></name><name name-style="western"><surname>Hsieh</surname><given-names>S.N.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ryanu</surname><given-names>H.H.</given-names></name><name name-style="western"><surname>Adiprabowo</surname><given-names>T.</given-names></name><name name-style="western"><surname>Purnomo</surname><given-names>A.T.</given-names></name></person-group><article-title>Radar System for Detecting Respiration Vital Sign of Live Victim Behind the Wall</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>22</volume><fpage>14670</fpage><lpage>14685</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3188165</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05232-f001" orientation="portrait"><label>Figure 1</label><caption><p>Two-stage unmanned search and rescue scheme.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g001.jpg"/></fig><fig position="float" id="sensors-25-05232-f002" orientation="portrait"><label>Figure 2</label><caption><p>Hardware structure of airborne radar system. (1). Onboard computer. (2). Transmission radio station of data collection end. (3). Flight controller. (4). X4M200 IR-UWB radar. (5). High-definition camera. (6) GPS module. (7) Transmission radio station of data reception end. (8) Ground station for visualizing.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g002.jpg"/></fig><fig position="float" id="sensors-25-05232-f003" orientation="portrait"><label>Figure 3</label><caption><p>Pictures of airborne radar system. (<bold>a</bold>) Front view of the system. (<bold>b</bold>) Bottom view of the system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g003.jpg"/></fig><fig position="float" id="sensors-25-05232-f004" orientation="portrait"><label>Figure 4</label><caption><p>Block diagram and detection principle of X4M200 IR-UWB radar.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g004.jpg"/></fig><fig position="float" id="sensors-25-05232-f005" orientation="portrait"><label>Figure 5</label><caption><p>Background environments of the human target: (<bold>a</bold>) Smooth ground. (<bold>b</bold>) Grassland.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g005.jpg"/></fig><fig position="float" id="sensors-25-05232-f006" orientation="portrait"><label>Figure 6</label><caption><p>Grass&#8211;surface clutter signal. (<bold>a</bold>) Time domain waveform. (<bold>b</bold>) Spectrum.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g006.jpg"/></fig><fig position="float" id="sensors-25-05232-f007" orientation="portrait"><label>Figure 7</label><caption><p>Statistical characteristics of clutter. (<bold>a</bold>) Frequency distribution histogram. (<bold>b</bold>) Fit curve, the probability density function is blue, and the normal distribution fitting curve is red.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g007.jpg"/></fig><fig position="float" id="sensors-25-05232-f008" orientation="portrait"><label>Figure 8</label><caption><p>The block diagram of the vital signal extraction method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g008.jpg"/></fig><fig position="float" id="sensors-25-05232-f009" orientation="portrait"><label>Figure 9</label><caption><p>Schematic diagram of range migration compensation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g009.jpg"/></fig><fig position="float" id="sensors-25-05232-f010" orientation="portrait"><label>Figure 10</label><caption><p>Field test result. (<bold>a</bold>) Raw data. (<bold>b</bold>) After compensation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g010.jpg"/></fig><fig position="float" id="sensors-25-05232-f011" orientation="portrait"><label>Figure 11</label><caption><p>Two-dimensional pseudo-color image of UWB radar. (<bold>a</bold>) Raw radar data. (<bold>b</bold>) Data after preprocessing.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g011.jpg"/></fig><fig position="float" id="sensors-25-05232-f012" orientation="portrait"><label>Figure 12</label><caption><p>Extracted vital signals at the target position. (<bold>a</bold>) Respiratory signal. (<bold>b</bold>) Heartbeat signal.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g012.jpg"/></fig><fig position="float" id="sensors-25-05232-f013" orientation="portrait"><label>Figure 13</label><caption><p>Amplitude&#8211;frequency curve of the feedback notch filter.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g013.jpg"/></fig><fig position="float" id="sensors-25-05232-f014" orientation="portrait"><label>Figure 14</label><caption><p>Experiment in scenario 1. (<bold>a</bold>) Scenario setup. (<bold>b</bold>) Raw radar data.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g014.jpg"/></fig><fig position="float" id="sensors-25-05232-f015" orientation="portrait"><label>Figure 15</label><caption><p>Target location. (<bold>a</bold>) Range FFT of raw data. (<bold>b</bold>) Cross-section view.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g015.jpg"/></fig><fig position="float" id="sensors-25-05232-f016" orientation="portrait"><label>Figure 16</label><caption><p>Observed signals.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g016.jpg"/></fig><fig position="float" id="sensors-25-05232-f017" orientation="portrait"><label>Figure 17</label><caption><p>Results of scenario 1. (<bold>a</bold>) Reference respiratory signal. (<bold>b</bold>) Recovered respiratory signal by proposed method. (<bold>c</bold>) Recovered respiratory signal by reference method. (<bold>d</bold>) Spectrum of signal (<bold>a</bold>). (<bold>e</bold>) Spectrum of signal (<bold>b</bold>). (<bold>f</bold>) Spectrum of signal (<bold>c</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g017.jpg"/></fig><fig position="float" id="sensors-25-05232-f018" orientation="portrait"><label>Figure 18</label><caption><p>Results of scenario 1. (<bold>a</bold>) Reference heartbeat signal. (<bold>b</bold>) Spectrum of signal (<bold>a</bold>). (<bold>c</bold>) Recovered heartbeat signal. (<bold>d</bold>) Spectrum of signal (<bold>c</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g018.jpg"/></fig><fig position="float" id="sensors-25-05232-f019" orientation="portrait"><label>Figure 19</label><caption><p>Experiment in scenario 2. (<bold>a</bold>) Scenario setup. (<bold>b</bold>) Raw radar data.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g019.jpg"/></fig><fig position="float" id="sensors-25-05232-f020" orientation="portrait"><label>Figure 20</label><caption><p>Results of scenario 2. (<bold>a</bold>) Reference respiratory signal. (<bold>b</bold>) Recovered respiratory signal by proposed method. (<bold>c</bold>) Recovered respiratory signal by reference method. (<bold>d</bold>) Spectrum of signal (<bold>a</bold>). (<bold>e</bold>) Spectrum of signal (<bold>b</bold>). (<bold>f</bold>) Spectrum of signal (<bold>c</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05232-g020.jpg"/></fig><table-wrap position="float" id="sensors-25-05232-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05232-t001_Table 1</object-id><label>Table 1</label><caption><p>Radar experimental parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Center Frequency</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bandwidth</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Detection Zone</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Range Resolution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Frame Rate</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.29 GHz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.4 GHz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4~5 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0514 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17 Hz</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05232-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05232-t002_Table 2</object-id><label>Table 2</label><caption><p>Results of respiratory rate estimation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Parameter</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">RR (Hz)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Accuracy (%)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">SNR (dB)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Scenario 1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3113</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3154</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3154</td><td align="center" valign="middle" rowspan="1" colspan="1">98.68</td><td align="center" valign="middle" rowspan="1" colspan="1">98.68</td><td align="center" valign="middle" rowspan="1" colspan="1">12.457</td><td align="center" valign="middle" rowspan="1" colspan="1">11.035</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scenario 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.332</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.332</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.002</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.983</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05232-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05232-t003_Table 3</object-id><label>Table 3</label><caption><p>Results of different distances.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RR (Hz)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">HR (Hz)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">2 m</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2366</td><td align="center" valign="middle" rowspan="1" colspan="1">98.50</td><td align="center" valign="middle" rowspan="1" colspan="1">1.017</td><td align="center" valign="middle" rowspan="1" colspan="1">98.44</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3 m</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3113</td><td align="center" valign="middle" rowspan="1" colspan="1">96.48</td><td align="center" valign="middle" rowspan="1" colspan="1">1.166</td><td align="center" valign="middle" rowspan="1" colspan="1">96.25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4 m</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2449</td><td align="center" valign="middle" rowspan="1" colspan="1">95.74</td><td align="center" valign="middle" rowspan="1" colspan="1">1.137</td><td align="center" valign="middle" rowspan="1" colspan="1">95.99</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2813</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.148</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.22</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>