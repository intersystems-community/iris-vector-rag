<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431018</article-id><article-id pub-id-type="pmcid-ver">PMC12431018.1</article-id><article-id pub-id-type="pmcaid">12431018</article-id><article-id pub-id-type="pmcaiid">12431018</article-id><article-id pub-id-type="doi">10.3390/s25175604</article-id><article-id pub-id-type="publisher-id">sensors-25-05604</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Communication</subject></subj-group></article-categories><title-group><article-title>Two-Stage Marker Detection&#8211;Localization Network for Bridge-Erecting Machine Hoisting Alignment</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="L">Lei</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5589-7423</contrib-id><name name-style="western"><surname>Xiao</surname><given-names initials="Z">Zelong</given-names></name><xref rid="c1-sensors-25-05604" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Hu</surname><given-names initials="T">Taiyang</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Vrochidis</surname><given-names initials="S">Stefanos</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Ioannidis</surname><given-names initials="K">Konstantinos</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Dourvas</surname><given-names initials="NI">Nikolaos I.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05604">School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing 210094, China</aff><author-notes><corresp id="c1-sensors-25-05604"><label>*</label>Correspondence: <email>zelongxiao@mail.njust.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>08</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5604</elocation-id><history><date date-type="received"><day>20</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>30</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>08</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05604.pdf"/><abstract><p>To tackle the challenges of complex construction environment interference (e.g., lighting variations, occlusion, and marker contamination) and the demand for high-precision alignment during the hoisting process of bridge-erecting machines, this paper presents a two-stage marker detection&#8211;localization network tailored to hoisting alignment. The proposed network adopts a &#8220;coarse detection&#8211;fine estimation&#8221; phased framework; the first stage employs a lightweight detection module, which integrates a dynamic hybrid backbone (DHB) and dynamic switching mechanism to efficiently filter background noise and generate coarse localization boxes of marker regions. Specifically, the DHB dynamically switches between convolutional and Transformer branches to handle features of varying complexity (using depthwise separable convolutions from MobileNetV3 for low-level geometric features and lightweight Transformer blocks for high-level semantic features). The second stage constructs a Transformer-based homography estimation module, which leverages multi-head self-attention to capture long-range dependencies between marker keypoints and the scene context. By integrating enhanced multi-scale feature interaction and position encoding (combining the absolute position and marker geometric priors), this module achieves the end-to-end learning of precise homography matrices between markers and hoisting equipment from the coarse localization boxes. To address data scarcity in construction scenes, a multi-dimensional data augmentation strategy is developed, including random homography transformation (simulating viewpoint changes), photometric augmentation (adjusting brightness, saturation, and contrast), and background blending with bounding box extraction. Experiments on a real bridge-erecting machine dataset demonstrate that the network achieves detection accuracy (mAP) of 97.8%, a homography estimation reprojection error of less than 1.2 mm, and a processing frame rate of 32 FPS. Compared with traditional single-stage CNN-based methods, it significantly improves the alignment precision and robustness in complex environments, offering reliable technical support for the precise control of automated hoisting in bridge-erecting machines.</p></abstract><kwd-group><kwd>bridge-erecting machine alignment</kwd><kwd>two-stage detection&#8211;localization</kwd><kwd>Transformer homography estimation</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05604"><title>1. Introduction</title><p>In modern bridge engineering, bridge-erecting machines serve as core equipment for the hoisting of precast girders/bridge panels, where the alignment during hoisting directly impacts the construction efficiency and operational safety. In automated construction scenarios, the high-precision detection and localization of markers (e.g., positioning targets, reference lines) is critical in achieving accurate docking between bridge-erecting machines and precast components. However, the complexity of real construction environments poses significant challenges for visual perception systems: dynamic lighting conditions (e.g., strong light reflection, shadow occlusion), random occlusion by dust/mechanical parts, and the surface contamination of markers after long-term use can all lead to false detection, missed detection, or localization errors in traditional vision methods.</p><p>Additionally, the dual requirements of real-time performance (to match the equipment movement speed) and alignment precision (millimeter-level error tolerance) make it difficult for traditional single-stage detection&#8211;localization methods based on convolutional neural networks (CNNs) to balance efficiency and robustness. Furthermore, the scarcity of annotated data in real construction scenarios (limited by construction cycles, safety regulations, etc.) further restricts the generalization capabilities of models.</p><p>To address the high precision requirements and data scarcity in marker detection&#8211;localization under complex construction environments, this paper proposes a two-stage marker detection&#8211;localization network for bridge-erecting machine hoisting alignment, with three key innovations. First, a &#8220;coarse detection&#8211;fine estimation&#8221; phased architecture is designed: the first stage uses a lightweight detection module with a dynamic hybrid backbone and adaptive feature switching to filter background interference and generate coarse marker region boxes; the second stage constructs a Transformer-based homography estimation module, which leverages self-attention to capture long-range dependencies between marker keypoints and the scene context, enhancing multi-scale feature interaction and positional encoding to achieve the end-to-end learning of precise homography matrices. Second, a multi-dimensional data augmentation strategy simulates lighting distortion, occlusion, and marker contamination to generate diverse training samples, improving the model&#8217;s adaptability to complex environments.</p><p>Experimental results show that the proposed method achieves detection accuracy (mAP) of 97.8%, a reprojection error of less than 1.2 mm in homography estimation, and a processing frame rate of 32 FPS on a real bridge-erecting machine construction dataset. Compared with traditional single-stage methods, it significantly enhances the alignment precision and robustness in complex environments, providing reliable technical support for the precise control of automated hoisting in bridge-erecting machines.</p></sec><sec id="sec2-sensors-25-05604"><title>2. Related&#160;Works</title><p>Marker detection and localization under complex construction environments have been a long-standing challenge in computer vision and construction robotics. This section reviews key advancements in three interrelated areas, namely traditional feature-based methods, deep learning-driven detection&#8211;localization frameworks, and data augmentation strategies, while highlighting their limitations in addressing the specific demands of bridge-erecting machine hoisting alignment.</p><p><bold>Traditional Feature-Based Methods.</bold> Early efforts in marker detection relied on handcrafted feature extraction and geometric matching. The Scale-Invariant Feature Transform (SIFT) [<xref rid="B1-sensors-25-05604" ref-type="bibr">1</xref>], a classic local feature descriptor, has been widely used to detect and match keypoints for homography estimation. SIFT&#8217;s invariance to scale and rotation makes it robust to viewpoint changes, but it struggles with dynamic lighting variations (e.g., strong reflections or shadows) and partial occlusion&#8212;common in construction sites&#8212;due to its sensitivity to intensity changes. Similarly, Harris corner detectors [<xref rid="B2-sensors-25-05604" ref-type="bibr">2</xref>] focus on local edge responses for corner detection, but they fail to capture the global context, leading to unstable performance when markers are contaminated (e.g., by dust or paint peeling). Other feature point extractors include Speeded-Up Robust Features (SURF) [<xref rid="B3-sensors-25-05604" ref-type="bibr">3</xref>], Oriented FAST and Rotated BRIEF (ORB) [<xref rid="B4-sensors-25-05604" ref-type="bibr">4</xref>] and the Boosted Efficient Binary Local Image Descriptor (BEBLID) [<xref rid="B5-sensors-25-05604" ref-type="bibr">5</xref>]. ORB builds on two foundational techniques: the Features from Accelerated Segment Test (FAST) method, a rapid corner detector that identifies keypoints by comparing the pixel intensities in a circular region, and Binary Robust Independent Elementary Features (BRIEF), a descriptor that generates binary feature vectors via random pixel pair comparisons. ORB enhances these with rotation invariance, making it suitable for real-time scenarios. These methods are widely used for marker detection but share similar limitations in complex construction environments.</p><p><bold>Deep Learning for Detection&#8211;Localization.</bold> The rise of deep learning has spurred the development of end-to-end detection&#8211;localization frameworks. Single-stage detectors like You Only Look Once (YOLO) [<xref rid="B6-sensors-25-05604" ref-type="bibr">6</xref>] and the Single-Shot MultiBox Detector (SSD) [<xref rid="B7-sensors-25-05604" ref-type="bibr">7</xref>] achieve real-time speeds by directly predicting bounding boxes and class scores in a single pass. However, their focus on efficiency often sacrifices precision, especially for small or occluded markers. For instance, YOLOv8 [<xref rid="B8-sensors-25-05604" ref-type="bibr">8</xref>], a state-of-the-art single-stage model, struggles to distinguish markers from cluttered backgrounds (e.g., machinery or debris) in construction scenes, resulting in high false-positive rates.</p><p>Two-stage frameworks, such as the Faster Region-Based Convolutional Neural Network (Faster R-CNN) [<xref rid="B9-sensors-25-05604" ref-type="bibr">9</xref>], improve the accuracy by first generating region proposals and then refining them. Faster R-CNN&#8217;s region proposal network (RPN) better handles object scale variations, but its computational complexity limits real-time performance&#8212;critical in aligning fast-moving hoisting equipment. Moreover, both single- and two-stage CNN-based methods rely on convolutional layers, which inherently model local spatial correlations but fail to capture long-range dependencies between marker keypoints and the scene context (e.g., the relationship between marker corners and nearby machinery). This limitation restricts their ability to estimate precise homography matrices under occluded or contaminated conditions.</p><p><bold>Homography Estimation with Deep Learning.</bold> Homography estimation, a core component of marker localization, has been explored using CNNs. Recent advances in deep learning have revolutionized homography estimation by replacing traditional multi-step pipelines with end-to-end trainable frameworks. These methods can be broadly categorized into supervised, unsupervised, and hybrid approaches (e.g., self-supervised, semi-supervised), each addressing distinct challenges in geometric alignment [<xref rid="B10-sensors-25-05604" ref-type="bibr">10</xref>].</p><p>Supervised methods leverage synthetic or labeled datasets to train networks for direct homography regression. Early work by DeTone et al. [<xref rid="B11-sensors-25-05604" ref-type="bibr">11</xref>] introduced the four-point parameterization of the homography matrix, enabling efficient network training by regressing offsets of keypoints rather than the full 3 &#215; 3 matrix. Subsequent studies optimized network architectures for efficiency, such as lightweight models based on ShuffleNet [<xref rid="B12-sensors-25-05604" ref-type="bibr">12</xref>], which reduced the parameters to under 9 MB while maintaining accuracy, making them suitable for edge devices. Hybrid frameworks like HomoNetComb [<xref rid="B13-sensors-25-05604" ref-type="bibr">13</xref>] combined deep learning with energy minimization, using CNNs to predict initial homographies and gradient descent to refine residuals, balancing speed and precision. Transformer-based models further improved the performance by capturing long-range feature dependencies, with attention mechanisms enhancing alignment in large-baseline scenarios [<xref rid="B14-sensors-25-05604" ref-type="bibr">14</xref>]. However, supervised methods face limitations in generalizing to real-world scenes due to the scarcity of labeled data and synthetic-to-real domain gaps.</p><p>Unsupervised methods eliminate the need for labeled data by optimizing the photometric consistency between warped and target images. A key innovation was the introduction of homography flow [<xref rid="B15-sensors-25-05604" ref-type="bibr">15</xref>], a low-rank representation of optical flow constrained by the homography subspace, enabling robust estimation by focusing on dominant planar motion. Generative adversarial networks (GANs) were also applied to enforce coplanarity constraints, where discriminators distinguish between original and warped images to guide homography prediction [<xref rid="B16-sensors-25-05604" ref-type="bibr">16</xref>]. Content-aware masks [<xref rid="B17-sensors-25-05604" ref-type="bibr">17</xref>] and contextual correlation layers [<xref rid="B18-sensors-25-05604" ref-type="bibr">18</xref>] were integrated to suppress dynamic objects and occlusions, improving the robustness in low-texture or noisy environments. Multi-scale and cascaded network structures [<xref rid="B19-sensors-25-05604" ref-type="bibr">19</xref>] further refined coarse-to-fine estimation, reducing the reprojection errors for large displacements. Despite progress, unsupervised methods struggle with training stability and may fail in scenes with significant depth variations.</p><p>Despite advancements, existing deep learning methods face trade-offs between accuracy, speed, and robustness in dynamic construction environments. Supervised models lack adaptability to unlabeled real-world data, while unsupervised approaches struggle with large parallax and occlusion. Single-stage CNNs often sacrifice precision for efficiency, failing to meet the millimeter-level alignment requirements of bridge-erecting machines. Thus, a two-stage framework combining lightweight detection and Transformer-based fine estimation is proposed to address these challenges.</p><p><bold>Computer Vision in Civil Engineering and Construction.</bold> Computer vision is widely applied in civil engineering and construction, facilitating tasks like safety monitoring (e.g., detecting non-hardhat wearers [<xref rid="B20-sensors-25-05604" ref-type="bibr">20</xref>]), tracking workers and equipment [<xref rid="B21-sensors-25-05604" ref-type="bibr">21</xref>], and progress monitoring via 3D point clouds [<xref rid="B22-sensors-25-05604" ref-type="bibr">22</xref>]. It enables activity recognition for earthmoving operations [<xref rid="B23-sensors-25-05604" ref-type="bibr">23</xref>] and interactions between excavators and dump trucks [<xref rid="B24-sensors-25-05604" ref-type="bibr">24</xref>]. Techniques include CNNs for object detection [<xref rid="B25-sensors-25-05604" ref-type="bibr">25</xref>] and two-stream networks for worker activity recognition [<xref rid="B26-sensors-25-05604" ref-type="bibr">26</xref>], using data from cameras, scanners, and unmanned aerial vehicles (UAVs) [<xref rid="B27-sensors-25-05604" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05604" ref-type="bibr">28</xref>]. These applications address site complexity, enhancing safety, productivity, and decision making.</p><p><bold>Data Augmentation for Construction Scenes.</bold> Data scarcity is a critical bottleneck in training robust models for construction environments, where annotated images are limited due to safety regulations and long construction cycles. Synthetic data generation, such as the SYNTHIA dataset [<xref rid="B29-sensors-25-05604" ref-type="bibr">29</xref>] for urban scenes, uses 3D rendering to simulate diverse environments. However, these datasets focus on urban settings (e.g., streets and buildings) and fail to simulate construction-specific interferences like dust occlusion, marker contamination, or dynamic lighting from heavy machinery. Existing augmentation strategies for object detection (e.g., random cropping or flipping) [<xref rid="B30-sensors-25-05604" ref-type="bibr">30</xref>] also do not address construction-specific challenges, leaving models poorly adapted to real-world scenarios.</p></sec><sec sec-type="methods" id="sec3-sensors-25-05604"><title>3. Methodology</title><sec id="sec3dot1-sensors-25-05604"><title>3.1. Overall&#160;Architecture</title><p>The proposed framework adopts a two-stage &#8220;coarse detection&#8211;fine estimation&#8221; architecture (<xref rid="sensors-25-05604-f001" ref-type="fig">Figure 1</xref>).</p><list list-type="bullet"><list-item><p>Stage 1: Lightweight Marker Detection Module for the rapid coarse localization of markers in complex environments.</p></list-item><list-item><p>Stage 2: Transformer-Based Homography Estimation Module for precise coordinate transformation between markers and hoisting equipment.</p></list-item></list></sec><sec id="sec3dot2-sensors-25-05604"><title>3.2. Stage 1: Lightweight Marker Detection&#160;Module</title><p>As shown in <xref rid="sensors-25-05604-f002" ref-type="fig">Figure 2</xref>, this stage focuses on efficient coarse localization through the dynamic hybrid block, which consists of two key components: a dynamic hybrid backbone (DHB) and a dynamic switching mechanism. Multi-scale feature maps are generated by the dynamic hybrid blocks, and these feature maps are further processed by the neck and detection head, which are consistent with YOLOv8 [<xref rid="B8-sensors-25-05604" ref-type="bibr">8</xref>].</p><sec><title>Dynamic Hybrid Block (DHB)</title><p>To balance geometric feature extraction and complex texture modeling, the DHB dynamically switches between convolutional and Transformer branches based on feature complexity, as shown in <xref rid="sensors-25-05604-f003" ref-type="fig">Figure 3</xref>. The input feature is processed by two branches of feature processing network and a dynamic switching block. The two-branch network is designed as follows:<list list-type="bullet"><list-item><p>The first branch uses MobileNetV3&#8217;s depthwise separable convolutions to extract geometric features (edges, corners). The depthwise convolution operation is defined as<disp-formula id="FD1-sensors-25-05604"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&#8859;</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>k</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the depthwise kernel, <italic toggle="yes">k</italic> is the kernel size, <italic toggle="yes">C</italic> is the input channel, &#8859; denotes depthwise convolution, and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the bias.</p></list-item><list-item><p>The second branch introduces lightweight Transformer blocks with scene-aware attention. The window multi-head self-attention (W-MSA) is computed as<disp-formula id="FD2-sensors-25-05604"><label>(2)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi><mml:mi>-</mml:mi><mml:mi>MSA</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>head</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>head</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>head</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Z</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and a scene prior bias <italic toggle="yes">B</italic> (encoding marker aspect ratio/position distribution) is added to the attention scores:<disp-formula id="FD3-sensors-25-05604"><label>(3)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mo>&#8868;</mml:mo></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mfenced><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item></list></p><p>For the dynamic switching module, a feature complexity discriminator calculates the entropy <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of feature map <italic toggle="yes">F</italic>:<disp-formula id="FD4-sensors-25-05604"><label>(4)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mi>F</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mfrac></mml:mstyle><mml:mo form="prefix">log</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mi>F</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The gating weight <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (via a sigmoid function) determines the branch contribution:<disp-formula id="FD5-sensors-25-05604"><label>(5)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow><mml:mi>&#948;</mml:mi></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>F</mml:mi><mml:mi>stage</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>trans</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>conv</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>=</mml:mo><mml:mn>3.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (threshold), <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (scaling factor). The threshold <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:math></inline-formula> acts as a criterion to judge the feature complexity. It determines whether the input feature maps have more high-level semantic information (when entropy <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>&gt;</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) or low-level geometric information (when <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). The scaling factor (<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) controls the smoothness of weight switching via the sigmoid function. A smaller <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow></mml:math></inline-formula> causes the weight change to be steeper. Here, it balances sensitivity and smoothness for stable adaptation to complex construction scenarios.</p><p>Threshold <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>=</mml:mo><mml:mn>3.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is derived from an entropy distribution analysis of 500 training images, where <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>3.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> indicated high-level semantic features (e.g., cluttered backgrounds) and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>3.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> indicated low-level geometric features (e.g., marker edges). Testing <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>2.5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3.0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3.5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>4.0</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> showed that this value minimized feature type misclassification.</p><p>Scaling factor <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> controls sigmoid smoothness. Evaluating <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#948;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0.7</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> revealed that this value balanced sensitivity to feature changes and training stability, avoiding abrupt weight fluctuations in mixed-complexity scenarios (e.g., partially occluded markers).</p></sec></sec><sec id="sec3dot3-sensors-25-05604"><title>3.3. Stage 2: Transformer-Based Homography Estimation&#160;Module</title><p>This stage refines marker localization using the coarse detection results from Stage 1, focusing on modeling long-range dependencies between marker keypoints and the scene context&#8212;critical for precise homography estimation under occlusion or contamination. The module architecture (<xref rid="sensors-25-05604-f004" ref-type="fig">Figure 4</xref>) consists of three core components: input feature processing, a Transformer encoder with fused positional encoding, and a homography decoding head.</p><sec id="sec3dot3dot1-sensors-25-05604"><title>3.3.1. Input&#160;Processing</title><p>Regions of interest (ROIs) cropped from the Stage 1 outputs (resized to 256 &#215; 256) are fed into ResNet50&#8217;s convolutional layers (C3&#8211;C5), which extract multi-scale features with 256, 512, and 1024 channels, respectively. These features are concatenated and flattened into a sequence <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for the final feature map, preserving the spatial and semantic information necessary for keypoint correlation.</p></sec><sec id="sec3dot3dot2-sensors-25-05604"><title>3.3.2. Transformer&#160;Encoder</title><p>The encoder (6 layers) uses multi-head self-attention to model the relationships between all pairs of features in the sequence, enabling the capture of long-range dependencies (e.g., between marker corners and nearby structural edges). A critical design is the fused positional encoding, which combines the following:<list list-type="order"><list-item><p>Absolute positional encoding: Encodes pixel coordinates <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the ROI to preserve spatial layout.</p></list-item><list-item><p>Geometric prior encoding: Encodes marker-specific priors (center coordinates: <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>center</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>center</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, width: <italic toggle="yes">w</italic>, height: <italic toggle="yes">h</italic>) derived from Stage 1&#8217;s coarse bounding box, anchoring attention to the marker geometry.</p></list-item></list></p><p>The positional encoding equation can be written as<disp-formula id="FD6-sensors-25-05604"><label>(6)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>pos</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>PE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>center</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>center</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>PE</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> encodes the marker center coordinates and aspect ratio. The multi-head attention operation is<disp-formula id="FD7-sensors-25-05604"><label>(7)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>MultiHead</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>head</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>head</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This splits features into subspaces to model diverse correlations (e.g., local shape vs. global context).</p></sec><sec id="sec3dot3dot3-sensors-25-05604"><title>3.3.3. Homography&#160;Decoding</title><p>After encoding, global average pooling (GAP) aggregates the sequence into a compact feature vector <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, capturing the global scene context. A 3-layer multi-layer perceptron (MLP) maps <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> to a 9-dimensional vector (flattened <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> matrix), normalized by the Frobenius norm to ensure scale invariance:<disp-formula><label>(8)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>MLP</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> prevents scale ambiguity in homography regression.</p></sec></sec><sec id="sec3dot4-sensors-25-05604"><title>3.4. Multi-Dimensional Data&#160;Augmentation</title><p>To address the challenge of limited annotated data for object detection tasks, we propose a synthetic sample generation pipeline that combines geometric transformations, photometric augmentations, and background integration. This approach generates diverse training samples with corresponding YOLO-formatted bounding box annotations, simulating real-world variations in object appearance and context.</p><sec id="sec3dot4dot1-sensors-25-05604"><title>3.4.1. Random Homography&#160;Transformation</title><p>First, we apply a small-magnitude random homography transformation to the target template to simulate viewpoint variations (e.g., rotation, scaling, shearing, and translation). The transformation matrix <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is constructed with perturbations bounded by a parameter <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>max_perturb</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (default: <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), which controls the maximum relative deviation from the identity matrix. Specifically,</p><list list-type="bullet"><list-item><p>Scaling factors for <italic toggle="yes">x</italic>- and <italic toggle="yes">y</italic>-axes are perturbed by &#177;<inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>max_perturb</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p>Shearing terms <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are perturbed by &#177;<inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>max_perturb</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>;</p></list-item><list-item><p>Translation offsets <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are limited to &#177;<inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>max_perturb</mml:mo></mml:mrow><mml:mo>&#215;</mml:mo><mml:mrow><mml:mo>template_width</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and &#177;<inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>max_perturb</mml:mo></mml:mrow><mml:mo>&#215;</mml:mo><mml:mrow><mml:mo>template_width</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively.</p></list-item></list><p>After transformation, the output size is dynamically adjusted to avoid cropping by aligning the minimum transformed coordinates to the top-left corner of the canvas.</p></sec><sec id="sec3dot4dot2-sensors-25-05604"><title>3.4.2. Photometric&#160;Augmentation</title><p>To enhance the sample diversity, we apply photometric transformations to the transformed template. This includes the following:<list list-type="bullet"><list-item><p>Color space adjustments: Random brightness (<inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>color_range</mml:mi></mml:mrow><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>1.3</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) and saturation modifications in the HSV space, followed by contrast (<inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>1.3</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) and brightness offset (<inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) adjustments in the BGR space;</p></list-item><list-item><p>Gaussian noise injection: Additive Gaussian noise with variance <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>noise_var</mml:mi></mml:mrow><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to simulate sensor noise or low-light conditions.</p></list-item></list></p></sec><sec id="sec3dot4dot3-sensors-25-05604"><title>3.4.3. Background Integration and Bounding Box&#160;Extraction</title><p>The augmented template is then randomly pasted onto a background image (selected from a predefined dataset: VOC2007) while ensuring full containment within the background dimensions. The VOC2007 dataset is part of the PASCAL VOC project; it encompasses 9963 images and covers diverse scenes like urban streets, indoor rooms, and natural landscapes. A binary mask (thresholded at 1 pixel intensity) is used to identify the non-black (foreground) region of the transformed template, from which the minimum bounding box <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is extracted. This bounding box is translated to absolute coordinates relative to the background image. Several examples of generated image samples are shown in <xref rid="sensors-25-05604-f005" ref-type="fig">Figure 5</xref>.</p></sec></sec><sec id="sec3dot5-sensors-25-05604"><title>3.5. Loss&#160;Function</title><p>The first-stage detection loss follows the YOLOv11 formulation, which includes localization, confidence, and classification components, as defined in the original YOLOv11 architecture.</p><p>The second-stage loss for homography matrix <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> uses Smooth L1 to regress the predicted <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> to the ground-truth <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mo>&#8727;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>:<disp-formula><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>homo</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>9</mml:mn></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>9</mml:mn></mml:munderover><mml:mrow><mml:mi>SmoothL</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec4-sensors-25-05604"><title>4. Experimental Setup and Result&#160;Analysis</title><sec id="sec4dot1-sensors-25-05604"><title>4.1. Experimental&#160;Setup</title><p>The proposed network is trained on a PC with an NVIDIA GeForce RTX 2080Ti GPU (Santa Clara, CA, USA) and 64 GB RAM. The network is implemented with PyTorch 1.10.0 with CUDA 11.3. Training employs a learning rate of 0.001 and batch size of 16 for 100 epochs, with the Adam optimizer and cosine annealing for learning rate scheduling.</p></sec><sec id="sec4dot2-sensors-25-05604"><title>4.2. Experimental Datasets and&#160;Preprocessing</title><p>The Third Construction Co., Ltd. of China Construction Eighth Engineering Bureau, Nanjing, China.</p><sec id="sec4dot2dot1-sensors-25-05604"><title>4.2.1. Real Construction&#160;Dataset</title><p>The dataset was collected from 3 bridge construction sites (The Third Construction Co., Ltd. of China Construction Eighth Engineering Bureau, Nanjing, China) over 2 months (April&#8211;May 2024) using a Intel RealSense Depth Camera D455 (Santa Clara, CA, USA) (12 MP resolution, 30 fps) mounted on the bridge-erecting machine&#8217;s hoist arm. The dataset includes 2000 images with the following:<list list-type="bullet"><list-item><p>Markers: 5 types of rectangular markers (10&#8211;20cm in size) affixed to precast girders;</p></list-item><list-item><p>Annotations: Bounding boxes (labeled via LabelImg v1.8.5) and 4 corner coordinates (manually verified for sub-pixel accuracy using OpenCV4.0&#8217;s cornerSubPix);</p></list-item><list-item><p>Disturbances: 32% with lighting variations (morning/afternoon sun, overcast), 28% with partial occlusion (crane arms, worker bodies), 21% with contamination (dust, paint peeling)&#8212;a distribution aligned with field observations.</p></list-item></list></p></sec><sec id="sec4dot2dot2-sensors-25-05604"><title>4.2.2. Augmented&#160;Dataset</title><p>Following the multi-dimensional augmentation strategy described in <xref rid="sec3dot4-sensors-25-05604" ref-type="sec">Section 3.4</xref>, 5000 synthetic images were generated by simulating interference factors (lighting distortion, random occlusion, marker contamination). These synthetic data were mixed with real data at a 1:1 ratio for training.</p></sec></sec><sec id="sec4dot3-sensors-25-05604"><title>4.3. Evaluation&#160;Metrics</title><p>For the detection performance evaluation, the mean average precision (<inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) with an intersection over union threshold of 0.5 was used to evaluate the marker detection accuracy.</p><p>For the localization precision evaluation, the homography reprojection error is adopted, which is the average pixel difference between the predicted and ground-truth coordinates of marker corners after applying the estimated homography matrix.</p><p>For the real-time performance evaluation, the frames per second (FPS) was measured to evaluate the computational efficiency.</p><sec><title>Robustness</title><p>Robustness was assessed by measuring the <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and treprojection error on three challenging subsets:<list list-type="bullet"><list-item><p>Images with lighting variations;</p></list-item><list-item><p>Images with partial marker occlusion;</p></list-item><list-item><p>Images with contaminated markers (e.g., dust adhesion, paint peeling).</p></list-item></list></p></sec></sec><sec id="sec4dot4-sensors-25-05604"><title>4.4. Comparative&#160;Experiments</title><p>Three comparative methods were tested to validate the proposed network&#8217;s superiority:<list list-type="order"><list-item><p><bold>Single-stage detection + traditional homography</bold>: YOLOv8 (single-stage detector) combined with SIFT feature matching for homography estimation;</p></list-item><list-item><p><bold>Two-stage CNN network</bold>: Faster R-CNN (two-stage detector) followed by a CNN-based homography regression module;</p></list-item><list-item><p><bold>Proposed method without data augmentation</bold>: Identical to the proposed network but trained using only real data (no augmented samples).</p></list-item></list></p></sec><sec id="sec4dot5-sensors-25-05604"><title>4.5. Ablation&#160;Studies</title><p>Three key components were ablated to verify their contributions:<list list-type="order"><list-item><p><bold>Transformer module</bold>: Performance comparison between the proposed network and a variant where the Transformer-based homography estimation module was replaced with a CNN regression network.</p></list-item><list-item><p><bold>Data augmentation</bold>: Comparison of <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and reprojection error with/without using augmented training data.</p></list-item><list-item><p><bold>Positional encoding</bold>: Evaluation of three variants:</p><list list-type="bullet"><list-item><p>Using only absolute positional encoding;</p></list-item><list-item><p>Using only geometric prior encoding (marker center coordinates and aspect ratio);</p></list-item><list-item><p>Using the proposed fused positional encoding (absolute + geometric).</p></list-item></list></list-item></list></p></sec><sec id="sec4dot6-sensors-25-05604"><title>4.6. Result&#160;Analysis</title><sec id="sec4dot6dot1-sensors-25-05604"><title>4.6.1. Quantitative&#160;Results</title><p><xref rid="sensors-25-05604-t001" ref-type="table">Table 1</xref> presents the performance metrics as mean values &#177; standard deviations from three independent training runs, confirming the result stability. The proposed method exhibits the lowest variability across all metrics (e.g., mAP std. dev. = 0.4%), indicating robust convergence and consistent performance.</p><p>During inference, the lightweight marker detection in Stage 1 has average memory usage of 4.2 GB and an inference time of 19 ms per image (1280 &#215; 960). The homography estimation in Stage 2 has average memory usage of 5.8 GB and an inference time of 12 ms per image. The total end-to-end inference time is 31 ms per image.</p></sec><sec id="sec4dot6dot2-sensors-25-05604"><title>4.6.2. Ablation Study&#160;Results</title><p><bold>Transformer Module vs. CNN Regression.</bold>&#160;<xref rid="sensors-25-05604-t002" ref-type="table">Table 2</xref> shows that replacing the Transformer-based homography module with a CNN regression network reduces the mAP by 2.8% and increases the reprojection error by 0.8 mm. This gap stems from CNNs&#8217; focus on local correlations; thus, they fail to model long-range dependencies between marker keypoints and distant scene structures&#8212;critical for handling occlusion or contamination in construction. The Transformer&#8217;s multi-head attention captures these global relationships, enabling robust estimation even when markers are partially obscured (e.g., by crane arms), whereas CNNs&#8217; limited receptive fields lead to underfitting.</p><p><bold>Impact of Data Augmentation.</bold> Without data augmentation, the mAP drops by 1.7% and the reprojection error rises by 0.6 mm, confirming that our multi-dimensional strategy (random homography, photometric changes, background blending) mitigates overfitting to limited real data. Synthetic samples simulate dust, variable lighting, and other disturbances, introducing diverse texture and illumination variations. This forces the model to learn invariant features, enhancing its generalization to the highly variable conditions of real construction sites.</p><p><bold>Positional Encoding Variants.</bold> Fused positional encoding (absolute + geometric priors) outperforms single components, with a 1.3% higher mAP and 0.4 mm lower error than absolute encoding alone and a 1.5% higher mAP with a 0.5 mm lower error than geometric priors alone. Absolute encoding preserves fine-grained spatial details (e.g., marker edges), while geometric priors (center, dimensions from Stage 1) provide structural constraints. Their fusion balances local precision and the global context, preventing drift from noise or occlusion&#8212;vital for millimeter-level hoisting alignment.</p><p><xref rid="sensors-25-05604-t003" ref-type="table">Table 3</xref> shows additional experiments on data ratios. The 2:5 ratio outperforms the others, confirming its rationality as it balances real-world representation and synthetic diversity. It is noted that the current disturbance definitions (e.g., marker contamination, occlusion) are simplified representations of real-world complexities. While we focused on dust and paint peeling for contamination and random shapes for occlusion, real scenarios may involve oil stains, rust, linear obstructions (e.g., mechanical arms), or diffuse obstructions (e.g., dust clouds). However, our data augmentation strategy prioritized capturing core disturbance features: texture degradation for contamination and spatial obstruction for occlusion. The proposed two-stage network, with its dynamic feature adaptation (via DHB) and long-range dependency modeling (via Transformer), learns generalized patterns beyond specific disturbance types. This is supported by its robust performance on the existing disturbed subsets, suggesting potential adaptability to unlisted disturbances. Future work will expand the disturbance library to cover more specific cases, further validating the model&#8217;s generalization ability.</p></sec><sec id="sec4dot6dot3-sensors-25-05604"><title>4.6.3. Scalability&#160;Analysis</title><p>To assess the performance under varying operational conditions, we generated test data by randomly stitching two to four original images (640 &#215; 480) to create higher resolutions (1280 &#215; 960, 1920 &#215; 1440, 2880 &#215; 2160), with 2, 4&#8211;5, and 6&#8211;8 markers per frame, respectively. The results are shown in <xref rid="sensors-25-05604-t004" ref-type="table">Table 4</xref>. The model maintains high accuracy (mAP &gt; 96%) and low errors (&lt;1.6 mm) across the stitched conditions.</p></sec><sec id="sec4dot6dot4-sensors-25-05604"><title>4.6.4. Qualitative&#160;Analysis</title><p><xref rid="sensors-25-05604-f006" ref-type="fig">Figure 6</xref> presents the partial marker detection results. The detector can effectively detect the targets, with no false positives or false negatives observed. <xref rid="sensors-25-05604-f007" ref-type="fig">Figure 7</xref> shows the precise estimation results for homography (the green bounding box in the figure). We can see that the output results of the detector cannot accurately locate the boundaries of the markers. However, after our homography estimation, the obtained bounding boxes precisely align with the markers, thus meeting the requirement for high-precision localization.</p></sec><sec id="sec4dot6dot5-sensors-25-05604"><title>4.6.5. Robustness&#160;Verification</title><p>In extreme scenarios, the results are as follows:<list list-type="bullet"><list-item><p>Occluded markers: <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>95.6</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (as shown in <xref rid="sensors-25-05604-t001" ref-type="table">Table 1</xref>);</p></list-item><list-item><p>Contaminated markers: reprojection error <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn><mml:mspace width="4pt"/><mml:mi>mm</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p><p>These results validate the network&#8217;s strong engineering applicability under complex construction conditions.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05604"><title>5. Conclusions and&#160;Discussion</title><p>The proposed two-stage network, which integrates lightweight detection with Transformer-driven homography estimation, successfully achieves high-precision and real-time marker detection and localization in complex construction environments. Additionally, the multi-dimensional data augmentation strategy effectively mitigates the challenge of limited real-world data availability, thereby enhancing the model&#8217;s generalization performance. Furthermore, the experimental results validate the robustness and practicality of the proposed method in engineering scenarios, providing critical technical support for automated hoisting operations by bridge-erecting machines.</p><p>To further advance this research, several key directions are identified. First, multi-modal fusion will be explored by incorporating laser point cloud or infrared sensor data to improve the alignment accuracy under low-light or dusty conditions. Second, model lightweighting will be pursued through techniques such as knowledge distillation or quantization compression, aiming to reduce the computational complexity and enable deployment on embedded devices (e.g., bridge-erecting machine controllers) while maintaining real-time performance. Third, dynamic scenario extension will be addressed by introducing temporal modeling (e.g., combining Transformer with LSTM) to handle dynamically moving markers during hoisting, thereby enhancing the stability of continuous frame alignment.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, L.L. and Z.X.; software, L.L. and T.H.; validation, L.L. and T.H.;writing&#8212;original draft preparation, L.L.; writing&#8212;review and editing, L.L., T.H. and Z.X. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in the study are included in the article; further inquiries can be directed to the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05604"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lowe</surname><given-names>D.G.</given-names></name></person-group><article-title>Distinctive image features from scale-invariant keypoints</article-title><source>Int. J. Comput. Vis.</source><year>2004</year><volume>60</volume><fpage>91</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000029664.99615.94</pub-id></element-citation></ref><ref id="B2-sensors-25-05604"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Harris</surname><given-names>C.</given-names></name><name name-style="western"><surname>Stephens</surname><given-names>M.</given-names></name></person-group><article-title>A combined corner and edge detector</article-title><source>Proceedings of the Alvey Vision Conference</source><conf-loc>Manchester, UK</conf-loc><conf-date>31 August&#8211;2 September 1988</conf-date><publisher-name>British Machine Vision Association (BMVA)</publisher-name><publisher-loc>London, UK</publisher-loc><year>1988</year><volume>Volume 15</volume><fpage>10</fpage><lpage>5244</lpage></element-citation></ref><ref id="B3-sensors-25-05604"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bay</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ess</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tuytelaars</surname><given-names>T.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>Speeded-up robust features (SURF)</article-title><source>Comput. Vis. Image Underst.</source><year>2008</year><volume>110</volume><fpage>346</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2007.09.014</pub-id></element-citation></ref><ref id="B4-sensors-25-05604"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rublee</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rabaud</surname><given-names>V.</given-names></name><name name-style="western"><surname>Konolige</surname><given-names>K.</given-names></name><name name-style="western"><surname>Bradski</surname><given-names>G.</given-names></name></person-group><article-title>ORB: An efficient alternative to SIFT or SURF</article-title><source>Proceedings of the 2011 International Conference on Computer Vision</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>6&#8211;13 November 2011</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2011</year><fpage>2564</fpage><lpage>2571</lpage></element-citation></ref><ref id="B5-sensors-25-05604"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su&#225;rez</surname><given-names>I.</given-names></name><name name-style="western"><surname>Sfeir</surname><given-names>G.</given-names></name><name name-style="western"><surname>Buenaposada</surname><given-names>J.M.</given-names></name><name name-style="western"><surname>Baumela</surname><given-names>L.</given-names></name></person-group><article-title>BEBLID: Boosted efficient binary local image descriptor</article-title><source>Pattern Recognit. Lett.</source><year>2020</year><volume>133</volume><fpage>366</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2020.04.005</pub-id></element-citation></ref><ref id="B6-sensors-25-05604"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You only look once: Unified, real-time object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, IL, USA</conf-loc><conf-date>26 June&#8211;1 July 2016</conf-date><fpage>779</fpage><lpage>788</lpage></element-citation></ref><ref id="B7-sensors-25-05604"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Reed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name></person-group><article-title>Ssd: Single shot multibox detector</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2016: 14th European Conference</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#8211;14 October 2016</conf-date><comment>Proceedings, Part I 14</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2016</year><fpage>21</fpage><lpage>37</lpage></element-citation></ref><ref id="B8-sensors-25-05604"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Varghese</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sambath</surname><given-names>M.</given-names></name></person-group><article-title>Yolov8: A novel object detection algorithm with enhanced performance and robustness</article-title><source>Proceedings of the 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)</source><conf-loc>Chennai, India</conf-loc><conf-date>18&#8211;19 April 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B9-sensors-25-05604"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Faster r-cnn: Towards real-time object detection with region proposal networks</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2015</year><volume>28</volume><fpage>1137</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id></element-citation></ref><ref id="B10-sensors-25-05604"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Shu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name></person-group><article-title>A review of homography estimation: Advances and challenges</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>4977</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12244977</pub-id></element-citation></ref><ref id="B11-sensors-25-05604"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>DeTone</surname><given-names>D.</given-names></name><name name-style="western"><surname>Malisiewicz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Rabinovich</surname><given-names>A.</given-names></name></person-group><article-title>Deep image homography estimation</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/arXiv.1606.03798</pub-id><pub-id pub-id-type="arxiv">1606.03798</pub-id></element-citation></ref><ref id="B12-sensors-25-05604"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>You</surname><given-names>Z.</given-names></name><name name-style="western"><surname>An</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name></person-group><article-title>Efficient and robust homography estimation using compressed convolutional neural network</article-title><source>Proceedings of the Digital TV and Multimedia Communication: 15th International Forum, IFTC 2018</source><conf-loc>Shanghai, China</conf-loc><conf-date>20&#8211;21 September 2018</conf-date><comment>Revised Selected Papers 15</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2019</year><fpage>156</fpage><lpage>168</lpage></element-citation></ref><ref id="B13-sensors-25-05604"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name></person-group><article-title>Combining convolutional neural network and photometric refinement for accurate homography estimation</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>109460</fpage><lpage>109473</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2933635</pub-id></element-citation></ref><ref id="B14-sensors-25-05604"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Deep Homography Estimation With Feature Correlation Transformer</article-title><source>Proceedings of the 2023 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Brisbane, Australia</conf-loc><conf-date>10&#8211;14 July 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>1397</fpage><lpage>1402</lpage></element-citation></ref><ref id="B15-sensors-25-05604"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>N.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Motion basis learning for unsupervised deep homography estimation with subspace projection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>13117</fpage><lpage>13125</lpage></element-citation></ref><ref id="B16-sensors-25-05604"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hong</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>N.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Unsupervised homography estimation with coplanarity-aware gan</article-title><source>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>21&#8211;24 June 2022</conf-date><fpage>17663</fpage><lpage>17672</lpage></element-citation></ref><ref id="B17-sensors-25-05604"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>N.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Content-aware unsupervised deep homography estimation</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><comment>Proceedings, Part I 16</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>653</fpage><lpage>669</lpage></element-citation></ref><ref id="B18-sensors-25-05604"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nie</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name></person-group><article-title>Depth-aware multi-grid deep homography estimation with contextual correlation</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2021</year><volume>32</volume><fpage>4460</fpage><lpage>4472</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2021.3125736</pub-id></element-citation></ref><ref id="B19-sensors-25-05604"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>W.</given-names></name></person-group><article-title>Unsupervised Multi-Scale-Stage Content-Aware Homography Estimation</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>1976</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12091976</pub-id></element-citation></ref><ref id="B20-sensors-25-05604"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>L.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Rose</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>An</surname><given-names>W.</given-names></name></person-group><article-title>Detecting non-hardhat-use by a deep learning method from far-field surveillance videos</article-title><source>Autom. Constr.</source><year>2018</year><volume>85</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2017.09.018</pub-id></element-citation></ref><ref id="B21-sensors-25-05604"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name></person-group><article-title>Visual tracking of construction jobsite workforce and equipment with particle filtering</article-title><source>J. Comput. Civ. Eng.</source><year>2016</year><volume>30</volume><fpage>04016023</fpage><pub-id pub-id-type="doi">10.1061/(ASCE)CP.1943-5487.0000573</pub-id></element-citation></ref><ref id="B22-sensors-25-05604"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharif</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Nahangi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Haas</surname><given-names>C.</given-names></name><name name-style="western"><surname>West</surname><given-names>J.</given-names></name></person-group><article-title>Automated model-based finding of 3D objects in cluttered construction point cloud models</article-title><source>Comput.-Aided Civ. Infrastruct. Eng.</source><year>2017</year><volume>32</volume><fpage>893</fpage><lpage>908</lpage><pub-id pub-id-type="doi">10.1111/mice.12306</pub-id></element-citation></ref><ref id="B23-sensors-25-05604"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Golparvar-Fard</surname><given-names>M.</given-names></name><name name-style="western"><surname>Heydarian</surname><given-names>A.</given-names></name><name name-style="western"><surname>Niebles</surname><given-names>J.C.</given-names></name></person-group><article-title>Vision-based action recognition of earthmoving equipment using spatio-temporal features and support vector machine classifiers</article-title><source>Advanced Engineering Informatics</source><year>2013</year><volume>27</volume><fpage>652</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1016/j.aei.2013.09.001</pub-id></element-citation></ref><ref id="B24-sensors-25-05604"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Seo</surname><given-names>J.</given-names></name></person-group><article-title>Interaction analysis for vision-based activity identification of earthmoving excavators and dump trucks</article-title><source>Autom. Constr.</source><year>2018</year><volume>87</volume><fpage>297</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2017.12.016</pub-id></element-citation></ref><ref id="B25-sensors-25-05604"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>B.</given-names></name><name name-style="western"><surname>Love</surname><given-names>P.E.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>H.</given-names></name></person-group><article-title>Automated detection of workers and heavy equipment on construction sites: A convolutional neural network approach</article-title><source>Adv. Eng. Inform.</source><year>2018</year><volume>37</volume><fpage>139</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.aei.2018.05.003</pub-id></element-citation></ref><ref id="B26-sensors-25-05604"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name></person-group><article-title>Towards efficient and objective work sampling: Recognizing workers&#8217; activities in site surveillance videos with two-stream convolutional networks</article-title><source>Autom. Constr.</source><year>2018</year><volume>94</volume><fpage>360</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2018.07.011</pub-id></element-citation></ref><ref id="B27-sensors-25-05604"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jeelani</surname><given-names>I.</given-names></name><name name-style="western"><surname>Han</surname><given-names>K.</given-names></name><name name-style="western"><surname>Albert</surname><given-names>A.</given-names></name></person-group><article-title>Automating and scaling personalized safety training using eye-tracking data</article-title><source>Autom. Constr.</source><year>2018</year><volume>93</volume><fpage>63</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2018.05.006</pub-id></element-citation></ref><ref id="B28-sensors-25-05604"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Irizarry</surname><given-names>J.</given-names></name><name name-style="western"><surname>Costa</surname><given-names>D.B.</given-names></name></person-group><article-title>Exploratory study of potential applications of unmanned aerial systems for construction management tasks</article-title><source>J. Manag. Eng.</source><year>2016</year><volume>32</volume><fpage>05016001</fpage><pub-id pub-id-type="doi">10.1061/(ASCE)ME.1943-5479.0000422</pub-id></element-citation></ref><ref id="B29-sensors-25-05604"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ros</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sellart</surname><given-names>L.</given-names></name><name name-style="western"><surname>Materzynska</surname><given-names>J.</given-names></name><name name-style="western"><surname>Vazquez</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lopez</surname><given-names>A.M.</given-names></name></person-group><article-title>The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>3234</fpage><lpage>3243</lpage></element-citation></ref><ref id="B30-sensors-25-05604"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shorten</surname><given-names>C.</given-names></name><name name-style="western"><surname>Khoshgoftaar</surname><given-names>T.M.</given-names></name></person-group><article-title>A survey on image data augmentation for deep learning</article-title><source>J. Big Data</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id><pub-id pub-id-type="pmcid">PMC8287113</pub-id><pub-id pub-id-type="pmid">34306963</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05604-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall framework of the proposed method. Red boxes denotes the results of marker detection, green boxes denotes the results of homography estimation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05604-g001.jpg"/></fig><fig position="float" id="sensors-25-05604-f002" orientation="portrait"><label>Figure 2</label><caption><p>The structure of the marker detection network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05604-g002.jpg"/></fig><fig position="float" id="sensors-25-05604-f003" orientation="portrait"><label>Figure 3</label><caption><p>The detailed structure of the dynamic hybrid block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05604-g003.jpg"/></fig><fig position="float" id="sensors-25-05604-f004" orientation="portrait"><label>Figure 4</label><caption><p>Framework of the Transformer-based homography estimation module. Input ROIs are processed by ResNet50 to extract multi-scale features, which are fed into a Transformer encoder with fused positional encoding. Global average pooling (GAP) and a multi-layer perceptron (MLP) then regress the 3 &#215; 3 homography matrix.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05604-g004.jpg"/></fig><fig position="float" id="sensors-25-05604-f005" orientation="portrait"><label>Figure 5</label><caption><p>Examples of generated image samples.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05604-g005.jpg"/></fig><fig position="float" id="sensors-25-05604-f006" orientation="portrait"><label>Figure 6</label><caption><p>Examples of detection results.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05604-g006.jpg"/></fig><fig position="float" id="sensors-25-05604-f007" orientation="portrait"><label>Figure 7</label><caption><p>Examples of homography estimation results.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05604-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05604-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05604-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of Detection, Localization and Real-Time Performance (mean &#177; std. dev. over 3 runs).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Detection</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Localization</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Real-Time</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Robustness</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">mAP</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>
(%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Reprojection Error (mm)
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
FPS
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">mAP</mml:mi><mml:mi mathvariant="bold">occlusion</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
(%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8+SIFT</td><td align="center" valign="middle" rowspan="1" colspan="1">90.2 &#177; 0.8</td><td align="center" valign="middle" rowspan="1" colspan="1">3.5 &#177; 0.3</td><td align="center" valign="middle" rowspan="1" colspan="1">45 &#177; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">82.1 &#177; 1.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Faster R-CNN+CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">93.4 &#177; 0.6</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3 &#177; 0.2</td><td align="center" valign="middle" rowspan="1" colspan="1">38 &#177; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">88.7 &#177; 0.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ours (no augmentation)</td><td align="center" valign="middle" rowspan="1" colspan="1">96.1 &#177; 0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">1.8 &#177; 0.2</td><td align="center" valign="middle" rowspan="1" colspan="1">32 &#177; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">92.4 &#177; 0.7</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Proposed Method</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>97.8 &#177; 0.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.2 &#177; 0.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>32 &#177; 1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>95.6 &#177; 0.5</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05604-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05604-t002_Table 2</object-id><label>Table 2</label><caption><p>Ablation study on key components.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Variant</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold-italic">mAP</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
(%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reprojection Error (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Full model (proposed)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.8</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Without Transformer (CNN regression)</td><td align="center" valign="middle" rowspan="1" colspan="1">95.0</td><td align="center" valign="middle" rowspan="1" colspan="1">2.0</td><td align="center" valign="middle" rowspan="1" colspan="1">33</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Without data augmentation</td><td align="center" valign="middle" rowspan="1" colspan="1">96.1</td><td align="center" valign="middle" rowspan="1" colspan="1">1.8</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Absolute pos. encoding only</td><td align="center" valign="middle" rowspan="1" colspan="1">96.5</td><td align="center" valign="middle" rowspan="1" colspan="1">1.6</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Geometric prior encoding only</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05604-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05604-t003_Table 3</object-id><label>Table 3</label><caption><p>Performance under different real-to-synthetic data ratios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Ratio (Real&#8211;Synthetic)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold-italic">mAP</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
(%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reprojection Error (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">1:1 (2000:2000)</td><td align="center" valign="middle" rowspan="1" colspan="1">96.9</td><td align="center" valign="middle" rowspan="1" colspan="1">1.4</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2:5 (2000:5000)</td><td align="center" valign="middle" rowspan="1" colspan="1">97.8</td><td align="center" valign="middle" rowspan="1" colspan="1">1.2</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3:5 (3000:5000)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05604-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05604-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance under stitched images (varying resolutions/marker counts).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Stitched Condition</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mi mathvariant="bold-italic">mAP</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> (%)
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reprojection Error (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">1280 &#215; 960, 2 markers</td><td align="center" valign="middle" rowspan="1" colspan="1">97.3</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">1920 &#215; 1440, 4&#8211;5 markers</td><td align="center" valign="middle" rowspan="1" colspan="1">96.5</td><td align="center" valign="middle" rowspan="1" colspan="1">1.5</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2880 &#215; 2160, 6&#8211;8 markers</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>