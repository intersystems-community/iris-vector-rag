<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431029</article-id><article-id pub-id-type="pmcid-ver">PMC12431029.1</article-id><article-id pub-id-type="pmcaid">12431029</article-id><article-id pub-id-type="pmcaiid">12431029</article-id><article-id pub-id-type="doi">10.3390/s25175253</article-id><article-id pub-id-type="publisher-id">sensors-25-05253</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Improved PPO Optimization for Robotic Arm Grasping Trajectory Planning and Real-Robot Migration</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7205-6885</contrib-id><name name-style="western"><surname>Li</surname><given-names initials="C">Chunlei</given-names></name><xref rid="af1-sensors-25-05253" ref-type="aff">1</xref><xref rid="af2-sensors-25-05253" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="Z">Zhe</given-names></name><xref rid="af2-sensors-25-05253" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="L">Liang</given-names></name><xref rid="af1-sensors-25-05253" ref-type="aff">1</xref><xref rid="af2-sensors-25-05253" ref-type="aff">2</xref><xref rid="c1-sensors-25-05253" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ji</surname><given-names initials="Z">Zeyu</given-names></name><xref rid="af2-sensors-25-05253" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="C">Chenbo</given-names></name><xref rid="af2-sensors-25-05253" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liang</surname><given-names initials="J">Jiaxing</given-names></name><xref rid="af2-sensors-25-05253" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="Y">Yafeng</given-names></name><xref rid="af3-sensors-25-05253" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Bao</surname><given-names initials="X">Xianqiang</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Wang</surname><given-names initials="S">Shuangyi</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Housden</surname><given-names initials="J">James</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05253"><label>1</label>Shaanxi Key Laboratory of Advanced Manufacturing and Evaluation of Robot Key Components, Baoji 721016, China; <email>602lcl-602lcl@163.com</email></aff><aff id="af2-sensors-25-05253"><label>2</label>School of Mechanical Engineering, Baoji University of Arts and Sciences, Baoji 721016, China; <email>liuzhe19991006@163.com</email> (Z.L.); <email>lcb_1231@163.com</email> (C.L.); </aff><aff id="af3-sensors-25-05253"><label>3</label>School of Computer Science and Technology, Baoji University of Arts and Sciences, Baoji 721013, China</aff><author-notes><corresp id="c1-sensors-25-05253"><label>*</label>Correspondence: <email>liliang@bjwlxy.edu.cn</email>; Tel.: +86-13509175674</corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5253</elocation-id><history><date date-type="received"><day>15</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>15</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>19</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>23</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05253.pdf"/><abstract><p>Addressing key challenges in unstructured environments, including local optimum traps, limited real-time interaction, and convergence difficulties, this research pioneers a hybrid reinforcement learning approach that combines simulated annealing (SA) with proximal policy optimization (PPO) for robotic arm trajectory planning. The framework enables the accurate, collision-free grasping of randomly appearing objects in dynamic obstacles through three key innovations: a probabilistically enhanced simulation environment with a 20% obstacle generation rate; an optimized state-action space featuring 12-dimensional environment coding and 6-DoF joint control; and an SA-PPO algorithm that dynamically adjusts the learning rate to balance exploration and convergence. Experimental results show a 6.52% increase in success rate (98% vs. 92%) and a 7.14% reduction in steps per set compared to the baseline PPO. A real deployment on the AUBO-i5 robotic arm enables real machine grasping, validating a robust transfer from simulation to reality. This work establishes a new paradigm for adaptive robot manipulation in industrial scenarios requiring a real-time response to environmental uncertainty.</p></abstract><kwd-group><kwd>robotic trajectory planning</kwd><kwd>proximal policy optimization</kwd><kwd>simulated annealing</kwd><kwd>unstructured environments</kwd><kwd>collision-free grasping</kwd><kwd>reinforcement learning</kwd><kwd>sim-to-real transfer</kwd></kwd-group><funding-group><award-group><funding-source>Key Project of Shaanxi Provincial Department of Science and Technology</funding-source><award-id>2024QY2-GJHX-38</award-id></award-group><funding-statement>Key Project of Shaanxi Provincial Department of Science and Technology (Program No. 2024QY2-GJHX-38), P.R. China, L.L.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05253"><title>1. Introduction</title><p>With the continuous advancement of robotic arm technology, industrial robotic arms have become indispensable assets in modern industrial production. By deploying robotic arms to replace human labor in performing various simple yet repetitive mechanical tasks, such as grasping and placement operations, current productivity levels can be significantly enhanced. In future smart factories, work environments will exhibit dynamic and unstructured characteristics, requiring robotic arms to operate efficiently and safely while performing complex tasks [<xref rid="B1-sensors-25-05253" ref-type="bibr">1</xref>]. However, with the growing demands in industrial production and service sectors, existing robotic grasping technologies can no longer meet future requirements. There is an increasing expectation for robotic arms to possess autonomous motion capabilities to handle various environmental uncertainties [<xref rid="B2-sensors-25-05253" ref-type="bibr">2</xref>]. Consequently, research on reinforcement learning-based trajectory planning for robotic arms carries significant importance.</p><p>Robotic grasping tasks exhibit remarkable versatility in industrial applications, with distinct operational scenarios dictated by varying environments and target objects. These systems also play pivotal roles in human&#8211;robot interaction and service robotics domains [<xref rid="B3-sensors-25-05253" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05253" ref-type="bibr">4</xref>]. Diverse trajectory planning methods have been developed for robotic arms across different application scenarios. Conventional kinematic and dynamic planning approaches [<xref rid="B5-sensors-25-05253" ref-type="bibr">5</xref>], which generate trajectories through mathematical analysis or numerical computation based on the arm&#8217;s kinematic model, prove effective for deterministic tasks in structured environments. However, their heavy dependence on precise kinematic modeling results in poor adaptability to unstructured environments and limited capability in handling dynamic obstacles. Geometry-based path planning algorithms [<xref rid="B6-sensors-25-05253" ref-type="bibr">6</xref>] model the robotic arm&#8217;s workspace as geometric constructs, generating collision-free paths through graph search or sampling methods. While effective in principle, these approaches suffer from high computational complexity and often fail to guarantee globally optimal solutions. In contrast, intelligent optimization algorithms [<xref rid="B7-sensors-25-05253" ref-type="bibr">7</xref>] employ bionic principles or mathematical optimization to search for optimal trajectories without relying on precise models, making them suitable for complex constrained optimization problems. However, these methods exhibit notable limitations, including slow convergence rates, susceptibility to local optima, and the need for manual parameter tuning. While these methods demonstrate application value across various scenarios, they inherently possess limitations. Recent advances in machine vision have endowed robotic arms with environmental perception capabilities, significantly enhancing their intelligence. Consequently, the development of grasping systems capable of handling unknown objects in unconstrained environments has emerged as a key research focus.</p><p>To overcome the limitations of existing approaches while leveraging their respective strengths, reinforcement learning [<xref rid="B8-sensors-25-05253" ref-type="bibr">8</xref>] offers a promising solution for robotic grasping. Through continuous agent&#8211;environment interactions, this approach eliminates the need for manual annotation by utilizing predefined reward signals as feedback mechanisms, thereby enabling autonomous environmental perception. Reinforcement learning has demonstrated significant practical achievements across diverse domains, including autonomous vehicle control [<xref rid="B9-sensors-25-05253" ref-type="bibr">9</xref>], cross-lingual translation, speech recognition, biomimetic agents, game playing (Go and esports), and robotic arm manipulation. These advancements have created new opportunities for integrating virtual reality and digital twin technologies into intelligent control systems [<xref rid="B10-sensors-25-05253" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05253" ref-type="bibr">11</xref>], thereby enabling novel approaches to robotic arm intelligent control. In 2013, Mnih et al. [<xref rid="B12-sensors-25-05253" ref-type="bibr">12</xref>] integrated Q-learning with convolutional neural networks, introducing the groundbreaking Deep Q-Network (DQN) framework. The team subsequently demonstrated the efficacy of this approach through successful applications in gaming environments [<xref rid="B13-sensors-25-05253" ref-type="bibr">13</xref>]. Wang et al. [<xref rid="B14-sensors-25-05253" ref-type="bibr">14</xref>] modified the DQN architecture by splitting it into two separate pathways for agent action selection and Q-value generation respectively. In an alternative approach, Hasselt et al. [<xref rid="B15-sensors-25-05253" ref-type="bibr">15</xref>]. improved DQN through a dual-network structure. The research group led by Li Heyu [<xref rid="B16-sensors-25-05253" ref-type="bibr">16</xref>] proposed a robotic arm control method based on the deep deterministic policy gradient algorithm, creating a Unity3D simulation environment that successfully demonstrated object grasping in production line scenarios. Researchers at UC Berkeley led by Schulman [<xref rid="B17-sensors-25-05253" ref-type="bibr">17</xref>] proposed the Trust Region Policy Optimization (TRPO) algorithm, whose core innovation involves modifying the cost function to enable better policy generation. The same team later developed the Proximal Policy Optimization (PPO) algorithm [<xref rid="B18-sensors-25-05253" ref-type="bibr">18</xref>], the experimental results of which showed it could enhance training stability without compromising efficiency.</p><p>To achieve successful robotic grasping and efficient training, this study establishes a 1:1 simulated environment using PyBullet, incorporating models of the robotic arm, gripper, gantry system, and cameras. Building upon PPO, we optimize the state space, action space, and reward mechanism. The training process integrates simulated annealing (SA) to dynamically adjust learning rates, initially assigning higher values to enhance exploration and escape local optima during early training phases. The learning rate is dynamically adjusted with the increase in the number of training rounds during training, and converges stably in the middle and late stages to achieve efficient learning and be able to train a model that can accomplish the grasping task. After obtaining the model, sim-to-real experiments were conducted. By addressing issues such as coordinate transformation, normalization, and data acquisition between the real and simulated environments, the model was successfully transferred to a real robotic arm.</p><p>The results of this experiment demonstrate the ability to achieve autonomous target recognition, grasping, and obstacle avoidance in a simulated environment, aiming to address the aforementioned shortcomings and enabling application in material handling operations performed by robotic arms in factories. We aim to enable robotic arms to automatically identify and grasp target materials in the unstructured or semi-structured environments commonly found in factory workshops. Ultimately, this research seeks to develop more adaptive, safer, and easier-to-deploy robotic grasping solutions for manufacturing, reducing reliance on fixed automated systems and lowering the implementation barriers for robotic automation in complex material handling scenarios. In the field of embodied intelligence, the results of this experiment can also provide a motion model foundation for tasks.</p></sec><sec id="sec2-sensors-25-05253"><title>2. Related Work</title><p>Although the PPO algorithm improves stability by adjusting the objective function, its monotonous policy update mechanism is prone to fall into local optimality in high-dimensional continuous control tasks. Xing Chen et al. [<xref rid="B19-sensors-25-05253" ref-type="bibr">19</xref>] demonstrated through a non-strategy metric that the success rate of the PPO algorithm in reward-sparse scenarios (e.g., multi-obstacle grasping) is less than 40%, and there is a limited space of strategies that can be explored. In addition, the existing PPO improvement schemes that integrate Simulated Annealing (SA) have the problem of decoupling between parameters and the environment. Taking the COAPG-PPO algorithm proposed by Zhou Yi et al. [<xref rid="B20-sensors-25-05253" ref-type="bibr">20</xref>] as an example, the fixed annealing scheme adopted by this algorithm reflects this problem. It leads to insufficient exploration in the early stage of the algorithm&#8217;s operation, and excessive disturbance in the later stage, which in turn triggers policy oscillations. In meeting the growing demand for more flexible and intelligent automation technologies in modern manufacturing, especially in the field of material handling tasks, current solutions typically rely on pre-programmed trajectories or specialized fixtures that are inflexible, require significant engineering resources to adapt to new tasks or components, and struggle to cope with dynamic environments or unexpected obstacles. We identified the main limitations of existing deployments, as follow: (1) it is challenging to reliably grasp random objects without extensive re-engineering; (2) the lack of intrinsic obstacle avoidance poses a safety risk and operational inefficiencies when human workers or other equipment are present; and (3) the high cost and time required to program and debug a new task hampers adaptability to small-quantity, multi-variety production.</p><p>To address the above deficiencies, this study proposes an adaptive PPO + SA framework with an innovative mechanism, which optimizes the reward and punishment mechanism of PPO, adjusts the parameters to achieve the function of obstacle-avoidance grasping and establish an exponential correlation between the temperature and the strategy entropy, realizes the self-attenuation of the exploration intensity with the convergence of the strategy, which makes it easier to jump out of the local optimum at the initial stage, and finally migrates the model obtained from training to a real machine for random target grasping or obstacle avoidance grasping, which fundamentally solves the local convergence problem of PPO and non-adaptive SA.</p></sec><sec id="sec3-sensors-25-05253"><title>3. Optimized PPO Algorithm Architecture for Robotic Arm Grasping</title><p>Advances in machine vision have endowed robotic arms with environmental perception capabilities. Reinforcement learning further enhances this perception through continuous environmental interaction. This study optimizes the state space, action space, reward function, and penalty function in the PPO algorithm. The trained model enables collision-free random grasping by robotic arms, establishing a foundation for real-world grasping applications.</p><sec id="sec3dot1-sensors-25-05253"><title>3.1. Proximal Policy Optimization (PPO)</title><p>As a simplified variant of the TRPO algorithm, PPO incorporates a probability ratio clipping mechanism. This approach maintains training stability while significantly reducing computational overhead, establishing PPO as both efficient and easily implementable. <xref rid="sensors-25-05253-f001" ref-type="fig">Figure 1</xref> illustrates the schematic of this probability ratio clipping mechanism.</p><p>In PPO, the policy represents a probability distribution over the agent&#8217;s actions, while the value function evaluates the current policy&#8217;s performance. The complete objective function is given by Equation (1):<disp-formula id="FD1-sensors-25-05253"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>V</mml:mi><mml:mi>F</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>S</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>&#960;</mml:mi><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the above equation, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">&#952;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the clipped policy objective, which constrains policy updates by clipping the probability ratio to prevent excessive deviation from the previous policy, thereby maintaining training stability. <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">&#952;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the value function loss term (typically implemented as mean squared error) for training the state-value baseline. <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>[</mml:mo><mml:mi mathvariant="sans-serif">&#960;</mml:mi><mml:mi mathvariant="normal">&#952;</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the policy entropy term that encourages exploration. The coefficients <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> balance the relative contributions of these loss components.</p></sec><sec id="sec3dot2-sensors-25-05253"><title>3.2. State and Action Space Design</title><p>The state space characterizes the current environmental features and serves as input to the policy network. For the six-degree-of-freedom robotic arm grasping task, the designed state vector <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> integrates critical features encompassing the arm&#8217;s kinematic state, target object information and environmental constraints to form a 12-dimensional representation, and the observed state space is shown in Equation (2):<disp-formula id="FD2-sensors-25-05253"><label>(2)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="sans-serif">&#916;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The observed state space integrates six core components essential for robotic grasping. The joint angles <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent current angular positions of all six rotational joints to fully capture the arm&#8217;s kinematic configuration, while the end-effector coordinates <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> specify its spatial position through the gripper center&#8217;s 3D global coordinates. Crucially, the target coordinates <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> define the desired grasping location, complemented by obstacle coordinates <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> mapping environmental constraints. The proximity metric <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="sans-serif">&#916;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> encodes the minimum joint-to-obstacle distance as a critical safety parameter, and the orientation angle <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> between the end-effector and world z-axis provides essential posture feedback. Collectively, these elements enable the policy network to synthesize real-time joint kinematics, precise tool positioning, target localization, and environmental awareness into actionable control decisions.</p><p>The gripper&#8217;s center point is determined through analysis of the linkage configuration shown in <xref rid="sensors-25-05253-f002" ref-type="fig">Figure 2</xref>, where circled numbers indicate joint indices defined in the Unified Robot Description Format (URDF) file [<xref rid="B21-sensors-25-05253" ref-type="bibr">21</xref>]. The URDF provides standardized kinematic and dynamic modeling for robotic systems. This study defines the gripper&#8217;s operational center as the midpoint between Link 7 and Link 10, selected for their symmetrical positioning and mechanical dominance in grasping operations.</p><p>The obstacle-aware system combines the 3D coordinates of an obstacle with the minimum distance between each robot joint and the obstacle, allowing for more efficient obstacle avoidance learning. A collision will immediately trigger the maximum penalty and terminate the current round of training that caused the collision, proceeding directly to the next round, while near misses (e.g., with a gap of less than 5 cm) are penalized proportional to the distance without interrupting training&#8212;the closer the distance, the heavier the penalty. This hierarchical reward structure strictly enforces safety constraints while preserving valuable training episodes, thus promoting a robust obstacle avoidance grasping strategy. The dual spatial representation (absolute coordinates + relative distances) provides comprehensive environment awareness, and the graded punishment scheme provides a clear optimization gradient for the strategy network. As illustrated in <xref rid="sensors-25-05253-f003" ref-type="fig">Figure 3</xref>, obstacle avoidance for the gripper is demonstrated. The spherical volumes outlined in red represent the influence zones of randomly generated obstacles, while the black line indicates the shortest distance between the robotic gripper and the nearest obstacle. Since the current minimum distance is measured relative to the obstacle marked by the red sphere, other randomly placed obstacles are disregarded in the immediate computation. During arm movement, this minimum distance is continuously recalibrated in real time, enabling comprehensive collision-free grasping through dynamic obstacle awareness. The system&#8217;s focus on the most critical obstacle (nearest violation risk) ensures computational efficiency while maintaining safety.</p><p>The system regulates vertical grasping through environmental constraints by calculating the angle between the end-effector and the global <italic toggle="yes">z</italic>-axis. This angular deviation serves as the primary metric for vertical alignment assessment, where smaller angles indicate closer conformity to ideal perpendicular grasping conditions. The implementation continuously monitors this orientation during approach phases, enabling real-time trajectory adjustments to achieve optimal perpendicular contact with target objects. As shown in <xref rid="sensors-25-05253-f004" ref-type="fig">Figure 4</xref>, the red line &#8220;R&#8221; indicates the orientation of the robotic arm&#8217;s end-effector, while the blue line &#8220;Z&#8221; represents the <italic toggle="yes">z</italic>-axis, which is consistent with the <italic toggle="yes">Z</italic>-axis of the world coordinate system and is used to observe the offset angle more intuitively. The angle between the red line and blue line is calculated, and when this angle exceeds a certain threshold, fine-tuning intervention will be performed. The adjustment continues until the angle falls below the predefined threshold, which confirms that the gripper&#8217;s end-effector has reached an approximately vertical state.</p><p>A six-degree-of-freedom (6-DoF) robotic arm is the standard kinematic configuration for industrial manipulation tasks, capable of positioning the end-effector at any three-dimensional spatial coordinate and achieving the desired posture. This capability stems from its articulated structure composed of six rotary joints&#8212;typically arranged in a serial mechanism configuration&#8212;enabling the independent control of Cartesian positions (x, y, z) and attitudes (roll, pitch, yaw). The industrial models used in this study, such as the AUBO-i5, feature a spherical workspace with a radius of approximately 886.5 mm and a repeatability accuracy of &#177;0.02 mm, making them an ideal choice for precise operations in unstructured environments. The action space describes the behavior output by the policy network and serves as the control command for the robotic arm. For the continuous motion control task of a 6-DoF robotic arm, this paper selects joint angle variations as the action output, constructing a 6-dimensional continuous action space a, as shown in Equation (3):<disp-formula id="FD3-sensors-25-05253"><label>(3)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the angular increments of the six rotary joints, controlling the robotic arm&#8217;s joint movements at each step. Additionally, the incremental value <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for each joint angle is constrained within a specified range to ensure the physical feasibility of the actions,<disp-formula id="FD4-sensors-25-05253"><label>(4)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equation (4), <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the maximum allowable angular increment per step for each joint. To integrate state and action, at each moment <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the robotic arm generates action <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> through the policy network based on the current state <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, as expressed in Equation (5).<disp-formula id="FD5-sensors-25-05253"><label>(5)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this equation, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the policy network, and its parameters <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#952;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are updated using an improved algorithm that combines PPO with simulated annealing. Finally, after receiving the action <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the robotic arm updates its joint angles, resulting in a new state <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, while the environment provides corresponding reward feedback.</p><p>The 12-dimensional state vector comprehensively incorporates the robotic arm&#8217;s joint angles, end-effector pose, target position, and environmental obstacle information. This design ensures the policy network can fully capture task-critical features, thereby enabling the efficient learning of complex grasping maneuvers.</p></sec><sec id="sec3dot3-sensors-25-05253"><title>3.3. Loss Function and Reward Design</title><p>In the optimized PPO algorithm, the design of the loss function and reward function is critical for enhancing the performance of the 6-DoF robotic arm in grasping tasks. A well-structured loss function ensures stable policy network updates, while an effective reward function guides the robotic arm toward optimal action strategies. The PPO algorithm&#8217;s core objective function, serving as the foundational loss function, is defined in Equation (6).<disp-formula id="FD6-sensors-25-05253"><label>(6)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mo>&#8712;</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>&#8712;</mml:mo></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the above equation, <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability ratio of the old and new strategies, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the dominance function, which measures the advantage of the current action over the baseline strategy, and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8712;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the trimming threshold, which is used to limit the magnitude of the strategy update to prevent drastic fluctuations in the training process.</p><p>To guide the 6-DoF robotic arm in efficiently performing grasping tasks, we design a compound reward mechanism. A significant positive reward is provided only upon successful grasping of the target object, creating a sparse but clear success signal. Collisions between the robotic arm and the environment or target object trigger a strong negative penalty to prevent harmful motions. Additionally, a distance-based reward incrementally encourages the end-effector to approach the target, reducing inefficient exploration, while an obstacle-avoidance reward ensures safe operation by penalizing proximity to obstacles. This multi-faceted reward structure balances task completion, motion safety, and operational efficiency.</p><p>The gripper distance reward can be defined by Equation (7), where <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the distance between the gripper and the target, while <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the reduction in this distance, and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the maximum distance at which the robot arm gripper can successfully grasp the target object. Within this distance range, tests have shown that the target object can be successfully grasped. The closer the distance, the better the grasping effect. Therefore, even if this distance has been reached, we still encourage further approach. A larger <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> indicates that the gripper is progressively approaching the target. <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> serves as a distance coefficient, and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a reward constant introduced to address the diminishing incentive issue. When the distance becomes sufficiently small, even with a large <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> coefficient, the product <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mo>&#215;</mml:mo><mml:mo>&#8710;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> would yield an insignificant value due to the minimal <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which fails to adequately encourage further approach. To overcome this limitation, the reward constant <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is incorporated to provide sustained motivation when the gripper is in close proximity to the target.<disp-formula id="FD7-sensors-25-05253"><label>(7)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mi>C</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mo>&#215;</mml:mo><mml:mo>&#8710;</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#8804;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mo>&#215;</mml:mo><mml:mo>&#8710;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The formula for the successful gripping reward of the clamping jaws is shown in Equation (8).<disp-formula id="FD8-sensors-25-05253"><label>(8)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The gripper jaw grasping obstacle avoidance reward formula is shown in Equation (9).<disp-formula id="FD9-sensors-25-05253"><label>(9)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>a</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>Robotic </mml:mi><mml:mo>&#160;</mml:mo><mml:mi>arms</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>collide</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>with</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>obstacles</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>or</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>do</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>not</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>meet</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>safety</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>distances</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The final composite reward function integrates these components through Equation (10), where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the factor of the distance reward, success reward, and obstacle avoidance reward, respectively. These factors are dynamically adjusted: <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is progressively reduced from 2.0 to 1.0, <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is progressively increased from 0.5 to 1.5, and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is fixed at 0.5. In early training when the robotic arm has not yet mastered the grasping skill, this dynamically adjusted coefficient scheme prioritizes shorter distances to minimize the effect of sparse success/failure signals. As training advances and the end-effector approaches the target, the increasing emphasis on success rewards promotes a grasping behavior that completely avoids obstacles. Most importantly, the reward for obstacle avoidance is smaller than the distance reward to prevent the robotic arm from over-prioritizing obstacle avoidance at the expense of approaching the target.<disp-formula id="FD10-sensors-25-05253"><label>(10)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The proposed design offers significant advantages through its multidimensional state space, which comprehensively incorporates both the robotic arm&#8217;s intrinsic state and extrinsic environmental information (target objects and obstacles). This serves as the critical foundation for enabling collision-free random grasping. The action space is strategically decomposed; joint0, joint1, and joint2 primarily drive the arm toward the target, while joint3 and joint4 execute fine adjustments for vertical grasping alignment, and joint5 rotates the gripper to the optimal grasping angle. This hierarchical actuation ensures efficient and precise target acquisition. Furthermore, the optimized reward&#8212;penalty mechanism systematically guides the arm to learn essential skills&#8212;including target approach, vertical grasping, and obstacle avoidance&#8212;throughout the training process. This results in a robotic system that simultaneously achieves safety (collision prevention), global robustness (adaptation to varying initial conditions), and task adaptability (generalization to unseen scenarios). The synergistic integration of these components guarantees reliable performance in complex grasping tasks.</p></sec></sec><sec id="sec4-sensors-25-05253"><title>4. Improved PPO Algorithm Design Based on Simulated Annealing Algorithm</title><p>The optimized PPO algorithm described above has yielded a preliminary trajectory planning model for grasping tasks capable of reproducing basic operations. However, several limitations persist, including insufficient initial exploration, susceptibility to local optima, and convergence difficulties. To address these issues, this study proposes an improved PPO algorithm incorporating simulated annealing principles. Simulation experiments demonstrate measurable improvements in overcoming these challenges.</p><sec id="sec4dot1-sensors-25-05253"><title>4.1. Simulated Annealing (SA)</title><p>The simulated annealing algorithm [<xref rid="B22-sensors-25-05253" ref-type="bibr">22</xref>] is a widely-used stochastic heuristic optimization method inspired by the thermodynamic annealing process, where a material is gradually a cooled from high temperature to allow its particles to reach the minimum energy state, ultimately forming a crystalline structure. The algorithm introduces a &#8220;probability of accepting inferior solutions,&#8221; enabling it to escape local optima and eventually converge to a global or near-optimal solution. The probability of accepting a new solution in simulated annealing is given by Equation (11), where <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the energy difference between the current and new solutions.<disp-formula id="FD11-sensors-25-05253"><label>(11)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#8710;</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec4dot2-sensors-25-05253"><title>4.2. Design of Improved PPO Algorithm for SA Based on Crawling</title><p>The SA algorithm aims at global optimization through dynamic parameter tuning, while the PPO algorithm maximizes cumulative rewards through strategy gradients. The basic goal of both is to optimize the decision-making strategy, reflecting theoretical compatibility and complementary exploration&#8212;development balance. As mentioned earlier, while PPO exhibits stable performance in complex tasks, its policy update relies on local gradient optimization and thus is prone to local optima in high-dimensional spaces, especially in scenarios involving sparse rewards or multi-objective functions. To address this limitation, this study employs a simulated annealing technique with a dynamically adjustable learning rate. During the initial search phase, this hybrid approach may accept sub-solutions, thereby escaping from local optima while significantly enhancing global search capabilities. The temperature scheduling mechanism in SA creates a natural synergy with the shear objective function of the PPO, creating a balanced optimization process that maintains the training stability of the PPO while overcoming its inherent tendency towards local convergence. This integration proves to be particularly effective for robot manipulation tasks that require both the precise improvement of the strategy and in-depth exploration of the state space. The core strength of the PPO combined with the SA algorithm lies in its controllable, adaptive exploration mechanism (realized through temperature parameters) and its ability to systematically accept temporarily inferior solutions to cross energy barriers.</p><p>In the PPO algorithm, SA is primarily employed to dynamically adjust the learning rate, maintaining a higher rate during initial training phases for enhanced exploration while gradually reducing it to facilitate convergence. This study implements an adaptive annealing scheme, where the learning rate adjustment follows Equation (12).<disp-formula id="FD12-sensors-25-05253"><label>(12)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>r</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this formulation, <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the learning rate at the nth update cycle, <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the initial learning rate at training commencement, and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> serves as the reduction factor for learning rate decay. The system automatically decreases the learning rate when the monitored performance metric fails to improve over consecutive training cycles, as governed by the above equation. This adaptive mechanism ensures comprehensive training across different learning rate regimes while maintaining optimization stability. The initially high learning rate corresponds to the &#8220;high temperature phase&#8221; of the SA, which allows for a large parameter update step size, enabling the model to be tuned significantly within the solution space. This aggressive updating strategy facilitates the escape from the initial local optimum and promotes the rapid exploration of different regions. As training proceeds, the system enters the &#8220;low-temperature phase&#8221; of the SA, characterized by a reduced learning rate, where smaller and more precise parameter adjustments enable fine-tuned convergence.</p><p>The simulation experiment for robotic arm trajectory planning using the SA-enhanced PPO algorithm comprises four key steps, as illustrated in <xref rid="sensors-25-05253-f005" ref-type="fig">Figure 5</xref>. First, a virtual environment capable of real-time training visualization is constructed to demonstrate the solution set. Subsequently, to accomplish the designated tasks, an algorithmic implementation environment is established specifically for model training. During the process, joint angle data are collected as input values, while comparative PPO training is conducted according to the designed reward function, with dynamic learning rate adjustments performed based on training outcomes. Finally, the trained robotic arm joint parameters are saved and deployed in the simulation environment to execute the target tasks.</p><p>This design greatly improves the algorithm&#8217;s ability to escape from local optima through an initial high learning rate, which enhances the global search capability and effectively prevents convergence to sub-optimal solutions. To achieve stable policy updates, the clipping mechanism in PPO ensures training stability throughout the process. By integrating simulated annealing, the low learning rate at a later stage achieves accurate and stable convergence, achieving an optimal balance between exploration and exploitation. <xref rid="sensors-25-05253-f006" ref-type="fig">Figure 6</xref> illustrates the detailed workflow of the combined PPO + SA algorithm.</p></sec></sec><sec id="sec5-sensors-25-05253"><title>5. Experiment of Robotic Arm Trajectory Planning Based on Improved PPO Algorithm</title><p>In the simulation experiments, this study employed Stable-Baselines3 [<xref rid="B23-sensors-25-05253" ref-type="bibr">23</xref>] and PyBullet [<xref rid="B24-sensors-25-05253" ref-type="bibr">24</xref>] as core frameworks. Stable-Baselines3 was utilized to implement both the PPO algorithm and its SA-enhanced variant, while PyBullet served as the physics engine for constructing the three-dimensional simulation environment. These components were integrated through the Gym interface to conduct trajectory planning simulations. Finally the obtained model was applied in a real environment for target grasping.</p><sec id="sec5dot1-sensors-25-05253"><title>5.1. 3D Simulation Modeling</title><p>A simulation environment is constructed using the Pybullet physics simulation library, which includes an AUBO-I5 robotic arm, a two-finger gripper jaw for the INSPIRE-ROBOTS, and a gantry-fixed depth camera. The robotic arm base is fixed at the origin of the spatial coordinate system. The gantry cross-section is 0.04 m in length and width, and 1.54 m in height, respectively in the left front and right front of the robotic arm, 0.3 m in front, and 0.6 m in the left and right. We build the above simulation environment, 1:1, and then restore the real experimental environment. The real environment is shown in <xref rid="sensors-25-05253-f007" ref-type="fig">Figure 7</xref>:</p><p>AUBO-i5 (AUBO Intelligent, Beijing, China) is a nationally certified collaborative robot that complies with the ISO 10218-1 standard. Its 6-Degree-of-Freedom modular joint design achieves a repeatability accuracy of &#177;0.02 mm and a load capacity of 5 kg, which is crucial for the random object grasping task in this study. Industrial application cases show that it performs well in areas such as loading and unloading materials for machining, welding and assembly, and vision-guided applications. We utilized its native Python control architecture, which was further developed to adapt to the action models generated by the experimental training, thereby meeting the requirements for collision-free random object grasping.</p><p>Additionally, a rectangular target object (shown in red in the figures) measuring 0.02 m in length and width and 0.05 m in height was placed in the environment for grasping tasks. The cuboid object was selected as the grasping target due to its simple yet stable geometric properties, which make it particularly suitable for evaluating the grasping performance of both the robotic arm and gripper. The complete simulation environment setup is illustrated in <xref rid="sensors-25-05253-f008" ref-type="fig">Figure 8</xref>.</p><p><xref rid="sensors-25-05253-t001" ref-type="table">Table 1</xref> describes the hardware and software configurations used in the training.</p></sec><sec id="sec5dot2-sensors-25-05253"><title>5.2. Grabbing Task Trajectory Planning</title><p>The path planning of the robotic arm is modeled using DH parameters, imported into the AUBO-i5 kinematic model and converted to a URDF file in order to establish coordinate transformations between joints. The system uses inverse kinematics for the point-to-point motion control of the end-effector, and the arm follows a trajectory from the initial position to the target grasping position. <xref rid="sensors-25-05253-f009" ref-type="fig">Figure 9</xref>a&#8211;d illustrate the four key phases of this motion in sequence: initialization, proximity, fine-tuning, and final grasping.</p><p>We employ a proximity-aware velocity modulation strategy; when the gripper&#8217;s center position enters a threshold near the target, the robotic arm automatically decelerates its motion. This deceleration allows for precise pose adjustment through finer kinematic corrections. This approach slightly increases the number of maneuvering steps, but significantly reduces the risk of collision in cluttered environments compared to a constant velocity.</p><p>The training program also includes obstacle avoidance adjustment of the robotic arm. During training, two different types of obstacles are encountered: static obstacles (e.g., gantry structures and cameras) that remain stationary throughout the training process, and dynamic obstacles that randomly appear with a probability of 20% during each training session. These dynamic obstacles include primitive shapes such as spheres, cylinders, and cubes, whose sizes and locations are randomized, as illustrated in <xref rid="sensors-25-05253-f010" ref-type="fig">Figure 10</xref>, which shows two spherical obstacles of varying sizes generated in an example scenario. This dual obstacle paradigm enhances the generalization ability of the strategy in terms of collision-free grasping. When randomly generated obstacles are too close to the target object, this training is automatically terminated and these cases are excluded from the policy update by labeling them as &#8220;no feasible grasping angle&#8221;. For all other cases, the agent learns to bypass the obstacle while completing the grasp. The spatial representations <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="sans-serif">&#916;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> enable the policy network to encode critical obstacle avoidance information. This geometric awareness is combined with a reward structure that penalizes collisions and proximity states, resulting in collision-free grasping in the test case.</p></sec><sec id="sec5dot3-sensors-25-05253"><title>5.3. Simulation and Analysis</title><p>In this experiment, the process of moving the robotic arm from its initial state to completing the grasping task or reaching the maximum single step length is considered a single grasping process. The advantages of the improved PPO + SA algorithm will be demonstrated by comparing and analyzing the relevant results of the optimized PPO algorithm and the improved PPO + SA algorithm. The specific training parameters are shown in <xref rid="sensors-25-05253-t002" ref-type="table">Table 2</xref>.</p><p>The 20-million-step training was completed within 14.77 h on a single RTX 4070 GPU, achieving 375.8 steps/second throughput with peak GPU utilization of 94.2%. Memory consumption remained at 12.1/16 GB (75.6%), confirming headroom for model complexity scaling. The 2.66 ms/step latency demonstrates feasibility for industrial-scale RL training on cost-effective hardware.</p><p>The performance of the optimized PPO algorithm and improved PPO + SA algorithm was evaluated by system simulation experiments. The main indicators included distance reward curves, mean reward curves, grasping success rates, and training efficiency. The distance reward curves and mean reward curves represent the changes in the magnitude of reward values for distance rewards and all rewards (including distance rewards, grasping rewards, and obstacle avoidance rewards) in the training task, respectively. The grasping success rates represent the probability change curve of successful captures in this round of capture actions relative to the total number of capture attempts. The training efficiency represents the change curve in the number of steps required for a single capture action. <xref rid="sensors-25-05253-f011" ref-type="fig">Figure 11</xref> shows the distance reward curve.</p><p>In <xref rid="sensors-25-05253-f011" ref-type="fig">Figure 11</xref>a,b, the blue curve represents the optimized PPO algorithm, while the orange curve corresponds to the SA-improved PPO + SA algorithm. As the training progresses, the distance reward value increases steadily and reaches a peak around 13 million interactions, and it then decreases gradually, which is consistent with the dynamic reward parameter scheme defined in our reward function. <xref rid="sensors-25-05253-f011" ref-type="fig">Figure 11</xref>a shows that the improved PPO + SA algorithm exhibits stronger initial oscillations compared to the optimized PPO algorithm, suggesting that it has an enhanced ability to escape from local optima in the early training phase. Furthermore, <xref rid="sensors-25-05253-f011" ref-type="fig">Figure 11</xref>b shows that the improved PPO + SA algorithm achieves higher distance rewards faster by approaching the target more efficiently. <xref rid="sensors-25-05253-f012" ref-type="fig">Figure 12</xref> shows the corresponding mean reward results.</p><p>As shown in <xref rid="sensors-25-05253-f012" ref-type="fig">Figure 12</xref>a,b, the reward value increases gradually as the number of training steps increases. When the number of training steps reaches 10 million, the performance index of the improved PPO + SA algorithm is significantly better than the optimized PPO algorithm. In addition, the average reward accumulation of the improved PPO + SA algorithm shows a steeper upward trend, indicating faster learning progress. Although the improved PPO + SA algorithm gradually stabilizes after 15 million interactions and no longer rises significantly, its reward value is consistently higher than that of the PPO algorithm throughout the training process. This persistent performance gap suggests that improved PPO + SA algorithm has the potential to achieve higher task success rates, providing empirical evidence for its superior effectiveness. The comparative results suggest that the inclusion of simulated annealing in PPO can significantly improve learning efficiency and final policy quality.</p><p>The reward function values in <xref rid="sensors-25-05253-f012" ref-type="fig">Figure 12</xref>b indirectly indicate the superior success rate of the improved PPO + SA algorithm compared to the optimized PPO algorithm. However, this study provides direct experimental evidence by comparing the final grasping success rates of both algorithms after 20 million training interactions. The conclusive results, presented in <xref rid="sensors-25-05253-f013" ref-type="fig">Figure 13</xref>, demonstrate the following.</p><p><xref rid="sensors-25-05253-f013" ref-type="fig">Figure 13</xref>a,b present comparative success rate curves between the optimized PPO algorithm and improved PPO + SA algorithm throughout 20 million training steps. In <xref rid="sensors-25-05253-f013" ref-type="fig">Figure 13</xref>a, the improved PPO + SA algorithm exhibits significantly larger performance jumps compared to the optimized PPO algorithm during the initial 0&#8211;5 million step phase, demonstrating simulated annealing&#8217;s effectiveness in escaping local optima. Between 5 and 15 million steps, the improved PPO + SA algorithm maintains a superior exploration capability through its dynamic learning rate adjustment mechanism, enabling faster convergence toward optimal policies. During the final 15&#8211;20 million step phase, the algorithm&#8217;s simulated annealing component gradually stabilizes the learning rate adjustments, allowing the policy to smoothly converge while maintaining its performance advantage. <xref rid="sensors-25-05253-f013" ref-type="fig">Figure 13</xref>b reveals that the improved PPO + SA algorithm achieves steeper learning progress compared to the optimized PPO algorithm throughout the training process.</p><p>This study further evaluates the execution efficiency of the improved PPO + SA algorithm. With the maximum steps per episode set to 200, experimental results demonstrate that the required steps for the robotic arm to reach grasping positions gradually decrease as training progresses. <xref rid="sensors-25-05253-f014" ref-type="fig">Figure 14</xref> presents a direct comparison between the optimized PPO algorithm and improved PPO + SA algorithm when both have completed 20 million environmental interactions, revealing the following.</p><p>As observed in <xref rid="sensors-25-05253-f014" ref-type="fig">Figure 14</xref>b, the comparative fitting curves of the optimized PPO algorithm and improved PPO + SA algorithm after 20 million training steps reveal two key advantages of the enhanced approach. First, the improved PPO + SA algorithm consistently requires fewer steps per episode throughout the entire training process compared to the optimized PPO algorithm. Second, the step-reduction rate of the improved PPO + SA algorithm shows a steeper decline, indicating the faster optimization of movement efficiency.</p><p>The statistical analysis of the grasping success rates demonstrates that after 20 million steps, the optimized PPO algorithm model achieves a 92% success rate, while the improved PPO + SA algorithm model reaches 98%. Furthermore, the improved PPO + SA algorithm reduces the average steps per successful grasp from the initial 200 steps to 143, outperforming the optimized PPO algorithm&#8217;s reduction to 154 steps. These results are visually summarized in <xref rid="sensors-25-05253-f015" ref-type="fig">Figure 15</xref>, which highlights the following.</p><p>After the validation of simulation experiments, it can be shown that the improved PPO + SA algorithm is superior to the previous optimized PPO algorithm in terms of distance reward, average reward, success rate of grasping, and the number of steps used to grasp the target. These results confirm that integrating simulated annealing with PPO yields superior convergence properties and accelerated training dynamics. Specifically, the hybrid algorithm achieves the following:<list list-type="order"><list-item><p>Increased ability to jump out of the local optimum at the beginning of training and fast convergence at the later stage;</p></list-item><list-item><p>A 6.5% absolute increase in optimal model crawl success rate (98% vs. 92%);</p></list-item><list-item><p>A 7.2% reduction in steps per successful episode (143 vs. 154).</p></list-item></list></p></sec><sec id="sec5dot4-sensors-25-05253"><title>5.4. Sim-to-Real</title><p>The simulation-to-reality (sim-to-real) transfer [<xref rid="B25-sensors-25-05253" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05253" ref-type="bibr">26</xref>] represents a crucial step in deploying robotic reinforcement learning to practical applications. In this study, we enhanced the feasibility of simulation-to-reality transfer through the following measures:<list list-type="order"><list-item><p>Position randomization. We randomly varied target positions and obstacles during training to improve the model&#8217;s generalization ability. This randomization helps the model adapt to complex scenarios in the real world caused by different target positions;</p></list-item><list-item><p>Adaptive noise injection. We implemented adaptive noise injection in the observation space and introduced noise into joint angle measurements to simulate the uncertainty of sensors in the real world;</p></list-item><list-item><p>Standardization of the unified observation space. We achieved consistent standardization between simulated and real-world observations to avoid inference failures caused by inconsistent observation spaces;</p></list-item><list-item><p>Coordinate System Transformation. Accurate coordinate transformation is achieved through hand-eye calibration to ensure consistency between the real and simulated worlds.</p></list-item></list></p><p>For target object data acquisition, this experiment utilizes the Mech-Mind depth camera (Mech-Eye Pro M Enhanced), which captures 2D images, 3D data, and point clouds, while the point cloud provides coordinates relative to the camera frame, as shown in <xref rid="sensors-25-05253-f016" ref-type="fig">Figure 16</xref>.</p><p>The training process uses the robotic arm&#8217;s base coordinate system. To maintain consistency, hand&#8211;eye calibration is performed to obtain the coordinate transformation between the camera and the robotic arm base. The hand&#8211;eye calibration result in this study is represented by a 4 &#215; 4 homogeneous transformation matrix <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi>cam</mml:mi></mml:mrow><mml:mrow><mml:mi>base</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in Equation (13).<disp-formula id="FD13-sensors-25-05253"><label>(13)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi>cam</mml:mi></mml:mrow><mml:mrow><mml:mi>base</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>0.9961</mml:mn></mml:mtd><mml:mtd><mml:mn>0.0219</mml:mn></mml:mtd><mml:mtd><mml:mn>0.0854</mml:mn></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mn>162.08</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.0241</mml:mn></mml:mtd><mml:mtd><mml:mn>0.9994</mml:mn></mml:mtd><mml:mtd><mml:mn>0.0246</mml:mn></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mn>403.37</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>0.0848</mml:mn></mml:mtd><mml:mtd><mml:mn>0.0265</mml:mn></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mn>0.9960</mml:mn></mml:mtd><mml:mtd><mml:mn>1408.24</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The matrix <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi>cam</mml:mi></mml:mrow><mml:mrow><mml:mi>base</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is used to transform point <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>cam</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> from the camera coordinate system to the robotic arm&#8217;s base coordinate system as <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>base</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, with the transformation formula given by Equation (14).<disp-formula id="FD14-sensors-25-05253"><label>(14)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>base</mml:mi><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mi>cam</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this formulation, <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (top-left 3 &#215; 3 submatrix) represents the rotation matrix, while <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (top-right 3 &#215; 1 vector, units: mm) denotes the translation vector, with the last row serving as homogeneous coordinate padding. Through this transformation, any point&#8217;s coordinates in the camera frame can be converted to the robotic arm&#8217;s base frame for model inference. Obstacle information is similarly acquired using the identical transformation.</p><p>During the process of robotic arm grasping, the detection of the target is accomplished by YOLO, which can mark the pixel position of the center point of the object to be grasped. Then, based on the point cloud, it is converted into coordinates. The actual marking effect is shown in <xref rid="sensors-25-05253-f017" ref-type="fig">Figure 17</xref>.</p><p>The robotic arm&#8217;s state information is acquired through built-in servo mechanisms at each joint. The AUBO-i5 robotic arm features integrated encoders within every joint&#8217;s servo motor, enabling the real-time measurement of motor shaft rotation angles. Based on forward kinematics modeling, the controller converts these joint angles into the end-effector&#8217;s spatial pose (position and orientation), forming the essential data foundation for physical grasping operations.</p><p>The crawling process in the real environment is shown in <xref rid="sensors-25-05253-f018" ref-type="fig">Figure 18</xref>.</p><p>It is demonstrated in <xref rid="sensors-25-05253-f018" ref-type="fig">Figure 18</xref>, in that the real environment, it is possible to perform actions acquired by reasoning to accomplish a random grasping function similar to the simulation.</p><p>We also tested placing obstacles in the environment, which undoubtedly made grasping the target much more difficult. Similar to grasping a random target without obstacles, the Mech-Mind depth camera acquired the information and coordinates of the target and obstacles and converted them into the robot arm&#8217;s reference coordinates. <xref rid="sensors-25-05253-f019" ref-type="fig">Figure 19</xref> shows the depth map obtained after adding obstacles, with blue representing the target to be grasped and red representing obstacles.</p><p><xref rid="sensors-25-05253-f020" ref-type="fig">Figure 20</xref> is a YOLO calibration image containing obstacles. In this image, the white objects outlined in red are obstacles, while the red objects outlined in green are the targets to be captured. From this image, the coordinates of the targets and obstacles can be obtained, and the distance from the robot arm joints to the obstacles can be calculated using these coordinates. This is one of the parameters required in the state space and represents very important data for obstacle avoidance and grasping operations.</p><p>Similar to obstacle-free grasping, with this data, model inference and robotic arm operation can be performed to achieve obstacle avoidance grasping consistent with the simulated environment. The obstacle avoidance grasping process is shown in <xref rid="sensors-25-05253-f021" ref-type="fig">Figure 21</xref>.</p><p><xref rid="app1-sensors-25-05253" ref-type="app">Video S1</xref> demonstrates the robot arm performing target grasping and obstacle avoidance grasping operations in a real-world environment.</p></sec><sec id="sec5dot5-sensors-25-05253"><title>5.5. Future Work</title><p>Although the current PPO algorithm integrated with the simulated annealing algorithm has achieved encouraging results, further optimization can be achieved by introducing other reinforcement learning methods. For example, the curriculum learning [<xref rid="B27-sensors-25-05253" ref-type="bibr">27</xref>] framework adopts an incremental training paradigm to systematically increase task difficulty. In this experiment, curriculum learning can be applied to gradually expand the generation range of target objects. This integration is expected to bring dual benefits, as follow: (1) Accelerated convergence&#8211; by deferring complex obstacle avoidance to a later stage, the algorithm focuses early training cycles on basic grasping skills, which is estimated to reduce training time in the initial phase. A simplified initial task allows for the faster stabilization of the strategy before complexity is introduced. (2) Enhancing universality&#8212;the staged difficulty increment builds robust feature representations, improves model generalization, and enhances learning for complex tasks.</p><p>The algorithm has achieved good results in grasping actions, but in order to more clearly demonstrate its applicability and robustness in various operating environments, future research will involve testing the proposed algorithm on a variety of different tasks (such as placement, assembly, or collaborative tasks).</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-05253"><title>6. Conclusions</title><p>This study focuses on the PPO algorithm for reinforcement learning in continuous action spaces. The research begins with a detailed introduction to the PPO algorithm, followed by the design of simulation strategies encompassing the robotic arm&#8217;s state representation, action space configuration, loss function formulation, and reward structure. Subsequently, the simulated annealing (SA) algorithm is presented along with a feasibility analysis of its integration with PPO. Through comparative experiments between the optimized PPO algorithm and improved PPO + SA algorithms, key performance metrics including average grasping reward, success rate, and step efficiency are systematically evaluated. The simulation results demonstrate that incorporating simulated annealing for dynamic learning rate adjustment yields comprehensive performance improvements. Furthermore, the implemented model successfully transitions from simulation to physical deployment, achieving the precise grasping of randomly positioned targets in real-world scenarios. The experimental outcomes validate the effectiveness of the proposed approach in both virtual and physical environments while maintaining operational robustness.</p><p>Subsequent experiments will continue to introduce concepts from other reinforcement learning algorithms and adjust the parameter relationships between PPO, SA, and other reinforcement learning algorithms. This is expected to accelerate model convergence speed and enhance its generalization capabilities. The experiments will be continuously optimized and upgraded, tested across various tasks to adapt to more complex and diverse scenarios, meet the future needs of smart factories, and improve current production efficiency levels.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-sensors-25-05253"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mdpi.com/article/10.3390/s25175253/s1">https://www.mdpi.com/article/10.3390/s25175253/s1</uri>, Video S1: Demonstration of robotic arm target grasping and obstacle avoidance grasping in a real environment.</p><supplementary-material id="sensors-25-05253-s001" position="float" content-type="local-data" orientation="portrait"><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sensors-25-05253-s001.zip" position="float" orientation="portrait"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Final draft review, C.L. (Chunlei Li); overall scheme design, L.L.; writing, Z.L.; creation of charts, C.L. (Chenbo Li).; data analysis, Z.J.; literature search, J.L.; data collection, Y.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05253"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>B.</given-names></name></person-group><article-title>A method on dynamic path planning for robotic manipulator autonomous obstacle avoidance based on an improved RRT algorithm</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>571</elocation-id><pub-id pub-id-type="doi">10.3390/s18020571</pub-id><pub-id pub-id-type="pmid">29438320</pub-id><pub-id pub-id-type="pmcid">PMC5856115</pub-id></element-citation></ref><ref id="B2-sensors-25-05253"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name></person-group><article-title>Application of deep reinforcement learning in mobile robot path planning</article-title><source>Proceedings of the 2017 Chinese Automation Congress (CAC)</source><conf-loc>Jinan, China</conf-loc><conf-date>20&#8211;22 October 2017</conf-date><fpage>7112</fpage><lpage>7116</lpage><pub-id pub-id-type="doi">10.1109/CAC.2017.8244061</pub-id></element-citation></ref><ref id="B3-sensors-25-05253"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rakshit</surname><given-names>A.</given-names></name><name name-style="western"><surname>Konar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Nagar</surname><given-names>A.K.</given-names></name></person-group><article-title>A hybrid brain-computer interface for closed-loop position control of a robot arm</article-title><source>IEEE/CAA J. Autom. Sin.</source><year>2020</year><volume>7</volume><fpage>1344</fpage><lpage>1360</lpage><pub-id pub-id-type="doi">10.1109/JAS.2020.1003336</pub-id></element-citation></ref><ref id="B4-sensors-25-05253"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>D.</given-names></name></person-group><article-title>Empowering Things with Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things</article-title><source>IEEE Internet Things J.</source><year>2021</year><volume>8</volume><fpage>7789</fpage><lpage>7817</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2020.3039359</pub-id></element-citation></ref><ref id="B5-sensors-25-05253"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shimizu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kakuya</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yoon</surname><given-names>W.-K.</given-names></name><name name-style="western"><surname>Kitagaki</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kosuge</surname><given-names>K.</given-names></name></person-group><article-title>Analytical Inverse Kinematic Computation for 7-DOF Redundant Manipulators With Joint Limits and Its Application to Redundancy Resolution</article-title><source>IEEE Trans. Robot.</source><year>2008</year><volume>24</volume><fpage>1131</fpage><lpage>1142</lpage><pub-id pub-id-type="doi">10.1109/TRO.2008.2003266</pub-id></element-citation></ref><ref id="B6-sensors-25-05253"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hart</surname><given-names>P.E.</given-names></name><name name-style="western"><surname>Nilsson</surname><given-names>N.J.</given-names></name><name name-style="western"><surname>Raphael</surname><given-names>B.</given-names></name></person-group><article-title>A Formal Basis for the Heuristic Determination of Minimum Cost Paths</article-title><source>IEEE Trans. Syst. Sci. Cybern.</source><year>1968</year><volume>4</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1109/TSSC.1968.300136</pub-id></element-citation></ref><ref id="B7-sensors-25-05253"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Katoch</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chauhan</surname><given-names>S.S.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>V.</given-names></name></person-group><article-title>A review on genetic algorithm: Past, present, and future</article-title><source>Multimed. Tools Appl.</source><year>2021</year><volume>80</volume><fpage>8091</fpage><lpage>8126</lpage><pub-id pub-id-type="doi">10.1007/s11042-020-10139-6</pub-id><pub-id pub-id-type="pmid">33162782</pub-id><pub-id pub-id-type="pmcid">PMC7599983</pub-id></element-citation></ref><ref id="B8-sensors-25-05253"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Morales</surname><given-names>E.F.</given-names></name><name name-style="western"><surname>Zaragoza</surname><given-names>J.H.</given-names></name></person-group><article-title>An introduction to reinforcement learning</article-title><source>IEEE Trans. Neural Netw.</source><year>2011</year><volume>11</volume><fpage>219</fpage><lpage>354</lpage></element-citation></ref><ref id="B9-sensors-25-05253"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Isele</surname><given-names>D.</given-names></name><name name-style="western"><surname>Rahimi</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cosgun</surname><given-names>A.</given-names></name><name name-style="western"><surname>Subramanian</surname><given-names>K.</given-names></name><name name-style="western"><surname>Fujimura</surname><given-names>K.</given-names></name></person-group><article-title>Navigating Occluded Intersections with Autonomous Vehicles Using Deep Reinforcement Learning</article-title><source>Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Brisbane, QLD, Australia</conf-loc><conf-date>21&#8211;25 May 2018</conf-date><fpage>2034</fpage><lpage>2039</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2018.8461233</pub-id></element-citation></ref><ref id="B10-sensors-25-05253"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Du</surname><given-names>Y.Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Q.H.</given-names></name><name name-style="western"><surname>L&#252;</surname><given-names>X.Y.</given-names></name></person-group><article-title>Virtual control technology of cantilever roadheader driven by digital twin</article-title><source>Comput. Integr. Manuf. Syst.</source><year>2021</year><volume>27</volume><fpage>1617</fpage><lpage>1628</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B11-sensors-25-05253"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name></person-group><article-title>Application of digital twin technology in petrochemical industry</article-title><source>Pet. Refin. Eng.</source><year>2022</year><volume>52</volume><fpage>44</fpage><lpage>49</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B12-sensors-25-05253"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mnih</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kavukcuoglu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Silver</surname><given-names>D.</given-names></name><name name-style="western"><surname>Rusu</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Veness</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bellemare</surname><given-names>M.G.</given-names></name><name name-style="western"><surname>Graves</surname><given-names>A.</given-names></name><name name-style="western"><surname>Riedmiller</surname><given-names>M.</given-names></name><name name-style="western"><surname>Fidjeland</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Ostrovski</surname><given-names>G.</given-names></name><etal/></person-group><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><year>2015</year><volume>518</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/nature14236</pub-id><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="B13-sensors-25-05253"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mnih</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kavukcuoglu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Silver</surname><given-names>D.</given-names></name><name name-style="western"><surname>Graves</surname><given-names>A.</given-names></name><name name-style="western"><surname>Antonoglou</surname><given-names>I.</given-names></name><name name-style="western"><surname>Wierstra</surname><given-names>D.</given-names></name><name name-style="western"><surname>Riedmiller</surname><given-names>M.</given-names></name></person-group><article-title>Playing Atari with Deep Reinforcement Learning</article-title><source>arXiv</source><year>2013</year><pub-id pub-id-type="doi">10.48550/arXiv.1312.5602</pub-id><pub-id pub-id-type="arxiv">1312.5602</pub-id></element-citation></ref><ref id="B14-sensors-25-05253"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Schaul</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hessel</surname><given-names>M.</given-names></name><name name-style="western"><surname>Van Hasselt</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lanctot</surname><given-names>M.</given-names></name><name name-style="western"><surname>De Freitas</surname><given-names>N.</given-names></name></person-group><article-title>Dueling Network Architec-tures for Deep Reinforcement Learning</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>New York, NY, USA</conf-loc><conf-date>19&#8211;24 June 2016</conf-date><fpage>1995</fpage><lpage>2003</lpage></element-citation></ref><ref id="B15-sensors-25-05253"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Van Hasselt</surname><given-names>H.</given-names></name><name name-style="western"><surname>Guez</surname><given-names>A.</given-names></name><name name-style="western"><surname>Silver</surname><given-names>D.</given-names></name></person-group><article-title>Deep Reinforcement Learning with Double Q-learning</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Phoenix, AZ, USA</conf-loc><conf-date>12&#8211;17 February 2016</conf-date><fpage>2094</fpage><lpage>2100</lpage><pub-id pub-id-type="doi">10.1609/aaai.v30i1.10295</pub-id></element-citation></ref><ref id="B16-sensors-25-05253"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lei</surname><given-names>G.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>T.</given-names></name></person-group><article-title>Robotic arm control method based on deep reinforcement learning</article-title><source>Syst. Simul.</source><year>2019</year><volume>31</volume><fpage>2452</fpage><lpage>2457</lpage><comment>(In Chinese)</comment></element-citation></ref><ref id="B17-sensors-25-05253"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Schulman</surname><given-names>J.</given-names></name><name name-style="western"><surname>Levine</surname><given-names>S.</given-names></name><name name-style="western"><surname>Moritz</surname><given-names>P.</given-names></name><name name-style="western"><surname>Jordan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Moritz</surname><given-names>P.</given-names></name></person-group><article-title>Trust Region Policy Optimization</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Lille, France</conf-loc><conf-date>6&#8211;11 July 2015</conf-date><fpage>1889</fpage><lpage>1897</lpage></element-citation></ref><ref id="B18-sensors-25-05253"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schulman</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wolski</surname><given-names>F.</given-names></name><name name-style="western"><surname>Dhariwal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name><name name-style="western"><surname>Klimov</surname><given-names>O.</given-names></name></person-group><article-title>Proximal Policy Optimization Algorithms</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="doi">10.48550/arXiv.1707.06347</pub-id><pub-id pub-id-type="arxiv">1707.06347</pub-id></element-citation></ref><ref id="B19-sensors-25-05253"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Diao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Piao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Goebel</surname><given-names>R.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>Y.</given-names></name></person-group><article-title>The Sufficiency of Off-Policyness and Soft Clipping: PPO Is Still Insufficient according to an Off-Policy Measure</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>7&#8211;14 February 2023</conf-date><volume>Volume 37</volume><fpage>7078</fpage><lpage>7086</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i6.25864</pub-id></element-citation></ref><ref id="B20-sensors-25-05253"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name></person-group><article-title>Proximal policy optimization algorithm based on clipping optimization and policy guidance</article-title><source>J. Comput. Appl.</source><year>2024</year><volume>44</volume><fpage>2334</fpage><lpage>2341</lpage><pub-id pub-id-type="doi">10.11772/j.issn.1001-9081.2023081079</pub-id></element-citation></ref><ref id="B21-sensors-25-05253"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><name name-style="western"><surname>Park</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>M.</given-names></name></person-group><article-title>From Unified Robot Description Format to DH Parameters: Examinations of Two Different Approaches for Manipulator</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>133441</fpage><lpage>133455</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3438626</pub-id></element-citation></ref><ref id="B22-sensors-25-05253"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Delahaye</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chaimatanan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mongeau</surname><given-names>M.</given-names></name></person-group><article-title>Simulated Annealing: From Basics to Applications</article-title><source>Handbook of Metaheuristics</source><edition>2nd ed.</edition><person-group person-group-type="editor"><name name-style="western"><surname>Gendreau</surname><given-names>M.</given-names></name><name name-style="western"><surname>Potvin</surname><given-names>J.-Y.</given-names></name></person-group><series>International Series in Operations Research &amp; Management Science</series><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><volume>Volume 272</volume><fpage>1</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-91086-4_1</pub-id></element-citation></ref><ref id="B23-sensors-25-05253"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raffin</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hill</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gleave</surname><given-names>A.</given-names></name></person-group><article-title>Stable-baselines3: Reliable reinforcement learning implementations</article-title><source>Mach. Learn. Res.</source><year>2021</year><volume>22</volume><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B24-sensors-25-05253"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>Y.K.</given-names></name></person-group><article-title>An Open-Source Multi-goal Reinforcement Learning Environment for Robotic Manipulation with Pybullet</article-title><source>Proceedings of the Towards Autonomous Robotic Systems (TAROS 2021)</source><conf-loc>Lincoln, UK</conf-loc><conf-date>8&#8211;10 September 2021</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Fox</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ghalamzan Esfahani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Saaj</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hanheide</surname><given-names>M.</given-names></name><name name-style="western"><surname>Parsons</surname><given-names>S.</given-names></name></person-group><comment>Lecture Notes in Computer Science</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><volume>Volume 13054</volume><pub-id pub-id-type="doi">10.1007/978-3-030-89177-0_2</pub-id></element-citation></ref><ref id="B25-sensors-25-05253"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Scheikl</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>Tagliabue</surname><given-names>E.</given-names></name><name name-style="western"><surname>Gyenes</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wagner</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dall&#8217;ALba</surname><given-names>D.</given-names></name><name name-style="western"><surname>Fiorini</surname><given-names>P.</given-names></name><name name-style="western"><surname>Mathis-Ullrich</surname><given-names>F.</given-names></name></person-group><article-title>Sim-to-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery</article-title><source>IEEE Robot. Autom. Lett.</source><year>2023</year><volume>8</volume><fpage>560</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1109/LRA.2022.3227873</pub-id></element-citation></ref><ref id="B26-sensors-25-05253"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Martin</surname><given-names>J.B.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Moutarde</surname><given-names>F.</given-names></name></person-group><article-title>Pre-trained image encoder for data-efficient reinforcement learning and sim-to-real transfer on robotic-manipulation tasks</article-title><source>Proceedings of the CoRL 2022 Workshop on Pre-training Robot Learning</source><conf-loc>Auckland, New Zealand</conf-loc><conf-date>14 December 2022</conf-date></element-citation></ref><ref id="B27-sensors-25-05253"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Louradour</surname><given-names>J.</given-names></name><name name-style="western"><surname>Collobert</surname><given-names>R.</given-names></name><name name-style="western"><surname>Weston</surname><given-names>J.</given-names></name></person-group><article-title>Curriculum learning</article-title><source>Proceedings of the 26th Annual International Conference on Machine Learning</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>14&#8211;18 June 2009</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2009</year><fpage>41</fpage><lpage>48</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05253-f001" orientation="portrait"><label>Figure 1</label><caption><p>Illustration of the probability ratio trimming mechanism of the PPO algorithm.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g001.jpg"/></fig><fig position="float" id="sensors-25-05253-f002" orientation="portrait"><label>Figure 2</label><caption><p>Representation of jaw linking points.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g002.jpg"/></fig><fig position="float" id="sensors-25-05253-f003" orientation="portrait"><label>Figure 3</label><caption><p>Minimum distance of clamping jaws from random obstacles and radiation map of random obstacles.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g003.jpg"/></fig><fig position="float" id="sensors-25-05253-f004" orientation="portrait"><label>Figure 4</label><caption><p>Schematic diagram of the angle between the end of the jaws and the <italic toggle="yes">z</italic>-axis of the world coordinate system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g004.jpg"/></fig><fig position="float" id="sensors-25-05253-f005" orientation="portrait"><label>Figure 5</label><caption><p>Block diagram of robotic arm trajectory planning training based on PPO + SA algorithm.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g005.jpg"/></fig><fig position="float" id="sensors-25-05253-f006" orientation="portrait"><label>Figure 6</label><caption><p>Flow chart of PPO + SA algorithm design.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g006.jpg"/></fig><fig position="float" id="sensors-25-05253-f007" orientation="portrait"><label>Figure 7</label><caption><p>Experimental environment diagram.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g007.jpg"/></fig><fig position="float" id="sensors-25-05253-f008" orientation="portrait"><label>Figure 8</label><caption><p>Reinforcement learning simulation training scenario.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g008.jpg"/></fig><fig position="float" id="sensors-25-05253-f009" orientation="portrait"><label>Figure 9</label><caption><p>Robotic arm gripping simulation path. (<bold>a</bold>) Initial attitude of the robot arm ready to grasp the target. (<bold>b</bold>) The end of the robot arm leans toward the target position. (<bold>c</bold>) The end of the robot arm is about to reach the target position. (<bold>d</bold>) The end of the robot arm reaches the gripping position.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g009.jpg"/></fig><fig position="float" id="sensors-25-05253-f010" orientation="portrait"><label>Figure 10</label><caption><p>Robotic arm grasping random obstacle generation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g010.jpg"/></fig><fig position="float" id="sensors-25-05253-f011" orientation="portrait"><label>Figure 11</label><caption><p>Comparison of robotic arm control process distance rewards. (<bold>a</bold>) Comparison of distance reward curves for PPO and PPO + SA algorithms. (<bold>b</bold>) Comparison of average distance reward curves for PPO and PPO + SA algorithms.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g011.jpg"/></fig><fig position="float" id="sensors-25-05253-f012" orientation="portrait"><label>Figure 12</label><caption><p>Comparison of robotic arm control process rewards. (<bold>a</bold>) Comparison of PPO and PPO + SA algorithm reward curves. (<bold>b</bold>) Comparison of average reward curves for PPO and PPO + SA algorithms.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g012.jpg"/></fig><fig position="float" id="sensors-25-05253-f013" orientation="portrait"><label>Figure 13</label><caption><p>Comparison of success rate of robotic arm gripping processes. (<bold>a</bold>) Comparison of PPO and PPO + SA algorithm crawling success curves. (<bold>b</bold>) Comparison of average crawl success rate between PPO and PPO + SA algorithms.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g013.jpg"/></fig><fig position="float" id="sensors-25-05253-f014" orientation="portrait"><label>Figure 14</label><caption><p>Comparison of single gripping steps of robotic arm. (<bold>a</bold>) Comparison of PPO and PPO + SA algorithms in terms of the number of steps captured in a single run. (<bold>b</bold>) Comparison of average number of steps captured by PPO and PPO + SA algorithms.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g014.jpg"/></fig><fig position="float" id="sensors-25-05253-f015" orientation="portrait"><label>Figure 15</label><caption><p>Average success rate vs. average number of steps.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g015.jpg"/></fig><fig position="float" id="sensors-25-05253-f016" orientation="portrait"><label>Figure 16</label><caption><p>Mech-Eye depth map.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g016.jpg"/></fig><fig position="float" id="sensors-25-05253-f017" orientation="portrait"><label>Figure 17</label><caption><p>YOLO Calibration of the center point of the target object.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g017.jpg"/></fig><fig position="float" id="sensors-25-05253-f018" orientation="portrait"><label>Figure 18</label><caption><p>Diagram of the grasping process in a real environment through model reasoning. (<bold>a</bold>) The robotic arm is in the initial position. (<bold>b</bold>) The robotic arm is approaching the target position. (<bold>c</bold>) The robotic arm is about to reach the target position. (<bold>d</bold>) The robotic arm reaches the grasping position.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g018.jpg"/></fig><fig position="float" id="sensors-25-05253-f019" orientation="portrait"><label>Figure 19</label><caption><p>Mech-Eye depth map with obstacles.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g019.jpg"/></fig><fig position="float" id="sensors-25-05253-f020" orientation="portrait"><label>Figure 20</label><caption><p>YOLO calibration map containing obstacles.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g020.jpg"/></fig><fig position="float" id="sensors-25-05253-f021" orientation="portrait"><label>Figure 21</label><caption><p>Schematic diagram of obstacle avoidance and grasping in a real environment through model inference. (<bold>a</bold>) The robotic arm is in the initial position. (<bold>b</bold>) The robotic arm is avoiding obstacles and approaching the target position. (<bold>c</bold>) The robotic arm has successfully avoided obstacles and is about to reach the target position. (<bold>d</bold>) The robotic arm reaches the grasping position.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05253-g021.jpg"/></fig><table-wrap position="float" id="sensors-25-05253-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05253-t001_Table 1</object-id><label>Table 1</label><caption><p>Software and hardware configuration for training.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Software and Hardware</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Detailed Information</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Processors</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel I7-12800HX</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Discrete Graphics Card</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA GeForce RTX 4070</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Operating technique</td><td align="center" valign="middle" rowspan="1" colspan="1">Windows-11</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Physical Simulation Library</td><td align="center" valign="middle" rowspan="1" colspan="1">Pybullet</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep Reinforcement Learning Library</td><td align="center" valign="middle" rowspan="1" colspan="1">Stable-Baselines3: 2.0.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Python Environment</td><td align="center" valign="middle" rowspan="1" colspan="1">3.10.16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Customizing the Environment to Create Libraries</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gymnasium: 0.28.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05253-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05253-t002_Table 2</object-id><label>Table 2</label><caption><p>Hyper-parameters of the PPO + SA algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">max_step (Maximum number of execution steps)</td><td align="center" valign="middle" rowspan="1" colspan="1">20,000,000</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">initial_lr (Initialized learning rate)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0003</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">min_lr (Minimum learning rate)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.000001</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">annealing_coefficient</td><td align="center" valign="middle" rowspan="1" colspan="1">0.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">batch_size (Number of samples in a single training session)</td><td align="center" valign="middle" rowspan="1" colspan="1">256</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">clip_range=</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Gmma (discount factor)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.99</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">n_steps (Number of environmental steps collected per update)</td><td align="center" valign="middle" rowspan="1" colspan="1">2048</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">n_epoches (Number of training rounds executed per update)</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single_length_max (Maximum length of a single training session)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>