<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431383</article-id><article-id pub-id-type="pmcid-ver">PMC12431383.1</article-id><article-id pub-id-type="pmcaid">12431383</article-id><article-id pub-id-type="pmcaiid">12431383</article-id><article-id pub-id-type="doi">10.3390/s25175363</article-id><article-id pub-id-type="publisher-id">sensors-25-05363</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Cross-Domain Object Detection with Hierarchical Multi-Scale Domain Adaptive YOLO</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5804-4538</contrib-id><name name-style="western"><surname>Zhu</surname><given-names initials="S">Sihan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="c1-sensors-25-05363" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhu</surname><given-names initials="P">Peipei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="Y">Yuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Qiao</surname><given-names initials="W">Wensheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Leo</surname><given-names initials="M">Marco</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05363">National Key Laboratory of Complex Aviation System Simulation, Southwest China Institute of Electronic Technology, Chengdu 610036, China; <email>zhupeipei0806@sina.com</email> (P.Z.); <email>wuyuaner@126.com</email> (Y.W.); <email>wshqiao0529@163.com</email> (W.Q.)</aff><author-notes><corresp id="c1-sensors-25-05363"><label>*</label>Correspondence: <email>sihanzhu96@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>29</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5363</elocation-id><history><date date-type="received"><day>13</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>21</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>29</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05363.pdf"/><abstract><p>To alleviate the performance degradation caused by domain shift, domain adaptive object
detection (DAOD) has achieved compelling success in recent years. DAOD aims to improve
the model&#8217;s detection performance on the target domain by reducing the distribution
discrepancy between different domains. However, most existing methods are built on
two-stage Faster RCNN, which is not suitable for real applications due to the detection
efficiency. In this paper, we propose a novel Hierarchical Multi-scale Domain Adaptive
(HMDA) method by integrating a simple but effective one-stage YOLO framework. HMDAYOLO
mainly consists of the hierarchical backbone adaptation and the multi-scale head
adaptation. The former performs hierarchical adaptation based on the differences in
representational information of features at different depths of the backbone network, which
promotes comprehensive distribution alignment and suppresses the negative transfer. The
latter makes full use of the rich discriminative information in the feature maps to be detected
for multi-scale adaptation. Additionally, it can reduce local instance divergence and ensure
the model&#8217;s multi-scale detection capability. In this way, HMDA can improve the model&#8217;s
generalization ability while ensuring its discriminating capability. We empirically verify the
effectiveness of our method on four cross-domain object detection scenarios, comprising
different domain shifts. Experimental results and analyses demonstrate that HMDA-YOLO
can achieve competitive performance with real-time detection efficiency.</p></abstract><kwd-group><kwd>object detection</kwd><kwd>domain adaptation</kwd><kwd>hierarchical</kwd><kwd>multi-scale</kwd><kwd>YOLO</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62303433</award-id></award-group><funding-statement>This paper is supported by the National Natural Science Foundation of China under Grant 62303433.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05363"><title>1. Introduction</title><p>Object detection is a core technique in the field of computer vision, which aims to recognize and localize objects of interest in images or videos. And it can help individuals to extract key information from complex visual environments. Traditional object detection algorithms rely on hand-crafted features, with drawbacks, such as high computational costs, poor robustness, and low accuracy. In recent years, deep learning has gained popularity [<xref rid="B1-sensors-25-05363" ref-type="bibr">1</xref>], which can learn more robust and generalizable deep feature representations. Specifically, object detection has made impressive progress thanks to convolutional neural networks (CNNs). And it has become an essential part of many real-world applications. The mainstream three detectors are Faster RCNN [<xref rid="B2-sensors-25-05363" ref-type="bibr">2</xref>], You Only Look Once (YOLO) [<xref rid="B3-sensors-25-05363" ref-type="bibr">3</xref>], and Single Shot Multi-box Detector (SSD) [<xref rid="B4-sensors-25-05363" ref-type="bibr">4</xref>]. However, the superior performance of advance detectors relies on the availability of large number of high-quality annotated data. Acquiring such data is time consuming and labour intensive. Additionally, well-trained detectors may experience a sudden drop in performance due to the domain shift problem when processing new data or tasks.</p><p>The aforementioned problems seriously affect the application and deployment of the detection models when data distributions are different. Unsupervised Domain Adaptation (UDA) [<xref rid="B5-sensors-25-05363" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05363" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05363" ref-type="bibr">7</xref>] has been proposed to deal with the domain shift problems and reduce the dependence of model training on target labels. It aims to transfer the source knowledge to the target and reduce the distribution discrepancy between different domains, which enhances the model&#8217;s generalization ability and discriminative capability. Deep UDA methods can be divided into two categories: Moment matching methods [<xref rid="B8-sensors-25-05363" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05363" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05363" ref-type="bibr">10</xref>] explicitly match the feature distribution across domains during network training based on pre-defined metrics. Adversarial learning methods [<xref rid="B11-sensors-25-05363" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05363" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05363" ref-type="bibr">13</xref>] implicitly learn domain-invariant representations in an adversarial paradigm.</p><p>Unlike classification and semantic segmentation, the object detection task predicts bounding box localization and corresponding object category [<xref rid="B14-sensors-25-05363" ref-type="bibr">14</xref>]. This brings potential problems and challenges for cross-domain object detection, but has likewise raised a lot of concerns. Recent studies have made significant efforts to improve the cross-domain detection capabilities. Following the first try on cross-domain object detection, domain adaptive Faster RCNN [<xref rid="B15-sensors-25-05363" ref-type="bibr">15</xref>], most existing DAOD methods are still built on the two-stage detector Faster RCNN [<xref rid="B16-sensors-25-05363" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05363" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05363" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05363" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05363" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05363" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05363" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05363" ref-type="bibr">23</xref>]. Few works have utilized one-stage detectors (e.g., SSD [<xref rid="B4-sensors-25-05363" ref-type="bibr">4</xref>] and FCOS [<xref rid="B24-sensors-25-05363" ref-type="bibr">24</xref>]) to consider the computational efficiency [<xref rid="B25-sensors-25-05363" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05363" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05363" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05363" ref-type="bibr">28</xref>]. And some methods have also been proposed for light-weight and practical use based on the YOLO series [<xref rid="B29-sensors-25-05363" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05363" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05363" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05363" ref-type="bibr">32</xref>]. Overall, the development of DAOD methods is closely related to the key technologies in the field of object detection and domain adaptation.</p><p>Since the scene layout, number of objects, patterns between objects, and the background may be quite different across domains in object detection tasks, a potential problem in DAOD tasks is that blindly and directly adapting feature distributions can lead to negative transfer, which degrades the model&#8217;s cross-domain performance. This is also the reason why strategies like prototype alignment and entropy regularization, which work well in image classification or segmentation, become inapplicable in object detection. In addition, some of the methods utilize image generation techniques to introduce auxiliary data [<xref rid="B22-sensors-25-05363" ref-type="bibr">22</xref>,<xref rid="B31-sensors-25-05363" ref-type="bibr">31</xref>], or adopt a student&#8211;teacher network paradigm [<xref rid="B32-sensors-25-05363" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05363" ref-type="bibr">33</xref>] for training. The former makes the training process not an end-to-end mode, and the latter significantly increases the model&#8217;s complexity and the difficulty of the training model. These types of methods greatly restrict the training and application scenarios of detectors. Finally, the efficiency and accuracy of the dominant Faster RCNN are outdated, which is not suitable for resource-limited and time-critical real applications [<xref rid="B32-sensors-25-05363" ref-type="bibr">32</xref>].</p><p>For the above reasons, we propose a novel Hierarchical Multi-scale Domain Adaptive method, HMDA-YOLO, based on the simple but effective one-stage YOLOv5 [<xref rid="B34-sensors-25-05363" ref-type="bibr">34</xref>] detector. HMDA-YOLO is easy to implement and has competitive performance, which consists of the hierarchical backbone adaptation and the multi-scale head adaptation. Considering the differences in representation information of features at various depths, we designed the hierarchical backbone adaptation strategy, which promotes comprehensive distribution alignment and suppresses the negative transfer. Specifically, we adopt the pixel-level adaptation, image-level adaptation, and weighted image-level adaptation for the adversarial training of shallow-level, middle-level, and deep-level feature maps, respectively. To make full use of the rich discriminative information of the feature maps to be detected, we further designed the multi-scale head adaptation strategy. It performs pixel-level adaptation across each detection scale, which reduces local instance discrepancy and the impact of background noise. Note that &#8220;pixel-level&#8221; denotes each location at the corresponding feature map and the &#8220;image-level&#8221; treats the entire feature map as a whole in this paper. The proposed HMDA-YOLO can significantly improve the model&#8217;s cross-domain capability. We empirically verify the HMDA-YOLO on four cross-domain object detection scenarios, comprising different domain shifts. Experimental results and analyses demonstrate that HMDA-YOLO can achieve competitive performance with high detection efficiency.</p><p>The contributions of this paper can be summarized as follows: (1) we propose a simple but effective DAOD method, HMDA-YOLO, for more accurate and efficient cross-domain detection; (2) a hierarchical adaptation strategy for the backbone network as well as a multi-scale adaptation strategy for the head network are designed to simultaneously ensure the model&#8217;s generalization and discriminating capability; (3) HMDA-YOLO can achieve competitive performance on several cross-domain object detection benchmarks, compared with state-of-the-art DAOD methods.</p><p>The remainder of this paper is organized as follows: <xref rid="sec2-sensors-25-05363" ref-type="sec">Section 2</xref> reviews the techniques related to DAOD. <xref rid="sec3-sensors-25-05363" ref-type="sec">Section 3</xref> explores the technical details of HMDA. <xref rid="sec4-sensors-25-05363" ref-type="sec">Section 4</xref> presents the experimental results and analysis. Finally, <xref rid="sec5-sensors-25-05363" ref-type="sec">Section 5</xref> provides a summary.</p></sec><sec id="sec2-sensors-25-05363"><title>2. Related&#160;Work</title><sec id="sec2dot1-sensors-25-05363"><title>2.1. Object&#160;Detection</title><p>The mainstream methods for object detection can be categorized into two-stage (e.g., Faster RCNN [<xref rid="B2-sensors-25-05363" ref-type="bibr">2</xref>] and Mask RCNN [<xref rid="B35-sensors-25-05363" ref-type="bibr">35</xref>]) and one-stage (e.g., SSD [<xref rid="B4-sensors-25-05363" ref-type="bibr">4</xref>], FCOS [<xref rid="B24-sensors-25-05363" ref-type="bibr">24</xref>], and YOLO series [<xref rid="B3-sensors-25-05363" ref-type="bibr">3</xref>]). Faster RCNN [<xref rid="B2-sensors-25-05363" ref-type="bibr">2</xref>] is the representative method for two-stage detectors. Its main contribution is the region proposal network (RPN), and it integrates feature extraction, proposal detection, bounding box regression, and classification into a single unified framework. SSD [<xref rid="B4-sensors-25-05363" ref-type="bibr">4</xref>] predicts bounding box and category from feature maps at different scales for fast and accurate object detection. The YOLO series has undergone several version upgrades. YOLO [<xref rid="B3-sensors-25-05363" ref-type="bibr">3</xref>] first regards the detection task as regression and it can realize real-time detection. YOLOv3 [<xref rid="B36-sensors-25-05363" ref-type="bibr">36</xref>] utilizes Darknet-53 as the backbone network and can obtain multi-scale predictions by adding the Spatial Pyramid Pooling (SPP) module. YOLOv4 [<xref rid="B37-sensors-25-05363" ref-type="bibr">37</xref>] examines the contribution of different tricks on the object detection task and extends the Darknet-53 backbone with Cross Stage Partial (CSP) connection. YOLOv5 [<xref rid="B34-sensors-25-05363" ref-type="bibr">34</xref>] modifies the CSPDarknet-53 and makes use of multiple augmentation techniques for faster and better capability. In recent years, DEtection TRansformer (DETR) [<xref rid="B38-sensors-25-05363" ref-type="bibr">38</xref>] shows competitive performance, which treats object detection as a set prediction task.</p></sec><sec id="sec2dot2-sensors-25-05363"><title>2.2. Unsupervised Domain&#160;Adaptation</title><p>As mentioned before, there are two mainstream methods of UDA. Moment matching methods usually measure the distribution discrepancy by a pre-defined metric (e.g., Maximum Mean Discrepancy), and explicitly match the feature distribution across domains by reducing such discrepancy during network training. The Deep Adaptation Network (DAN) [<xref rid="B8-sensors-25-05363" ref-type="bibr">8</xref>] extends the maximum mean discrepancy with multi-kernel and optimizes it with multiple layers. The Joint Adaptation Network (JAN) [<xref rid="B9-sensors-25-05363" ref-type="bibr">9</xref>] tries to simultaneously match the margin and conditional distribution. The Deep Sub-domain Adaptation Network (DSAN) [<xref rid="B10-sensors-25-05363" ref-type="bibr">10</xref>] aligns sub-domains for fine-grained distribution matching. Adversarial learning methods extend the network with a domain discriminator or more classifiers, and implicitly learn domain-invariant representations in an adversarial paradigm. The Domain Adversarial Neural Network (DANN) [<xref rid="B11-sensors-25-05363" ref-type="bibr">11</xref>] firstly combines adversarial training with domain adaptation. And it proposes the gradient reverse layer, which is the core of most adversarial methods. The Conditional Domain Adversarial Network (CDAN) [<xref rid="B12-sensors-25-05363" ref-type="bibr">12</xref>] proposes multi-linear conditioning to make use of discriminative information for better adversarial training. The Dynamic Adversarial Adaptation Network (DAAN) [<xref rid="B13-sensors-25-05363" ref-type="bibr">13</xref>] implements adversarial training in each category and dynamically adapts their weights.</p></sec><sec id="sec2dot3-sensors-25-05363"><title>2.3. Domain Adaptive Object&#160;Detection</title><p>Object detection is a fundamental task in the field of computer vision and has a wide range of real-world applications. Therefore, researchers pay more and more attention to cross-domain detection techniques [<xref rid="B14-sensors-25-05363" ref-type="bibr">14</xref>]. As mentioned earlier, the Faster RCNN-based methods are currently the dominant. Domain Adaptive Faster RCNN (DAF) [<xref rid="B15-sensors-25-05363" ref-type="bibr">15</xref>] is the first work to address the DAOD task by designing the image-level and instance-level adaptation. Strong-Weak Distribution Alignment (SWDA) [<xref rid="B16-sensors-25-05363" ref-type="bibr">16</xref>] adjusts the adaptation balance between different network layers to avoid potential risks of blind transfer. Reference [<xref rid="B18-sensors-25-05363" ref-type="bibr">18</xref>] used a domain randomization technique to generate source domains with different domain shifts, and unified multiple source domains to learn domain-invariant features by representation learning. Reference [<xref rid="B22-sensors-25-05363" ref-type="bibr">22</xref>] proposed the Hierarchical Transferability Calibration Network (HTCN) to improve transferability while maintaining discriminability, which extends existing adaptation strategies with image-to-image translation.</p><p>Comparatively, DAOD approaches based on one-stage detectors do not obtain enough attention. Reference [<xref rid="B25-sensors-25-05363" ref-type="bibr">25</xref>] enhanced the SSD framework with weakly supervised learning for cross-domain detection. Reference [<xref rid="B26-sensors-25-05363" ref-type="bibr">26</xref>] used the style transfer and pseudo-labeling approach for pixel-level adaptation and noise reduction. Reference [<xref rid="B27-sensors-25-05363" ref-type="bibr">27</xref>] proposed an Implicit Instance-Invariant Network (I<sup>3</sup>Net) to learn instance-invariant features by designing multiple strategies and modules. Specifically, domain-adaptive YOLO architectures are proposed for resource-limited and time-critical real applications. References [<xref rid="B29-sensors-25-05363" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05363" ref-type="bibr">30</xref>] extended YOLOv3 and YOLOv4 at different scales to learn domain-invariant features, respectively. Stepwise Domain Adaptive YOLO (S-DAYOLO) framework [<xref rid="B31-sensors-25-05363" ref-type="bibr">31</xref>] was proposed to bridge the domain gap by constructing an auxiliary domain for autonomous driving. Semi-supervised Domain Adaptive YOLO (SSDA-YOLO) [<xref rid="B32-sensors-25-05363" ref-type="bibr">32</xref>] combines the knowledge distillation framework and scene style transfer with the mean teacher model, and further proposes a consistency loss for distribution alignment based on the YOLOv5.</p><p>It is worth noting that with the development of DETR detectors, related cross-domain methods have also received widespread attention. Sequence Feature Alignment (SFA) designs a domain query-based feature alignment module and a token-wise feature alignment module for the adaptation of detection transformers [<xref rid="B39-sensors-25-05363" ref-type="bibr">39</xref>]. The Domain Adaptive Detection Transformer (DA-DETR) introduces information fusion for effective knowledge transfer [<xref rid="B40-sensors-25-05363" ref-type="bibr">40</xref>].</p><p>As mentioned earlier, there are numerous DAOD algorithms built on the outdated Faster RCNN. In this paper, we select the lightweight YOLO framework, which greatly improves inference speed while ensuring feature extraction capability. By comprehensively aligning the features in the backbone and head network, HMDA not only effectively enhances the model&#8217;s generalization ability but also avoids the degradation of target discriminability. It is shown that, combined with the proposed hierarchical multi-scale domain adaptation approach, very competitive results on various cross-domain detection scenarios can be obtained even by the small version of the YOLO framework.</p></sec></sec><sec id="sec3-sensors-25-05363"><title>3. Method</title><p>In this section, we introduce the technical details of the proposed HMDA. An overview of the HMDA based on YOLOv5 architecture is shown in <xref rid="sensors-25-05363-f001" ref-type="fig">Figure 1</xref>. HMDA-YOLO hierarchically extracts the deep features of the image and performs specific adaptations at different depths by embedding multiple domain discriminators, which are distributed in the backbone network and the head network of YOLOv5. More information is described below.</p><sec id="sec3dot1-sensors-25-05363"><title>3.1. Preliminary&#160;Knowledge</title><p>We will briefly introduce the YOLOv5 framework. YOLOv5 is a simple but effective one-stage detector and has been widely used in real-world applications. It mainly contains three parts, which is shown in the top part of <xref rid="sensors-25-05363-f001" ref-type="fig">Figure 1</xref>. (1) The backbone network is responsible for deep feature extraction. The CSPDarknet53 [<xref rid="B37-sensors-25-05363" ref-type="bibr">37</xref>] is the most commonly used backbone network, composed of convolutional module, C3 module, and the Spatial Pyramid Pooling (SPP) module [<xref rid="B41-sensors-25-05363" ref-type="bibr">41</xref>]; (2) The neck network further processes the extracted features and performs feature fusion to enhance the feature representative capability. It can achieve top-down and bottom-up feature fusion by the utilization of the Feature Pyramid Network (FPN) [<xref rid="B42-sensors-25-05363" ref-type="bibr">42</xref>] and Path Aggregation Network (PAN) [<xref rid="B43-sensors-25-05363" ref-type="bibr">43</xref>]; (3) The head network implements multi-scale detection of small, medium, and large objects.</p><p>In the cross-domain object detection tasks, there are two domains: a labeled source domain with <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> images, denoted as <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the source image, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the coordinate of bounding box, and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the object category. An unlabeled target domain with <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> images, denoted as <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. The label space of the source and target domains are identical, but their data distributions are different. The goal of domain adaptive object detection is to train a detector to reduce the domain shift and learn transferable representations that the model can generalize well to the target objects.</p></sec><sec id="sec3dot2-sensors-25-05363"><title>3.2. Hierarchical Multi-Scale&#160;Adaptation</title><p>The adversarial DAOD methods usually learn domain-invariant feature representations with the help of the domain discriminator (i.e., domain classifier). Assuming that the framework is composed of a feature extractor <italic toggle="yes">F</italic> and a domain discriminator <italic toggle="yes">G</italic>. The feature extractor <italic toggle="yes">F</italic> tries to confuse the domain discriminator <italic toggle="yes">G</italic>, which means maximizing the domain classification loss. Conversely, the domain discriminator <italic toggle="yes">D</italic> is trained to distinguish the source samples from the target samples, which means minimizing the domain classification loss. The feature extractor <italic toggle="yes">F</italic> also learns to minimize the source supervision loss. The adversarial loss can be written as follows:<disp-formula id="FD1-sensors-25-05363"><label>(1)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>F</mml:mi></mml:munder><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> denote the expected domain classification error over the source and target domain, respectively. By utilizing the Gradient Reverse Layer (GRL) [<xref rid="B11-sensors-25-05363" ref-type="bibr">11</xref>], the min&#8211;max adversarial training can be unified in one back-propagation.</p><p>For object detection tasks, each activation on the feature map corresponds to a patch of the input image, and detectors will perform classification and regression on each location. Therefore, the domain discriminator can implement not only image-level distribution alignment, but also pixel-level distribution alignment. Since HMDA-YOLO hierarchically aligns the feature distribution in the backbone network and the head network, we will focus on these two parts.</p><sec id="sec3dot2dot1-sensors-25-05363"><title>3.2.1. Hierarchical Backbone&#160;Adaptation</title><p>The backbone network hierarchically adapts the output features of each C3 module (i.e., the layer 4, 6, and 9 in YOLOv5 framework) through different ways, according to the representation information of the features at different depths.</p><p>The shallow feature can capture a lot of detailed information (e.g., edges, colors, textures, and angles), which not only facilitates the detection of small objects, but also helps the alignment of local features of cross-domain objects. According to [<xref rid="B44-sensors-25-05363" ref-type="bibr">44</xref>], the least-square loss function can stabilize the training process of the domain discriminator and have advantages on aligning shallow representations. Therefore, we use it as the criterion to implement shallow-level feature adaptation. The domain discriminator <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a fully convolutional network with <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> kernel to predict the pixel-level domain label of the source and target feature maps. The shallow-level adaptation loss can be formulated as follows:<disp-formula id="FD2-sensors-25-05363"><label>(2)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are the source and target shallow-level feature map of the input image <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. And <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the domain prediction in location <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the corresponding feature map.</p><p>The medium-level feature contains information about localized shapes and simple objects. And the Binary Cross Entropy (BCE) loss is used for adversarial loss calculation. The domain discriminator <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is different from <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> treats each feature map as a whole and predicts image-level domain label. Specifically, <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is composed of common convolutional layers, a global average pooling layer, and a fully connected layer. The middle-level adaptation loss can be formulated as follows:<disp-formula id="FD3-sensors-25-05363"><label>(3)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the medium-level feature map of the input image <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. And <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the image-level domain prediction of <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the ground truth domain label, which is 0 for the source domain and 1 for the target domain in this paper.</p><p>The scene layout, number of objects, patterns between objects, and the background may be quite different across domains. According to [<xref rid="B16-sensors-25-05363" ref-type="bibr">16</xref>], it is likely to hurt performance for larger shifts. As an example, the source domain that contains rural images and the target domain that contains urban images may have large domain discrepancy, even if they share the same object category. In this case, blind alignment may lead to the negative transfer and impair the model&#8217;s capability between different domains. HMDA-YOLO alleviates the above problem by adjusting the weights of hard-to-classify and easy-to-classify samples to more robustly improve the model&#8217;s cross-domain performance. Specifically, for the deep-level feature adaptation, the BCE loss is extended with focal loss [<xref rid="B45-sensors-25-05363" ref-type="bibr">45</xref>] to re-weight different samples. The domain discriminator <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is trained to distinguish the source from the target, which is similar to <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The deep-level adaptation loss can be written as follows:<disp-formula id="FD4-sensors-25-05363"><label>(4)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>&#947;</mml:mi></mml:msup><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mi>&#947;</mml:mi></mml:msup><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the deep-level feature map of the input image <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> is the parameter to control the weight of different samples. If a sample is easy-to-classify (i.e., far from the decision boundary), it is desired to have a low loss to avoid negative transfer. Conversely, if it is hard-to-classify, we want it to have a high loss for domain confusion. Therefore, the value of <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:math></inline-formula> needs to be greater than 1 to assign low weight for the easy-to-classify samples and high weight for the hard-to-classify samples.</p><p>Combining the hierarchical domain adaptation loss, HMDA-YOLO promotes comprehensive distribution alignment and suppresses the negative transfer. The overall adaptation loss function of the backbone network can be written as follows:<disp-formula id="FD5-sensors-25-05363"><label>(5)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot2dot2-sensors-25-05363"><title>3.2.2. Multi-Scale Head&#160;Adaptation</title><p>The feature maps of the head network play an important role in multi-scale object detection, which are crucial for object recognition and localization. Therefore, HMDA-YOLO proposes to implement fine-grained pixel-level adaptation at each scale for efficient cross-domain training, based on the rich discriminative information of the feature maps to be detected. On the one hand, multi-scale adaptation can reduce the local instance divergence and guarantee the model&#8217;s multi-scale detection capability. On the other hand, it helps to distinguish between the object and the background, which reduces the impact of background noise. In this way, the model can learn more robust and generalizable representations to improve the detection accuracy.</p><p>Specifically, the head adaptation is performed at three different scales, i.e., layer 17, 20, and 23 in YOLOv5 framework. The pixel-level domain discriminator <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is similar to <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which predicts pixel-level domain labels. And BCE loss function is used for loss calculation. The adaptation loss at each scale can be written as follows:<disp-formula id="FD6-sensors-25-05363"><label>(6)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the feature map to be detected at each scale. Note that <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in three different scales do not share the weight. Considering all three scales feature adaptation, the head adaptation loss can be unified as follows:<disp-formula id="FD7-sensors-25-05363"><label>(7)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">s</italic> denotes different scales.</p></sec></sec><sec id="sec3dot3-sensors-25-05363"><title>3.3. Overall&#160;Formulation</title><p>The main discriminative capability of the network is learned from the source labeled samples in supervised way. And the source supervised training loss of YOLOv5 can be formulated as follows:<disp-formula id="FD8-sensors-25-05363"><label>(8)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denote the set of <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the classification loss and defaults to the BCE loss. <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the object confidence loss and defaults to the BCE loss. <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the bounding box regression loss and defaults to the CIoU loss.</p><p>The domain adversarial loss combined with the hierarchical backbone adaptation strategy and the multi-scale head adaptation strategy can be written as follows:<disp-formula id="FD9-sensors-25-05363"><label>(9)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Combining the source supervised loss function with the cross-domain adversarial loss function, the overall optimization objective can be formulated:<disp-formula id="FD10-sensors-25-05363"><label>(10)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>D</mml:mi></mml:munder><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mi>G</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#955;</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">D</italic> denotes the set of all domain discriminators, including <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> is the trade-off parameter to balance the detection loss and the domain adversarial loss. The network can be trained in an end-to-end manner using a standard stochastic gradient descent algorithm. And the adversarial training can be achieved by the utilization of GRL, which reverses the gradient during propagation. Structures of these domain discriminators are summarized in <xref rid="sensors-25-05363-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec3dot4-sensors-25-05363"><title>3.4. Theoretical&#160;Analysis</title><p>Reference [<xref rid="B46-sensors-25-05363" ref-type="bibr">46</xref>] designed <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mo>&#916;</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-distance to measure the divergence between two sets of samples that have different data distributions. Let us consider a source domain <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, a target domain <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and a domain discriminator <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>:</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8614;</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which tries to predict the source and target domain label to be 0 and 1, respectively. Assuming that <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:math></inline-formula> to be a set of possible domain discriminators, the <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mo>&#916;</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-distance can be defined as follows:<disp-formula id="FD11-sensors-25-05363"><label>(11)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mo>&#916;</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:munder><mml:mo movablelimits="true" form="prefix">sup</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8764;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8800;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8764;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8800;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8764;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8764;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denote the expected domain classification errors over the source domain and the target domain, respectively. The combined error of the ideal hypothesis (e.g., domain discriminator) can be denoted as follows:<disp-formula id="FD12-sensors-25-05363"><label>(12)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>&#955;</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8727;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8727;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>where</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8727;</mml:mo></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo form="prefix">arg</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8727;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the ideal joint hypothesis. The terms <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are the expected risks on source and target domains, respectively. It can be used to measure the adaptability between different domains. If the ideal joint hypothesis performs poorly, i.e., the error <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> is large, the domain adaptation process is difficult to realize. Based on the above knowledge, Reference [<xref rid="B46-sensors-25-05363" ref-type="bibr">46</xref>] gives a upper bound on the target error as follows:<disp-formula id="FD13-sensors-25-05363"><label>(13)</label><mml:math id="mm73" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8704;</mml:mo><mml:mi>h</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">H</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8804;</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mo>&#916;</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
the target error is upper bounded by three terms, including the expected error on the source domain <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the domain divergence <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mo>&#916;</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and few constant terms. Since the <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> can be directly minimized by the network supervised learning and the third term is difficult to handle, with the majority of existing methods minimize the upper bound on the target error by reducing the domain divergence between source and target domains. Our proposed method not only hierarchically optimizes the distribution discrepancy between different domains at the backbone network, but also alignment the multi-scale features before the detection in the head network. Thus, it can effectively reduce the distribution discrepancy and significantly improve the cross-domain detection performance.</p></sec></sec><sec id="sec4-sensors-25-05363"><title>4. Experiment</title><p>In this section, we verify the proposed HMDA-YOLO on different cross-domain object detection tasks. Related datasets consist of Cityscapes, Foggy Cityscapes, Sim10k, KAIST, Pascal VOC, and Clipart. Quantitative results and related analyses demonstrate the superiority of the proposed HMDA-YOLO.</p><sec id="sec4dot1-sensors-25-05363"><title>4.1. Datasets</title><p>Cityscapes [<xref rid="B47-sensors-25-05363" ref-type="bibr">47</xref>] is a large-scale street scene dataset for driving scenarios. The images are captured by a car-mounted video camera from 50 different cities. It consists 2975 images in the training set and 500 images in the validation set.</p><p>Foggy Cityscapes [<xref rid="B48-sensors-25-05363" ref-type="bibr">48</xref>] dataset is rendered from Cityscapes by adding synthetic fog to real, clear-weather images using incomplete depth information. The data split and semantic annotations of Foggy Cityscapes are inherited from Cityscapes.</p><p>Sim10k [<xref rid="B49-sensors-25-05363" ref-type="bibr">49</xref>] dataset is generated by the game engine of Grand Theft Auto V (GTA V). It contains 10,000 images of synthetic driving scene with 58,701 bounding boxes of car for training.</p><p>KAIST [<xref rid="B50-sensors-25-05363" ref-type="bibr">50</xref>] dataset has both RGB and long-wavelength infrared (LWIR) images for multi-spectral pedestrian detection and multi-source fusion. Since successive frames of images are similar, we refined it with 7373 images for training and 1229 images for testing. All the three categories are integrated as <italic toggle="yes">person</italic>.</p><p>Pascal VOC [<xref rid="B51-sensors-25-05363" ref-type="bibr">51</xref>] is a realistic scenes dataset for segmentation and detection. The train-val datasets of Pascal VOC 2007 and 2012 are utilized together. And it consists of 16,551 images with 20 distinct object categories.</p><p>Clipart [<xref rid="B25-sensors-25-05363" ref-type="bibr">25</xref>], which is a graphical image dataset with complex backgrounds, has the same 20 object categories with Pascal VOC. Specifically, it has 500 images for training and 500 images for testing.</p><p>We evaluate our method in the following scenarios: adaptation across adverse weather (Cityscapes &#8594; Foggy Cityscapes), adaptation from synthetic to real (Sim10k &#8594; Cityscapes), adaptation across heterogeneous data (KAIST RGB &#8594; KAIST LWIR), and adaptation across large domain shift (Pascal VOC &#8594; Clipart). More details of the datasets can be found in <xref rid="sensors-25-05363-t001" ref-type="table">Table 1</xref>. C, F, S, KR, KL, P, and CL denote Cityscapes, Foggy Cityscapes, Sim10K, KAIST RGB, KAIST LWIR, Pascal VOC, and Clipart, respectively.</p></sec><sec id="sec4dot2-sensors-25-05363"><title>4.2. Experiment&#160;Setup</title><p>All experiments are implemented based on the PyTorch framework with a NVIDIA A800 GPU (TSMC, Hsinchu, Taiwan). And we choose YOLO framework with small parameters (YOLOv5s and YOLOv8s) as the base detector. All training and testing images are resized in the shape of <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>960</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>960</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The stochastic gradient descent (SGD) is utilized for optimizing the model, where the learning rate is 0.01, weight decay is 0.0005 and momentum is 0.937. The batch size is set to 32 with 200 training epochs during the training stage. Other settings can be referred to [<xref rid="B32-sensors-25-05363" ref-type="bibr">32</xref>]. For each cross-domain detection tasks, we report the average precisions (AP, %) and mean average precisions (mAP, %) with a threshold of 0.5. The tradeoff parameter <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> is set to 1.0 without specified.</p><p>We compare our proposed HMDA-YOLO with some state-of-the-art DAOD methods, including Faster RCNN [<xref rid="B2-sensors-25-05363" ref-type="bibr">2</xref>], YOLOv5s, YOLOv8s [<xref rid="B34-sensors-25-05363" ref-type="bibr">34</xref>], DAF [<xref rid="B15-sensors-25-05363" ref-type="bibr">15</xref>], SWDA [<xref rid="B16-sensors-25-05363" ref-type="bibr">16</xref>], HTCN [<xref rid="B22-sensors-25-05363" ref-type="bibr">22</xref>], ATF [<xref rid="B52-sensors-25-05363" ref-type="bibr">52</xref>], UMT [<xref rid="B33-sensors-25-05363" ref-type="bibr">33</xref>], TDD [<xref rid="B53-sensors-25-05363" ref-type="bibr">53</xref>], TIA [<xref rid="B54-sensors-25-05363" ref-type="bibr">54</xref>], Deformable-DETR [<xref rid="B55-sensors-25-05363" ref-type="bibr">55</xref>], SFA [<xref rid="B39-sensors-25-05363" ref-type="bibr">39</xref>], DA-DETR [<xref rid="B40-sensors-25-05363" ref-type="bibr">40</xref>], S-DAYOLO [<xref rid="B31-sensors-25-05363" ref-type="bibr">31</xref>] and ConfMix [<xref rid="B56-sensors-25-05363" ref-type="bibr">56</xref>]. Faster RCNN, D-DETR, YOLOv5s, and YOLOv8s means the model is only trained with source samples (source only) and it then inferences directly on the target domain samples. Bold in each table indicates the optimal result, except for YOLOv8s.</p></sec><sec id="sec4dot3-sensors-25-05363"><title>4.3. Quantitative&#160;Results</title><sec id="sec4dot3dot1-sensors-25-05363"><title>4.3.1. Adaptation Across Different&#160;Visibility</title><p>Differences in data distribution due to weather changes are very common in real-world applications, e.g., autonomous driving. The ability of the detection model to adapt to different weather is crucial. Therefore, we firstly evaluate our proposed HMDA-YOLO across adverse weather. Specifically, the Cityscapes dataset with clear weather and the Foggy Cityscapes with fog are used in this task. The quantitative results are shown in <xref rid="sensors-25-05363-t002" ref-type="table">Table 2</xref>. Methods based on different detectors are separated in each table. It can be seen that the proposed HMDA-YOLO outperforms all the compared methods and achieves 45.9% mAP. And it is 2.8% higher than the second best TDD. HMDA-YOLO has an absolute mAP improvement of 19.6% and a relative improvement of 74.5% over the baseline YOLOv5s. Through the hierarchical backbone adaptation and multi-scale head adaptation, the affection of adverse weather on the model&#8217;s discriminative ability can be significantly mitigated. Additionally, HMDA-YOLO is extremely efficient for knowledge transfer on multiple categories, including car, mbike, bus, and train.</p></sec><sec id="sec4dot3dot2-sensors-25-05363"><title>4.3.2. Adaptation from Synthetic to&#160;Real</title><p>With rapid development of game engine and simulation software, we have easy access to a large number of lifelike scene images. Therefore, we can try to improve the model&#8217;s generalization ability with the help of synthetic images, by building cross domain models from synthetic to real. In particular, we employ synthetic Sim10K and real Cityscapes to compose this cross-domain task, with one category car to be detected. Detection results are shown in <xref rid="sensors-25-05363-t003" ref-type="table">Table 3</xref>. Note that <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> is set to 0.1 to obtain better performance according existing studies [<xref rid="B16-sensors-25-05363" ref-type="bibr">16</xref>,<xref rid="B22-sensors-25-05363" ref-type="bibr">22</xref>]. Our method achieves the best accuracy of 58.6%. And it outperforms the baseline YOLOv5s, UMT, and DA-DETR by 7.0%, 15.5%, and 3.9%. Note that all methods based on Faster RCNN perform poorly in this task, even lower than the yolov5s without adaptation.</p></sec><sec id="sec4dot3dot3-sensors-25-05363"><title>4.3.3. Adaptation Across Heterogeneous&#160;Data</title><p>There are many heterogeneous sensors that can acquire multiple types of heterogeneous data, such as RGB images, infrared images, and multi-spectral images. And the scenario of heterogeneous data adaptation is often ignored in DAOD research. We validate whether the proposed HMDA-YOLO can deal with heterogeneous data adaptation. The RGB and long-wavelength infrared (LWIR) images in the KAIST dataset are used as the source and target domains, respectively. The results are reported in <xref rid="sensors-25-05363-t004" ref-type="table">Table 4</xref>. It can be seen that the YOLOv5s model trained with RGB images is completely unable to detect the person in LWIR images. And HMDA-YOLO can drastically improve the cross-domain performance of the model and achieves the accuracy of 45.4%, which demonstrates that the HMDA framework can efficiently implement the knowledge transfer even when the data are heterogeneous.</p></sec><sec id="sec4dot3dot4-sensors-25-05363"><title>4.3.4. Adaptation Across Large Domain&#160;Shift</title><p>Finally, we evaluate the proposed method on real-to-artistic adaptation datasets from Pascal VOC to Clipart. In this task, the source and target domains have large domain shift. The results of Pascal VOC to Clipart are shown in <xref rid="sensors-25-05363-t005" ref-type="table">Table 5</xref>. The proposed HMDA-YOLO do not obtain the best performance with the accuracy of 39.1%. However, it still achieves the performance close to the SOTA methods. It is worth noting that it has a huge improvement of 18.7% over the baseline YOLOv5s (91.7% relative improvement), which also proves its superiority.</p></sec><sec id="sec4dot3dot5-sensors-25-05363"><title>4.3.5. Flexibility on the YOLO&#160;Series</title><p>The bottom of <xref rid="sensors-25-05363-t002" ref-type="table">Table 2</xref>, <xref rid="sensors-25-05363-t003" ref-type="table">Table 3</xref>, <xref rid="sensors-25-05363-t004" ref-type="table">Table 4</xref> and <xref rid="sensors-25-05363-t005" ref-type="table">Table 5</xref> present the quantitative results of YOLOv8s and HMDA based on it. The shallow-level, middle-level, deep-level, and multi-scale feature maps of YOLOv8 are layers 4, 6, 9, 15, 18, and 21, respectively. It can be seen that YOLOv8s has a stronger feature learning ability compared with YOLOv5s, and its source only model generally performs better than YOLOv5s. Moreover, the proposed HMDA can be flexibly integrated into other YOLO frameworks. HMDA based on YOLOv8s can obtain the accuracy of 54.4%, 64.8%, 33.8%, and 42.8% on task C&#8594;F, S&#8594;C, KR&#8594;KL, and P&#8594;CL, respectively. And it achieves the best results in tasks C&#8594;F, S&#8594;C, and P&#8594;CL, which demonstrates the flexibility and effectiveness of the proposed HMDA.</p></sec></sec><sec id="sec4dot4-sensors-25-05363"><title>4.4. Analysis</title><sec id="sec4dot4dot1-sensors-25-05363"><title>4.4.1. Ablation&#160;Study</title><p>To evaluate the contribution of different adaptation strategies, we recorded the ablation results in <xref rid="sensors-25-05363-t006" ref-type="table">Table 6</xref>. There are 3 findings as follows: (1) Each component of our proposed HMDA-YOLO positively impacts the model&#8217;s cross-domain detection capability. (2) The hierarchical adaptation of the backbone network is more effective than the multi-scale adaptation of head network, especially the medium-level feature adaptation. (3) Combined with backbone and head adaptation strategy, our proposed HMDA-YOLO can achieve advanced performance. It demonstrate that HMDA-YOLO not only improves the generalization capability, but also guaranties the discriminative ability of the model.</p></sec><sec id="sec4dot4dot2-sensors-25-05363"><title>4.4.2. Detection&#160;Examples</title><p><xref rid="sensors-25-05363-f003" ref-type="fig">Figure 3</xref> presents some object detection results of the ground truth, YOLOv5, YOLOv5+HMDA, YOLOv8, and YOLOv8+HMDA on tasks Cityscapes &#8594; Foggy Cityscapes, KAIST RGB &#8594; KAIST LWIR, Sim10k &#8594; Cityscapes, and Pascal VOC &#8594; Clipart. The proposed HMDA shows the better cross-domain detection performance compared with YOLOv5s and YOLOv8s.</p></sec><sec id="sec4dot4dot3-sensors-25-05363"><title>4.4.3. Convergence</title><p>The model&#8217;s convergence in the training stage is important for DAOD algorithms. We analyzed the convergence of YOLOv5s and HMDA-YOLO on task Cityscapes &#8594; Foggy Cityscapes, by recording the training and validation error curves (composed of <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) in <xref rid="sensors-25-05363-f004" ref-type="fig">Figure 4</xref>. It can be seen that the convergence trend of the source supervision is basically the same for both methods, but HMDA-YOLO can converge better. As for the target validation, the proposed method is obviously superior, with more stable convergence and less loss. In contrast, YOLOv5s has a progressively higher validation error without transfer learning strategy. The above analyses demonstrate that HMDA can converge faster and better, and can guarantee the performance of model on both source and target domain samples.</p></sec><sec id="sec4dot4dot4-sensors-25-05363"><title>4.4.4. Influence of IOU&#160;Threshold</title><p><xref rid="sensors-25-05363-f005" ref-type="fig">Figure 5</xref> illustrates the performance of YOLOv5s and HMDA-YOLO on two different tasks as the IoU threshold changes from 0.5 to 0.95 (at intervals of 0.05). Generally, the overall detection accuracy is gradually decreasing as the IoU threshold increases. However, we have two insightful findings. The first one is that the YOLOv5-based method can localize the object accurately, which makes it possible to maintain good results when the threshold is increasing. The second point is that the proposed HMDA-YOLO greatly improves the cross-domain detection capability. And it still outperforms the baseline YOLOv5s with a IoU threshold of 0.5 even when the threshold of HMDA-YOLO is set to 0.95, which are separated by dash lines. The above findings demonstrate the effectiveness of the proposed HMDA-YOLO.</p></sec><sec id="sec4dot4dot5-sensors-25-05363"><title>4.4.5. Model&#160;Complexity</title><p>An important reason for choosing YOLO framework is its efficiency in real-time detection. Therefore, we analyzed the model complexity to show its advantage. The model&#8217;s parameters and inference time of different detectors are shown in <xref rid="sensors-25-05363-t007" ref-type="table">Table 7</xref>. Compared to other architectures, YOLOv5s and YOLOv8s are light-weight, less complex, and more efficient. HMDA+YOLOv5s has about 12.9M parameters because of multiple domain discriminators during the training stage. In inference time, there is no need to load the weight of domain discriminators and vanilla YOLO with adaptive weight is used. The YOLO series framework has huge advantages in terms of both model parameter quantity and inference speed.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05363"><title>5. Conclusions</title><p>In this paper, we propose a novel DAOD method, HMDA-YOLO, to address the problem of domain shift in cross-domain object detection tasks. Considering various factors, such as performance, efficiency, and engineering practice, we adopted YOLOv5 as the baseline instead of the outdated Faster RCNN. The core of HMDA-YOLO is the hierarchical backbone adaptation and multi-scale head adaptation. The backbone adaptation is hierarchically performed at different levels depending on the level of feature abstraction, which promotes efficient adaptation and suppresses negative transfer. The head adaptation is adopted in three scales to enhance the model&#8217;s cross-domain performance and not impair its discriminative ability. The experimental results of multiple cross-domain tasks demonstrate that the proposed HMDA-YOLO is characterized by excellent cross-domain detection performance and fast detection speed.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, S.Z.; methodology, S.Z.; validation, S.Z., P.Z., Y.W. and W.Q.; formal analysis, S.Z., P.Z. and W.Q.; investigation, S.Z. and W.Q.; resources, Y.W.; data curation, S.Z. and P.Z.; writing&#8212;original draft, S.Z.; writing&#8212;review and editing, S.Z.; visualization, S.Z.; supervision, Y.W.; project administration, Y.W.; funding acquisition, P.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data used in the experiment can be obtained at: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.cityscapes-dataset.com/">https://www.cityscapes-dataset.com/</uri>, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://host.robots.ox.ac.uk/pascal/VOC/">http://host.robots.ox.ac.uk/pascal/VOC/</uri>, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/naoto0804/cross-domain-detection/tree/master/datasets">https://github.com/naoto0804/cross-domain-detection/tree/master/datasets</uri>, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://soonminhwang.github.io/rgbt-ped-detection/">https://soonminhwang.github.io/rgbt-ped-detection/</uri>, and <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://fcav.engin.umich.edu/projects/driving-in-the-matrix">https://fcav.engin.umich.edu/projects/driving-in-the-matrix</uri> (accessed on 12 July 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05363"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="B2-sensors-25-05363"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Faster r-cnn: Towards real-time object detection with region proposal networks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>39</volume><fpage>1137</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id></element-citation></ref><ref id="B3-sensors-25-05363"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You only look once: Unified, real-time object detection</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>779</fpage><lpage>788</lpage></element-citation></ref><ref id="B4-sensors-25-05363"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style="western"><surname>Reed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name></person-group><article-title>Ssd: Single shot multibox detector</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2016: 14th European Conference</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#8211;14 October 2016</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2016</year><fpage>21</fpage><lpage>37</lpage></element-citation></ref><ref id="B5-sensors-25-05363"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>S.J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name></person-group><article-title>A survey on transfer learning</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2009</year><volume>22</volume><fpage>1345</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2009.191</pub-id></element-citation></ref><ref id="B6-sensors-25-05363"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yosinski</surname><given-names>J.</given-names></name><name name-style="western"><surname>Clune</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lipson</surname><given-names>H.</given-names></name></person-group><article-title>How transferable are features in deep neural networks?</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2014</year><volume>27</volume><fpage>3320</fpage><lpage>3328</lpage></element-citation></ref><ref id="B7-sensors-25-05363"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>W.</given-names></name></person-group><article-title>Deep visual domain adaptation: A survey</article-title><source>Neurocomputing</source><year>2018</year><volume>312</volume><fpage>135</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2018.05.083</pub-id></element-citation></ref><ref id="B8-sensors-25-05363"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jordan</surname><given-names>M.</given-names></name></person-group><article-title>Learning transferable features with deep adaptation networks</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Lille, France</conf-loc><conf-date>6&#8211;11 July 2015</conf-date><fpage>97</fpage><lpage>105</lpage></element-citation></ref><ref id="B9-sensors-25-05363"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jordan</surname><given-names>M.I.</given-names></name></person-group><article-title>Deep transfer learning with joint adaptation networks</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Sydney, Australia</conf-loc><conf-date>6&#8211;11 August 2017</conf-date><fpage>2208</fpage><lpage>2217</lpage></element-citation></ref><ref id="B10-sensors-25-05363"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ke</surname><given-names>G.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bian</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>Q.</given-names></name></person-group><article-title>Deep subdomain adaptation network for image classification</article-title><source>IEEE Trans. Neural Networks Learn. Syst.</source><year>2020</year><volume>32</volume><fpage>1713</fpage><lpage>1722</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2020.2988928</pub-id><pub-id pub-id-type="pmid">32365037</pub-id></element-citation></ref><ref id="B11-sensors-25-05363"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ganin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ustinova</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ajakan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Germain</surname><given-names>P.</given-names></name><name name-style="western"><surname>Larochelle</surname><given-names>H.</given-names></name><name name-style="western"><surname>Laviolette</surname><given-names>F.</given-names></name><name name-style="western"><surname>Marchand</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lempitsky</surname><given-names>V.</given-names></name></person-group><article-title>Domain-adversarial training of neural networks</article-title><source>J. Mach. Learn. Res.</source><year>2016</year><volume>17</volume><fpage>2030</fpage><lpage>2096</lpage></element-citation></ref><ref id="B12-sensors-25-05363"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jordan</surname><given-names>M.I.</given-names></name></person-group><article-title>Conditional adversarial domain adaptation</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2018</year><volume>31</volume><fpage>1647</fpage><lpage>1657</lpage></element-citation></ref><ref id="B13-sensors-25-05363"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>M.</given-names></name></person-group><article-title>Transfer learning with dynamic adversarial adaptation network</article-title><source>Proceedings of the 2019 IEEE International Conference on Data Mining (ICDM)</source><conf-loc>Beijing, China</conf-loc><conf-date>8&#8211;11 November 2019</conf-date><fpage>778</fpage><lpage>786</lpage></element-citation></ref><ref id="B14-sensors-25-05363"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Oza</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sindagi</surname><given-names>V.A.</given-names></name><name name-style="western"><surname>Sharmini</surname><given-names>V.V.</given-names></name><name name-style="western"><surname>Patel</surname><given-names>V.M.</given-names></name></person-group><article-title>Unsupervised Domain Adaptation of Object Detectors: A Survey</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>46</volume><fpage>4018</fpage><lpage>4040</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3217046</pub-id><pub-id pub-id-type="pmid">37030853</pub-id></element-citation></ref><ref id="B15-sensors-25-05363"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sakaridis</surname><given-names>C.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>D.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>Domain adaptive faster r-cnn for object detection in the wild</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>3339</fpage><lpage>3348</lpage></element-citation></ref><ref id="B16-sensors-25-05363"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Saito</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ushiku</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Harada</surname><given-names>T.</given-names></name><name name-style="western"><surname>Saenko</surname><given-names>K.</given-names></name></person-group><article-title>Strong-weak distribution alignment for adaptive object detection</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>6956</fpage><lpage>6965</lpage></element-citation></ref><ref id="B17-sensors-25-05363"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>D.</given-names></name></person-group><article-title>Adapting object detectors via selective cross-domain alignment</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>687</fpage><lpage>696</lpage></element-citation></ref><ref id="B18-sensors-25-05363"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>T.</given-names></name><name name-style="western"><surname>Jeong</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>C.</given-names></name></person-group><article-title>Diversify and match: A domain adaptive representation learning paradigm for object detection</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>12456</fpage><lpage>12465</lpage></element-citation></ref><ref id="B19-sensors-25-05363"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Cross-domain object detection through coarse-to-fine feature adaptation</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>13766</fpage><lpage>13775</lpage></element-citation></ref><ref id="B20-sensors-25-05363"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Multi-adversarial faster-rcnn for unrestricted object detection</article-title><source>Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>6668</fpage><lpage>6677</lpage></element-citation></ref><ref id="B21-sensors-25-05363"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>W.F.</given-names></name><name name-style="western"><surname>Chew</surname><given-names>C.M.</given-names></name></person-group><article-title>Pixel and feature level based domain adaptation for object detection in autonomous driving</article-title><source>Neurocomputing</source><year>2019</year><volume>367</volume><fpage>31</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2019.08.022</pub-id></element-citation></ref><ref id="B22-sensors-25-05363"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dou</surname><given-names>Q.</given-names></name></person-group><article-title>Harmonizing transferability and discriminability for adapting object detectors</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>8869</fpage><lpage>8878</lpage></element-citation></ref><ref id="B23-sensors-25-05363"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>C.D.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.R.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.S.</given-names></name></person-group><article-title>Exploring categorical regularization for domain adaptive object detection</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>11724</fpage><lpage>11733</lpage></element-citation></ref><ref id="B24-sensors-25-05363"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>T.</given-names></name></person-group><article-title>FCOS: A simple and strong anchor-free object detector</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>44</volume><fpage>1922</fpage><lpage>1933</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.3032166</pub-id><pub-id pub-id-type="pmid">33074804</pub-id></element-citation></ref><ref id="B25-sensors-25-05363"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Inoue</surname><given-names>N.</given-names></name><name name-style="western"><surname>Furuta</surname><given-names>R.</given-names></name><name name-style="western"><surname>Yamasaki</surname><given-names>T.</given-names></name><name name-style="western"><surname>Aizawa</surname><given-names>K.</given-names></name></person-group><article-title>Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>5001</fpage><lpage>5009</lpage></element-citation></ref><ref id="B26-sensors-25-05363"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rodriguez</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Mikolajczyk</surname><given-names>K.</given-names></name></person-group><article-title>Domain adaptation for object detection via style consistency</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1911.10033</pub-id><pub-id pub-id-type="arxiv">1911.10033</pub-id></element-citation></ref><ref id="B27-sensors-25-05363"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Y.</given-names></name></person-group><article-title>I3net: Implicit instance-invariant network for adapting one-stage object detectors</article-title><source>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>12576</fpage><lpage>12585</lpage></element-citation></ref><ref id="B28-sensors-25-05363"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style="western"><surname>Du</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name></person-group><article-title>Multi-Granularity Alignment Domain Adaptation for Object Detection</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>9581</fpage><lpage>9590</lpage></element-citation></ref><ref id="B29-sensors-25-05363"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hnewa</surname><given-names>M.</given-names></name><name name-style="western"><surname>Radha</surname><given-names>H.</given-names></name></person-group><article-title>Multiscale domain adaptive yolo for cross-domain object detection</article-title><source>Proceedings of the 2021 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>19&#8211;22 September 2021</conf-date><fpage>3323</fpage><lpage>3327</lpage></element-citation></ref><ref id="B30-sensors-25-05363"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tuo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jing</surname><given-names>Z.</given-names></name></person-group><article-title>Domain adaptive yolo for one-stage cross-domain detection</article-title><source>Proceedings of the Asian Conference on Machine Learning</source><conf-loc>Virtual</conf-loc><conf-date>17&#8211;19 November 2021</conf-date><fpage>785</fpage><lpage>797</lpage></element-citation></ref><ref id="B31-sensors-25-05363"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>R.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>D.</given-names></name></person-group><article-title>Cross-domain object detection for autonomous driving: A stepwise domain adaptative YOLO approach</article-title><source>IEEE Trans. Intell. Veh.</source><year>2022</year><volume>7</volume><fpage>603</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1109/TIV.2022.3165353</pub-id></element-citation></ref><ref id="B32-sensors-25-05363"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name></person-group><article-title>SSDA-YOLO: Semi-supervised domain adaptive YOLO for cross-domain object detection</article-title><source>Comput. Vis. Image Underst.</source><year>2023</year><volume>229</volume><fpage>103649</fpage><pub-id pub-id-type="doi">10.1016/j.cviu.2023.103649</pub-id></element-citation></ref><ref id="B33-sensors-25-05363"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>L.</given-names></name></person-group><article-title>Unbiased mean teacher for cross-domain object detection</article-title><source>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>4091</fpage><lpage>4101</lpage></element-citation></ref><ref id="B34-sensors-25-05363"><label>34.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Jocher</surname><given-names>G.</given-names></name><name name-style="western"><surname>Nishimura</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mineeva</surname><given-names>T.</given-names></name><name name-style="western"><surname>Vilari&#241;o</surname><given-names>R.</given-names></name></person-group><article-title>YOLOv5. GitHub Repository</article-title><year>2020</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ultralytics/yolov5" ext-link-type="uri">https://github.com/ultralytics/yolov5</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-12">(accessed on 12 July 2025)</date-in-citation></element-citation></ref><ref id="B35-sensors-25-05363"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gkioxari</surname><given-names>G.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><article-title>Mask r-cnn</article-title><source>Proceedings of the IEEE international Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2961</fpage><lpage>2969</lpage></element-citation></ref><ref id="B36-sensors-25-05363"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>Yolov3: An incremental improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id><pub-id pub-id-type="arxiv">1804.02767</pub-id></element-citation></ref><ref id="B37-sensors-25-05363"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.Y.M.</given-names></name></person-group><article-title>Yolov4: Optimal speed and accuracy of object detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2004.10934</pub-id><pub-id pub-id-type="arxiv">2004.10934</pub-id></element-citation></ref><ref id="B38-sensors-25-05363"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Carion</surname><given-names>N.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Synnaeve</surname><given-names>G.</given-names></name><name name-style="western"><surname>Usunier</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zagoruyko</surname><given-names>S.</given-names></name></person-group><article-title>End-to-end object detection with transformers</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Virtual</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><fpage>213</fpage><lpage>229</lpage></element-citation></ref><ref id="B39-sensors-25-05363"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>He</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zha</surname><given-names>Z.J.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>D.</given-names></name></person-group><article-title>Exploring sequence feature alignment for domain adaptive detection transformers</article-title><source>Proceedings of the 29th ACM International Conference on Multimedia</source><conf-loc>Chengdu, China</conf-loc><conf-date>20&#8211;24 October 2021</conf-date><fpage>1730</fpage><lpage>1738</lpage></element-citation></ref><ref id="B40-sensors-25-05363"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>S.</given-names></name></person-group><article-title>Da-detr: Domain adaptive detection transformer with information fusion</article-title><source>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>23787</fpage><lpage>23798</lpage></element-citation></ref><ref id="B41-sensors-25-05363"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Spatial pyramid pooling in deep convolutional networks for visual recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year><volume>37</volume><fpage>1904</fpage><lpage>1916</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2389824</pub-id><pub-id pub-id-type="pmid">26353135</pub-id></element-citation></ref><ref id="B42-sensors-25-05363"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name></person-group><article-title>Feature pyramid networks for object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>2117</fpage><lpage>2125</lpage></element-citation></ref><ref id="B43-sensors-25-05363"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Path aggregation network for instance segmentation</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>8759</fpage><lpage>8768</lpage></element-citation></ref><ref id="B44-sensors-25-05363"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lau</surname><given-names>R.Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Paul Smolley</surname><given-names>S.</given-names></name></person-group><article-title>Least squares generative adversarial networks</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2794</fpage><lpage>2802</lpage></element-citation></ref><ref id="B45-sensors-25-05363"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name><name name-style="western"><surname>Goyal</surname><given-names>P.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Doll&#225;r</surname><given-names>P.</given-names></name></person-group><article-title>Focal loss for dense object detection</article-title><source>Proceedings of the IEEE international Conference on Computer Vision (ICCV)</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>2980</fpage><lpage>2988</lpage></element-citation></ref><ref id="B46-sensors-25-05363"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ben-David</surname><given-names>S.</given-names></name><name name-style="western"><surname>Blitzer</surname><given-names>J.</given-names></name><name name-style="western"><surname>Crammer</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kulesza</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pereira</surname><given-names>F.</given-names></name><name name-style="western"><surname>Vaughan</surname><given-names>J.W.</given-names></name></person-group><article-title>A theory of learning from different domains</article-title><source>Mach. Learn.</source><year>2010</year><volume>79</volume><fpage>151</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1007/s10994-009-5152-4</pub-id></element-citation></ref><ref id="B47-sensors-25-05363"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cordts</surname><given-names>M.</given-names></name><name name-style="western"><surname>Omran</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rehfeld</surname><given-names>T.</given-names></name><name name-style="western"><surname>Enzweiler</surname><given-names>M.</given-names></name><name name-style="western"><surname>Benenson</surname><given-names>R.</given-names></name><name name-style="western"><surname>Franke</surname><given-names>U.</given-names></name><name name-style="western"><surname>Roth</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name></person-group><article-title>The Cityscapes Dataset for Semantic Urban Scene Understanding</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>3213</fpage><lpage>3223</lpage></element-citation></ref><ref id="B48-sensors-25-05363"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sakaridis</surname><given-names>C.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>D.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>Semantic foggy scene understanding with synthetic data</article-title><source>Int. J. Comput. Vis.</source><year>2018</year><volume>126</volume><fpage>973</fpage><lpage>992</lpage><pub-id pub-id-type="doi">10.1007/s11263-018-1072-8</pub-id></element-citation></ref><ref id="B49-sensors-25-05363"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Johnson-Roberson</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barto</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sridhar</surname><given-names>S.N.</given-names></name><name name-style="western"><surname>Rosaen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Vasudevan</surname><given-names>R.</given-names></name></person-group><article-title>Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks?</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1610.01983</pub-id></element-citation></ref><ref id="B50-sensors-25-05363"><label>50.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hwang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>N.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>So Kweon</surname><given-names>I.</given-names></name></person-group><article-title>Multispectral Pedestrian Detection: Benchmark Dataset and Baseline</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>1037</fpage><lpage>1045</lpage></element-citation></ref><ref id="B51-sensors-25-05363"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Everingham</surname><given-names>M.</given-names></name><name name-style="western"><surname>Eslami</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name><name name-style="western"><surname>Williams</surname><given-names>C.K.</given-names></name><name name-style="western"><surname>Winn</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>The pascal visual object classes challenge: A retrospective</article-title><source>Int. J. Comput. Vis.</source><year>2015</year><volume>111</volume><fpage>98</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1007/s11263-014-0733-5</pub-id></element-citation></ref><ref id="B52-sensors-25-05363"><label>52.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Domain adaptive object detection via asymmetric tri-way faster-rcnn</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>309</fpage><lpage>324</lpage></element-citation></ref><ref id="B53-sensors-25-05363"><label>53.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name></person-group><article-title>Cross domain object detection by target-perceived dual branch distillation</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>9570</fpage><lpage>9580</lpage></element-citation></ref><ref id="B54-sensors-25-05363"><label>54.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Task-specific Inconsistency Alignment for Domain Adaptive Object Detection</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>14217</fpage><lpage>14226</lpage></element-citation></ref><ref id="B55-sensors-25-05363"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Su</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name></person-group><article-title>Deformable detr: Deformable transformers for end-to-end object detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2010.04159</pub-id></element-citation></ref><ref id="B56-sensors-25-05363"><label>56.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mattolin</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zanella</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ricci</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>Confmix: Unsupervised domain adaptation for object detection via confidence-based mixing</article-title><source>Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>2&#8211;7 January 2023</conf-date><fpage>423</fpage><lpage>433</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05363-f001" orientation="portrait"><label>Figure 1</label><caption><p>The overall structure of HMDA-YOLO. It mainly consists of two main parts, the backbone adaptation and the head adaptation. The hierarchical adaptation of the feature distribution in the backbone part of YOLOv5 framework alleviates the affection of negative transfer and makes the adaptation more comprehensive. The multi-scale head adaptation significantly reduces local instance discrepancy and the impact of background noise. In this way, HMDA enhances the model&#8217;s cross-domain performance and not impairs its discriminative ability, which enables real-time and accurate detection across domains.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05363-g001.jpg"/></fig><fig position="float" id="sensors-25-05363-f002" orientation="portrait"><label>Figure 2</label><caption><p>The structure of two types of domain discriminator. Top: pixel-level domain discriminator. Bottom: image-level domain discriminator. GAP means global average pooling layer. The information in the convolutional layer indicates the size of the convolutional kernels, the number of output channels, and whether or not to downsample, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05363-g002.jpg"/></fig><fig position="float" id="sensors-25-05363-f003" orientation="portrait"><label>Figure 3</label><caption><p>Illustration of the detection results on the target domain. From left to right are results of C&#8594;F, S&#8594;C, KR&#8594;KL, and P&#8594;Cl, respectively. (<bold>a</bold>&#8211;<bold>e</bold>) denote the groud truth, YOLOv5, YOLOv5+HMDA, YOLOv8, and YOLOv8+HMDA (Zoom in for better view).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05363-g003.jpg"/></fig><fig position="float" id="sensors-25-05363-f004" orientation="portrait"><label>Figure 4</label><caption><p>Training and validation error curves of YOLOv5s and HMDA-YOLO on task Cityscapes &#8594; Foggy Cityscapes.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05363-g004.jpg"/></fig><fig position="float" id="sensors-25-05363-f005" orientation="portrait"><label>Figure 5</label><caption><p>The performance with the variation in IOU thresholds on two tasks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05363-g005.jpg"/></fig><table-wrap position="float" id="sensors-25-05363-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05363-t001_Table 1</object-id><label>Table 1</label><caption><p>The number of images in different domains.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Tasks</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Training Set</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Validation Set</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Classes</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Source Domain</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Target Domain</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Target Domain</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">C&#8594;F</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cityscapes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Foggy Cityscapes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Foggy Cityscapes</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2975</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2975</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">S&#8594;C</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sim10K</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cityscapes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cityscapes</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10,000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2975</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">KR&#8594;KL</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KAIST RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KAIST LWIR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KAIST LWIR</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7601</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7601</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2252</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">P&#8594;Cl</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pascal VOC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Clipart</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Clipart</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">20</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16,551</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">500</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05363-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05363-t002_Table 2</object-id><label>Table 2</label><caption><p>Detection results (%) across different visibility (C&#8594;F).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bicycle</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Person</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Rider</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Mbike</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bus</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Truck</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Train</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster RCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">39.9</td><td align="center" valign="middle" rowspan="1" colspan="1">28.3</td><td align="center" valign="middle" rowspan="1" colspan="1">28.5</td><td align="center" valign="middle" rowspan="1" colspan="1">34.2</td><td align="center" valign="middle" rowspan="1" colspan="1">23.4</td><td align="center" valign="middle" rowspan="1" colspan="1">26.3</td><td align="center" valign="middle" rowspan="1" colspan="1">14.7</td><td align="center" valign="middle" rowspan="1" colspan="1">11.4</td><td align="center" valign="middle" rowspan="1" colspan="1">25.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DAF</td><td align="center" valign="middle" rowspan="1" colspan="1">40.5</td><td align="center" valign="middle" rowspan="1" colspan="1">27.1</td><td align="center" valign="middle" rowspan="1" colspan="1">25.0</td><td align="center" valign="middle" rowspan="1" colspan="1">31.0</td><td align="center" valign="middle" rowspan="1" colspan="1">20.0</td><td align="center" valign="middle" rowspan="1" colspan="1">35.3</td><td align="center" valign="middle" rowspan="1" colspan="1">22.1</td><td align="center" valign="middle" rowspan="1" colspan="1">20.4</td><td align="center" valign="middle" rowspan="1" colspan="1">27.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SWDA</td><td align="center" valign="middle" rowspan="1" colspan="1">43.5</td><td align="center" valign="middle" rowspan="1" colspan="1">35.3</td><td align="center" valign="middle" rowspan="1" colspan="1">29.9</td><td align="center" valign="middle" rowspan="1" colspan="1">42.3</td><td align="center" valign="middle" rowspan="1" colspan="1">30.0</td><td align="center" valign="middle" rowspan="1" colspan="1">36.2</td><td align="center" valign="middle" rowspan="1" colspan="1">24.5</td><td align="center" valign="middle" rowspan="1" colspan="1">32.6</td><td align="center" valign="middle" rowspan="1" colspan="1">34.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ATF</td><td align="center" valign="middle" rowspan="1" colspan="1">50.0</td><td align="center" valign="middle" rowspan="1" colspan="1">38.8</td><td align="center" valign="middle" rowspan="1" colspan="1">34.6</td><td align="center" valign="middle" rowspan="1" colspan="1">47.0</td><td align="center" valign="middle" rowspan="1" colspan="1">33.4</td><td align="center" valign="middle" rowspan="1" colspan="1">43.3</td><td align="center" valign="middle" rowspan="1" colspan="1">23.7</td><td align="center" valign="middle" rowspan="1" colspan="1">38.7</td><td align="center" valign="middle" rowspan="1" colspan="1">38.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HTCN</td><td align="center" valign="middle" rowspan="1" colspan="1">47.9</td><td align="center" valign="middle" rowspan="1" colspan="1">37.1</td><td align="center" valign="middle" rowspan="1" colspan="1">33.2</td><td align="center" valign="middle" rowspan="1" colspan="1">47.5</td><td align="center" valign="middle" rowspan="1" colspan="1">32.3</td><td align="center" valign="middle" rowspan="1" colspan="1">47.4</td><td align="center" valign="middle" rowspan="1" colspan="1">31.6</td><td align="center" valign="middle" rowspan="1" colspan="1">40.9</td><td align="center" valign="middle" rowspan="1" colspan="1">39.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UMT</td><td align="center" valign="middle" rowspan="1" colspan="1">48.6</td><td align="center" valign="middle" rowspan="1" colspan="1">37.4</td><td align="center" valign="middle" rowspan="1" colspan="1">33.0</td><td align="center" valign="middle" rowspan="1" colspan="1">46.7</td><td align="center" valign="middle" rowspan="1" colspan="1">30.4</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>56.5</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>34.1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">46.8</td><td align="center" valign="middle" rowspan="1" colspan="1">41.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TIA</td><td align="center" valign="middle" rowspan="1" colspan="1">49.7</td><td align="center" valign="middle" rowspan="1" colspan="1">38.1</td><td align="center" valign="middle" rowspan="1" colspan="1">34.8</td><td align="center" valign="middle" rowspan="1" colspan="1">46.3</td><td align="center" valign="middle" rowspan="1" colspan="1">37.7</td><td align="center" valign="middle" rowspan="1" colspan="1">52.1</td><td align="center" valign="middle" rowspan="1" colspan="1">31.1</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>48.6</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">42.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TDD</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D-DETR</td><td align="center" valign="middle" rowspan="1" colspan="1">44.2</td><td align="center" valign="middle" rowspan="1" colspan="1">35.5</td><td align="center" valign="middle" rowspan="1" colspan="1">37.7</td><td align="center" valign="middle" rowspan="1" colspan="1">39.1</td><td align="center" valign="middle" rowspan="1" colspan="1">21.6</td><td align="center" valign="middle" rowspan="1" colspan="1">26.8</td><td align="center" valign="middle" rowspan="1" colspan="1">17.2</td><td align="center" valign="middle" rowspan="1" colspan="1">5.8</td><td align="center" valign="middle" rowspan="1" colspan="1">28.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SFA</td><td align="center" valign="middle" rowspan="1" colspan="1">62.6</td><td align="center" valign="middle" rowspan="1" colspan="1">44.0</td><td align="center" valign="middle" rowspan="1" colspan="1">46.5</td><td align="center" valign="middle" rowspan="1" colspan="1">48.6</td><td align="center" valign="middle" rowspan="1" colspan="1">28.3</td><td align="center" valign="middle" rowspan="1" colspan="1">46.2</td><td align="center" valign="middle" rowspan="1" colspan="1">25.1</td><td align="center" valign="middle" rowspan="1" colspan="1">29.4</td><td align="center" valign="middle" rowspan="1" colspan="1">41.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DA-DETR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>46.3</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s</td><td align="center" valign="middle" rowspan="1" colspan="1">46.9</td><td align="center" valign="middle" rowspan="1" colspan="1">29.7</td><td align="center" valign="middle" rowspan="1" colspan="1">35.8</td><td align="center" valign="middle" rowspan="1" colspan="1">37.9</td><td align="center" valign="middle" rowspan="1" colspan="1">15.4</td><td align="center" valign="middle" rowspan="1" colspan="1">26.7</td><td align="center" valign="middle" rowspan="1" colspan="1">11.5</td><td align="center" valign="middle" rowspan="1" colspan="1">6.6</td><td align="center" valign="middle" rowspan="1" colspan="1">26.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">S-DAYOLO</td><td align="center" valign="middle" rowspan="1" colspan="1">61.9</td><td align="center" valign="middle" rowspan="1" colspan="1">37.3</td><td align="center" valign="middle" rowspan="1" colspan="1">42.6</td><td align="center" valign="middle" rowspan="1" colspan="1">42.1</td><td align="center" valign="middle" rowspan="1" colspan="1">24.4</td><td align="center" valign="middle" rowspan="1" colspan="1">40.5</td><td align="center" valign="middle" rowspan="1" colspan="1">23.5</td><td align="center" valign="middle" rowspan="1" colspan="1">39.5</td><td align="center" valign="middle" rowspan="1" colspan="1">39.0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Confmix</td><td align="center" valign="middle" rowspan="1" colspan="1">62.6</td><td align="center" valign="middle" rowspan="1" colspan="1">33.5</td><td align="center" valign="middle" rowspan="1" colspan="1">45.0</td><td align="center" valign="middle" rowspan="1" colspan="1">43.4</td><td align="center" valign="middle" rowspan="1" colspan="1">28.6</td><td align="center" valign="middle" rowspan="1" colspan="1">45.8</td><td align="center" valign="middle" rowspan="1" colspan="1">27.3</td><td align="center" valign="middle" rowspan="1" colspan="1">40.0</td><td align="center" valign="middle" rowspan="1" colspan="1">40.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>66.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>49.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>51.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>38.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>45.9</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" rowspan="1" colspan="1">54.8</td><td align="center" valign="middle" rowspan="1" colspan="1">43.3</td><td align="center" valign="middle" rowspan="1" colspan="1">45.0</td><td align="center" valign="middle" rowspan="1" colspan="1">51.0</td><td align="center" valign="middle" rowspan="1" colspan="1">22.0</td><td align="center" valign="middle" rowspan="1" colspan="1">38.1</td><td align="center" valign="middle" rowspan="1" colspan="1">16.6</td><td align="center" valign="middle" rowspan="1" colspan="1">3.2</td><td align="center" valign="middle" rowspan="1" colspan="1">34.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.4</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05363-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05363-t003_Table 3</object-id><label>Table 3</label><caption><p>Detection results (%) from synthetic to real (S&#8594;C).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster RCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">34.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DAF</td><td align="center" valign="middle" rowspan="1" colspan="1">38.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SWDA</td><td align="center" valign="middle" rowspan="1" colspan="1">40.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HTCN</td><td align="center" valign="middle" rowspan="1" colspan="1">42.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UMT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D-DETR</td><td align="center" valign="middle" rowspan="1" colspan="1">47.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SFA</td><td align="center" valign="middle" rowspan="1" colspan="1">52.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DA-DETR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s</td><td align="center" valign="middle" rowspan="1" colspan="1">51.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Confmix</td><td align="center" valign="middle" rowspan="1" colspan="1">56.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>58.6</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" rowspan="1" colspan="1">58.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05363-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05363-t004_Table 4</object-id><label>Table 4</label><caption><p>Detection results (%) across heterogeneous data (KR&#8594;KL).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster RCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">9.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DAF</td><td align="center" valign="middle" rowspan="1" colspan="1">21.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SWDA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s</td><td align="center" valign="middle" rowspan="1" colspan="1">5.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>45.4</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" rowspan="1" colspan="1">10.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05363-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05363-t005_Table 5</object-id><label>Table 5</label><caption><p>Detection results (%) across large domain shift (P&#8594;Cl).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Aero</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bike</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bird</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Boat</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bottle</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bus</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cat</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Chair</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cow</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster RCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">35.6</td><td align="center" valign="middle" rowspan="1" colspan="1">52.5</td><td align="center" valign="middle" rowspan="1" colspan="1">24.3</td><td align="center" valign="middle" rowspan="1" colspan="1">23.0</td><td align="center" valign="middle" rowspan="1" colspan="1">20.0</td><td align="center" valign="middle" rowspan="1" colspan="1">43.9</td><td align="center" valign="middle" rowspan="1" colspan="1">32.8</td><td align="center" valign="middle" rowspan="1" colspan="1">10.7</td><td align="center" valign="middle" rowspan="1" colspan="1">30.6</td><td align="center" valign="middle" rowspan="1" colspan="1">11.7</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DAF</td><td align="center" valign="middle" rowspan="1" colspan="1">15.0</td><td align="center" valign="middle" rowspan="1" colspan="1">34.6</td><td align="center" valign="middle" rowspan="1" colspan="1">12.4</td><td align="center" valign="middle" rowspan="1" colspan="1">11.9</td><td align="center" valign="middle" rowspan="1" colspan="1">19.8</td><td align="center" valign="middle" rowspan="1" colspan="1">21.1</td><td align="center" valign="middle" rowspan="1" colspan="1">23.2</td><td align="center" valign="middle" rowspan="1" colspan="1">3.1</td><td align="center" valign="middle" rowspan="1" colspan="1">22.1</td><td align="center" valign="middle" rowspan="1" colspan="1">26.3</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SWDA</td><td align="center" valign="middle" rowspan="1" colspan="1">26.2</td><td align="center" valign="middle" rowspan="1" colspan="1">48.5</td><td align="center" valign="middle" rowspan="1" colspan="1">32.6</td><td align="center" valign="middle" rowspan="1" colspan="1">33.7</td><td align="center" valign="middle" rowspan="1" colspan="1">38.5</td><td align="center" valign="middle" rowspan="1" colspan="1">54.3</td><td align="center" valign="middle" rowspan="1" colspan="1">37.1</td><td align="center" valign="middle" rowspan="1" colspan="1">18.6</td><td align="center" valign="middle" rowspan="1" colspan="1">34.8</td><td align="center" valign="middle" rowspan="1" colspan="1">58.3</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HTCN</td><td align="center" valign="middle" rowspan="1" colspan="1">33.6</td><td align="center" valign="middle" rowspan="1" colspan="1">58.9</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>34.0</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">23.4</td><td align="center" valign="middle" rowspan="1" colspan="1">45.6</td><td align="center" valign="middle" rowspan="1" colspan="1">57.0</td><td align="center" valign="middle" rowspan="1" colspan="1">39.8</td><td align="center" valign="middle" rowspan="1" colspan="1">12.0</td><td align="center" valign="middle" rowspan="1" colspan="1">39.7</td><td align="center" valign="middle" rowspan="1" colspan="1">51.3</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ATF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>67.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>36.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D-DETR</td><td align="center" valign="middle" rowspan="1" colspan="1">24.8</td><td align="center" valign="middle" rowspan="1" colspan="1">50.5</td><td align="center" valign="middle" rowspan="1" colspan="1">14.0</td><td align="center" valign="middle" rowspan="1" colspan="1">22.8</td><td align="center" valign="middle" rowspan="1" colspan="1">11.5</td><td align="center" valign="middle" rowspan="1" colspan="1">50.7</td><td align="center" valign="middle" rowspan="1" colspan="1">28.7</td><td align="center" valign="middle" rowspan="1" colspan="1">3.0</td><td align="center" valign="middle" rowspan="1" colspan="1">26.5</td><td align="center" valign="middle" rowspan="1" colspan="1">32.6</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SFA</td><td align="center" valign="middle" rowspan="1" colspan="1">35.2</td><td align="center" valign="middle" rowspan="1" colspan="1">47.6</td><td align="center" valign="middle" rowspan="1" colspan="1">33.5</td><td align="center" valign="middle" rowspan="1" colspan="1">38.3</td><td align="center" valign="middle" rowspan="1" colspan="1">39.6</td><td align="center" valign="middle" rowspan="1" colspan="1">40.4</td><td align="center" valign="middle" rowspan="1" colspan="1">38.5</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>27.2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">37.6</td><td align="center" valign="middle" rowspan="1" colspan="1">43.1</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DA-DETR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>43.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s</td><td align="center" valign="middle" rowspan="1" colspan="1">12.8</td><td align="center" valign="middle" rowspan="1" colspan="1">39.7</td><td align="center" valign="middle" rowspan="1" colspan="1">8.2</td><td align="center" valign="middle" rowspan="1" colspan="1">11.5</td><td align="center" valign="middle" rowspan="1" colspan="1">34.0</td><td align="center" valign="middle" rowspan="1" colspan="1">33.4</td><td align="center" valign="middle" rowspan="1" colspan="1">19.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">34.4</td><td align="center" valign="middle" rowspan="1" colspan="1">6.3</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>50.3</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>70.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>44.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>45.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" rowspan="1" colspan="1">22.4</td><td align="center" valign="middle" rowspan="1" colspan="1">60.1</td><td align="center" valign="middle" rowspan="1" colspan="1">27.5</td><td align="center" valign="middle" rowspan="1" colspan="1">33.2</td><td align="center" valign="middle" rowspan="1" colspan="1">44.0</td><td align="center" valign="middle" rowspan="1" colspan="1">37.9</td><td align="center" valign="middle" rowspan="1" colspan="1">23.5</td><td align="center" valign="middle" rowspan="1" colspan="1">10.1</td><td align="center" valign="middle" rowspan="1" colspan="1">57.2</td><td align="center" valign="middle" rowspan="1" colspan="1">7.8</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Method</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Table</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Dog</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Hrs</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Mbike</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Prsn</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Plnt</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Sheep</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Sofa</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Train</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>TV</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>mAP</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Faster RCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">13.8</td><td align="center" valign="middle" rowspan="1" colspan="1">6.0</td><td align="center" valign="middle" rowspan="1" colspan="1">36.8</td><td align="center" valign="middle" rowspan="1" colspan="1">45.9</td><td align="center" valign="middle" rowspan="1" colspan="1">48.7</td><td align="center" valign="middle" rowspan="1" colspan="1">41.9</td><td align="center" valign="middle" rowspan="1" colspan="1">16.5</td><td align="center" valign="middle" rowspan="1" colspan="1">7.3</td><td align="center" valign="middle" rowspan="1" colspan="1">22.9</td><td align="center" valign="middle" rowspan="1" colspan="1">32.0</td><td align="center" valign="middle" rowspan="1" colspan="1">27.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DAF</td><td align="center" valign="middle" rowspan="1" colspan="1">10.6</td><td align="center" valign="middle" rowspan="1" colspan="1">10.0</td><td align="center" valign="middle" rowspan="1" colspan="1">19.6</td><td align="center" valign="middle" rowspan="1" colspan="1">39.4</td><td align="center" valign="middle" rowspan="1" colspan="1">34.6</td><td align="center" valign="middle" rowspan="1" colspan="1">29.3</td><td align="center" valign="middle" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" rowspan="1" colspan="1">17.1</td><td align="center" valign="middle" rowspan="1" colspan="1">19.7</td><td align="center" valign="middle" rowspan="1" colspan="1">24.8</td><td align="center" valign="middle" rowspan="1" colspan="1">19.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SWDA</td><td align="center" valign="middle" rowspan="1" colspan="1">12.5</td><td align="center" valign="middle" rowspan="1" colspan="1">12.5</td><td align="center" valign="middle" rowspan="1" colspan="1">33.8</td><td align="center" valign="middle" rowspan="1" colspan="1">65.5</td><td align="center" valign="middle" rowspan="1" colspan="1">54.5</td><td align="center" valign="middle" rowspan="1" colspan="1">52.0</td><td align="center" valign="middle" rowspan="1" colspan="1">9.3</td><td align="center" valign="middle" rowspan="1" colspan="1">24.9</td><td align="center" valign="middle" rowspan="1" colspan="1">54.1</td><td align="center" valign="middle" rowspan="1" colspan="1">49.1</td><td align="center" valign="middle" rowspan="1" colspan="1">38.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HTCN</td><td align="center" valign="middle" rowspan="1" colspan="1">20.1</td><td align="center" valign="middle" rowspan="1" colspan="1">20.1</td><td align="center" valign="middle" rowspan="1" colspan="1">39.1</td><td align="center" valign="middle" rowspan="1" colspan="1">72.8</td><td align="center" valign="middle" rowspan="1" colspan="1">61.3</td><td align="center" valign="middle" rowspan="1" colspan="1">43.1</td><td align="center" valign="middle" rowspan="1" colspan="1">19.3</td><td align="center" valign="middle" rowspan="1" colspan="1">30.1</td><td align="center" valign="middle" rowspan="1" colspan="1">50.2</td><td align="center" valign="middle" rowspan="1" colspan="1">51.8</td><td align="center" valign="middle" rowspan="1" colspan="1">40.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ATF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>41.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>42.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>42.1</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D-DETR</td><td align="center" valign="middle" rowspan="1" colspan="1">22.1</td><td align="center" valign="middle" rowspan="1" colspan="1">17.4</td><td align="center" valign="middle" rowspan="1" colspan="1">19.6</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>73.1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">54.2</td><td align="center" valign="middle" rowspan="1" colspan="1">20.8</td><td align="center" valign="middle" rowspan="1" colspan="1">11.5</td><td align="center" valign="middle" rowspan="1" colspan="1">12.6</td><td align="center" valign="middle" rowspan="1" colspan="1">55.2</td><td align="center" valign="middle" rowspan="1" colspan="1">30.3</td><td align="center" valign="middle" rowspan="1" colspan="1">29.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SFA</td><td align="center" valign="middle" rowspan="1" colspan="1">23.9</td><td align="center" valign="middle" rowspan="1" colspan="1">31.6</td><td align="center" valign="middle" rowspan="1" colspan="1">32.5</td><td align="center" valign="middle" rowspan="1" colspan="1">72.5</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>66.8</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">43.0</td><td align="center" valign="middle" rowspan="1" colspan="1">18.5</td><td align="center" valign="middle" rowspan="1" colspan="1">29.0</td><td align="center" valign="middle" rowspan="1" colspan="1">53.0</td><td align="center" valign="middle" rowspan="1" colspan="1">44.9</td><td align="center" valign="middle" rowspan="1" colspan="1">39.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DA-DETR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>35.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>27.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.3</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s</td><td align="center" valign="middle" rowspan="1" colspan="1">8.0</td><td align="center" valign="middle" rowspan="1" colspan="1">10.1</td><td align="center" valign="middle" rowspan="1" colspan="1">17.0</td><td align="center" valign="middle" rowspan="1" colspan="1">8.9</td><td align="center" valign="middle" rowspan="1" colspan="1">28.6</td><td align="center" valign="middle" rowspan="1" colspan="1">45.7</td><td align="center" valign="middle" rowspan="1" colspan="1">2.5</td><td align="center" valign="middle" rowspan="1" colspan="1">20.4</td><td align="center" valign="middle" rowspan="1" colspan="1">24.3</td><td align="center" valign="middle" rowspan="1" colspan="1">42.5</td><td align="center" valign="middle" rowspan="1" colspan="1">20.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>55.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>36.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>61.3</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
YOLOv8s
</td><td align="center" valign="middle" rowspan="1" colspan="1">28.5</td><td align="center" valign="middle" rowspan="1" colspan="1">14.8</td><td align="center" valign="middle" rowspan="1" colspan="1">39.7</td><td align="center" valign="middle" rowspan="1" colspan="1">44.8</td><td align="center" valign="middle" rowspan="1" colspan="1">44.6</td><td align="center" valign="middle" rowspan="1" colspan="1">58.7</td><td align="center" valign="middle" rowspan="1" colspan="1">15.8</td><td align="center" valign="middle" rowspan="1" colspan="1">19.8</td><td align="center" valign="middle" rowspan="1" colspan="1">29.1</td><td align="center" valign="middle" rowspan="1" colspan="1">54.8</td><td align="center" valign="middle" rowspan="1" colspan="1">33.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>HMDA</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05363-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05363-t006_Table 6</object-id><label>Table 6</label><caption><p>Ablation study (%) of HMDA-YOLO based on task C&#8594;F.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Car</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bicycle</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Person</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Rider</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Mbike</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bus</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Truck</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Train</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">mAP</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv5s</td><td align="center" valign="middle" rowspan="1" colspan="1">46.9</td><td align="center" valign="middle" rowspan="1" colspan="1">29.7</td><td align="center" valign="middle" rowspan="1" colspan="1">35.8</td><td align="center" valign="middle" rowspan="1" colspan="1">37.9</td><td align="center" valign="middle" rowspan="1" colspan="1">15.4</td><td align="center" valign="middle" rowspan="1" colspan="1">26.7</td><td align="center" valign="middle" rowspan="1" colspan="1">11.5</td><td align="center" valign="middle" rowspan="1" colspan="1">6.6</td><td align="center" valign="middle" rowspan="1" colspan="1">26.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Base + <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">52.0</td><td align="center" valign="middle" rowspan="1" colspan="1">38.7</td><td align="center" valign="middle" rowspan="1" colspan="1">42.2</td><td align="center" valign="middle" rowspan="1" colspan="1">47.4</td><td align="center" valign="middle" rowspan="1" colspan="1">29.3</td><td align="center" valign="middle" rowspan="1" colspan="1">37.6</td><td align="center" valign="middle" rowspan="1" colspan="1">21.2</td><td align="center" valign="middle" rowspan="1" colspan="1">15.6</td><td align="center" valign="middle" rowspan="1" colspan="1">35.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Base + <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">60.3</td><td align="center" valign="middle" rowspan="1" colspan="1">39.5</td><td align="center" valign="middle" rowspan="1" colspan="1">45.0</td><td align="center" valign="middle" rowspan="1" colspan="1">48.5</td><td align="center" valign="middle" rowspan="1" colspan="1">26.0</td><td align="center" valign="middle" rowspan="1" colspan="1">39.3</td><td align="center" valign="middle" rowspan="1" colspan="1">23.9</td><td align="center" valign="middle" rowspan="1" colspan="1">35.3</td><td align="center" valign="middle" rowspan="1" colspan="1">39.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Base + <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">59.2</td><td align="center" valign="middle" rowspan="1" colspan="1">40.9</td><td align="center" valign="middle" rowspan="1" colspan="1">46.0</td><td align="center" valign="middle" rowspan="1" colspan="1">48.4</td><td align="center" valign="middle" rowspan="1" colspan="1">31.4</td><td align="center" valign="middle" rowspan="1" colspan="1">37.6</td><td align="center" valign="middle" rowspan="1" colspan="1">22.5</td><td align="center" valign="middle" rowspan="1" colspan="1">17.6</td><td align="center" valign="middle" rowspan="1" colspan="1">37.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Base + <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">64.4</td><td align="center" valign="middle" rowspan="1" colspan="1">40.5</td><td align="center" valign="middle" rowspan="1" colspan="1">48.2</td><td align="center" valign="middle" rowspan="1" colspan="1">51.2</td><td align="center" valign="middle" rowspan="1" colspan="1">33.3</td><td align="center" valign="middle" rowspan="1" colspan="1">44.9</td><td align="center" valign="middle" rowspan="1" colspan="1">25.0</td><td align="center" valign="middle" rowspan="1" colspan="1">41.8</td><td align="center" valign="middle" rowspan="1" colspan="1">43.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Base + <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">59.7</td><td align="center" valign="middle" rowspan="1" colspan="1">45.0</td><td align="center" valign="middle" rowspan="1" colspan="1">47.3</td><td align="center" valign="middle" rowspan="1" colspan="1">50.3</td><td align="center" valign="middle" rowspan="1" colspan="1">30.2</td><td align="center" valign="middle" rowspan="1" colspan="1">38.6</td><td align="center" valign="middle" rowspan="1" colspan="1">24.3</td><td align="center" valign="middle" rowspan="1" colspan="1">23.7</td><td align="center" valign="middle" rowspan="1" colspan="1">39.9</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HMDA-YOLO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>66.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>40.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>49.1</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>51.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>38.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>54.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>27.8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>48.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>45.9</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05363-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05363-t007_Table 7</object-id><label>Table 7</label><caption><p>Model complexity of different detectors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Detector</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Faster RCNN</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Faster RCNN</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">D-DETR</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">YOLOv5s</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">YOLOv8s</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Backbone</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">CSPDarknet</td><td align="center" valign="middle" rowspan="1" colspan="1">CSPDarknet</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Params (M)</td><td align="center" valign="middle" rowspan="1" colspan="1">138.4</td><td align="center" valign="middle" rowspan="1" colspan="1">25.6</td><td align="center" valign="middle" rowspan="1" colspan="1">40.2</td><td align="center" valign="middle" rowspan="1" colspan="1">7.2</td><td align="center" valign="middle" rowspan="1" colspan="1">11.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (FPS)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7&#8211;15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>