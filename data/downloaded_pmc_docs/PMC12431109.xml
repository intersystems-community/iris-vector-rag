<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431109</article-id><article-id pub-id-type="pmcid-ver">PMC12431109.1</article-id><article-id pub-id-type="pmcaid">12431109</article-id><article-id pub-id-type="pmcaiid">12431109</article-id><article-id pub-id-type="doi">10.3390/s25175567</article-id><article-id pub-id-type="publisher-id">sensors-25-05567</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Tracking-Based Denoising: A Trilateral Filter-Based Denoiser for Real-World Surveillance Video in Extreme Low-Light Conditions <xref rid="fn1-sensors-25-05567" ref-type="author-notes">&#8224;</xref></article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3345-9665</contrib-id><name name-style="western"><surname>Jiang</surname><given-names initials="H">He</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-05567" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-5277-0423</contrib-id><name name-style="western"><surname>Wu</surname><given-names initials="P">Peilin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05567" ref-type="aff">1</xref><xref rid="c1-sensors-25-05567" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-5588-5090</contrib-id><name name-style="western"><surname>Zheng</surname><given-names initials="Z">Zhou</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-05567" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-7422-5803</contrib-id><name name-style="western"><surname>Gu</surname><given-names initials="H">Hao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05567" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-8280-2519</contrib-id><name name-style="western"><surname>Yi</surname><given-names initials="F">Fudi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-05567" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-4516-4313</contrib-id><name name-style="western"><surname>Cui</surname><given-names initials="W">Wen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05567" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2079-9417</contrib-id><name name-style="western"><surname>Lv</surname><given-names initials="C">Chen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af2-sensors-25-05567" ref-type="aff">2</xref><xref rid="c1-sensors-25-05567" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Lu</surname><given-names initials="ZM">Zhe-Ming</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05567"><label>1</label>School of Information and Control Engineering, China University of Mining and Technology, Xuzhou 221116, China; <email>jianghe@cumt.edu.cn</email> (H.J.); <email>zhengzhou@cumt.edu.cn</email> (Z.Z.); <email>guhao@cumt.edu.cn</email> (H.G.); <email>yifudi@cumt.edu.cn</email> (F.Y.); <email>cuiwen@cumt.edu.cn</email> (W.C.)</aff><aff id="af2-sensors-25-05567"><label>2</label>School of Software, Taiyuan University of Technology, Taiyuan 030024, China</aff><author-notes><corresp id="c1-sensors-25-05567"><label>*</label>Correspondence: <email>peilin.wu@cumt.edu.cn</email> (P.W.); <email>chenlv@cumt.edu.cn</email> (C.L.); Tel.: +86-189-2876-5690 (P.W.); +86-130-9233-3199 (C.L.)</corresp><fn id="fn1-sensors-25-05567"><label>&#8224;</label><p>This paper is an extended version of our paper published in Proceedings of the 2nd International Conference on Internet of Things, Communication and Intelligent Technology, Xuzhou, China, 22&#8211;24 September 2023.</p></fn></author-notes><pub-date pub-type="epub"><day>06</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5567</elocation-id><history><date date-type="received"><day>30</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>25</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>06</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05567.pdf"/><abstract><p>Video denoising in extremely low-light surveillance scenarios is a challenging task in computer vision, as it suffers from harsh noise and insufficient signal to reconstruct fine details. The denoising algorithm for these scenarios encounters challenges such as the lack of <italic toggle="yes">ground truth</italic>, and the noise distribution in the real world is far more complex than in a normal scene. Consequently, recent state-of-the-art (SOTA) methods like VRT and Turtle for video denoising perform poorly in this low-light environment. Additionally, some methods rely on raw video data, which is difficult to obtain from surveillance systems. In this paper, a denoising method is proposed based on the trilateral filter, which aims to denoise real-world low-light surveillance videos. Our trilateral filter is a weighted filter, allocating reasonable weights to different inputs to produce an appropriate output. Our idea is inspired by an experimental finding: noise on stationary objects can be easily suppressed by averaging adjacent frames. This led us to believe that if we can track moving objects accurately and filter along their trajectories, the noise may be effectively removed. Our proposed method involves four main steps. First, coarse motion vectors are obtained by bilateral search. Second, an amplitude-phase filter is used to judge and correct erroneous vectors. Third, these vectors are refined by a full search in a small area for greater accuracy. Finally, the trilateral filter is applied along the trajectory to denoise the noisy frame. Extensive experiments have demonstrated that our method achieves superior performance in terms of visual effects and quantitative tests.</p></abstract><kwd-group><kwd>video denoising</kwd><kwd>trilateral filter</kwd><kwd>amplitude-phase filter</kwd><kwd>low light</kwd><kwd>surveillance video</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>52304182</award-id></award-group><award-group><funding-source>National Key Research and Development Program of China</funding-source><award-id>2023YFC2907600</award-id><award-id>2021YFC2902701</award-id><award-id>2021YFC2902702</award-id></award-group><award-group><funding-source>Open Fund of the Key Laboratory of System Control and Information Processing of the Ministry of Education of China</funding-source><award-id>SCIP20240105</award-id></award-group><award-group><funding-source>Graduate Innovation Program of China University of Mining and Technology</funding-source><award-id>2024WLKXJ090</award-id></award-group><award-group><funding-source>Postgraduate Research &amp; Practice Innovation Program of Jiangsu Province</funding-source><award-id>KYCX24_2778</award-id></award-group><funding-statement>This research was funded by the National Natural Science Foundation of China (grant no. 52304182), the National Key Research and Development Program of China (grant no. 2023YFC2907600, 2021YFC2902701, 2021YFC2902702), the Open Fund of the Key Laboratory of System Control and Information Processing of the Ministry of Education of China (grant no. SCIP20240105), the Graduate Innovation Program of China University of Mining and Technology (grant no. 2024WLKXJ090), and the Postgraduate Research &amp; Practice Innovation Program of Jiangsu Province (grant no. KYCX24_2778).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05567"><title>1. Introduction</title><p>Reducing the noise inherent in video sensors is a critical challenge, particularly under the harsh conditions of low-light surveillance. The demand for reliable, high-quality video from surveillance systems is ever-increasing, being driven by security needs such as preventing nighttime theft. This quality is also directly essential for applications like autonomous driving, object detection, and action recognition. This necessitates effective denoising techniques that can operate under severe signal degradation. The principle of video denoising is to reconstruct the true signal corrupted by this sensor noise. This is achieved by exploiting the spatiotemporal redundancy inherent in the video data stream. The method involves identifying patches with high similarity to a target region across both space and time. A weighted combination of these similar patches is then used to reconstruct the original feature, effectively restoring the clean signal from its noisy frames. The challenges originate directly at the sensor level. First, the scarcity of incident photons in low-light environments results in a fundamentally low signal-to-noise ratio [<xref rid="B1-sensors-25-05567" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05567" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05567" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05567" ref-type="bibr">4</xref>] and significant signal distortion. Second, unlike the sophisticated sensors in professional cameras from manufacturers like Sony or Panasonic, the sensors in cost-effective surveillance hardware are inherently more prone to thermal and read noise, thus severely amplifying issues when the captured signal itself is weak. &#65279;</p><p>A low-light video denoising method relies on raw video data as input [<xref rid="B5-sensors-25-05567" ref-type="bibr">5</xref>]. However, these raw data are difficult to obtain from standard surveillance systems, which typically output processed and compressed formats like H.264 or YUV/RGB. Attempting to reverse the on-camera Image Signal Processing (ISP) pipeline to recover the raw data often leads to severe artifacts and information loss, as critical sensor-level information is irrevocably discarded during processing. A recent method [<xref rid="B6-sensors-25-05567" ref-type="bibr">6</xref>] divides surveillance video data into moving and static areas, aiming to separate static and moving regions. It introduces an optimization solution based on Dynamic Mode Decomposition (DMD) and Plug-and-Play Alternating Direction Method of Multipliers (PnP-ADMM), which involves a minimization problem equation and incorporates an implicit regularization term to reduce noise. However, it fails to effectively remove noise in extremely low-light and high-noise scenarios. Moreover, it primarily focuses on dynamic mode decomposition, neglecting structure and texture. Additionally, DMD relies on linear system approximations, which can be inadequate for complex, nonlinear motion, and high-frequency DMD modes may be over-smoothed, adversely affecting details. Another video denoising method [<xref rid="B3-sensors-25-05567" ref-type="bibr">3</xref>] embeds the BM3D [<xref rid="B7-sensors-25-05567" ref-type="bibr">7</xref>] algorithm into the HEVC processing workflow, which reduces redundant computations by replacing BM3D [<xref rid="B7-sensors-25-05567" ref-type="bibr">7</xref>]&#8217;s block matching with HEVC&#8217;s motion estimation. This approach is considered efficient for video denoising but also faces challenges in extremely noisy low-light conditions, as such hybrid frameworks often prioritize computational efficiency over adaptive noise modeling, leading to insufficient robustness against complex noise patterns in practical surveillance scenarios.</p><p>In this paper, we address the task of denoising surveillance videos from low-light environments. This task has the following difficulties. First, we lack <italic toggle="yes">ground truth</italic>. Second, the noise distribution in each RGB channel is different. Third, the noise varies greatly across frames, and the low-light environment further complicates the problem. Fourth, the images obtained by the many well-known denoising algorithms are either too smooth or a little noisy, which indicates that it is difficult to provide a good balance between noise removal and detail retention in such a harsh environment. In the experiment, noise is largely removed after averaging the adjacent frames. In <xref rid="sensors-25-05567-f001" ref-type="fig">Figure 1</xref>, the first two images contain only stationary objects, whose noise is largely reduced by multi-frame temporal averaging. The second two images include moving objects, but the moving electronic bike (as shown in <xref rid="sensors-25-05567-f001" ref-type="fig">Figure 1</xref>d) disappears and leaves a trajectory blur. Inspired by this phenomenon, we believe that if the trajectory of the moving object is accurately found, and we filter along the trajectory, the noise will be well suppressed, and reversal artifacts can be efficiently reduced like [<xref rid="B8-sensors-25-05567" ref-type="bibr">8</xref>]. In this paper, a tracking-based denoising algorithm is proposed, and our contributions can be summarized in the following aspects:<list list-type="order"><list-item><p>A simple but efficient motion vector estimation method is put forward, which can be applied to another computer vision task.</p></list-item><list-item><p>A motion updating filter called an amplitude-phase filter is proposed, which improves the accuracy of these motion vectors. In addition, a denoising filter, namely the trilateral filter, is proposed by considering the gradient information, and it can suppress the gradient reversal artifact of the bilateral filter.</p></list-item><list-item><p>A tracking-based video denoising method is proposed.</p></list-item></list></p><fig position="anchor" id="sensors-25-05567-f001" orientation="portrait"><label>Figure 1</label><caption><p>Comparison of noisy images in video and their corresponding temporal average. (<bold>a</bold>) A single noisy frame from the static sequence. (<bold>b</bold>) The result of averaging adjacent frames in the static region. (<bold>c</bold>) A single noisy frame from the moving sequence. (<bold>d</bold>) The result of averaging adjacent frames in the moving region.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g001.jpg"/></fig></sec><sec id="sec2-sensors-25-05567"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05567"><title>2.1. Traditional Methods</title><p>Early methods, like content-adaptive filtering [<xref rid="B9-sensors-25-05567" ref-type="bibr">9</xref>], aimed to suppress noise variance in flat areas while maintaining crisp edge delineation through minimizing the weighted least square error. Subsequently, wavelet-based algorithms [<xref rid="B10-sensors-25-05567" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05567" ref-type="bibr">11</xref>] demonstrated superior capabilities in distinguishing signal and noise components, while they tend to introduce artifacts. While the Kalman filter-based method [<xref rid="B12-sensors-25-05567" ref-type="bibr">12</xref>] primarily targets the reduction of temporal flicker artifacts, its reliance on linear system assumptions fundamentally limits its effectiveness in real-world low-light scenarios characterized by nonlinear noise, resulting in inconsistent denoising performance.</p><p>Later, researchers also proposed patch-based techniques to exploit temporal coherence. For instance, ref. [<xref rid="B13-sensors-25-05567" ref-type="bibr">13</xref>] uses optical flow for motion-compensated patch matching for denoising. Other representative methods like VBM3D [<xref rid="B14-sensors-25-05567" ref-type="bibr">14</xref>] and its extension VBM4D [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>] utilize block matching and collaborative filtering in higher-dimensional transform domains for noise removal. However, VBM4D&#8217;s [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>] primary bottleneck is its computationally intensive motion estimation, which is required for grouping spatiotemporal volumes. This process lacks the efficiency of VBM3D&#8217;s [<xref rid="B14-sensors-25-05567" ref-type="bibr">14</xref>] staged transform approach, leading to a significant increase in complexity. The method in [<xref rid="B16-sensors-25-05567" ref-type="bibr">16</xref>] models groups of similar spatiotemporal patches within an empirical Bayesian framework, avoiding motion estimation errors by not relying on motion compensation. This approach leverages the low intrinsic dimensionality of patch groups to enhance denoising but depends heavily on noise characteristic assumptions and may face challenges in accurately estimating statistics due to the high dimensionality of spatiotemporal patches and limited sample sizes. Even some more recent deep learning approaches that also leverage patch-based strategies such as those proposed in [<xref rid="B17-sensors-25-05567" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05567" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05567" ref-type="bibr">19</xref>] can still suffer from long running times despite their different underlying mechanisms.</p><p>A method specialized for low-light surveillance video denoising in [<xref rid="B20-sensors-25-05567" ref-type="bibr">20</xref>] employs a combination of filtering techniques and adaptively estimates noise levels. By conducting denoising recursively using only the previous frame, it is well suited for hardware implementation due to its efficiency. Its primary limitation, however, arises from its core reliance on a Kalman filter for temporal prediction. This filter inherently assumes a linear motion model, which is often violated in dynamic surveillance scenarios involving abrupt or complex movements. Consequently, this can lead to motion blur and ghosting artifacts around fast-moving objects, undermining the algorithm&#8217;s performance in complex scenes. Additionally, the method is formulated for grayscale video, limiting its direct application to color surveillance systems.</p><p>The method in [<xref rid="B21-sensors-25-05567" ref-type="bibr">21</xref>] employs a multi-stage pipeline including a Kalman filter and nonlocal means (NLM). Although its low memory footprint makes it suitable for resource-constrained hardware, the architecture suffers from a major drawback: its sequential processing and computationally intensive NLM stage create an inherent performance bottleneck, resulting in high latency. This intrinsic slowness, combined with the need for extensive hyperparameter tuning and its reliance on a linear motion assumption, limits its effectiveness in complex, real-time scenarios.</p></sec><sec id="sec2dot2-sensors-25-05567"><title>2.2. Supervised Deep Learning Methods</title><p>The rise of Convolutional Neural Networks (CNNs) [<xref rid="B22-sensors-25-05567" ref-type="bibr">22</xref>] has dramatically enhanced video denoising performance. Several studies [<xref rid="B23-sensors-25-05567" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05567" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05567" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05567" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>] have explored their potential. For example, ref. [<xref rid="B23-sensors-25-05567" ref-type="bibr">23</xref>] consists of two stages: spatial denoising followed by a motion compensation stage to reduce flickering. While effective, the reliance on optical flow in such methods [<xref rid="B23-sensors-25-05567" ref-type="bibr">23</xref>,<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>,<xref rid="B29-sensors-25-05567" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-05567" ref-type="bibr">30</xref>] comes at a steep computational price. Unlike coarser block-matching techniques that estimate a single displacement for a patch, optical flow computes a dense motion vector for every pixel. This per-pixel granularity is computationally intensive, rendering these methods too slow and costly for practical deployment. Furthermore, their accuracy is compromised under challenging conditions like low light and high noise, where the foundational assumption of brightness constancy is often violated. To enhance performance, various architectures have been proposed, including U-Net-inspired designs [<xref rid="B24-sensors-25-05567" ref-type="bibr">24</xref>,<xref rid="B26-sensors-25-05567" ref-type="bibr">26</xref>] and multi-scale feature extraction frameworks [<xref rid="B31-sensors-25-05567" ref-type="bibr">31</xref>]. A prominent example, FastdvdNet [<xref rid="B24-sensors-25-05567" ref-type="bibr">24</xref>], employs a U-Net architecture with depthwise separable convolutions to implicitly handle motion, thereby avoiding the costly estimation of optical flow. However, its architecture relies solely on standard convolutional layers, lacking mechanisms like attention or advanced feature fusion. This structural simplicity limits its ability to capture long-range temporal dependencies, making it less effective for scenes with significant motion.</p><p>Recursive Neural Network (RNN)-based algorithms [<xref rid="B32-sensors-25-05567" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05567" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05567" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05567" ref-type="bibr">35</xref>] exhibit excellent performance in utilizing temporal modeling and leveraging inter-frame correlations. FloRNN [<xref rid="B32-sensors-25-05567" ref-type="bibr">32</xref>], for example, is designed for online processing by replacing the true backward pass of a bidirectional RNN with a finite &#8217;look-ahead&#8217; module. While this design enables real-time operation, it introduces a fundamental asymmetry: context from past frames is propagated directly via recurrence, while future context is an approximation derived from a limited look-ahead window and then warped back to the current frame. This indirect estimation of future information is inherently less robust than a true bidirectional pass. The approach in [<xref rid="B33-sensors-25-05567" ref-type="bibr">33</xref>] uses stacked RNN layers to capture temporal dynamics, and EMVD [<xref rid="B35-sensors-25-05567" ref-type="bibr">35</xref>] introduces multi-stage fusion to reduce computational load, though this may limit their capacity to model long-range dependencies. The sequential nature of RNNs creates a core architectural dilemma: a choice between maximum accuracy with slow, offline bidirectional models and speed with faster but less robust online approximations. This trade-off fundamentally limits their practical application in real-time systems.</p><p>In contrast, transformer-based methods [<xref rid="B4-sensors-25-05567" ref-type="bibr">4</xref>,<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>,<xref rid="B36-sensors-25-05567" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05567" ref-type="bibr">37</xref>] achieve superior denoising accuracy but come with high computational demands. Video Restoration Transformer (VRT) [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>] exemplifies such trade-offs. While its Temporal Mutual Self-Attention mechanism achieves state-of-the-art performance, the model relies on window-based attention for implicit motion modeling, which limits its ability to capture long-range temporal dependencies. This constraint becomes particularly evident in the presence of large motions, where the model struggles to efficiently model distant frame relationships and fully leverage motion information, potentially compromising restoration quality. To mitigate these issues, VRT processes videos in segmented clips and employs sequence shifting&#8212;strategies that add architectural complexity and computational overhead. Similarly, the work in [<xref rid="B4-sensors-25-05567" ref-type="bibr">4</xref>], which focuses on low-light environments, integrates 3D Shifted Window Transformer blocks within a U-Net-like encoder&#8211;decoder architecture. This design cleverly avoids explicit motion estimation modules like optical flow. However, its primary limitation lies in its final temporal fusion stage. This module relies on a simplified, implicit alignment, using a single similarity score to re-weight entire feature maps before fusion via convolution. While computationally simpler than dedicated alignment techniques, this soft re-weighting approach may be insufficient for handling large or complex motions, potentially leading to motion blur or ghosting artifacts. This architecture, combined with the deep stack of 3D attention blocks, still results in significant computational and memory overhead.</p></sec><sec id="sec2dot3-sensors-25-05567"><title>2.3. Unsupervised Methods</title><p>In the realm of unsupervised video denoising [<xref rid="B25-sensors-25-05567" ref-type="bibr">25</xref>,<xref rid="B38-sensors-25-05567" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05567" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05567" ref-type="bibr">40</xref>], recent deep learning approaches have explored diverse architectures to address spatiotemporal noise without clean data supervision. Recent approaches, such as UDVD [<xref rid="B25-sensors-25-05567" ref-type="bibr">25</xref>], employ &#8220;blind-spot&#8221; convolutions [<xref rid="B41-sensors-25-05567" ref-type="bibr">41</xref>] to model intricate spatiotemporal relationships. By using rotated and asymmetric kernels, this method enhances feature representation. However, its performance degrades in scenarios involving large-scale, rapid motion. This limitation stems from a constrained search area, which is confined to orthogonal axes (e.g., below and right) while overlooking diagonal orientations like the upper left. Another approach, Temporal As a Plugin (TAP) [<xref rid="B39-sensors-25-05567" ref-type="bibr">39</xref>], extends pre-trained image denoisers by inserting trainable temporal modules into their skip connections. These modules use deformable convolutions for motion alignment, simplifying the transition from image to video denoising. However, a significant domain gap arises when applying a model trained on standard, well-lit images to complex low-light video. The image denoiser backbone, optimized for simple noise patterns, struggles to handle severe degradations such as signal-dependent noise and motion blur commonly found in low-light conditions. Crucially, its inaccurate denoising results introduce residual noise and artifacts into the generated "pseudo-clean" frames, which misguide the learning of temporal modules and lead to error propagation and amplification. Turtle [<xref rid="B42-sensors-25-05567" ref-type="bibr">42</xref>] also introduces a novel framework based on U-Net, but it employs a truncated causal history mechanism. It uses Cross-Frame Attention to implicitly align features and Cross-Channel Attention to aggregate features from historical frames. However, its core design choice to use only a truncated history of past frames imposes a significant limitation. By design, the model has no access to future frames, creating an information bottleneck. This hard truncation restricts the model&#8217;s receptive field in the temporal dimension, preventing it from capturing long-range dependencies.</p><p>Building on unsupervised learning, self-supervised methods like [<xref rid="B43-sensors-25-05567" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05567" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05567" ref-type="bibr">45</xref>] further minimize reliance on paired data. While [<xref rid="B25-sensors-25-05567" ref-type="bibr">25</xref>] explores untrained denoising networks, self-supervised video methods directly address temporal noise by leveraging noisy video itself for training. The approach in [<xref rid="B45-sensors-25-05567" ref-type="bibr">45</xref>] utilizes the translation equivariance of Fully Convolutional Networks (FCNs) and a progressive fine-tuning strategy. By learning from &#8220;pseudo-clean&#8221; data, it can theoretically achieve a lower noise level than methods that learn directly from noisy data. However, unlike in normal lighting scenarios, the noise type and intensity in low-light conditions are harsh and not easy to estimate. This indirectly affects the performance of both the pre-trained model and the final output.</p><p>In summary, recent SOTA methods, including TAP [<xref rid="B39-sensors-25-05567" ref-type="bibr">39</xref>], Turtle [<xref rid="B42-sensors-25-05567" ref-type="bibr">42</xref>], and VRT [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>], all demonstrate limited robustness in low-light scenarios. Although their performance is commendable in daylight, it degrades substantially under low-illumination conditions with severe noise, where they struggle with effective noise removal. Furthermore, these deep learning methods inherently suffer from the common disadvantage of high memory consumption.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05567"><title>3. Methods</title><p>This work is a significantly extended version of our conference paper [<xref rid="B46-sensors-25-05567" ref-type="bibr">46</xref>]. In this section, we will introduce our tracking-based algorithm shown in <xref rid="sensors-25-05567-f002" ref-type="fig">Figure 2</xref>.</p><sec id="sec3dot1-sensors-25-05567"><title>3.1. Coarse Motion Estimation</title><p>The coarse motion estimation module is founded upon the classic Block-Matching Estimation (<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) framework, as exemplified by the method in [<xref rid="B47-sensors-25-05567" ref-type="bibr">47</xref>]. This approach is widely adopted for its computational efficiency. Our primary contribution in this context is not the invention of the search mechanism itself, but its novel application and adaptation to the domain of low-light surveillance video denoising, which is a departure from its conventional use in Frame Rate Up-Conversion (<inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>R</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). The objective is also fundamentally different: while in <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>R</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, motion vectors are primarily used for interpolating missing frames via methods like Overlapped Blocks Motion Compensation (<inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>B</mml:mi><mml:mi>M</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), our implementation leverages them to enhance the robustness of cross-frame matching specifically for denoising purposes. We selected this bilateral search strategy because it is not only computationally efficient due to the simple Sum of Absolute Differences (<inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) matching criterion, but it also provides accurate results. It can effectively avoid artifacts such as overlaps or holes that can result from unidirectional motion estimation. Furthermore, the resulting motion vectors are well suited for the subsequent vector refinement stage. The process of our bilateral search is illustrated in <xref rid="sensors-25-05567-f003" ref-type="fig">Figure 3</xref>.</p><p>In this process, each frame is segmented into <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> blocks, where 8 is the block size (<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). To improve matching robustness, we adopt (<inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>B</mml:mi><mml:mi>M</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), which extends the boundary of the original block by a margin of Overlapped pixels (<inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). To facilitate the processing of these edge blocks, the frame is padded with a certain width of Padding pixels (<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). Based on these steps, the search area is defined as <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#215;</mml:mo><mml:mi>B</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>p</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>B</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#215;</mml:mo><mml:mi>B</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>p</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>y</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>b</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>B</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where subscripts <italic toggle="yes">a</italic> and <italic toggle="yes">b</italic> denote the block&#8217;s row and column indices, respectively. Naturally, the ranges of these indices are constrained by the dimensions of the input video frame (the resolution of motion vectors field), which we denote as <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The lower bound for both <italic toggle="yes">a</italic> and <italic toggle="yes">b</italic> indices is 1, and the upper bound is determined by the frame&#8217;s boundaries. Specifically, for index <italic toggle="yes">a</italic>, it must satisfy <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Once the block&#8217;s spatial range is defined, <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is used to calculate the pixel differences.<disp-formula id="FD1-sensors-25-05567"><label>(1)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In Equation (<xref rid="FD1-sensors-25-05567" ref-type="disp-formula">1</xref>), <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents a block with the starting point <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the previous frame, and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined analogously for the subsequent frame. <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the search area in the previous frame, and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>&#8804;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>&#8804;</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, in which <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are integer offsets, and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Search window size. The motion vector <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that minimizes the <inline-formula><mml:math id="mm784" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> value is selected, as shown in Equation (<xref rid="FD2-sensors-25-05567" ref-type="disp-formula">2</xref>).<disp-formula id="FD2-sensors-25-05567"><label>(2)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mi>min</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot2-sensors-25-05567"><title>3.2. Motion Vector Updating</title><p>The initial motion vector <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is often coarse, and thus an update mechanism is introduced. We represent <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in terms of its amplitude and phase, where amplitude <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> and phase <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arctan</mml:mo><mml:mo>(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Therefore, the vector can be modified by altering <italic toggle="yes">A</italic> and <italic toggle="yes">P</italic>, transforming it as follows: <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo form="prefix">cos</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo form="prefix">sin</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8594;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo form="prefix">cos</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo form="prefix">sin</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The mechanism, therefore, is designed to transform <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> be the vector to be updated, which belongs to the block in the <italic toggle="yes">a</italic>th row and the <italic toggle="yes">b</italic>th column, and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are its 8-neighbor vectors, as illustrated in <xref rid="sensors-25-05567-f004" ref-type="fig">Figure 4</xref>.</p><p>To reduce singular motions, the local continuity of the motion vector field is considered. Local continuity is measured as the Manhattan distance in terms of amplitude (<inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and phase (<inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), as defined in Equation (<xref rid="FD3-sensors-25-05567" ref-type="disp-formula">3</xref>). Here, <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the amplitude and phase of vector <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Similarly, <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the amplitude and phase of the neighbor vector <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively, where <italic toggle="yes">i</italic> ranges from 1 to 8.<disp-formula id="FD3-sensors-25-05567"><label>(3)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mstyle><mml:munderover><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>8</mml:mn></mml:munderover><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mspace width="1.em"/><mml:mi>P</mml:mi></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mstyle><mml:munderover><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>8</mml:mn></mml:munderover><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A Rationality matrix <italic toggle="yes">R</italic> is used to judge whether the current motion vector is reliable. Since it is a binary classification problem, the element <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in this matrix can only be 0 or 1, and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicates the rationality of the block at the <italic toggle="yes">a</italic>th row and the <italic toggle="yes">b</italic>th column. Equation (<xref rid="FD3-sensors-25-05567" ref-type="disp-formula">3</xref>) is used to estimate the motion vector <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as well as calculating the local continuity in amplitude <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and phase <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, Equation (<xref rid="FD4-sensors-25-05567" ref-type="disp-formula">4</xref>) is used for the rationality matrix initialization. In Equation (<xref rid="FD4-sensors-25-05567" ref-type="disp-formula">4</xref>), <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the maximum and minimum phase offset thresholds.<disp-formula id="FD4-sensors-25-05567"><label>(4)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mspace width="1.em"/><mml:mi>A</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>P</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mspace width="1.em"/><mml:mi>A</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mn>16</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>P</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Vectors with a rationality of 1 are considered reasonable, while those with a rationality of 0 are deemed unreasonable. For reasonable vectors, we leave them unchanged. But for unreasonable vectors, a median filter is applied to update them. This process is called the Amplitude-Phase Filter (<inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), and Equation (<xref rid="FD5-sensors-25-05567" ref-type="disp-formula">5</xref>) shows the details.<disp-formula id="FD5-sensors-25-05567"><label>(5)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="{" close=""><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Furthermore, a stop condition of this iterative mechanism must be designed. Our design follows two principles: the number of updates cannot be excessively large, and the majority of motion vectors must be reasonable. The first principle can be easily satisfied by setting the maximum number of iterations. The second condition can be measured by the sum of the rationality matrix, namely <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. In Equation (<xref rid="FD6-sensors-25-05567" ref-type="disp-formula">6</xref>), <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the resolution of the motion vector field as mentioned before, and <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> ranges between 0 and 1, which describes the percentage of reasonable vectors in all vectors.<disp-formula id="FD6-sensors-25-05567"><label>(6)</label><mml:math id="mm57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munderover><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:munderover><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:mi>&#946;</mml:mi><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Our proposed APF introduces a novel approach to motion vector updating. Unlike the approach in [<xref rid="B48-sensors-25-05567" ref-type="bibr">48</xref>], which relies on the Bidirectional Prediction Difference and subsequent Outlier Detection, our classification is performed directly using the amplitude and phase of the motion vectors. This allows for a more nuanced evaluation of vector reliability, as the phase component explicitly accounts for directional variations in motion. Moreover, our adaptive, iterative update offers a more flexible solution than the fixed two-stage (median-then-mean) smoothing process in [<xref rid="B48-sensors-25-05567" ref-type="bibr">48</xref>] without introducing significant computational overhead.</p></sec><sec id="sec3dot3-sensors-25-05567"><title>3.3. Motion Vector Refinement</title><p>After the second step, the motion vectors may not be precise enough, and their accuracy cannot reach the pixel level without a full search. A full search is often avoided by many methods due to its time complexity. However, its idea can be adopted. Our approach is to compensate for the motion vectors by doing full search only in a small area. In this way, we can obtain more accurate motion vectors, and the small area helps the system avoid significant computational cost, as shown in <xref rid="sensors-25-05567-f005" ref-type="fig">Figure 5</xref>.</p><p>Suppose a vector <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is initialized by coarse motion estimation, and it is then updated to <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. After the final refinement step, the vector becomes <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the compensation vector on the X/Y-axis obtained from a full search in a small area.<disp-formula id="FD7-sensors-25-05567"><label>(7)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mrow><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>In Equation (<xref rid="FD7-sensors-25-05567" ref-type="disp-formula">7</xref>), <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is short for the Sum of the Absolute Difference, and the footnote <italic toggle="yes">p</italic> denotes pixel-level precision. <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents an image block <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> starting at <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the previous frame, and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined analogously.</p><p><inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a new search area, which is different from the search area <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. First, <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the search area used in vector initialization, and its size should be large to improve the likelihood of capturing a wide range of motions. However, <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the small search area used in vector compensation. Second, the search method used in <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is bilateral search, but the search method in <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a full search. In fact, <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> here denotes small search window size.</p><p>The final optimal vector <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> can be acquired using Equation (<xref rid="FD8-sensors-25-05567" ref-type="disp-formula">8</xref>), and <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is the best vector for the block in row <italic toggle="yes">a</italic> and column <italic toggle="yes">b</italic>. The entire process for motion vector estimation, updating, and refinement is summarized in Algorithm 1.<disp-formula id="FD8-sensors-25-05567"><label>(8)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mi>min</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<array orientation="portrait"><tbody><tr><td style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold> Motion vectors estimation, updating, and refinement.</td></tr><tr><td style="border-bottom:solid thin" rowspan="1" colspan="1"><list list-type="simple"><list-item><p><bold>Require:</bold> Low-light Surveillance Video <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">I</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, Block Size <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, Padding Pixel <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, Overlap Pixel <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, Search Window Size <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, Vector Updating Number <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, Rationality Matrix <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:math></inline-formula>, Reasonable Degree <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.96</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and Small Search Window Size <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><bold>Ensure:</bold> The Final Vector <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow></mml:math></inline-formula> and Rationality Matrix <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:math></inline-formula>.
<list list-type="simple"><list-item><label>1:</label><p>&#160;&#160;Initialization: <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>m</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>n</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, Zero Matrix <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>2:</label><p>&#160;&#160;<bold>for</bold> each <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>3:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;<bold>for</bold> each <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>4:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>for</bold> each <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#160;<bold>do</bold></p></list-item><list-item><label>5:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Get the areas <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>6:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Solve Equation (<xref rid="FD2-sensors-25-05567" ref-type="disp-formula">2</xref>) to get the vector <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>7:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>Repeat:</bold> Compute <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> using Equation (<xref rid="FD3-sensors-25-05567" ref-type="disp-formula">3</xref>).</p></list-item><list-item><label>8:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Update <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> using Equations (<xref rid="FD4-sensors-25-05567" ref-type="disp-formula">4</xref>) and (<xref rid="FD5-sensors-25-05567" ref-type="disp-formula">5</xref>).</p></list-item><list-item><label>9:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>10:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>While:</bold>&#160;<inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msubsup><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#8805;</mml:mo><mml:mi>&#946;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>11:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Get the updated vector <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>12:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Solve Equation (<xref rid="FD8-sensors-25-05567" ref-type="disp-formula">8</xref>) to get the best vector.</p></list-item><list-item><label>13:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>14:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Update <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> using Equation (<xref rid="FD5-sensors-25-05567" ref-type="disp-formula">5</xref>) for each vector.</p></list-item><list-item><label>15:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>16:</label><p>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>17:</label><p>&#160;&#160;&#160;&#160;<bold>end for</bold></p></list-item><list-item><label>18:</label><p><bold>end for</bold></p></list-item><list-item><label>19:</label><p><bold>return</bold> The final vector <bold>V</bold> and rationality matrix <bold>R</bold>;</p></list-item></list></p></list-item></list></td></tr></tbody></array></p></sec><sec id="sec3dot4-sensors-25-05567"><title>3.4. Trilateral Filter</title><p>Motion vectors can be classified into three categories using the criteria depicted in <xref rid="sensors-25-05567-f006" ref-type="fig">Figure 6</xref>, namely type 1, type 2 and type 3 vectors. Type 1 vectors are zero vectors; thus, only stationary objects can be found in their corresponding blocks. Type 2 vectors are not zero vectors but they are reasonable, meaning their corresponding blocks have found well-matched counterparts in the adjacent frames. Type 3 vectors are correspondingly unreasonable, signifying that their corresponding blocks fail to find the matching pairs in adjacent frames. Since the trilateral filter is based on the bilateral filter [<xref rid="B49-sensors-25-05567" ref-type="bibr">49</xref>], we consider that a bilateral filter is an edge-preserving filter, and it can preserve the image&#8217;s edge features while denoising. However, the bilateral filter has two disadvantages. The first is its slow speed, which has already been solved by methods [<xref rid="B50-sensors-25-05567" ref-type="bibr">50</xref>]. The second is that the images filtered by a bilateral filter [<xref rid="B49-sensors-25-05567" ref-type="bibr">49</xref>] have the gradient reversal artifact.</p><p>To address this artifact, we propose an innovative trilateral filter designed to suppress the gradient reversal artifacts often associated with bilateral filters. While conventional bilateral filters operate on the spatial and pixel domain, and other trilateral filters like those in [<xref rid="B51-sensors-25-05567" ref-type="bibr">51</xref>,<xref rid="B52-sensors-25-05567" ref-type="bibr">52</xref>] have incorporated third domains such as depth edges or motion vector similarity, our method uniquely introduces a gradient similarity weight. This design choice is specifically tailored to our task of video denoising, as it effectively preserves texture and fine details that are critical in the restored images. Inspired by this concept, our trilateral filter considers three aspects of information: the spatial domain, the pixel domain, and the gradient domain.</p><p>The spatial domain information is measured by the local Euclidean distance in Equation (<xref rid="FD9-sensors-25-05567" ref-type="disp-formula">9</xref>)), and <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the similarity coefficient for the block in row <italic toggle="yes">a</italic> and column <italic toggle="yes">b</italic>. <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a constant, which is related to the block variance <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#948;</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and it equals <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>&#948;</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in Equation (<xref rid="FD10-sensors-25-05567" ref-type="disp-formula">10</xref>) and <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in Equation (<xref rid="FD11-sensors-25-05567" ref-type="disp-formula">11</xref>) have similar meanings.</p><p>The pixel domain information is measured by the Euclidean distance between the pixel values. In Equation (<xref rid="FD10-sensors-25-05567" ref-type="disp-formula">10</xref>), <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is adopted as the similarity metric for the block in row <italic toggle="yes">a</italic> and column <italic toggle="yes">b</italic> in the pixel domain.</p><p>The gradient domain information is measured by the Euclidean distance between the gradient images, namely <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. In Equation (<xref rid="FD12-sensors-25-05567" ref-type="disp-formula">12</xref>), <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the gradient-domain similarity constant for the block in row <italic toggle="yes">a</italic> and column <italic toggle="yes">b</italic>, while <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the gradient of blocks <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In the experiment, <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are used for the computation of gradients along the <italic toggle="yes">x</italic>-axis and <italic toggle="yes">y</italic>-axis.<disp-formula id="FD9-sensors-25-05567"><label>(9)</label><mml:math id="mm128" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05567"><label>(10)</label><mml:math id="mm129" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05567"><label>(11)</label><mml:math id="mm130" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#961;</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05567"><label>(12)</label><mml:math id="mm131" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>g</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8722;</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mrow><mml:mo>)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The final coefficient <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This coefficient integrates the information of the spatial domain, pixel domain, and gradient domains of an image, and it is therefore more robust. In addition, <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; thus, <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is suitable as a weighting factor for the block in row <italic toggle="yes">a</italic> and column <italic toggle="yes">b</italic>. Based on these, trilateral filter <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for vector <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is defined in Equation (<xref rid="FD13-sensors-25-05567" ref-type="disp-formula">13</xref>), in which &#8727; means a starting point <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in block <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD13-sensors-25-05567"><label>(13)</label><mml:math id="mm140" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#8727;</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The trilateral filter suppresses noise differently for each vector type shown in <xref rid="sensors-25-05567-f006" ref-type="fig">Figure 6</xref>. For type 1 vectors (zero vectors), noise is reduced by averaging 10 adjacent frames. Type 2 vectors are denoised directly using Equation (<xref rid="FD13-sensors-25-05567" ref-type="disp-formula">13</xref>). For type 3 vectors, nonlocal similarity and a large search window (<inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) are employed to find matching pairs, after which Equation (<xref rid="FD13-sensors-25-05567" ref-type="disp-formula">13</xref>) is applied for denoising.</p></sec></sec><sec id="sec4-sensors-25-05567"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05567"><title>4.1. Datasets for Real-World Low-Light Surveillance Video Denoising</title><p>The evaluation of denoisers for low-light surveillance video heavily relies on representative datasets, yet existing benchmarks present significant limitations for this specific task. While widely used benchmarks such as Set8 [<xref rid="B23-sensors-25-05567" ref-type="bibr">23</xref>] and DAVIS [<xref rid="B53-sensors-25-05567" ref-type="bibr">53</xref>] exist, they primarily feature well-lit common scenarios and are thus unsuitable. Several datasets have been proposed specifically for low-light environments, including CRVD [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>], and those in [<xref rid="B4-sensors-25-05567" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05567" ref-type="bibr">5</xref>]. CRVD [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>] provides 1080p videos from an IMX385 sensor across five ISO levels, while [<xref rid="B5-sensors-25-05567" ref-type="bibr">5</xref>] targets even more extreme conditions (&lt;0.1 lux) at 4K resolution. However, both datasets generate motion by manually controlling static objects frame by frame, resulting in discontinuous motion patterns confined to indoor environments, which fail to capture the fluidity of real-world dynamics. Although the dataset in [<xref rid="B4-sensors-25-05567" ref-type="bibr">4</xref>] offers more realistic and complex motion models across its 210 video pairs, it shares a fundamental issue with CRVD [<xref rid="B5-sensors-25-05567" ref-type="bibr">5</xref>,<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>]: they are all provided in RAW format. This creates a critical domain gap, as consumer-grade surveillance cameras (Hikvision, Dahua) typically output compressed streams like YUV or H.264 due to hardware and bandwidth constraints. Attempting to reverse-engineer RAW data from these formats via an inverse ISP pipeline is an ill-posed problem that introduces substantial estimation errors. Compounding these issues, the datasets from [<xref rid="B4-sensors-25-05567" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05567" ref-type="bibr">5</xref>] are not publicly available. Finally, while the public DID dataset [<xref rid="B55-sensors-25-05567" ref-type="bibr">55</xref>] offers multi-camera diversity and is accessible, it is designed for video enhancement, and its dynamics are generated by camera motion across static scenes, making it inappropriate for evaluating denoising on scenes with independent object motion.</p><p>To address this, we collected real-world extreme low-light sequences with resolution 1920 &#215; 1072 using Hikvision DS-IPC surveillance cameras. The sequences feature wide-angle residential views with static and slow traffic and top&#8211;down road perspectives with fast vehicles, encompassing complicated motions and challenging backgrounds. Our dataset is provided in the RGB color space, matching the typical output of surveillance systems and thus eliminating the domain gap associated with RAW-based datasets, and it comprises 14 video clips (around 800 noisy samples) covering static and dynamic regions for quantitative evaluation, which is shown in <xref rid="sensors-25-05567-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-05567-t002" ref-type="table">Table 2</xref>. Equations (<xref rid="FD14-sensors-25-05567" ref-type="disp-formula">14</xref>)&#8211;(<xref rid="FD16-sensors-25-05567" ref-type="disp-formula">16</xref>) show the definitions of PSNR and SSIM. In Equation (<xref rid="FD14-sensors-25-05567" ref-type="disp-formula">14</xref>), <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mn>255</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is defined in Equation (<xref rid="FD15-sensors-25-05567" ref-type="disp-formula">15</xref>). In Equation (<xref rid="FD15-sensors-25-05567" ref-type="disp-formula">15</xref>), <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the resolution of the <italic toggle="yes">ground truth</italic> image <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the denoised result of the input noisy image. In Equation (<xref rid="FD16-sensors-25-05567" ref-type="disp-formula">16</xref>), <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> are two signals and <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are their average values. <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are variance <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, and <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the covariance of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>. <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are small values, and both of them equal 0.001. They serve to stabilize the division when the denominators are close to zero.<disp-formula id="FD14-sensors-25-05567"><label>(14)</label><mml:math id="mm154" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD15-sensors-25-05567"><label>(15)</label><mml:math id="mm155" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:munderover><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-05567"><label>(16)</label><mml:math id="mm156" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We provide six visual results for qualitative comparison in <xref rid="sensors-25-05567-f007" ref-type="fig">Figure 7</xref>, <xref rid="sensors-25-05567-f008" ref-type="fig">Figure 8</xref>, <xref rid="sensors-25-05567-f009" ref-type="fig">Figure 9</xref>, <xref rid="sensors-25-05567-f010" ref-type="fig">Figure 10</xref>, <xref rid="sensors-25-05567-f011" ref-type="fig">Figure 11</xref> and <xref rid="sensors-25-05567-f012" ref-type="fig">Figure 12</xref>. Static areas use pseudo-GT via multi-frame averaging; dynamic areas lack reliable GT due to motion blur invalidating temporal averaging. This provides a stringent testbed for evaluating denoising robustness on actual surveillance artifacts.</p></sec><sec id="sec4dot2-sensors-25-05567"><title>4.2. Implementation Details</title><p>To evaluate the denoising performance of our method, nine popular denoising methods are used to make comparisons with our method, namely VBM4D [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>], FastdvdNet [<xref rid="B24-sensors-25-05567" ref-type="bibr">24</xref>], UDVD [<xref rid="B25-sensors-25-05567" ref-type="bibr">25</xref>], FloRNN [<xref rid="B32-sensors-25-05567" ref-type="bibr">32</xref>], RCD [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>], ShiftNet [<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>], TAP [<xref rid="B40-sensors-25-05567" ref-type="bibr">40</xref>], Turtle [<xref rid="B42-sensors-25-05567" ref-type="bibr">42</xref>], and VRT [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>].</p><p>The experimental setup employed Matlab R2023a and PyCharm 2023.1 (python3.8, CUDA11.3) as the primary software environments. The hardware configuration included a single NVIDIA V100-SMX2-32GB GPU, a 12-core Intel Xeon Platinum 8255C CPU operating at 2.50 GHz, and 43 GB of system memory. All of the source code is available on Github, and we follow the default parameters. We use PSNR and SSIM as they are widely used evaluation metrics in the video denoising. PSNR mainly measures the pixel-wise error between two images, while SSIM mainly measures the structural similarity between two images in video.</p></sec><sec id="sec4dot3-sensors-25-05567"><title>4.3. Quantitative Tests and Visual Evaluations</title><p><xref rid="sensors-25-05567-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-05567-t002" ref-type="table">Table 2</xref> show the average values of PSNR and SSIM of 14 video sequences. In both PSNR and SSIM tests, our algorithm performs best in all video sequences, demonstrating that our method can retain the structural features of each frame effectively and it can suppress the pixel-wise error of each frame to some extent.</p><p>The final denoising performance also needs to be judged by the human eye; thus, the visual quality of each denoised image is important. A low-light environment is filled with the noise of different categories; therefore, in the process of noise removal, the high-frequency information, such as image edge and texture, can easily to be mistaken for the noise and then suppressed by the denoising algorithm. For instance, in <xref rid="sensors-25-05567-f008" ref-type="fig">Figure 8</xref>, our method sharply delineates the edges and contours of the window, whereas results from competing methods are still plagued by complex noise artifacts. In <xref rid="sensors-25-05567-f010" ref-type="fig">Figure 10</xref>, our approach adeptly restores the structure of the car, rendering its wheels and the overhead fence distinctly discernible. In contrast, TAP [<xref rid="B39-sensors-25-05567" ref-type="bibr">39</xref>] exhibits severe color distortion, and other algorithms also yield suboptimal outcomes. VBM4D [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>], for example, achieves noise reduction but at the expense of sacrificing fine details. Finally, in <xref rid="sensors-25-05567-f012" ref-type="fig">Figure 12</xref>, several methods show limitations. UDVD [<xref rid="B26-sensors-25-05567" ref-type="bibr">26</xref>] introduces noticeable green artifacts on the tree, and TAP [<xref rid="B39-sensors-25-05567" ref-type="bibr">39</xref>] once again suffers from severe color deviation. While Turtle [<xref rid="B42-sensors-25-05567" ref-type="bibr">42</xref>] maintains structural details, it visually amplifies the noise.</p><p>Although ShiftNet [<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>] and RCD [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>] achieve commendable results, our method demonstrates a superior trade-off, producing a perceptibly cleaner result that better preserves the tree&#8217;s intricate structure. In general, in the low-light environment, our method has a strong ability to protect image structures, and it is also robust to the environment with extremely low luminance.</p></sec><sec id="sec4dot4-sensors-25-05567"><title>4.4. Speed</title><p>Speed is an important metric for evaluating an algorithm. As shown in <xref rid="sensors-25-05567-t003" ref-type="table">Table 3</xref>, our method is not the fastest compared to other algorithms, achieving approximately 1.17 s to process a single frame. These tests were all conducted on a system equipped with an RTX 4090D GPU with 24 GB VRAM, paired with an AMD EPYC 9754 128-Core Processor (18 vCPUs utilized, max frequency 3.1 GHz) and 60 GB of system memory, using Python 3.8, and CUDA 11.3. However, our proposed method operates on a CPU and does not leverage specialized hardware like GPUs, which is in contrast to many contemporary deep learning techniques that rely on GPUs for computational acceleration. It is noteworthy that the majority of the time consumption in our algorithm arises from search operations, which are relatively independent and thus amenable to GPU acceleration. We intend to investigate GPU-accelerated versions in future work.</p></sec><sec id="sec4dot5-sensors-25-05567"><title>4.5. Intensity Curve</title><p>The intensity curve is a visualization method. Suppose <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">F</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a frame in the noisy video, and <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is its luminance channel. A single line of pixels <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is sampled from <inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:math></inline-formula>. Similarly, the same line from the denoised results and <italic toggle="yes">ground truth</italic> are also extracted. In <xref rid="sensors-25-05567-f013" ref-type="fig">Figure 13</xref>, the red curve <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math></inline-formula> is a noisy signal from row 120 of frame 26 in video sequence 2, while the black signal <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the <italic toggle="yes">ground truth</italic>.</p><p>The green curve represents the denoising result for each method. It can be seen that deep learning-based methods, such as FastdvdNet [<xref rid="B24-sensors-25-05567" ref-type="bibr">24</xref>], ShiftNet [<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>], and Turtle [<xref rid="B42-sensors-25-05567" ref-type="bibr">42</xref>], perform poorly on low-light videos. This is because in such a harsh environment, it is difficult to accurately model the complex noise distribution solely through the adjustment of network weights, leading to insufficient smoothing. Moreover, the intensity curve of the traditional VBM4D [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>] is overly smoothed, and significant residual noise remains around pixel column 80. This indicates a loss of detail, which is corroborated by the texture of the staircase in <xref rid="sensors-25-05567-f011" ref-type="fig">Figure 11</xref>. Finally, the overall difference between our curve and <inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the smallest, and its profile follows the <italic toggle="yes">ground truth</italic> more closely and smoothly, which again proves that our algorithm achieves the best denoising performance.</p></sec><sec id="sec4dot6-sensors-25-05567"><title>4.6. Video Denoising Performance on DAVIS and CRVD Benchmarks</title><p>To further validate the generalizability and robustness of our algorithm, we also evaluated its performance on standard benchmark datasets. Additional experiments were carried out on the DAVIS [<xref rid="B53-sensors-25-05567" ref-type="bibr">53</xref>] and CRVD [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>] datasets. For CRVD [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>], we converted the RAW data to RGB format to ensure a fair comparison. For DAVIS [<xref rid="B53-sensors-25-05567" ref-type="bibr">53</xref>], we followed common practice in video denoising by adding Gaussian noise with a standard deviation of 50. We compared our method against several representative top-performing approaches from our main experiments&#8212;VBM4D [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>], ShiftNet [<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>], and RCD [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>]&#8212;using both qualitative and quantitative evaluations. Representative results are presented below under the same evaluation protocol as in our main study. As shown in <xref rid="sensors-25-05567-f014" ref-type="fig">Figure 14</xref>, which depicts flowers and their pots, our method performs competitively, effectively suppressing prominent color noise artifacts while achieving performance comparable to other state-of-the-art methods. In the results, <bold>bold</bold> and <underline>underlined</underline> values indicate the best and second-best performance, respectively.</p><p>As shown in <xref rid="sensors-25-05567-f015" ref-type="fig">Figure 15</xref>, our algorithm achieves the highest PSNR and SSIM values, particularly in regions where the ground and foliage intersect. However, we observe that VBM4D [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>] preserves fine textures slightly better, resulting in less blurring. This can be explained by the continuous camera motion in this scene, which results in an absence of static regions. Since our method employs distinct strategies for static and dynamic areas, the lack of static regions necessitates a motion-oriented approach throughout the entire frame, leading to a minor performance trade-off. The superior quantitative results of our approach are likely attributable to its enhanced capability to suppress dominant color noise, which significantly influences PSNR and SSIM metrics.</p><p><xref rid="sensors-25-05567-f016" ref-type="fig">Figure 16</xref> presents a detailed view of the ground texture from Scene 1 of the CRVD dataset [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>].</p><p>To ensure consistency across experiments, the RAW data from CRVD [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>] were converted to the RGB color space using a standard linear transformation. This conversion follows well-established industrial imaging pipelines and may introduce minor color shifts, yet it does not significantly influence the denoising performance comparison. Since CRVD [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>] includes both ground-truth and noisy image pairs, no synthetic noise was introduced. Although the dataset involves a static camera setup consistent with our application scenario, it does not represent a low-light or complex-noise environment. In this setting, our method achieves the second-best performance, effectively preserving horizontal ground textures. While the visual smoothness of our result is slightly inferior to that of RCD [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>], our approach demonstrates significantly better noise suppression capability compared to both ShiftNet [<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>] and VBM4D [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>].</p><p><xref rid="sensors-25-05567-f017" ref-type="fig">Figure 17</xref> displays the wall texture from Scene 9, in which our method again achieves the second-best performance. It successfully preserves the fine white gaps between tiles while providing effective noise removal. In summary, while our algorithm is specifically optimized for denoising low-light surveillance videos with complex real-world noise, its competitive performance on general benchmark datasets demonstrates that it is not narrowly specialized. These results indicate that our approach maintains strong generalization capability across different scenes and noise characteristics.</p></sec></sec><sec id="sec5-sensors-25-05567"><title>5. Model Analysis</title><sec id="sec5dot1-sensors-25-05567"><title>5.1. Why Low-Light Environment Is Extremely Harsh?</title><p>In the traditional denoising tasks, Gaussian noise is artificially added to an image. Usually, the variance is used to measure the noise complexity, with larger variances indicating more complex noise. The variance of Gaussian noise is set as 15, 25, 30, 50, 75, etc. Gaussian noise with a variance of 75 is considered to have a complex distribution. However, noise in the real world is far more complex than Gaussian noise, and a low-light environment intensifies this complexity. Our 14 video sequences are divided into different RGB channels, and then the mean-variance of noise in each channel is calculated, and <xref rid="sensors-25-05567-f018" ref-type="fig">Figure 18</xref> and <xref rid="sensors-25-05567-f019" ref-type="fig">Figure 19</xref> show the statistical results. In <xref rid="sensors-25-05567-f018" ref-type="fig">Figure 18</xref>, the mean variance of noise in each color channel is different, and all the mean variances are much larger than 75. In <xref rid="sensors-25-05567-f019" ref-type="fig">Figure 19</xref>, noise varies from frame to frame, and the variation is almost random. In other words, the noise in such an environment is constantly changing spatially and temporally, which undoubtedly increases the denoising difficulty.</p></sec><sec id="sec5dot2-sensors-25-05567"><title>5.2. Why Our Model Can Work?</title><statement><label><bold>Lemma</bold>&#160;<bold>1.</bold></label><p><italic toggle="yes">Matrix</italic>&#160;<bold>D</bold><italic toggle="yes">&#160;is made up of K random values <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and </italic>&#160;<bold>D</bold>&#160;<italic toggle="yes">can be decomposed into the sum of L symmetric matrices without considering the zero values in </italic>&#160;<bold>D</bold><italic toggle="yes">, namely </italic>&#160;<bold>D</bold>&#160;<italic toggle="yes">= <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#8230;</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</italic></p></statement><statement><label><bold>Proof.</bold>&#160;</label><p>
<list list-type="simple"><list-item><p><italic toggle="yes">Case 1:</italic> If <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, we set <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mover><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>&#65079;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>&#65079;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mover><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm169" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; thus, <inline-formula><mml:math id="mm170" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and the proof ends.</p></list-item><list-item><p><italic toggle="yes">Case 2:</italic> If <inline-formula><mml:math id="mm171" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, a technique called mathematical induction is used, and this technique consists of three steps.</p></list-item></list>
<list list-type="bullet"><list-item><p>Step 1: When number <inline-formula><mml:math id="mm172" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, we set <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm174" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which meets the lemma.</p></list-item><list-item><p>Step 2: When number <inline-formula><mml:math id="mm175" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and if the lemma is satisfied, that is, <inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm177" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Step 3: Based on step 2, and when number <inline-formula><mml:math id="mm178" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm179" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We set <inline-formula><mml:math id="mm180" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mover><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>&#65079;</mml:mo></mml:mover></mml:mrow><mml:mi>K</mml:mi></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>&#8658;</mml:mo><mml:mi mathvariant="bold">D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>&#8943;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm181" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; thus, the lemma holds when <inline-formula><mml:math id="mm182" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> if it holds when <inline-formula><mml:math id="mm183" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. By the property of mathematical induction, this lemma holds for all natural numbers.</p></list-item></list>
</p><p>For instance, if <inline-formula><mml:math id="mm184" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, three symmetric matrices, namely <inline-formula><mml:math id="mm185" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm186" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm187" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, can be found to support the lemma, in which <inline-formula><mml:math id="mm188" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">D</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm189" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm190" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. &#9633;</p></statement><p>Suppose noise is <inline-formula><mml:math id="mm191" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">N</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and a part of <bold>N</bold> is selected, namely the red signal <inline-formula><mml:math id="mm192" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm193" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mn>3.9</mml:mn><mml:mo>,</mml:mo><mml:mn>0.4</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in <xref rid="sensors-25-05567-f020" ref-type="fig">Figure 20</xref>. Based on the lemma above, <inline-formula><mml:math id="mm194" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> can be theoretically decomposed into the sum of several symmetrical signals, and symmetrical signals can be fitted with different Gaussian signals approximately. In this case, <inline-formula><mml:math id="mm195" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, in which <inline-formula><mml:math id="mm196" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm197" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm198" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mn>3.9</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm199" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For example, <inline-formula><mml:math id="mm200" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> can be fitted by Gaussian distribution <inline-formula><mml:math id="mm201" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>0.18</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Suppose the noise in frame <italic toggle="yes">i</italic> is <inline-formula><mml:math id="mm202" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and its variance is <inline-formula><mml:math id="mm203" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In our experiment, we average 10 adjacent frames; thus, the denoisied data are <inline-formula><mml:math id="mm204" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac></mml:mstyle><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>10</mml:mn></mml:msubsup><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. By the property of the Gaussian distribution, namely <inline-formula><mml:math id="mm205" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#945;</mml:mi><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi mathvariant="double-struck">D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the variance of <italic toggle="yes">X</italic> is <inline-formula><mml:math id="mm206" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>100</mml:mn></mml:mfrac></mml:mstyle><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>10</mml:mn></mml:msubsup><mml:mi mathvariant="double-struck">D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm207" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is a constant. When averaging 10 adjacent frames, the noise variance of each frame is reduced by a factor of 100. Assuming that the mean variance of noise in each channel is 300, 400, and 900, after averaging, the mean variance of noise changes to 3, 4, and 9, which greatly weakens the impact of noise.</p></sec><sec id="sec5dot3-sensors-25-05567"><title>5.3. Why Is Our Model Fast?</title><p>Our algorithm is very fast due to its complexity of <inline-formula><mml:math id="mm208" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">k</italic> is a constant, and <italic toggle="yes">N</italic> is the block number.</p><p>Our algorithm consists of four parts. In the first part, the search exhausts all the steps, and the search radius is <inline-formula><mml:math id="mm209" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. However, the search method is not a full search; thus, the search scope is <inline-formula><mml:math id="mm210" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, and the total steps in the first part are <inline-formula><mml:math id="mm211" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The second part is vector updating. For each vector, Equation (<xref rid="FD3-sensors-25-05567" ref-type="disp-formula">3</xref>) consumes <inline-formula><mml:math id="mm212" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>16</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> steps, and Equations (<xref rid="FD4-sensors-25-05567" ref-type="disp-formula">4</xref>) and (<xref rid="FD5-sensors-25-05567" ref-type="disp-formula">5</xref>) consume <inline-formula><mml:math id="mm213" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> steps. Since the updating number is <inline-formula><mml:math id="mm214" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, therefore, the maximum steps in the second part are <inline-formula><mml:math id="mm215" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>19</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The third part is a full search and the search area is <inline-formula><mml:math id="mm216" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm217" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the small search window size, so the total steps in the third part is <inline-formula><mml:math id="mm218" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The fourth part is the denoising. In this part, the vectors are classified into three categories, and their numbers are <inline-formula><mml:math id="mm219" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm220" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm221" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="mm222" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For type 1 vectors, the system consumes <inline-formula><mml:math id="mm223" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> steps. For type 2 vectors, the system consumes <inline-formula><mml:math id="mm224" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>28</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> steps. For type 3 vectors, the system consumes <inline-formula><mml:math id="mm225" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> steps, where <inline-formula><mml:math id="mm226" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is short for big search window size. The total steps in the fourth part are <inline-formula><mml:math id="mm227" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>28</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>10</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>N</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. For simplicity, the upper bound <inline-formula><mml:math id="mm228" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is used as the total step count.</p><p>To sum up, the total steps are <inline-formula><mml:math id="mm229" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>19</mml:mn><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>B</mml:mi><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; thus, the complexity of our system is <inline-formula><mml:math id="mm230" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="sec6-sensors-25-05567"><title>6. Ablation Study</title><sec id="sec6dot1-sensors-25-05567"><title>6.1. Ablation on the Loop Count <inline-formula><mml:math id="mm231" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as the Stopping Criterion</title><p>In our method, <inline-formula><mml:math id="mm232" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of loops, and a larger <inline-formula><mml:math id="mm233" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> results in more elements equaling 1 in the rationality matrix while also increasing the running time. Thus, <inline-formula><mml:math id="mm234" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> needs to be set appropriately to balance efficiency. The sum of the rationality matrix, namely <inline-formula><mml:math id="mm235" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is defined as its value. It can be observed that the increment in this matrix value gradually decreases with the increase of <inline-formula><mml:math id="mm236" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In <xref rid="sensors-25-05567-f021" ref-type="fig">Figure 21</xref>, different frames are sampled, and their increment curves nearly coincide. Due to the randomness of sampling, the distribution demonstrates strong statistical significance. It is clear that when <inline-formula><mml:math id="mm237" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> exceeds 15, it is difficult to bring obvious increment to the rationality matrix; thus, <inline-formula><mml:math id="mm238" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is set as 15.</p></sec><sec id="sec6dot2-sensors-25-05567"><title>6.2. Ablation on <inline-formula><mml:math id="mm239" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>, the Proportion of Reasonable Vectors for Stopping</title><p>In the experiment, block size <inline-formula><mml:math id="mm240" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 16. Therefore, if the all motion vectors are reasonable, the number of element 1 (<inline-formula><mml:math id="mm241" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) in the rationality matrix is <inline-formula><mml:math id="mm242" overflow="scroll"><mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1920</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1072</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>16</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>16</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mn>8040</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. However, it is unrealistic for all motion vectors to be reasonable due to motion complexity. Some motion vectors converge to a stable value after many loop times (<inline-formula><mml:math id="mm243" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). <inline-formula><mml:math id="mm244" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is defined as <inline-formula><mml:math id="mm245" overflow="scroll"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>8040</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula>, and <xref rid="sensors-25-05567-f022" ref-type="fig">Figure 22</xref> shows the relationship between <inline-formula><mml:math id="mm246" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm247" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula>. Considering the convergence of <inline-formula><mml:math id="mm248" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the time complexity of the system, parameter <inline-formula><mml:math id="mm249" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> is set as 0.96.</p></sec><sec id="sec6dot3-sensors-25-05567"><title>6.3. Ablation on Search Window Size for Search Scope</title><p>The search window size controls the block matching range. Theoretically, a larger size increases the likelihood of matching similar blocks and improving performance. In our experiments, we varied sizes from 2 to 10 in steps of 2 and evaluated them using the PSNR. <xref rid="sensors-25-05567-f023" ref-type="fig">Figure 23</xref> shows that the PSNR gains plateau beyond size 6. However, a larger range leads to increased computational and time costs. Thus, we selected 6 as the optimal balance point for the search window size.</p></sec><sec id="sec6dot4-sensors-25-05567"><title>6.4. Ablation on <inline-formula><mml:math id="mm250" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm251" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> Used as Phase Clamping Thresholds in Amplitude Phase Filter</title><p><inline-formula><mml:math id="mm252" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm253" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are parameters defined in Equation (<xref rid="FD4-sensors-25-05567" ref-type="disp-formula">4</xref>). As visualized in <xref rid="sensors-25-05567-f024" ref-type="fig">Figure 24</xref>, the sum of the rationality matrix, denoted as <inline-formula><mml:math id="mm254" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo mathsize="85%">&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, increases with higher values of <inline-formula><mml:math id="mm255" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm256" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and it eventually converges within a specific range (<inline-formula><mml:math id="mm257" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>44</mml:mn><mml:mo>&#176;</mml:mo><mml:mo>&#8804;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>&#8804;</mml:mo><mml:mn>48</mml:mn><mml:mo>&#176;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm258" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>29</mml:mn><mml:mo>&#176;</mml:mo><mml:mo>&#8804;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>min</mml:mi></mml:msub><mml:mo>&#8804;</mml:mo><mml:mn>35</mml:mn><mml:mo>&#176;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). In this paper, the values of <inline-formula><mml:math id="mm259" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm260" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>min</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are set to 45&#176; and 30&#176;, respectively. This configuration not only ensures the rationality of the matrix but also maintains stable system runtime performance.</p></sec><sec id="sec6dot5-sensors-25-05567"><title>6.5. Ablation on Comparing Motion Estimation Methods Used for Denoising</title><p>Our algorithm mainly consists of motion estimation and denoising. Consider two adjacent frames in the video, which are denoted as <inline-formula><mml:math id="mm261" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm262" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The estimated motion vectors are used to artificially insert a new frame <inline-formula><mml:math id="mm263" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between <inline-formula><mml:math id="mm264" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm265" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. This frame <inline-formula><mml:math id="mm266" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is generated algorithmically to improve video smoothness, and its quality depends on the accuracy of motion estimation, thus serving as a means to evaluate motion estimation performance. For example, if the first frame <inline-formula><mml:math id="mm267" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and the third frame <inline-formula><mml:math id="mm268" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are used to interpolate the second frame <inline-formula><mml:math id="mm269" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, comparing <inline-formula><mml:math id="mm270" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> with the ground truth <inline-formula><mml:math id="mm271" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> can measure the validity of the estimated motion vectors.</p><p>Several motion estimation algorithms are compared, such as BME (Bidirectional Motion Estimation) [<xref rid="B47-sensors-25-05567" ref-type="bibr">47</xref>], FBJME (Forward&#8211;Backward Joint Motion Estimation) [<xref rid="B56-sensors-25-05567" ref-type="bibr">56</xref>], DME (Dual Motion Estimation) [<xref rid="B57-sensors-25-05567" ref-type="bibr">57</xref>], DSME (Direction-Select Motion Estimation) [<xref rid="B58-sensors-25-05567" ref-type="bibr">58</xref>], and LQME (Linear Quadratic Motion Estimation) [<xref rid="B59-sensors-25-05567" ref-type="bibr">59</xref>]. PSNR and SSIM are adopted as evaluation metrics to quantify the accuracy of motion vectors, and the dataset from the Experiments section is used. <xref rid="sensors-25-05567-t004" ref-type="table">Table 4</xref> shows that our method achieves first or second place in most cases, indirectly indicating that our motion estimation algorithm is effective and reasonable.</p></sec><sec id="sec6dot6-sensors-25-05567"><title>6.6. Ablation on How Component Changes Affect Speed</title><p>Our algorithm mainly includes four steps, namely coarse motion estimation, vector updating, vector refinement, and denoising. If step 1 is omitted, steps 2 and 3 cannot be performed. Similarly, step 4 is related to denoising, and omitting it would render the entire method ineffective. Therefore, the ablation study can only be conducted on steps 2 and 3 with four cases to consider.</p><p><xref rid="sensors-25-05567-t005" ref-type="table">Table 5</xref> shows the results of the ablation study. In the experiment, only stationary sequences are used. Since no motion exists, omitting step 2 or step 3 has little impact on PSNR or SSIM values. For video sequences containing moving objects, only speed ablation experiments can be performed due to the lack of <italic toggle="yes">ground truth</italic>.</p></sec></sec><sec id="sec7-sensors-25-05567"><title>7. Limitations</title><p>Although our method achieves competitive results, it also has certain limitations. The first is that it is less effective at removing temporally correlated noise, such as certain types of Gaussian noise where the disturbance in each frame is nearly identical. Suppose the Gaussian noise in frame <italic toggle="yes">i</italic> is <inline-formula><mml:math id="mm272" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm273" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8776;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm274" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8800;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. When 10 adjacent frames are averaged, the noise variance <inline-formula><mml:math id="mm275" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">D</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac></mml:mstyle><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>10</mml:mn></mml:msubsup><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>&#8776;</mml:mo><mml:mi mathvariant="double-struck">D</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>10</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Therefore, the averaging operation cannot suppress such noise. However, when the noise varies temporally, namely <inline-formula><mml:math id="mm276" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8800;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm277" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8800;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, such as a in low-light environment, the averaging operation can suppress the noise. Second, it is difficult to capture some extremely complex or isolated motions, as the search radius in our method is finite. However, these motions are rare in the real world, justifying our decision to limit the search radius to a reasonable range.</p></sec><sec sec-type="conclusions" id="sec8-sensors-25-05567"><title>8. Conclusions</title><p>Denoising low-light surveillance video presents a formidable challenge, stemming from the severe and complex noise patterns inherent in such conditions. The core difficulty lies in the critical trade-off between effectively suppressing intense noise and simultaneously preserving fine-grained texture details. Achieving this intricate balance is paramount for practical surveillance applications, as failure to do so can result in the loss of crucial visual information. In this paper, we proposed a tracking-based denoising algorithm designed for surveillance videos captured in extremely low-light environments. Our algorithm integrates coarse motion estimation via bilateral search for initial vector accuracy, motion vector updating using an amplitude-phase filter and rationality matrix to ensure local continuity, motion vector refinement with a small-area full search to achieve pixel-level precision without excessive computational cost, and a trilateral filter that combines spatial, pixel, and gradient domain information to effectively suppress noise by classifying motion vectors into three types and applying adaptive denoising strategies. Our method can effectively suppress noise while retaining detailed information, as demonstrated by extensive quantitative experiments and visual comparisons. In the future, we intend to explore other weighting dimensions for the trilateral filter, such as temporal consistency and texture complexity. Furthermore, we aim to replace the rigid three-category block classification with a more flexible, adaptive mechanism that allows for a smoother transition between categories.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, H.J.; methodology, H.J.; validation, P.W.; formal analysis, P.W. and C.L.; writing&#8212;original draft preparation, H.J. and P.W.; writing&#8212;review and editing, P.W. and Z.Z.; visualization, P.W., H.G. and F.Y.; supervision, H.J., C.L. and W.C.; project administration, H.J.; funding acquisition, H.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05567"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Malyugina</surname><given-names>A.</given-names></name><name name-style="western"><surname>Anantrasirichai</surname><given-names>N.</given-names></name><name name-style="western"><surname>Bull</surname><given-names>D.</given-names></name></person-group><article-title>Wavelet-based topological loss for low-light image denoising</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>2047</elocation-id><pub-id pub-id-type="doi">10.3390/s25072047</pub-id><pub-id pub-id-type="pmid">40218560</pub-id><pub-id pub-id-type="pmcid">PMC11990961</pub-id></element-citation></ref><ref id="B2-sensors-25-05567"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Q.</given-names></name></person-group><article-title>Guided filter-inspired network for low-light RAW image enhancement</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>2637</elocation-id><pub-id pub-id-type="doi">10.3390/s25092637</pub-id><pub-id pub-id-type="pmid">40363077</pub-id><pub-id pub-id-type="pmcid">PMC12074441</pub-id></element-citation></ref><ref id="B3-sensors-25-05567"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>S.-Y.</given-names></name><name name-style="western"><surname>Rhee</surname><given-names>C.E.</given-names></name></person-group><article-title>Motion estimation-assisted denoising for an efficient combination with an HEVC encoder</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>895</elocation-id><pub-id pub-id-type="doi">10.3390/s19040895</pub-id><pub-id pub-id-type="pmid">30795517</pub-id><pub-id pub-id-type="pmcid">PMC6412397</pub-id></element-citation></ref><ref id="B4-sensors-25-05567"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Low-light raw video denoising with a high-quality realistic motion dataset</article-title><source>IEEE Trans. Multimedia</source><year>2023</year><volume>25</volume><fpage>8119</fpage><lpage>8131</lpage><pub-id pub-id-type="doi">10.1109/TMM.2022.3233247</pub-id></element-citation></ref><ref id="B5-sensors-25-05567"><label>5.</label><element-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Im</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pak</surname><given-names>J.</given-names></name><name name-style="western"><surname>Na</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ryu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Moon</surname><given-names>S.</given-names></name><name name-style="western"><surname>Koo</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kang</surname><given-names>S.-J.</given-names></name></person-group><article-title>Supervised denoising for extreme low-light raw videos</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2025</year><comment><italic toggle="yes">in press</italic></comment><pub-id pub-id-type="doi">10.1109/tcsvt.2025.3572547</pub-id></element-citation></ref><ref id="B6-sensors-25-05567"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yamamoto</surname><given-names>H.</given-names></name><name name-style="western"><surname>Anami</surname><given-names>S.</given-names></name><name name-style="western"><surname>Matsuoka</surname><given-names>R.</given-names></name></person-group><article-title>Optimizing dynamic mode decomposition for video denoising via plug-and-play alternating direction method of multipliers</article-title><source>Signals</source><year>2024</year><volume>5</volume><fpage>202</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.3390/signals5020011</pub-id></element-citation></ref><ref id="B7-sensors-25-05567"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dabov</surname><given-names>K.</given-names></name><name name-style="western"><surname>Foi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Katkovnik</surname><given-names>V.</given-names></name><name name-style="western"><surname>Egiazarian</surname><given-names>K.</given-names></name></person-group><article-title>Image Denoising with Block-Matching and 3D Filtering</article-title><source>Proceedings of the SPIE Electronic Imaging 2006: Image Processing</source><conf-loc>San Jose, CA, USA</conf-loc><conf-date>15&#8211;19 January 2006</conf-date><pub-id pub-id-type="doi">10.1117/12.643267</pub-id></element-citation></ref><ref id="B8-sensors-25-05567"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title>Guided image filtering</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2013</year><volume>35</volume><fpage>1397</fpage><lpage>1409</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.213</pub-id><pub-id pub-id-type="pmid">23599054</pub-id></element-citation></ref><ref id="B9-sensors-25-05567"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chan</surname><given-names>T.W.</given-names></name><name name-style="western"><surname>Au</surname><given-names>O.C.</given-names></name><name name-style="western"><surname>Chong</surname><given-names>T.S.</given-names></name><name name-style="western"><surname>Chau</surname><given-names>W.S.</given-names></name></person-group><article-title>A Novel Content-Adaptive Video Denoising Filter</article-title><source>Proceedings of the 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>18&#8211;23 March 2005</conf-date><fpage>649</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2005.1415488</pub-id></element-citation></ref><ref id="B10-sensors-25-05567"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Selesnick</surname><given-names>I.W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.Y.</given-names></name></person-group><article-title>Video denoising using 2D and 3D dual-tree complex wavelet transforms</article-title><source>Proceedings of the Wavelets: Applications in Signal and Image Processing X</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>13 November 2003</conf-date><fpage>607</fpage><lpage>618</lpage><pub-id pub-id-type="doi">10.1117/12.504896</pub-id></element-citation></ref><ref id="B11-sensors-25-05567"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jovanov</surname><given-names>L.</given-names></name><name name-style="western"><surname>Pizurica</surname><given-names>A.</given-names></name><name name-style="western"><surname>Schulte</surname><given-names>S.</given-names></name><name name-style="western"><surname>Schelkens</surname><given-names>P.</given-names></name><name name-style="western"><surname>Munteanu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kerre</surname><given-names>E.</given-names></name><name name-style="western"><surname>Philips</surname><given-names>W.</given-names></name></person-group><article-title>Combined Wavelet-Domain and Motion-Compensated Video Denoising Based on Video Codec Motion Estimation Methods</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2009</year><volume>19</volume><fpage>417</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2009.2013491</pub-id></element-citation></ref><ref id="B12-sensors-25-05567"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dugad</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ahuja</surname><given-names>N.</given-names></name></person-group><article-title>Video Denoising by Combining Kalman and Wiener Estimates</article-title><source>Proceedings of the 1999 International Conference on Image Processing</source><conf-loc>Kobe, Japan</conf-loc><conf-date>24&#8211;28 October 1999</conf-date><fpage>152</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1109/icip.1999.819568</pub-id></element-citation></ref><ref id="B13-sensors-25-05567"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buades</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lisani</surname><given-names>J.-L.</given-names></name><name name-style="western"><surname>Miladinovic</surname><given-names>M.</given-names></name></person-group><article-title>Patch-Based Video Denoising with Optical Flow Estimation</article-title><source>IEEE Trans. Image Process.</source><year>2016</year><volume>25</volume><fpage>2573</fpage><lpage>2586</lpage><pub-id pub-id-type="doi">10.1109/TIP.2016.2551639</pub-id><pub-id pub-id-type="pmid">27071173</pub-id></element-citation></ref><ref id="B14-sensors-25-05567"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dabov</surname><given-names>K.</given-names></name><name name-style="western"><surname>Foi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Katkovnik</surname><given-names>V.</given-names></name><name name-style="western"><surname>Egiazarian</surname><given-names>K.</given-names></name></person-group><article-title>Image Restoration by Sparse 3D Transform-Domain Collaborative Filtering</article-title><source>Proceedings of the SPIE Electronic Imaging 2008: Image Processing: Algorithms and Systems VI</source><conf-loc>San Jose, CA, USA</conf-loc><conf-date>27&#8211;31 January 2008</conf-date><pub-id pub-id-type="doi">10.1117/12.766355</pub-id></element-citation></ref><ref id="B15-sensors-25-05567"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maggioni</surname><given-names>M.</given-names></name><name name-style="western"><surname>Boracchi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Foi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Egiazarian</surname><given-names>K.</given-names></name></person-group><article-title>Video Denoising, Deblocking, and Enhancement Through Separable 4-D Nonlocal Spatiotemporal Transforms</article-title><source>IEEE Trans. Image Process.</source><year>2012</year><volume>21</volume><fpage>3952</fpage><lpage>3966</lpage><pub-id pub-id-type="doi">10.1109/TIP.2012.2199324</pub-id><pub-id pub-id-type="pmid">22614644</pub-id></element-citation></ref><ref id="B16-sensors-25-05567"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arias</surname><given-names>P.</given-names></name><name name-style="western"><surname>Morel</surname><given-names>J.-M.</given-names></name></person-group><article-title>Video Denoising via Empirical Bayesian Estimation of Space-Time Patches</article-title><source>J. Math. Imaging Vis.</source><year>2017</year><volume>60</volume><fpage>70</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1007/s10851-017-0742-4</pub-id></element-citation></ref><ref id="B17-sensors-25-05567"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaksman</surname><given-names>G.</given-names></name><name name-style="western"><surname>Elad</surname><given-names>M.</given-names></name><name name-style="western"><surname>Milanfar</surname><given-names>P.</given-names></name></person-group><article-title>Patch Craft: Video Denoising by Deep Modeling and Patch Matching</article-title><source>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>2137</fpage><lpage>2146</lpage><pub-id pub-id-type="doi">10.1109/iccv48922.2021.00216</pub-id></element-citation></ref><ref id="B18-sensors-25-05567"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Davy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ehret</surname><given-names>T.</given-names></name><name name-style="western"><surname>Morel</surname><given-names>J.-M.</given-names></name><name name-style="western"><surname>Arias</surname><given-names>P.</given-names></name><name name-style="western"><surname>Facciolo</surname><given-names>G.</given-names></name></person-group><article-title>A Non-Local CNN for Video Denoising</article-title><source>Proceedings of the 2019 IEEE International Conference on Image Processing</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>22&#8211;25 September 2019</conf-date><fpage>2409</fpage><lpage>2413</lpage><pub-id pub-id-type="doi">10.1109/icip.2019.8803314</pub-id></element-citation></ref><ref id="B19-sensors-25-05567"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Davy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ehret</surname><given-names>T.</given-names></name><name name-style="western"><surname>Morel</surname><given-names>J.-M.</given-names></name><name name-style="western"><surname>Arias</surname><given-names>P.</given-names></name><name name-style="western"><surname>Facciolo</surname><given-names>G.</given-names></name></person-group><article-title>Video Denoising by Combining Patch Search and CNNs</article-title><source>J. Math. Imaging Vis.</source><year>2020</year><volume>63</volume><fpage>73</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1007/s10851-020-00995-0</pub-id></element-citation></ref><ref id="B20-sensors-25-05567"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name></person-group><article-title>Recursive Video Denoising Algorithm for Low Light Surveillance Applications</article-title><source>Proceedings of the 2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics</source><conf-loc>Shanghai, China</conf-loc><conf-date>22&#8211;24 October 2021</conf-date><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/cisp-bmei53629.2021.9624462</pub-id></element-citation></ref><ref id="B21-sensors-25-05567"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>M.</given-names></name><name name-style="western"><surname>Park</surname><given-names>D.</given-names></name><name name-style="western"><surname>Han</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ko</surname><given-names>H.</given-names></name></person-group><article-title>A Novel Approach for Denoising and Enhancement of Extremely Low-Light Video</article-title><source>IEEE Trans. Consum. Electron.</source><year>2015</year><volume>61</volume><fpage>72</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1109/TCE.2015.7064113</pub-id></element-citation></ref><ref id="B22-sensors-25-05567"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sutskever</surname><given-names>I.</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><article-title>ImageNet classification with deep convolutional neural networks</article-title><source>Commun. ACM</source><year>2017</year><volume>60</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="B23-sensors-25-05567"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tassano</surname><given-names>M.</given-names></name><name name-style="western"><surname>Delon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Veit</surname><given-names>T.</given-names></name></person-group><article-title>DVDNET: A Fast Network for Deep Video Denoising</article-title><source>Proceedings of the 2019 IEEE International Conference on Image Processing</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>22&#8211;25 September 2019</conf-date><fpage>1805</fpage><lpage>1809</lpage><pub-id pub-id-type="doi">10.1109/icip.2019.8803136</pub-id></element-citation></ref><ref id="B24-sensors-25-05567"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tassano</surname><given-names>M.</given-names></name><name name-style="western"><surname>Delon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Veit</surname><given-names>T.</given-names></name></person-group><article-title>FastDVDnet: Towards real-time deep video denoising without flow estimation</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>1354</fpage><lpage>1363</lpage><pub-id pub-id-type="doi">10.1109/cvpr42600.2020.00144</pub-id></element-citation></ref><ref id="B25-sensors-25-05567"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sheth</surname><given-names>D.Y.</given-names></name><name name-style="western"><surname>Mohan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Vincent</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Manzorro</surname><given-names>R.</given-names></name><name name-style="western"><surname>Crozier</surname><given-names>P.A.</given-names></name><name name-style="western"><surname>Khapra</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>E.P.</given-names></name><name name-style="western"><surname>Fernandez-Granda</surname><given-names>C.</given-names></name></person-group><article-title>Unsupervised Deep Video Denoising</article-title><source>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>1739</fpage><lpage>1748</lpage><pub-id pub-id-type="doi">10.1109/iccv48922.2021.00178</pub-id></element-citation></ref><ref id="B26-sensors-25-05567"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name></person-group><article-title>Real-time Streaming Video Denoising with Bidirectional Buffers</article-title><source>Proceedings of the 30th ACM International Conference on Multimedia</source><conf-loc>Lisbon, Portugal</conf-loc><conf-date>10&#8211;14 October 2022</conf-date><fpage>2758</fpage><lpage>2766</lpage><pub-id pub-id-type="doi">10.1145/3503161.3547934</pub-id></element-citation></ref><ref id="B27-sensors-25-05567"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>P.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name></person-group><article-title>Real-Time Controllable Denoising for Image and Video</article-title><source>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date><fpage>14028</fpage><lpage>14038</lpage><pub-id pub-id-type="doi">10.1109/CVPR52729.2023.01348</pub-id></element-citation></ref><ref id="B28-sensors-25-05567"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cheung</surname><given-names>K.C.</given-names></name><name name-style="western"><surname>See</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>A Simple Baseline for Video Restoration with Grouped Spatial-Temporal Shift</article-title><source>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date><fpage>9822</fpage><lpage>9832</lpage><pub-id pub-id-type="doi">10.1109/cvpr52729.2023.00947</pub-id></element-citation></ref><ref id="B29-sensors-25-05567"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Bian</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cheung</surname><given-names>K.C.</given-names></name><name name-style="western"><surname>See</surname><given-names>S.</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation</article-title><source>Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>12435</fpage><lpage>12446</lpage><pub-id pub-id-type="doi">10.1109/iccv51070.2023.01146</pub-id></element-citation></ref><ref id="B30-sensors-25-05567"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name></person-group><article-title>Spatiotemporal Blind-Spot Network with Calibrated Flow Alignment for Self-Supervised Video Denoising</article-title><source>Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>25 February&#8211;4 March 2025</conf-date><fpage>2411</fpage><lpage>2419</lpage><pub-id pub-id-type="doi">10.1609/aaai.v39i3.32242</pub-id></element-citation></ref><ref id="B31-sensors-25-05567"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>K.C.K.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>C.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name></person-group><article-title>EDVR: Video Restoration With Enhanced Deformable Convolutional Networks</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>16&#8211;20 June 2019</conf-date><pub-id pub-id-type="doi">10.1109/cvprw.2019.00247</pub-id></element-citation></ref><ref id="B32-sensors-25-05567"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zuo</surname><given-names>W.</given-names></name></person-group><article-title>Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-Ahead Forward Ones</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><fpage>592</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-19797-0_34</pub-id></element-citation></ref><ref id="B33-sensors-25-05567"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Song</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>Deep RNNs for Video Denoising</article-title><source>Proceedings of the Applications of Digital Image Processing XXXIX</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>28 August&#8211;1 September 2016</conf-date><pub-id pub-id-type="doi">10.1117/12.2239260</pub-id></element-citation></ref><ref id="B34-sensors-25-05567"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name></person-group><article-title>Versatile recurrent neural network for wide types of video restoration</article-title><source>Pattern Recognit.</source><year>2023</year><volume>138</volume><fpage>109360</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2023.109360</pub-id></element-citation></ref><ref id="B35-sensors-25-05567"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Maggioni</surname><given-names>M.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Song</surname><given-names>F.</given-names></name></person-group><article-title>Efficient Multi-Stage Video Denoising with Recurrent Spatio-Temporal Fusion</article-title><source>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>19&#8211;25 June 2021</conf-date><fpage>3465</fpage><lpage>3474</lpage><pub-id pub-id-type="doi">10.1109/cvpr46437.2021.00347</pub-id></element-citation></ref><ref id="B36-sensors-25-05567"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ranjan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ilg</surname><given-names>E.</given-names></name><name name-style="western"><surname>Green</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Timofte</surname><given-names>R.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>Recurrent video restoration transformer with guided deformable attention</article-title><source>Proceedings of the 36th Conference on Neural Information Processing Systems</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>28 November&#8211;9 December 2022</conf-date><fpage>378</fpage><lpage>393</lpage></element-citation></ref><ref id="B37-sensors-25-05567"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yue</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2025</year><volume>1</volume><pub-id pub-id-type="doi">10.1109/TCSVT.2025.3553160</pub-id></element-citation></ref><ref id="B38-sensors-25-05567"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Aiyetigbo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ravichandran</surname><given-names>D.</given-names></name><name name-style="western"><surname>Chalhoub</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kalivas</surname><given-names>P.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>N.</given-names></name></person-group><article-title>Unsupervised Coordinate-Based Video Denoising</article-title><source>Proceedings of the 2024 IEEE International Conference on Image Processing</source><conf-loc>Abu Dhabi, United Arab Emirates</conf-loc><conf-date>27&#8211;30 October 2024</conf-date><fpage>1438</fpage><lpage>1444</lpage><pub-id pub-id-type="doi">10.1109/icip51287.2024.10647922</pub-id></element-citation></ref><ref id="B39-sensors-25-05567"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>B.</given-names></name></person-group><article-title>Temporal As a Plugin: Unsupervised Video Denoising with Pre-trained Image Denoisers</article-title><source>Proceedings of the European Conference on Computer Vision. Cham: Springer Nature Switzerland</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date><fpage>349</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-72992-8_20</pub-id></element-citation></ref><ref id="B40-sensors-25-05567"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>H.</given-names></name></person-group><article-title>Unsupervised Deep Video Denoising with Untrained Network</article-title><source>Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>7&#8211;14 February 2023</conf-date><fpage>3651</fpage><lpage>3659</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i3.25476</pub-id></element-citation></ref><ref id="B41-sensors-25-05567"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Laine</surname><given-names>S.</given-names></name><name name-style="western"><surname>Karras</surname><given-names>T.</given-names></name><name name-style="western"><surname>Lehtinen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Aila</surname><given-names>T.</given-names></name></person-group><article-title>High-Quality Self-Supervised Deep Image Denoising</article-title><source>Proceedings of the 33rd Conference on Neural Information Processing Systems</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>8&#8211;14 December 2019</conf-date><fpage>6966</fpage><lpage>6976</lpage></element-citation></ref><ref id="B42-sensors-25-05567"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ghasemabadi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Janjua</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Salameh</surname><given-names>M.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>D.</given-names></name></person-group><article-title>Learning Truncated Causal History Model for Video Restoration</article-title><source>Proceedings of the 38th Conference on Neural Information Processing Systems</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>9&#8211;15 December 2024</conf-date><fpage>27584</fpage><lpage>27615</lpage></element-citation></ref><ref id="B43-sensors-25-05567"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ling</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>E.</given-names></name></person-group><article-title>Masked Video Pretraining Advances Real-World Video Denoising</article-title><source>IEEE Trans. Multimed.</source><year>2025</year><volume>27</volume><fpage>622</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1109/TMM.2024.3521818</pub-id></element-citation></ref><ref id="B44-sensors-25-05567"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dewil</surname><given-names>V.</given-names></name><name name-style="western"><surname>Anger</surname><given-names>J.</given-names></name><name name-style="western"><surname>Davy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ehret</surname><given-names>T.</given-names></name><name name-style="western"><surname>Facciolo</surname><given-names>G.</given-names></name><name name-style="western"><surname>Arias</surname><given-names>P.</given-names></name></person-group><article-title>Self-supervised Training for Blind Multi-Frame Video Denoising</article-title><source>Proceedings of the 2021 IEEE Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>5&#8211;9 January 2021</conf-date><fpage>2723</fpage><lpage>2733</lpage><pub-id pub-id-type="doi">10.1109/wacv48630.2021.00277</pub-id></element-citation></ref><ref id="B45-sensors-25-05567"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>T.H.</given-names></name></person-group><article-title>Restore from Restored: Video Restoration with Pseudo Clean Video</article-title><source>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>19&#8211;25 June 2021</conf-date><fpage>3536</fpage><lpage>3545</lpage><pub-id pub-id-type="doi">10.1109/cvpr46437.2021.00354</pub-id></element-citation></ref><ref id="B46-sensors-25-05567"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Geng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name></person-group><article-title>Denoising Real-World Low Light Surveillance Videos Based on Trilateral Filter</article-title><source>Proceedings of the 2nd International Conference on Internet of Things, Communication and Intelligent Technology</source><conf-loc>Xuzhou, China</conf-loc><conf-date>22&#8211;24 September 2023</conf-date><person-group person-group-type="editor"><name name-style="western"><surname>Dong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>D.</given-names></name></person-group><comment>Lecture Notes in Electrical Engineering</comment><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2024</year><volume>Volume 1197</volume><fpage>602</fpage><lpage>615</lpage></element-citation></ref><ref id="B47-sensors-25-05567"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Choi</surname><given-names>B.T.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Ko</surname><given-names>S.J.</given-names></name></person-group><article-title>New Frame Rate Up-Conversion Using Bi-Directional Motion Estimation</article-title><source>IEEE Trans. Consumer Electron.</source><year>2000</year><volume>46</volume><fpage>603</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1109/30.883418</pub-id></element-citation></ref><ref id="B48-sensors-25-05567"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Z.</given-names></name></person-group><article-title>Motion-Compensated Frame Interpolation with Weighted Motion Estimation and Hierarchical Vector Refinement</article-title><source>Neurocomputing</source><year>2016</year><volume>181</volume><fpage>76</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2015.06.102</pub-id></element-citation></ref><ref id="B49-sensors-25-05567"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tomasi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Manduchi</surname><given-names>R.</given-names></name></person-group><article-title>Bilateral Filtering for Gray and Color Images</article-title><source>Proceedings of the Sixth International Conference on Computer Vision</source><conf-loc>Bombay, India</conf-loc><conf-date>4&#8211;7 January 1998</conf-date><fpage>839</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1109/iccv.1998.710815</pub-id></element-citation></ref><ref id="B50-sensors-25-05567"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paris</surname><given-names>S.</given-names></name><name name-style="western"><surname>Durand</surname><given-names>F.</given-names></name></person-group><article-title>A Fast Approximation of the Bilateral Filter Using a Signal Processing Approach</article-title><source>Int. J. Comput. Vis.</source><year>2007</year><volume>81</volume><fpage>24</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1007/s11263-007-0110-8</pub-id></element-citation></ref><ref id="B51-sensors-25-05567"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ardabilian</surname><given-names>M.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name></person-group><article-title>Depth Edge Based Trilateral Filter Method for Stereo Matching</article-title><source>Proceedings of the 2015 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Quebec City, QC, Canada</conf-loc><conf-date>27&#8211;30 September 2015</conf-date><fpage>2280</fpage><lpage>2284</lpage><pub-id pub-id-type="doi">10.1109/ICIP.2015.7351208</pub-id></element-citation></ref><ref id="B52-sensors-25-05567"><label>52.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Stoll</surname><given-names>M.</given-names></name><name name-style="western"><surname>Volz</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bruhn</surname><given-names>A.</given-names></name></person-group><article-title>Joint Trilateral Filtering for Multiframe Optical Flow</article-title><source>Proceedings of the 2013 IEEE International Conference on Image Processing</source><conf-loc>Melbourne, VIC, Australia</conf-loc><conf-date>15&#8211;18 September 2013</conf-date><fpage>3845</fpage><lpage>3849</lpage><pub-id pub-id-type="doi">10.1109/ICIP.2013.6738792</pub-id></element-citation></ref><ref id="B53-sensors-25-05567"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pont-Tuset</surname><given-names>J.</given-names></name><name name-style="western"><surname>Perazzi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Caelles</surname><given-names>S.</given-names></name><name name-style="western"><surname>Arbel&#225;ez</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sorkine-Hornung</surname><given-names>A.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>The 2017 DAVIS Challenge on Video Object Segmentation</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1704.00675</pub-id></element-citation></ref><ref id="B54-sensors-25-05567"><label>54.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yue</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>Supervised Raw Video Denoising With a Benchmark Dataset on Dynamic Scenes</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>2298</fpage><lpage>2307</lpage><pub-id pub-id-type="doi">10.1109/cvpr42600.2020.00237</pub-id></element-citation></ref><ref id="B55-sensors-25-05567"><label>55.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name></person-group><article-title>Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement</article-title><source>Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>12831</fpage><lpage>12840</lpage><pub-id pub-id-type="doi">10.1109/iccv51070.2023.01183</pub-id></element-citation></ref><ref id="B56-sensors-25-05567"><label>56.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vinh</surname><given-names>T.Q.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>Y.-C.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>S.-H.</given-names></name></person-group><article-title>Frame Rate Up-Conversion Using Forward-Backward Jointing Motion Estimation and Spatio-Temporal Motion Vector Smoothing</article-title><source>Proceedings of the 2009 International Conference on Computer Engineering &amp; Systems</source><conf-loc>Cairo, Egypt</conf-loc><conf-date>14&#8211;16 December 2009</conf-date><fpage>605</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1109/icces.2009.5383061</pub-id></element-citation></ref><ref id="B57-sensors-25-05567"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>S.J.</given-names></name><name name-style="western"><surname>Yoo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>Y.H.</given-names></name></person-group><article-title>Dual Motion Estimation for Frame Rate Up-Conversion</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2010</year><volume>20</volume><fpage>1909</fpage><lpage>1914</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2010.2087832</pub-id></element-citation></ref><ref id="B58-sensors-25-05567"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yoo</surname><given-names>D.G.</given-names></name><name name-style="western"><surname>Kang</surname><given-names>S.J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>Y.H.</given-names></name></person-group><article-title>Direction-Select Motion Estimation for Motion-Compensated Frame Rate Up-Conversion</article-title><source>J. Display Technol.</source><year>2013</year><volume>9</volume><fpage>840</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1109/jdt.2013.2263374</pub-id></element-citation></ref><ref id="B59-sensors-25-05567"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Frame Rate Up-Conversion Using Linear Quadratic Motion Estimation and Trilateral Filtering Motion Smoothing</article-title><source>J. Display Technol.</source><year>2015</year><volume>12</volume><fpage>89</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1109/JDT.2015.2466104</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05567-f002" orientation="portrait"><label>Figure 2</label><caption><p>Flowchart of our method. The process begins with coarse motion estimation to generate initial motion vectors, which are then iteratively updated and refined. Finally, a trilateral filter produces the clean frames.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g002.jpg"/></fig><fig position="float" id="sensors-25-05567-f003" orientation="portrait"><label>Figure 3</label><caption><p>Coarse motion estimation via bilateral search. The search areas in the previous and subsequent frames are positioned symmetrically with respect to the location of the current block.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g003.jpg"/></fig><fig position="float" id="sensors-25-05567-f004" orientation="portrait"><label>Figure 4</label><caption><p>The Amplitude-Phase Filtering process. The filter first computes the difference between each motion vector and the average of its eight neighbors. This difference is then thresholded to validate the vector&#8217;s reasonableness. Finally, an iterative median filter is applied to ensure spatial smoothness.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g004.jpg"/></fig><fig position="float" id="sensors-25-05567-f005" orientation="portrait"><label>Figure 5</label><caption><p>Motion vector refinement. A final adjustment is performed on the vectors using an exhaustive search within a small, local window.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g005.jpg"/></fig><fig position="float" id="sensors-25-05567-f006" orientation="portrait"><label>Figure 6</label><caption><p>Motion vectors classification. Each motion vector is classified into one of three types based on <inline-formula><mml:math id="mm278" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm279" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g006.jpg"/></fig><fig position="float" id="sensors-25-05567-f007" orientation="portrait"><label>Figure 7</label><caption><p>Qualitative comparison on a dynamic scene with moving barriers.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g007.jpg"/></fig><fig position="float" id="sensors-25-05567-f008" orientation="portrait"><label>Figure 8</label><caption><p>Qualitative comparison on a static scene with residential buildings.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g008.jpg"/></fig><fig position="float" id="sensors-25-05567-f009" orientation="portrait"><label>Figure 9</label><caption><p>Qualitative comparison on a dynamic scene with moving car.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g009.jpg"/></fig><fig position="float" id="sensors-25-05567-f010" orientation="portrait"><label>Figure 10</label><caption><p>Qualitative comparison on a static scene with static vehicle.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g010.jpg"/></fig><fig position="float" id="sensors-25-05567-f011" orientation="portrait"><label>Figure 11</label><caption><p>Qualitative comparison on a static scene with roadside staircase.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g011.jpg"/></fig><fig position="float" id="sensors-25-05567-f012" orientation="portrait"><label>Figure 12</label><caption><p>Qualitative comparison on a static scene with tree and street lamp.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g012.jpg"/></fig><fig position="float" id="sensors-25-05567-f013" orientation="portrait"><label>Figure 13</label><caption><p>One example of the intensity curve. Our method demonstrates the most stable and effective noise removal, as evidenced by its signal (green line) being in the closest alignment with the ground truth (black line).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g013.jpg"/></fig><fig position="float" id="sensors-25-05567-f014" orientation="portrait"><label>Figure 14</label><caption><p>Comparison of the orchid scene from the DAVIS [<xref rid="B53-sensors-25-05567" ref-type="bibr">53</xref>] dataset. Visual comparison of methods on a zoomed-in patch (left) with metrics; red box in the image (right) shows the patch location.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g014.jpg"/></fig><fig position="float" id="sensors-25-05567-f015" orientation="portrait"><label>Figure 15</label><caption><p>Comparison of the skate-jump scene from the DAVIS [<xref rid="B53-sensors-25-05567" ref-type="bibr">53</xref>] dataset. Visual comparison of methods on a zoomed-in patch (left) with metrics; red box in the image (right) shows the patch location.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g015.jpg"/></fig><fig position="float" id="sensors-25-05567-f016" orientation="portrait"><label>Figure 16</label><caption><p>Comparison of Scene 1 from the CRVD [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>] dataset. Visual comparison of methods on a zoomed-in patch (left) with metrics; red box in the image (right) shows the patch location.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g016.jpg"/></fig><fig position="float" id="sensors-25-05567-f017" orientation="portrait"><label>Figure 17</label><caption><p>Comparison of Scene 9 from the CRVD [<xref rid="B54-sensors-25-05567" ref-type="bibr">54</xref>] dataset. Visual comparison of methods on a zoomed-in patch (left) with metrics; red box in the image (right) shows the patch location.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g017.jpg"/></fig><fig position="float" id="sensors-25-05567-f018" orientation="portrait"><label>Figure 18</label><caption><p>The average variance of RGB channels across video sequences and noisy grayscale images confirms the extremely harsh noise in our low-light surveillance environment.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g018.jpg"/></fig><fig position="float" id="sensors-25-05567-f019" orientation="portrait"><label>Figure 19</label><caption><p>The average variances of RGB channels across frames in Video 2, demonstrating significant noise level fluctuations over time due to complex environmental conditions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g019.jpg"/></fig><fig position="float" id="sensors-25-05567-f020" orientation="portrait"><label>Figure 20</label><caption><p>The blue line represents an example of a noisy signal, <inline-formula><mml:math id="mm280" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow></mml:math></inline-formula>, while the orange line shows the selected noise component, <inline-formula><mml:math id="mm281" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">N</mml:mi><mml:mo>&#8217;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, in a magnified view.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g020.jpg"/></fig><fig position="float" id="sensors-25-05567-f021" orientation="portrait"><label>Figure 21</label><caption><p>The ablation analysis of <inline-formula><mml:math id="mm282" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to verify the appropriate stopping criterion.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g021.jpg"/></fig><fig position="float" id="sensors-25-05567-f022" orientation="portrait"><label>Figure 22</label><caption><p>Ablation analysis of the relation between the value of the R matrix and the parameter <inline-formula><mml:math id="mm283" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> as the stopping criterion. The figure shows that after <inline-formula><mml:math id="mm284" overflow="scroll"><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:math></inline-formula> exceeds 0.96, the increase in the R value is very limited.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g022.jpg"/></fig><fig position="float" id="sensors-25-05567-f023" orientation="portrait"><label>Figure 23</label><caption><p>Analysis of search window size, as the PSNR improvement is very weak when the search window size exceeds 6.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g023.jpg"/></fig><fig position="float" id="sensors-25-05567-f024" orientation="portrait"><label>Figure 24</label><caption><p>Ablation analysis of <inline-formula><mml:math id="mm285" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm286" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for the appropriate division of reasonable vectors.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05567-g024.jpg"/></fig><table-wrap position="float" id="sensors-25-05567-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05567-t001_Table 1</object-id><label>Table 1</label><caption><p>Quantitative tests: average PSNR(dB)/SSIM values for the No. 1&#8211;7 video sequences, where <bold>bold</bold> and <underline>underlined</underline> texts indicate the best and second-best performance, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods&#8217;<break/> Names</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 1<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 2<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 3<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 4<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 5<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 6<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 7<break/>PSNR/SSIM</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>VBM4D</bold> [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">28.01/<underline>0.6650</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>27.04</underline>/ <underline>0.5716</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">27.51/0.5931</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.97</underline>/ <underline>0.6164</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>26.38</underline>/ <underline>0.5874</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.87</underline>/<underline>0.6438</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>29.15</underline>/<underline>0.6197</underline></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>FastdvdNet</bold> [<xref rid="B24-sensors-25-05567" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.37/0.4629</td><td align="center" valign="middle" rowspan="1" colspan="1">23.30/0.2848</td><td align="center" valign="middle" rowspan="1" colspan="1">23.63/0.3474</td><td align="center" valign="middle" rowspan="1" colspan="1">24.46/0.3801</td><td align="center" valign="middle" rowspan="1" colspan="1">23.01/0.3221</td><td align="center" valign="middle" rowspan="1" colspan="1">24.54/0.3860</td><td align="center" valign="middle" rowspan="1" colspan="1">24.52/0.3778</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>UDVD</bold> [<xref rid="B25-sensors-25-05567" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">26.15/0.5615</td><td align="center" valign="middle" rowspan="1" colspan="1">25.42/0.4109</td><td align="center" valign="middle" rowspan="1" colspan="1">25.93/0.4766</td><td align="center" valign="middle" rowspan="1" colspan="1">26.92/0.4871</td><td align="center" valign="middle" rowspan="1" colspan="1">25.11/0.4532</td><td align="center" valign="middle" rowspan="1" colspan="1">26.82/0.4895</td><td align="center" valign="middle" rowspan="1" colspan="1">27.14/0.4932</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>FloRNN</bold> [<xref rid="B32-sensors-25-05567" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.82/0.4851</td><td align="center" valign="middle" rowspan="1" colspan="1">23.66/0.3044</td><td align="center" valign="middle" rowspan="1" colspan="1">24.04/0.3693</td><td align="center" valign="middle" rowspan="1" colspan="1">25.02/0.4049</td><td align="center" valign="middle" rowspan="1" colspan="1">23.44/0.3474</td><td align="center" valign="middle" rowspan="1" colspan="1">25.04/0.4119</td><td align="center" valign="middle" rowspan="1" colspan="1">25.09/0.4031</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>RCD</bold> [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.22</underline>/0.6540</td><td align="center" valign="middle" rowspan="1" colspan="1">26.52/0.5160</td><td align="center" valign="middle" rowspan="1" colspan="1">27.44/0.5692</td><td align="center" valign="middle" rowspan="1" colspan="1">28.78/0.5892</td><td align="center" valign="middle" rowspan="1" colspan="1">26.11/0.5454</td><td align="center" valign="middle" rowspan="1" colspan="1">28.66/0.6065</td><td align="center" valign="middle" rowspan="1" colspan="1">28.90/0.5886</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>ShiftNet</bold> [<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">25.68/0.6548</td><td align="center" valign="middle" rowspan="1" colspan="1">26.73/ 0.5286</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>27.56</underline>/<underline>0.6079</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">28.69/ 0.6154</td><td align="center" valign="middle" rowspan="1" colspan="1">26.21/0.5744</td><td align="center" valign="middle" rowspan="1" colspan="1">27.34/0.6340</td><td align="center" valign="middle" rowspan="1" colspan="1">29.04/0.6188</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>TAP</bold> [<xref rid="B39-sensors-25-05567" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.95/0.4969</td><td align="center" valign="middle" rowspan="1" colspan="1">23.99/0.3405</td><td align="center" valign="middle" rowspan="1" colspan="1">24.43/0.3916</td><td align="center" valign="middle" rowspan="1" colspan="1">24.80/0.4263</td><td align="center" valign="middle" rowspan="1" colspan="1">23.34/0.3775</td><td align="center" valign="middle" rowspan="1" colspan="1">25.54/0.4361</td><td align="center" valign="middle" rowspan="1" colspan="1">24.64/0.4287</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>Turtle</bold> [<xref rid="B42-sensors-25-05567" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">23.30/0.4652</td><td align="center" valign="middle" rowspan="1" colspan="1">20.26/0.2581</td><td align="center" valign="middle" rowspan="1" colspan="1">21.99/0.3393</td><td align="center" valign="middle" rowspan="1" colspan="1">22.23/0.3640</td><td align="center" valign="middle" rowspan="1" colspan="1">20.87/0.3044</td><td align="center" valign="middle" rowspan="1" colspan="1">23.44/0.3902</td><td align="center" valign="middle" rowspan="1" colspan="1">22.01/0.3556</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>VRT</bold> [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">25.84/0.5293</td><td align="center" valign="middle" rowspan="1" colspan="1">24.67/0.3614</td><td align="center" valign="middle" rowspan="1" colspan="1">25.08/0.4198</td><td align="center" valign="middle" rowspan="1" colspan="1">26.20/0.4518</td><td align="center" valign="middle" rowspan="1" colspan="1">24.29/0.3966</td><td align="center" valign="middle" rowspan="1" colspan="1">26.14/0.4612</td><td align="center" valign="middle" rowspan="1" colspan="1">26.29/0.4500</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Our method</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>31.14</bold>/<bold>0.7687</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>29.81</bold>/<bold>0.6524</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>30.28</bold>/<bold>0.7038</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>31.62</bold>/<bold>0.7131</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>29.03</bold>/<bold>0.6772</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>31.50</bold>/<bold>0.7220</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>31.75</bold>/<bold>0.7143</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05567-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05567-t002_Table 2</object-id><label>Table 2</label><caption><p>Quantitative tests: average PSNR(dB)/SSIM values for the No. 8&#8211;14 video sequences, where <bold>bold</bold> and <underline>underlined</underline> texts indicate the best and second-best performance, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Methods&#8217;<break/> Names</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 8<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 9<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 10<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 11<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 12<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 13<break/>PSNR/SSIM</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Video 14<break/>PSNR/SSIM</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>VBM4D</bold> [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">29.79/0.7318</td><td align="center" valign="middle" rowspan="1" colspan="1">27.71/0.7250</td><td align="center" valign="middle" rowspan="1" colspan="1">24.01/0.7035</td><td align="center" valign="middle" rowspan="1" colspan="1">30.98/0.7483</td><td align="center" valign="middle" rowspan="1" colspan="1">28.94/0.7485</td><td align="center" valign="middle" rowspan="1" colspan="1">32.86/0.7899</td><td align="center" valign="middle" rowspan="1" colspan="1">28.02/0.7599</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>FastdvdNet</bold> [<xref rid="B24-sensors-25-05567" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">28.57/0.7010</td><td align="center" valign="middle" rowspan="1" colspan="1">26.62/0.6927</td><td align="center" valign="middle" rowspan="1" colspan="1">23.37/0.6745</td><td align="center" valign="middle" rowspan="1" colspan="1">29.22/0.6840</td><td align="center" valign="middle" rowspan="1" colspan="1">26.85/0.6292</td><td align="center" valign="middle" rowspan="1" colspan="1">30.95/0.6985</td><td align="center" valign="middle" rowspan="1" colspan="1">25.93/0.5812</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>UDVD</bold> [<xref rid="B26-sensors-25-05567" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">28.96/0.7169</td><td align="center" valign="middle" rowspan="1" colspan="1">27.16/0.7187</td><td align="center" valign="middle" rowspan="1" colspan="1">23.81/0.6990</td><td align="center" valign="middle" rowspan="1" colspan="1">29.70/0.7053</td><td align="center" valign="middle" rowspan="1" colspan="1">27.50/0.6703</td><td align="center" valign="middle" rowspan="1" colspan="1">30.85/0.6919</td><td align="center" valign="middle" rowspan="1" colspan="1">26.39/0.6136</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>FloRNN</bold> [<xref rid="B32-sensors-25-05567" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">28.52/0.6968</td><td align="center" valign="middle" rowspan="1" colspan="1">26.63/0.6922</td><td align="center" valign="middle" rowspan="1" colspan="1">23.36/0.6740</td><td align="center" valign="middle" rowspan="1" colspan="1">29.21/0.6834</td><td align="center" valign="middle" rowspan="1" colspan="1">26.90/0.6327</td><td align="center" valign="middle" rowspan="1" colspan="1">30.85/0.6948</td><td align="center" valign="middle" rowspan="1" colspan="1">25.96/0.5830</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>RCD</bold> [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>30.31</underline>/<underline>0.7704</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.01</underline>/<underline>0.7597</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>24.17</underline>/<underline>0.7351</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>31.70</underline>/<underline>0.7890</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>29.16</underline>/<underline>0.7807</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>33.85</underline>/<underline>0.8285</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">28.01/<underline>0.7809</underline></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>ShiftNet</bold> [<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">29.25/0.6977</td><td align="center" valign="middle" rowspan="1" colspan="1">27.21/0.6752</td><td align="center" valign="middle" rowspan="1" colspan="1">23.62/0.6442</td><td align="center" valign="middle" rowspan="1" colspan="1">30.26/0.7044</td><td align="center" valign="middle" rowspan="1" colspan="1">28.91/0.7335</td><td align="center" valign="middle" rowspan="1" colspan="1">32.62/0.7927</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.21</underline>/0.7748</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>TAP</bold> [<xref rid="B39-sensors-25-05567" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">28.28/0.6982</td><td align="center" valign="middle" rowspan="1" colspan="1">26.37/0.6919</td><td align="center" valign="middle" rowspan="1" colspan="1">23.35/0.6743</td><td align="center" valign="middle" rowspan="1" colspan="1">28.86/0.6975</td><td align="center" valign="middle" rowspan="1" colspan="1">26.66/0.6671</td><td align="center" valign="middle" rowspan="1" colspan="1">31.20/0.7293</td><td align="center" valign="middle" rowspan="1" colspan="1">26.12/0.6415</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>Turtle</bold> [<xref rid="B42-sensors-25-05567" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">27.39/0.6644</td><td align="center" valign="middle" rowspan="1" colspan="1">25.76/0.6652</td><td align="center" valign="middle" rowspan="1" colspan="1">22.91/0.6494</td><td align="center" valign="middle" rowspan="1" colspan="1">27.92/0.6520</td><td align="center" valign="middle" rowspan="1" colspan="1">25.84/0.6037</td><td align="center" valign="middle" rowspan="1" colspan="1">29.54/0.6394</td><td align="center" valign="middle" rowspan="1" colspan="1">25.12/0.5420</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1"><bold>VRT</bold> [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">29.48/0.7463</td><td align="center" valign="middle" rowspan="1" colspan="1">27.27/0.7353</td><td align="center" valign="middle" rowspan="1" colspan="1">23.64/0.7092</td><td align="center" valign="middle" rowspan="1" colspan="1">30.40/0.7406</td><td align="center" valign="middle" rowspan="1" colspan="1">27.75/0.6917</td><td align="center" valign="middle" rowspan="1" colspan="1">32.21/0.7562</td><td align="center" valign="middle" rowspan="1" colspan="1">26.67/0.6475</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Our method</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>31.85</bold>/<bold>0.8467</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>29.50</bold>/<bold>0.8409</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>25.53</bold>/<bold>0.8112</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>33.02</bold>/<bold>0.8544</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>30.39</bold>/<bold>0.8306</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>34.69</bold>/<bold>0.8581</bold></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>28.90</bold>/<bold>0.7925</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05567-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05567-t003_Table 3</object-id><label>Table 3</label><caption><p>Average speed (FPS) of different denoising algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Speed (FPS)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Device</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Speed (FPS)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Device</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">VBM4D [<xref rid="B15-sensors-25-05567" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.20</td><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">FastdvdNet [<xref rid="B24-sensors-25-05567" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">129.87</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">UDVD [<xref rid="B25-sensors-25-05567" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">20.58</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">FloRNN [<xref rid="B32-sensors-25-05567" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">37.31</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RCD [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">67.56</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">ShiftNet [<xref rid="B28-sensors-25-05567" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TAP [<xref rid="B39-sensors-25-05567" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">15.75</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Turtle [<xref rid="B42-sensors-25-05567" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">8.44</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VRT [<xref rid="B27-sensors-25-05567" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Our method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CPU</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05567-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05567-t004_Table 4</object-id><label>Table 4</label><caption><p>Quantitative comparison of different motion estimation algorithms on widely used video datasets. Results are reported as PSNR (dB) / SSIM. <bold>bold</bold> and <underline>underlined</underline> texts indicate the best and second-best performance, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">BME [<xref rid="B46-sensors-25-05567" ref-type="bibr">46</xref>]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FBJME [<xref rid="B55-sensors-25-05567" ref-type="bibr">55</xref>]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DME [<xref rid="B56-sensors-25-05567" ref-type="bibr">56</xref>]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DSME [<xref rid="B57-sensors-25-05567" ref-type="bibr">57</xref>]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LQME [<xref rid="B58-sensors-25-05567" ref-type="bibr">58</xref>]</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Our Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Akiyo</td><td align="center" valign="middle" rowspan="1" colspan="1">44.26/0.9926</td><td align="center" valign="middle" rowspan="1" colspan="1">45.76/0.9950</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>47.20</underline>/<underline>0.9961</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>47.39</bold>/<bold>0.9962</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">46.61/0.9960</td><td align="center" valign="middle" rowspan="1" colspan="1">47.14/0.9956</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Paris</td><td align="center" valign="middle" rowspan="1" colspan="1">34.24/0.9745</td><td align="center" valign="middle" rowspan="1" colspan="1">35.35/0.9795</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>36.42</underline>/<underline>0.9834</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>36.80</bold>/<bold>0.9847</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">36.14/0.9830</td><td align="center" valign="middle" rowspan="1" colspan="1">36.15/0.9822</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Silent</td><td align="center" valign="middle" rowspan="1" colspan="1">34.54/0.9518</td><td align="center" valign="middle" rowspan="1" colspan="1">35.35/<bold>0.9795</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">35.88/0.9636</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>36.11</underline>/<underline>0.9656</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm287" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>/</mml:mo><mml:mo>&#8722;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>36.13</bold>/0.9645</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Crew</td><td align="center" valign="middle" rowspan="1" colspan="1">28.46/0.8328</td><td align="center" valign="middle" rowspan="1" colspan="1">31.15/0.8962</td><td align="center" valign="middle" rowspan="1" colspan="1">31.05/0.8938</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>31.59</underline>/<underline>0.9067</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm288" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>/</mml:mo><mml:mo>&#8722;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>31.91</bold>/<bold>0.9074</bold></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Foreman</td><td align="center" valign="middle" rowspan="1" colspan="1">28.65/0.8636</td><td align="center" valign="middle" rowspan="1" colspan="1">31.72/0.8991</td><td align="center" valign="middle" rowspan="1" colspan="1">32.64/0.8939</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>33.15</underline>/0.9042</td><td align="center" valign="middle" rowspan="1" colspan="1">32.64/<underline>0.9049</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>33.70</bold>/<bold>0.9334</bold></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Football</td><td align="center" valign="middle" rowspan="1" colspan="1">22.58/0.6981</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>23.16</underline>/<underline>0.7125</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">22.49/0.6803</td><td align="center" valign="middle" rowspan="1" colspan="1">22.86/0.7046</td><td align="center" valign="middle" rowspan="1" colspan="1">22.96/0.6689</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>23.90</bold>/<bold>0.7549</bold></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mobile</td><td align="center" valign="middle" rowspan="1" colspan="1">20.63/0.7095</td><td align="center" valign="middle" rowspan="1" colspan="1">27.61/0.9383</td><td align="center" valign="middle" rowspan="1" colspan="1">28.31/0.9435</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>28.63</underline>/<bold>0.9602</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">27.71/0.9438</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>29.68</bold>/<underline>0.9558</underline></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Soccer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.48/0.7552</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><underline>25.30</underline>/<underline>0.8154</underline></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.32/0.7808</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.89/0.8071</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm289" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>/</mml:mo><mml:mo>&#8722;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>26.70</bold>/<bold>0.8487</bold></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Average</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.61/0.8473</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.93/0.9019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.28/0.8919</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><underline>32.68</underline>/<underline>0.9036</underline></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm290" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo>/</mml:mo><mml:mo>&#8722;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>33.16</bold>/<bold>0.9178</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05567-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05567-t005_Table 5</object-id><label>Table 5</label><caption><p>Impact of different step combinations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Case</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Step 1</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Step 2</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Step 3</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Step 4</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Speed/s</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Case 1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm291" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.903</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Case 2</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm292" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.123</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Case 3</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm293" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.026</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Case 4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm294" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.169</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>