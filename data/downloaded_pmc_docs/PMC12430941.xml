<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430941</article-id><article-id pub-id-type="pmcid-ver">PMC12430941.1</article-id><article-id pub-id-type="pmcaid">12430941</article-id><article-id pub-id-type="pmcaiid">12430941</article-id><article-id pub-id-type="doi">10.3390/s25175475</article-id><article-id pub-id-type="publisher-id">sensors-25-05475</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>OTVLD-Net: An Omni-Dimensional Dynamic Convolution-Transformer Network for Lane Detection</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-8600-9721</contrib-id><name name-style="western"><surname>Wu</surname><given-names initials="Y">Yunhao</given-names></name><xref rid="af1-sensors-25-05475" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-3824-6035</contrib-id><name name-style="western"><surname>Zhang</surname><given-names initials="Z">Ziyao</given-names></name><xref rid="af2-sensors-25-05475" ref-type="aff">2</xref><xref rid="af3-sensors-25-05475" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7164-5840</contrib-id><name name-style="western"><surname>Chen</surname><given-names initials="H">Haifeng</given-names></name><xref rid="af1-sensors-25-05475" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Jian</surname><given-names initials="L">Li</given-names></name><xref rid="af1-sensors-25-05475" ref-type="aff">1</xref><xref rid="c1-sensors-25-05475" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Zhai</surname><given-names initials="G">Guangtao</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05475"><label>1</label>College of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi&#8217;an 710021, China; <email>221611034@sust.edu.cn</email> (Y.W.); <email>chenhaifeng@sust.edu.cn</email> (H.C.)</aff><aff id="af2-sensors-25-05475"><label>2</label>School of Physics, Peking University, Beijing 100871, China; <email>1901110194@pku.edu.cn</email></aff><aff id="af3-sensors-25-05475"><label>3</label>State Key Laboratory of Artificial Microstructure and Mesoscopic Physics, Beijing 100871, China</aff><author-notes><corresp id="c1-sensors-25-05475"><label>*</label>Correspondence: <email>lijianjsj@sust.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>03</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5475</elocation-id><history><date date-type="received"><day>08</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>14</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>03</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05475.pdf"/><abstract><p>With the vigorous development of deep learning technology, lane detection tasks have achieved phased results. However, existing lane detection models do not consider the unique geometric and visual features of lanes when dealing with some challenging scenarios, resulting in many difficulties and limitations. To this end, we propose a lane detection network based on full-dimensional convolutional Transformer (OTVLD-Net) to improve the adaptability of the model under extreme road conditions and better handle complex lane topology. In order to extract richer contextual features, we designed ODVT-Net, which uses full-dimensional dynamic convolution combined with improved feature flip fusion layer and non-local network layer, and aggregates lane symmetry features by utilizing the horizontal symmetry of lanes. A feature weight generation mechanism based on Transformer is designed, and a cross-attention mechanism between feature maps and lane requests is added in the decoding stage to enable the network to aggregate global feature information. At the same time, a vanishing point detection module is introduced, and a joint weighted loss function is designed to be trained in coordination with the lane detection task to improve the generalization ability of the lane detection model. Experimental results on the OpenLane and CurveLanes datasets show that the detection effect of the OTVLD-Net model has reached the current advanced level. In particular, the accuracy on the OpenLane dataset is 6.4% higher than the F1 score of the second-ranked model, and the average performance in different challenging scenarios is also improved by 8.9%. At the same time, when ResNet-18 is used as the template feature extraction network, the model achieves a speed of 103FPS and a computing power of 14.2 GFlops, achieving good performance while ensuring real-time performance.</p></abstract><kwd-group><kwd>lane detection</kwd><kwd>Vision Transformer</kwd><kwd>feature extraction</kwd><kwd>vanishing point detection</kwd></kwd-group><funding-group><award-group><funding-source>Key project of National Natural Science Foundation of China</funding-source><award-id>62306172</award-id></award-group><funding-statement>This work was supported by the Key project of National Natural Science Foundation of China, Grant Number 62306172. The fund sponsor is Haifeng Chen.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05475"><title>1. Introduction</title><p>With the increasing advancement of autonomous driving technology, lane line detection, a critical component of vehicular environmental perception, is widely utilized in intelligent driver-assistance systems (IDAS) such as lane keeping assist, lane departure warning, blind spot monitoring, adaptive cruise control, and forward collision warning. However, robust lane line detection in complex environments remains highly challenging. The inherent structural characteristics of lane lines their slender morphology, potential for bends, merges, and bifurcations, small footprint, and sparse pixel distribution make them inherently more difficult to identify than conventional objects. Furthermore, actual driving scenarios are complicated by numerous uncontrollable external factors, including adverse weather, extreme illumination, traffic interferences, and abnormal markings.</p><p>Early research in lane detection primarily relied on fundamental image features such as color [<xref rid="B1-sensors-25-05475" ref-type="bibr">1</xref>] and edge information [<xref rid="B2-sensors-25-05475" ref-type="bibr">2</xref>]. Although traditional methods based on handcrafted features offer straightforward implementation and rapid detection speeds, they exhibit limited robustness and poor interference resistance in complex environments. Consequently, these approaches often struggle to meet the high-precision requirements of autonomous driving systems for lane detection. Recent advances in deep neural networks, coupled with the availability of large-scale annotated datasets, have provided powerful new avenues for addressing this problem. Lane detection methods based on pixel-wise segmentation identify lane markings through pixel-level segmentation. Subsequent post-processing techniques&#8212;such as clustering and curve fitting&#8212;are then applied to derive parametric representations of the lane lines. These approaches can be categorized into semantic segmentation and instance segmentation based on their segmentation output format. Semantic segmentation methods first perform binary segmentation to extract lane contours, followed by clustering algorithms to group lane pixels and distinguish individual lane instances. A key distinction between these methodologies primarily lies in their employed clustering algorithms. For instance, VPGNet [<xref rid="B3-sensors-25-05475" ref-type="bibr">3</xref>] employs a density-based clustering algorithm. LaneAF [<xref rid="B4-sensors-25-05475" ref-type="bibr">4</xref>] introduces a direction-aware clustering technique utilizing horizontal and vertical affine fields to constrain the spatial arrangement of adjacent lane points, thereby enhancing the interpretability of the clustering process. In contrast, instance segmentation-based methods directly predict segmentation maps for individual lane instances, deriving precise lane geometry through subsequent curve fitting. For example, ConvLSTM [<xref rid="B5-sensors-25-05475" ref-type="bibr">5</xref>] leverages temporal information from consecutive frames by integrating a hybrid convolutional-recurrent neural network architecture, further improving lane segmentation accuracy. While segmentation-based approaches offer conceptually straightforward design, the computational resources and inference time required for dense pixel-wise prediction impose non-trivial overhead.</p><p>Inspired by rectangular anchor boxes in object detection, researchers introduced dense linear anchors as structural priors for lane detection. These methods predict lane point offsets relative to the predefined linear anchors, which span diverse image regions to accommodate lanes with varying spatial distributions. This design facilitates feature extraction by providing dedicated reference structures. LineCNN [<xref rid="B6-sensors-25-05475" ref-type="bibr">6</xref>] pioneered this paradigm shift, enumerating potential lane origins at image boundaries to generate predictions rather than explicitly sampling features via linear anchors. Conversely, PointLaneNet [<xref rid="B7-sensors-25-05475" ref-type="bibr">7</xref>] and CurveLanesNAS [<xref rid="B8-sensors-25-05475" ref-type="bibr">8</xref>] employ vertical linear anchors to associate each feature map cell with ground truth lane annotations, subsequently regressing positional offsets relative to these anchors. Despite their efficacy, the fixed geometry and predetermined placement of linear anchors constrain adaptability to lanes of arbitrary shapes and orientations, potentially resulting in suboptimal feature sampling.</p><p>Parameter prediction-based lane detection methods represent lane markings as parameterized curves and directly regress the corresponding curve parameters via end-to-end network architectures. Bert et al. [<xref rid="B9-sensors-25-05475" ref-type="bibr">9</xref>] proposed a differentiable least squares fitting module to optimize lane curve parameters. Compared to the conventional two-stage segmentation-and-fitting pipeline, this end-to-end paradigm enhances stability during model fitting and yields superior performance and interpretability. To address complex lane geometries, PRNet [<xref rid="B10-sensors-25-05475" ref-type="bibr">10</xref>] employs piecewise-defined polynomials with distinct coefficients to represent lane structures. LSTR [<xref rid="B11-sensors-25-05475" ref-type="bibr">11</xref>] utilizes a hybrid convolutional Transformer architecture to capture global contextual information for lanes and directly regresses lane parameters. However, LSTR&#8217;s curve formulation, designed around specific vehicle camera parameters, exhibits high complexity, complicating model training.</p><p>In summary, identifying lane lines by pixel-by-pixel classification requires high-resolution feature maps, which leads to a large number of model parameters and high inference latency, making it difficult to meet the real-time requirements of the vehicle system. The clustering algorithm required is prone to failure due to pixel breakage in occlusion or intersection scenes, resulting in lane line merging or breakage. Lane points are sampled by predefined anchor lines and offsets are predicted. The sampling points are unevenly distributed in curved scenes, resulting in curvature estimation deviation. Horizontal anchors are difficult to adapt to steep curves, and high-density anchors improve coverage but increase the amount of calculation. Low-density anchors may miss sparse lane lines. The parameterized curve prediction of direct regression lane lines is insufficiently aware of local slender structures, and the regression error of curve edge points is large. At the same time, it is difficult to strike a balance between the number of model parameters and feature extraction accuracy.</p><p>Based on the preceding analysis, we propose ODVT-Net, a novel lane detection model integrating omni-dimensional dynamic convolution with Vision Transformer. To address the limited representational capacity of conventional deep learning models for lane detection, we introduce Omni-Dimensional Dynamic Non-local Fusion Network as the core feature extraction module. Concurrently, recognizing the critical need for robust spatial relationships and global context in lane detection, we incorporate a Transformer-based feature weighting mechanism and a vanishing point detection module. This design enables ODVT-Net to capture rich global information across both spatial and channel dimensions, significantly enhancing the model global perception capability. Furthermore, ODVT-Net achieves end-to-end pixel-level lane detection and effectively adapts to complex topological structures, including bifurcations and discontinuities. The principal contributions of this work are four fold:<list list-type="simple"><list-item><label>(1)</label><p>In order to extract features with richer contextual information in the feature extraction stage and to improve the adaptive ability of the model under extreme conditions, this paper proposes a full-dimensional dynamic non-local feature extraction module to extract features from the input images. The Omni-Dimensional Dynamic Convolution (ODConv) is used to make the model dynamically adapt to the inputs of different types of complex situations of lane lines, and an improved Non-local network layer is incorporated to help the model capture the long-range dependencies of spatial dimensions, and a Feature Flip Fusion Level is also incorporated to utilize the horizontal symmetry of lane lines, aggregating features of lane symmetry.</p></list-item><list-item><label>(2)</label><p>In order to aggregate global features, a Transformer-based feature weight generation mechanism is designed, which utilizes the self-attention mechanism of the Transformer encoder while adding a cross-attention mechanism between the feature map and the lane request at the decoder stage, so that the network both aggregates the global feature information while intuitively capturing the correlation between the feature sequences and the a priori sequences of the lane lines, to reducing the complexity of the network.</p></list-item><list-item><label>(3)</label><p>To further enhance the model&#8217;s global perception capability, we introduce a vanishing point detection auxiliary task following feature extraction. This module undergoes joint optimization with the primary lane detection task through shared weight parameters. This design exploits vanishing point localization as a global positional prior while concurrently enriching extracted features and improving model generalization.</p></list-item><list-item><label>(4)</label><p>To enhance detection performance for complex lane topologies, our method directly predicts pixel-level lane representations and employs bipartite matching against ground truth annotations. The final loss function combines the bipartite matching loss with the vanishing point detection loss in a weighted formulation.</p></list-item></list></p><p>The structure of the rest of this paper is described as follows: <xref rid="sec2-sensors-25-05475" ref-type="sec">Section 2</xref> deeply explores the existing deep learning-based lane detection methods, including the technical ideas based on semantic segmentation, object detection, and parameter prediction. <xref rid="sec3-sensors-25-05475" ref-type="sec">Section 3</xref> describes the lane detection model of full-dimensional dynamic convolution-VIT and the design details of its main modules. <xref rid="sec4-sensors-25-05475" ref-type="sec">Section 4</xref> describes our comparative experiments and ablation experiment results. Finally, <xref rid="sec5-sensors-25-05475" ref-type="sec">Section 5</xref> summarizes our contributions.</p></sec><sec id="sec2-sensors-25-05475"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05475"><title>2.1. Semantic Segmentation Based Methods</title><p>Lane line detection methods based on semantic segmentation, such as the Spatial Convolutional Neural Network SpatialCNN [<xref rid="B12-sensors-25-05475" ref-type="bibr">12</xref>], which extends the traditional deep layer-by-layer convolution to slice-by-slice convolution in feature mapping to realize message passing between pixels in rows and columns in the layer; structural association-based lane line detection method SAD [<xref rid="B13-sensors-25-05475" ref-type="bibr">13</xref>], which is applicable to the accurate detection and tracking of lane lines in automated vehicle systems; group channel local connection-based lane line detection method GCLNet [<xref rid="B14-sensors-25-05475" ref-type="bibr">14</xref>], which is applicable to the efficient and accurate detection and tracking of lane lines in automated vehicle systems; and CurveLanesNAS [<xref rid="B8-sensors-25-05475" ref-type="bibr">8</xref>], which is based on neural structure search for lane line detection. LaneNet [<xref rid="B15-sensors-25-05475" ref-type="bibr">15</xref>] was proposed by the team of Neven et al. Similarly to the clustering method Deep Clustering [<xref rid="B16-sensors-25-05475" ref-type="bibr">16</xref>], LaneNet deploys two decoders for segmentation and instance embedding, respectively, which enable it to finish clustering lane lines in the main body of the model without the need of tedious post-processing tasks. In previous segmentation-based lane line detection tasks, such as CooNet [<xref rid="B17-sensors-25-05475" ref-type="bibr">17</xref>] and SCNN [<xref rid="B18-sensors-25-05475" ref-type="bibr">18</xref>], the maximum number of lane lines is rigidly limited to four, which weakens the flexibility of lane line detection, while LaneNet is able to adaptively detect different numbers of lane lines according to different scenarios. Wang et al. proposed the FENET [<xref rid="B19-sensors-25-05475" ref-type="bibr">19</xref>] model inspired by the concentration of human drivers and utilized focus sampling and partial field of view evaluation. focus sampling and partial field of view evaluation to emphasize important long-distance details. GroupLane [<xref rid="B20-sensors-25-05475" ref-type="bibr">20</xref>] first applied the row classification strategy to 3D lane detection, performed row classification in the bird&#8217;s-eye view (BEV) space, and designed a dual detection head group to process lanes in different directions, respectively, solving the problem that traditional methods only support vertical lanes. Han et al. [<xref rid="B20-sensors-25-05475" ref-type="bibr">20</xref>] proposed a new detection head structure that processes lane geometry information and environmental information (discrete key point height regression), respectively, regards 2D lanes as projections of 3D lanes in perspective space, and achieves unified representation through camera parameters, effectively combining integrity and local flexibility.</p><p>Early segmentation-based methods typically preset a fixed maximum number of lane lines and were unable to flexibly adapt to changes in the number of lane lines in actual road scenarios. Standard segmentation methods work in perspective image space and are primarily designed for vertical lane lines, making it difficult to effectively model the three-dimensional geometric information of lanes. They also perform poorly for detecting curved lanes, horizontal lanes, or other non-vertical lanes. More importantly, in complex scenarios (such as occlusion, lighting changes, and road wear) or when focusing on distant, small lane lines, the model may struggle to capture sufficiently clear and robust features.</p></sec><sec id="sec2dot2-sensors-25-05475"><title>2.2. Object Detection Based Methods</title><p>Target-based detection methods, such as PointLaneNet [<xref rid="B7-sensors-25-05475" ref-type="bibr">7</xref>], a point-based lane line detection network for lane line identification and tracking by detecting lane line points in images; LineCNN [<xref rid="B6-sensors-25-05475" ref-type="bibr">6</xref>], a line-based convolutional neural network method for lane line detection by detecting line segments in images; and IntRA-KD [<xref rid="B21-sensors-25-05475" ref-type="bibr">21</xref>], a regional affinity kernel density-based lane line detection method for lane line detection and tracking by capturing correlations between different regions in images; UFLDV2 [<xref rid="B22-sensors-25-05475" ref-type="bibr">22</xref>], a lane line detection method that integrates feature learning and detection to achieve accurate lane line detection and tracking by jointly learning image features and detector parameters; SGNet [<xref rid="B23-sensors-25-05475" ref-type="bibr">23</xref>], a semantic grouping-based lane line detection method that improves lane line detection accuracy by grouping pixels in an image into semantic regions; and CondLaneNet [<xref rid="B8-sensors-25-05475" ref-type="bibr">8</xref>], a conditional network based lane line detection method that improves the accuracy and robustness of lane line detection by introducing conditional information, first generates a set of predefined anchor lines, inputs them into a deep learning model to extract the anchor features, calculates the offset between the lane lines and the anchor lines in the image, and finally regresses them back to the predefined anchor lines to obtain the lane line prediction results. DecoupleLane [<xref rid="B20-sensors-25-05475" ref-type="bibr">20</xref>] proposes a new detection head structure, which processes lane geometry information (third-order polynomial curve modeling in BEV space) and environmental information (height regression of discrete key points), respectively, and regards 2D lanes as projections of 3D lanes in perspective space, achieving unified representation through camera parameters, effectively combining integrity and local flexibility. LaneCorrect [<xref rid="B24-sensors-25-05475" ref-type="bibr">24</xref>] proposes a completely unlabeled lane detection framework, which extracts candidate lane points from the ground point cloud through threshold segmentation based on the high reflectivity difference in special paint on lane markings in point clouds, aggregates candidate points into 3D lane instances, and projects them to 2D images to generate noisy pseudo-labels. From pseudo-label generation to final detection, no manual labeling is required, which greatly reduces the labeling cost. Compared with the segmentation-based lane line detection method, the target detection-based method can accomplish the lane line detection task more quickly due to the use of anchor lines that incorporate the a priori knowledge of the lane line topology. And it is more resistant to interference. However, when encountering special lane line representations such as bifurcations, disconnections, or bends, this approach performs poorly despite its ability to simplify the modeling process.</p><p>Although lane detection methods based on target detection have shown advantages in detection speed, topology preservation, and anti-interference ability by introducing prior knowledge such as anchor lines, regional relationships, conditional information, or geometric modeling, predefined models or anchor lines are difficult to flexibly adapt to these irregular or geometric structures that exceed the preset pattern; methods such as DecoupleLane, which treat two-dimensional lanes as three-dimensional projections, are highly dependent on accurate camera internal and external parameters for perspective conversion. Camera calibration errors or dynamic changes will directly affect the accuracy of the detection results; LaneCorrect, a method that uses point clouds to generate unsupervised pseudo-labels, has its detection results limited by the noise generated by the point cloud segmentation and aggregation steps. The inaccuracy of threshold segmentation and the error in aggregating point clouds into three-dimensional instances will cause the pseudo-labels projected onto the image to contain noise, which in turn affects the effect of the final supervised training.</p></sec><sec id="sec2dot3-sensors-25-05475"><title>2.3. Parameter Prediction-Based Methods</title><p>Parameter-based prediction methods, such as PolylaneNet [<xref rid="B25-sensors-25-05475" ref-type="bibr">25</xref>], LSTR [<xref rid="B11-sensors-25-05475" ref-type="bibr">11</xref>], BezierLaneNet [<xref rid="B26-sensors-25-05475" ref-type="bibr">26</xref>], etc., are different from the aforementioned point-based prediction methods, in that parameter prediction methods directly output parameter lines represented by curve equations, which makes the model more end-to-end, reduces the complexity of the model, and makes it easy to obtain a lightweight model. However, the performance of parametric prediction-based lane line detection methods is often not as good as that of point-based methods because modeling lane lines as parametric curves restricts the degree of freedom of the lane line points, and is not compatible with special lane line representations such as irregularities and bifurcations, etc. The PolyLaneNet method was proposed by Lucas Tabelini et al. This model is a convolutional neural network for end-to-end lane line detection estimation and outputs polynomials representing each lane marker in the image, as well as domain lane polynomials and confidence scores for each lane, but its simple network structure and polynomial parameter curves are insufficient to cope with the complex lane line topology, which seriously affects its network performance. The team of Ma et al. proposed the BezierLaneNet method, and unlike the above methods, the model proposes to use a third order. Bessel curves to fit lane lines, and also proposed and used a deformation convolution-based feature flip fusion module in order to fuse the symmetric properties of the lanes in the driving scenario into the features.</p><p>Lane detection methods based on parameter prediction offer advantages in terms of model lightweightness and end-to-end deployment. However, parameterized methods force lane lines to conform to a predefined curve equation, limiting the degrees of freedom of lane points and resulting in overly strong geometric constraints. Compared with methods based on object detection, parameterized models often struggle to achieve comparable detection accuracy. The fundamental reason is that the curve equation requires a finite number of parameters to summarize the global shape, while local details of lane lines (such as worn markings and shadow interference) are easily ignored during smoothing, resulting in keypoint offsets. Slight deviations in curve parameters can significantly distort the predicted lane shape, making model training more dependent on fine-tuning parameters and increasing the difficulty of optimization.</p></sec></sec><sec id="sec3-sensors-25-05475"><title>3. Method</title><sec id="sec3dot1-sensors-25-05475"><title>3.1. OTVPA-Net Framework</title><p>The network architecture of OTVLD-Net consists of the dynamic feature extraction network ODVT-Net, the vanishing point detection module, the Transformer global feature aggregation module ODVT-Net, the lane detection module and the joint loss module, as shown in <xref rid="sensors-25-05475-f001" ref-type="fig">Figure 1</xref>. The model consists of a sequence of modules that process information in different ways:<list list-type="simple"><list-item><label>(1)</label><p>Dynamic feature extraction module: We propose a omni-dimensional dynamic non-local fusion feature extraction network, which performs dynamic convolution on the feature map after fusing the lane line symmetry features, and captures the global view of the feature map in the spatial dimension through the non-local layer.</p></list-item><list-item><label>(2)</label><p>Vanishing point detection module: Use the feature map output in the feature extraction stage to detect vanishing points and generate a vanishing point quadrant prediction map.</p></list-item><list-item><label>(3)</label><p>Transformer global feature aggregation module: The Transformer encoder is used to perform global feature aggregation on the input feature map in the channel dimension, and the Transformer decoder is used to calculate the cross attention between the feature sequence and the lane request sequence to further expand the global field of view of the feature sequence.</p></list-item><list-item><label>(4)</label><p>Lane detection module: Utilizes a series of outputs with global feature information extracted by the previous module to perform pixel-level detection of lane lines.</p></list-item><list-item><label>(5)</label><p>Joint loss module: Calculates the weighted loss of vanishing point detection and lane line matching, and the model is trained based on this loss.</p></list-item></list></p></sec><sec id="sec3dot2-sensors-25-05475"><title>3.2. Omni-Dimensional Dynamic Non-Local Fusion Feature Extraction Network ODVT-Net</title><p>A single CNN network is limited by the narrow receptive field of CNNs and the limitations of the universal convolution kernel. The feature representations it generates are often simple and limited. This fixed feature extraction mechanism limits the model&#8217;s ability to adapt to complex scenes. Therefore, this paper designs a full-dimensional dynamic non-local fusion feature extraction network ODVT-Net, whose main structure is shown in <xref rid="sensors-25-05475-f002" ref-type="fig">Figure 2</xref>. First, ODVT-Net replaces the convolution operations of the first two layers of residual blocks in the ResNet model with full-dimensional dynamic convolution operations to form a full-dimensional dynamic convolution layer. This operation enables the model to dynamically adapt to lane line images in different scenarios; secondly, ODVT-Net introduces a feature flip fusion module to introduce the horizontal symmetry of lane lines to the model; finally, ODVT-Net replaces the convolution operations of the last two layers of residual blocks in the ResNet model with improved Non-local Blocks to give the model a global vision in the spatial dimension.</p><p>The omni-dimensional dynamic convolution operation (<xref rid="sensors-25-05475-f002" ref-type="fig">Figure 2</xref>, left) uses a new multi-dimensional attention mechanism and parallel strategy. ODConv dynamically learns the attention of the convolution kernel along the four dimensions of the kernel space in the convolution layer, thereby better capturing the feature information of the input data, allowing the network to better adapt to different input data and improve the perception and generalization capabilities of the model. At the same time, there is no great sacrifice in the amount of model calculation. The dynamic convolution operation can be regarded as a dynamic perceptron <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and its calculation process is as follows:<disp-formula id="FD1-sensors-25-05475"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05475"><label>(2)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05475"><label>(3)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#8804;</mml:mo><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent weights, biases, and activation functions, respectively, and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the attention weight. The attention weight is not fixed, but changes with the input. Therefore, compared with static convolution, dynamic convolution has stronger feature expression ability.The additional computation it brings is much less than the convolution itself:<disp-formula id="FD4-sensors-25-05475"><label>(4)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#8811;</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="true">&#8721;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#960;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, applying ODConv to lane line detection models can improve performance without destroying their real-time requirements.</p><p>Lane lines on the road often have horizontal symmetry, that is, the lane lines are symmetrically distributed on both sides of the lane, and in most scenarios, the shapes of two symmetrical lane lines are similar. Inspired by the BezierLaneNet [<xref rid="B27-sensors-25-05475" ref-type="bibr">27</xref>], ODVT-Net adds a feature flip fusion module (<xref rid="sensors-25-05475-f002" ref-type="fig">Figure 2</xref>, middle) to enable the model to consider the horizontal symmetry characteristics of lane lines during reasoning and provide a reference for symmetrical lane lines for the detection of single lane lines. Considering that the lane lines in the camera-captured image may not be aligned, for example, image angle rotation, vehicle turning, unpaired lane lines, etc., 3 &#215; 3 deformable convolution is used in the convolution layer of the flipped feature map, and the model learns the offset features to make residual connections with the original feature map. Thanks to the fact that OTVLD-Net outputs pixel-level predictions of lane lines in the lane detection task, the positioning of pixel points can build a more accurate spatial feature map, which will support the accurate fusion between flipped features in the feature flip fusion module, help the model better adapt to different road conditions, and effectively improve the performance of the model in complex road environments.</p><p>In order to obtain a global view of the spatial dimension at this stage, ODVT-Net focuses on the extraction of non-local information in the spatial dimension in the height H and width W directions. Unlike the classic Non-Local Block, the input feature map size in the Non-Local Block of ODVT-Net (<xref rid="sensors-25-05475-f002" ref-type="fig">Figure 2</xref>, right) is modified to <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, making it more suitable for image detection and focusing on the global information extraction in the spatial dimension. At the same time, in order to reduce the amount of model calculation and not destroy the real-time requirements in the lane detection model, after the two 1 &#215; 1 convolutions of the Non-Local Block, the spatial dimension of the feature map is sampled with a maximum pooling of 2 steps. The impact on network performance is within an acceptable range, but the amount of calculation is only 1/4 of the original. By introducing the Non-Local Block, ODVT-Net can more easily capture long-distance dependencies in the spatial dimension, and at the same time, with the Transformer global feature aggregation, it focuses on the attention extraction in the channel dimension, so that the OTVLD-Net model can obtain a more comprehensive global view. The model will be more suitable for handling lane detection tasks in complex scenes, improving the accuracy and robustness of detection.</p></sec><sec id="sec3dot3-sensors-25-05475"><title>3.3. Vanishing Point Prediction Module</title><p>Adverse conditions&#8212;including inclement weather, poor illumination, and occlusions&#8212;degrade lane visibility. To address this, OTVLD-Net adapts VPGNet [<xref rid="B3-sensors-25-05475" ref-type="bibr">3</xref>] four-quadrant vanishing point localization method. This approach partitions the image plane using quadrant masks, defining the vanishing point (VP) as the intersection of these four regions. Through this structural decomposition, the vanishing point prediction (VPP) module infers the VP position by analyzing quadrant-specific features within the segmented global scene structure.</p><p>OTVLD-Net uses a lightweight semantic segmentation module in the vanishing point prediction head, which contains a 6 &#215; 6 residual block, two 1 &#215; 1 residual blocks, and finally performs a Tiling upsampling operation on the obtained feature map to obtain a four-quadrant vanishing point mask map with 5 channels. In the four-quadrant vanishing point mask map, VP is the intersection of the four quadrants, so the confidence values of VP in the four quadrant channels are roughly the same. Based on this, the formula for calculating VP is as follows:<disp-formula id="FD5-sensors-25-05475"><label>(5)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05475"><label>(6)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the probability of the existence of VP in the image; <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the confidence of the point <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> quadrant channel, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the size of the confidence map, and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the position coordinates of the final predicted VP.</p><p>In OTVLD-Net, the VPP module serves as an auxiliary component exclusively during training to enhance spatial perception. This module is omitted during inference to optimize computational efficiency.</p></sec><sec id="sec3dot4-sensors-25-05475"><title>3.4. Transformer Global Feature Aggregation</title><p>To generate dynamic feature weights for each lane line candidate, the Transformer global feature aggregation module is designed in this paper, as shown in <xref rid="sensors-25-05475-f003" ref-type="fig">Figure 3</xref>. The encoder of the model takes the feature sequence <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> as input, uses the Transformer self-attention mechanism to capture each of the most relevant input features in I, and outputs the feature sequence <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. However, this module discards the mask-self-attention module in a typical Transformer decoder, and defines a learnable lane query sequence <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> vectors of length <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent different lane line candidates, and this query sequence <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> will be directly used as the query sequence <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of the decoder&#8217;s self-attention module after a linear transformation. At this time, the decoder self-attention module integrates the cross-attention mechanism to capture the most relevant features from the feature sequence <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for each lane line query in <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, the decoder will output the dynamic lane feature sequence <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The functional relationship between the query sequence <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the feature sequence <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the cross-self-attention mechanism is as follows:<disp-formula id="FD7-sensors-25-05475"><label>(7)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-05475"><label>(8)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent the key vector and value vector obtained by linear transformation of <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the <italic toggle="yes">i</italic>-th and <italic toggle="yes">j</italic>-th unknown features of <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is used to describe the attention map of the pairwise relationship <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the lane feature of <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> output corresponding to <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> lane query <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a nonlinear mapping.</p><p>Within this architecture, the dynamic lane feature sequence <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is generated through cross-attention between the lane query sequence <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the globally enhanced pixel feature sequence <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Consequently, each lane feature <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> effectively captures channel-wise global context for its corresponding lane in <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. When integrated with spatial global features from Non-Local Blocks in the feature extraction module, OTVLD-Net achieves comprehensive global perception across dimensions. This enables precise localization of image pixels most relevant to lane queries <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> demonstrating robustness against challenging conditions including occlusions and complex topological variations.</p></sec><sec id="sec3dot5-sensors-25-05475"><title>3.5. Lane Detection Module</title><p>To generate pixel-level lane predictions, the model leverages dynamically generated weights from preceding modules within its detection pipeline. At this stage, the model reshapes the output feature sequence <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> of the Transformer encoder into a feature map <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, multiplies it by the heat map kernel <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the offset kernel <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and obtains the lane line pixel point heat map <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the offset map <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Then the model will use the inter-row <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to further process the lane line pixel point heat map <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD9-sensors-25-05475"><label>(9)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the probability that <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> predicted lane line exists in <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> row and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> column of the lane line pixel point heat map <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For simplicity, the same name <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> will be used to represent the above results in the following.</p><p>Each lane feature <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to a heat map <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and an offset map <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The heat map <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> predicts the probability of each pixel being a lane point (foreground) for each predicted lane line, and the offset map <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> predicts the horizontal offset from the lane point of each pixel in the same row to each predicted lane line (at this time, it is assumed that each <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> has at most one lane pixel in each row). After the model inputs the lane line pixel heat map and offset map into a post-processing step, it combines the vertical range vector <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the object score vector <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to obtain the final lane line pixel point. This post-processing process is shown in the following formula:<disp-formula id="FD10-sensors-25-05475"><label>(10)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05475"><label>(11)</label><mml:math id="mm79" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the expected horizontal coordinate of each lane line in each row generated initially using the lane line pixel point heat map <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the resultant horizontal coordinate of each lane line in each row obtained by combining the expected <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> with the position offset of each lane line in the lane line pixel point offset map <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The calculation process of the final model predicting the lane line point is as follows:<disp-formula id="FD12-sensors-25-05475"><label>(12)</label><mml:math id="mm85" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8804;</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-05475"><label>(13)</label><mml:math id="mm86" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the point set of <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> lane line predicted by the model, among which only the lane points between the starting row <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the ending row <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are retained; <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the set of lane lines finally predicted by the model, among which only the lane lines with foreground probability <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> higher than the threshold <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are retained.</p></sec><sec id="sec3dot6-sensors-25-05475"><title>3.6. Joint Loss Function</title><p>The proposed OTVLD-Net employs joint training for its two complementary subtasks: lane detection and vanishing point detection. Accordingly, we design a joint loss function to evaluate model performance and optimize parameters. The overall loss function for lane detection in OTVLD-Net is formulated as:<disp-formula id="FD14-sensors-25-05475"><label>(14)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The vanishing point prediction VPP loss calculates the loss of VPP prediction by calculating the Euclidean distance between the predicted VP and the true value VP. By converting the distance between the predicted point and the true value point into a probability distribution, assuming that the predicted point is <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> and the true value point is <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, their Euclidean distance in two-dimensional space is <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. VPP loss uses a Gaussian distribution, sets a standard deviation <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to convert the square of the Euclidean distance into a probability density function of a Gaussian distribution, and uses the negative log-likelihood loss as the loss function for the result:<disp-formula id="FD15-sensors-25-05475"><label>(15)</label><mml:math id="mm99" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>P</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the lane detection stage, the final predicted lane pixel set <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, lane pixel heat map <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, offset map <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, vertical range vector <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and object score vector <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are generated. First, the model will calculate the pairwise bipartite matching loss <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> predicted lane lines <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> true lane lines <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is expressed as the weighted sum of the object score loss <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, lane pixel heat map loss <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, lane pixel offset map loss <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and vertical range loss <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Among them, the calculation process of the object score loss <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is:<disp-formula id="FD16-sensors-25-05475"><label>(16)</label><mml:math id="mm115" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the probability that the <italic toggle="yes">i</italic>-th lane line is the foreground lane line. The calculation process of the lane line pixel heat map loss <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is:<disp-formula id="FD17-sensors-25-05475"><label>(17)</label><mml:math id="mm118" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the probability that the pixel point of <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> lane line in <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> row and <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> column is a foreground pixel point; <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the horizontal coordinate of <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> true value lane line in <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> row; and <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represent the vertical coordinates of the starting point and the ending point of true value lane line, that is, the pixel point heat map loss is only calculated within the valid range of the true value lane. The calculation process of the lane line pixel offset map loss <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is:<disp-formula id="FD18-sensors-25-05475"><label>(18)</label><mml:math id="mm129" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi><mml:mo>&#8722;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted horizontal offset of the pixel at <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> row and <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> column of <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> lane line; <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the horizontal coordinate of <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> lane line predicted by the model at <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> row; the pixel offset map loss is the same as the pixel point heat map loss, which is only calculated within the valid range of the true value lane. The calculation process of the vertical range loss <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is:<disp-formula id="FD19-sensors-25-05475"><label>(19)</label><mml:math id="mm138" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced open="&#x2016;" close="&#x2016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the predicted starting and ending ordinates of <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> predicted lane line. The calculation process of the final lane line bipartite matching loss is:<disp-formula id="FD20-sensors-25-05475"><label>(20)</label><mml:math id="mm142" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are weighted balance coefficients of <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. Subsequently, the model will use a mapping function <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to represent the optimal prediction-true value match, that is, the index of the predicted lane line assigned to <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> true value lane line in the optimal match. The minimum matching loss guided by it is obtained:<disp-formula id="FD21-sensors-25-05475"><label>(21)</label><mml:math id="mm153" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">arg</mml:mi></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After obtaining the optimal match <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between the predicted value and the true value, the calculation process of the final lane line two-part matching loss function is as follows:<disp-formula id="FD22-sensors-25-05475"><label>(22)</label><mml:math id="mm155" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mspace linebreak="newline"/><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msub><mml:mo stretchy="false">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8713;</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>&#981;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>&#981;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the probability that the <italic toggle="yes">i</italic>-th predicted lane line is the background.</p></sec></sec><sec id="sec4-sensors-25-05475"><title>4. Experimental and Results</title><sec id="sec4dot1-sensors-25-05475"><title>4.1. Lane Line Dataset</title><p>To comprehensively evaluate the proposed method and validate its efficacy, we conduct experiments on two benchmark lane detection datasets: OpenLane [<xref rid="B28-sensors-25-05475" ref-type="bibr">28</xref>] and CurveLanes [<xref rid="B29-sensors-25-05475" ref-type="bibr">29</xref>]. OpenLane comprises 160,000 training images and 40,000 validation images across six challenging scenarios: curves, intersections, nighttime conditions, extreme weather, merging/splitting lanes, and uphill/downhill terrain. The dataset features annotations for 14 lane categories, including road edges and double yellow solid lines. CurveLanes contains 100,000 training, 20,000 validation, and 30,000 test images, with extensive coverage of complex scenarios such as high-curvature roads, bifurcations, and dense lane configurations.</p></sec><sec id="sec4dot2-sensors-25-05475"><title>4.2. Evaluation Indicators</title><p>Both OpenLane and CurveLanes datasets employ the F1 score as the primary evaluation metric. True positives (TP), false positives (FP), and false negatives (FN) are determined by computing the Intersection-over-Union (IoU) between predicted lane markings and ground truth annotations. The IoU of two lane lines is defined as the IoU between their masks, where the width of the mask is fixed to 30 pixels. The F1 score is calculated as follows:<disp-formula id="FD23-sensors-25-05475"><label>(23)</label><mml:math id="mm158" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD24-sensors-25-05475"><label>(24)</label><mml:math id="mm159" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD25-sensors-25-05475"><label>(25)</label><mml:math id="mm160" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec4dot3-sensors-25-05475"><title>4.3. Experimental Setup</title><p>To ensure the accuracy and stability of the experiment, the following configuration was used: GPU is NVIDIA GeForce RTX3080Ti, single card is used. Python version is 3.8.13, NVCC version is Cuda compilation tool, version 11.7, V11.7.99. GCC version is gcc 9.4.0. PyTorch version is 1.13.0, CuDNN version is 8.5. OpenCV version is 4.2.0.</p><p>In terms of model structure, the number of lane line queries L is set to 80 to cover the number of all possible lane lines in a complex road scene. Meanwhile, in order to strike a balance between model performance and computational efficiency, the number of layers of the Transformer encoder and decoder are set to 2 and 4, respectively. In the vanishing point detection loss, the initial standard deviation <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 0.5, and when the model is trained to the 30th epoch, the standard deviation <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is adjusted to 0.1 in order to prevent the gradient from decreasing too fast or oscillating, and in the two-part lane line matching loss, in order to make the individual loss terms consistent in order to promote the model to be consistent in terms of magnitude. In the experiments, the target detection loss weight <inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 5, the heat map loss weight <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 1, the offset loss weight <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 1, and the range loss weight <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is set to 10. In the post-processing stage, the target score threshold t is set to 0.7 to filter out the lane lines with higher confidence. Lane line prediction results with higher confidence.</p></sec><sec id="sec4dot4-sensors-25-05475"><title>4.4. Comparative Experiment</title><p>The performance evaluation results on the OpenLane dataset are shown in <xref rid="sensors-25-05475-t001" ref-type="table">Table 1</xref> OTVLD-Net achieved F1 scores of 59.8%, 61.2%, and 62.5% when using ResNet-18, ResNet-34, and ResNet-101 as the template feature extraction networks, respectively, outperforming the best-performing experimental one, which used ResNet- 101 as the backbone network for CondLaneNet, with corresponding improvements of 1.9%, 4.2%, and 6.4%, respectively. In different challenging lane scenarios, OTVLD-Net achieved the best F1 scores, as shown in <xref rid="sensors-25-05475-t002" ref-type="table">Table 2</xref>, demonstrating the robustness of the model. In particular, in the case of the feature extraction network using ResNet-18 as a template, OTVLD-Net achieves F1 scores of 63.9%, 54.5%, and 62.5% in the &#8220;curves&#8221;, &#8220;intersections&#8221;, and &#8220;merging and diverging&#8221; scenarios, respectively, which are 6.4%, 6.1%, and 17.0% better than CondLaneNet, respectively. These results show that the OTVLD-Net architecture can well handle lane lines with complex topologies. The reason is that by using ODVT-Net, a full-dimensional dynamic nonlocal fusion feature extraction network, the model is able to dynamically adjust the extraction weights of features according to different lane line scenarios, and the hybrid attention mechanism of nonlocal non-local combined with Transformer is able to capture the rich global information of lane lines from the feature maps across space and dimension, which makes it better able to distinguish different lane lines than other methods that do not use the attention mechanism or only combine channel or spatial attention. The hybrid attention mechanism can capture the rich global information of lane lines from the feature map across space and dimensions, making it better able to distinguish different lane lines compared to other methods that do not use an attention mechanism or only combine channel or spatial attention. In terms of speed, the feature extraction network (ODVT-Net-18) using ResNet-18 as the template achieves 103 FPS and 14.2 GFlops of computation with an F1 score of 59.8%, which is a good performance while maintaining real-time performance.</p><p>The performance on the CurveLanes dataset is shown in <xref rid="sensors-25-05475-t003" ref-type="table">Table 3</xref>. The CurveLanes dataset contains lane lines with complex topologies and occlusion phenomena, such as curved, bifurcated, dense, and blocked lane lines. Using ResNet-18, ResNet-34, and ResNet-101 as templates for the feature extraction network, OTVLD-Net achieved F1 scores of 87.86%, 88.16%, and 88.35%, respectively, and improved 2.90%, 2.51%, and 2.38%, respectively, over CondLaneNet, which had the next highest performance. Meanwhile, some qualitative comparison results on the CurveLanes dataset, which are classified into four categories: curvilinear, blocked, dense, and bifurcated lane lines, are presented in <xref rid="sensors-25-05475-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-05475-f005" ref-type="fig">Figure 5</xref> The experimental results show that OTVLD-Net is able to effectively deal with the obstruction problem as well as lane lines with complex topologies.</p></sec><sec id="sec4dot5-sensors-25-05475"><title>4.5. Ablation Experiment</title><p>To evaluate the impact of the Feature Flip Fusion module on model performance, we conducted an ablation study while maintaining identical model architecture elsewhere. As shown in <xref rid="sensors-25-05475-t004" ref-type="table">Table 4</xref>, incorporating the Feature Flip Fusion module yields a substantial 4.34% improvement in F1 score. This enhancement demonstrates that the module&#8217;s exploitation of inherent lane symmetry provides complementary information critical for lane detection accuracy.</p><p>To validate the efficacy of the ODVT-Net feature extraction network, we employ ODVT-Net-34&#8212;a ResNet-34 variant embedding ODConv and Non-Local layers&#8212;for comparative evaluation against standard backbone networks. Ablation results in <xref rid="sensors-25-05475-t005" ref-type="table">Table 5</xref> demonstrate that ODVT-Net exhibits significantly enhanced dynamic adaptation capabilities while effectively leveraging spatial global context to deliver superior feature representations for subsequent lane detection modules.</p><p>To investigate the effect of varying the number of lane queries <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> on model performance, the experimental results are presented in <xref rid="sensors-25-05475-t006" ref-type="table">Table 6</xref> Increasing <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> from 20 to 80 enhances the model ability to capture and represent lane information characterized by diverse topologies. However, further increasing S introduces excessive redundancy, which adversely affects further performance improvement.</p><p>To investigate the effect of varying the number of decoder layers on model performance, experimental results are presented in <xref rid="sensors-25-05475-t007" ref-type="table">Table 7</xref>. Increasing the number of decoder layers from 1 to 4 enhances the dynamic convolutional kernel&#8217;s capacity to capture global lane line information, thereby improving detection accuracy. However, further increasing the number of layers yields diminishing returns for global information capture due to introduced computational redundancy, with no significant performance improvement observed.</p><p>In order to explore the impact of the vanishing point prediction module on the model performance, the ablation experiment conducted in this section is shown in <xref rid="sensors-25-05475-t008" ref-type="table">Table 8</xref>. While keeping the other structures of the model the same, the experimental results are compared and found that after the introduction of the vanishing point prediction module, the F1 score of the model reaches 88.23%, which is 4.07% higher than that without the introduction of the module. This result shows that the global geometric prior information provided by the vanishing point prediction module can significantly enhance the robustness of lane line detection. Especially in curved roads, occluded scenes, and low light conditions, helping the model to correct local perception bias. The addition of the vanishing point prediction module enables the model to utilize the topological characteristics of the scene depth to enhance the geometric reasoning ability of the lane line direction, and ultimately improves the detection accuracy in complex driving environments.</p><p>In order to explore the impact of the lane detection module on the model performance, the ablation experiments conducted in this section are shown in <xref rid="sensors-25-05475-t009" ref-type="table">Table 9</xref>. While keeping the other structures of the model unchanged, the F1 score is improved by 5.35% after introducing the designed lane detection head. This result shows that the design and performance of the lane detection module are crucial to accurately decode features into precise lane line predictions. The proposed detection head significantly enhances the model&#8217;s ability to aggregate local features and resolve spatial ambiguity, especially in challenging scenarios involving severe perspective distortion, long-distance lane lines, or dense adjacent lanes. The detection module designed in this paper is crucial in maintaining detection integrity and suppressing false positives.</p><p>To verify whether the proposed Omni-Dimensional Dynamic Non-Local Fusion Feature Extraction Network (ODVT-Net) and Transformer Global Feature Aggregation (TGFA) can improve lane detection performance, we designed different combinations of the three feature fusion modules, ODVT-Net, TGFA, RESA [<xref rid="B30-sensors-25-05475" ref-type="bibr">30</xref>], and SCNN, and analyzed them on the CULane dataset. The quantitative results are shown in <xref rid="sensors-25-05475-t010" ref-type="table">Table 10</xref> below. The results show that for the backbone network, ODVT-Net-34 leads the residual network in accuracy, while significantly reducing the number of parameters and inference time, verifying that its application-side matching is higher than that of the residual network; for the feature aggregation module, while maintaining the detection accuracy and number of parameters, the inference time of the Transformer global feature aggregation module is relatively slow, but the gap is not large, thus verifying the superiority of the newly designed module in this chapter in the lane line detection task.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05475"><title>5. Conclusions</title><p>This paper proposes a lane detection model OTVLD-Net based on full-dimensional dynamic convolutional Transformer. In the feature extraction stage, the model designs a full-dimensional dynamic non-local feature extraction network ODVT-Net, which dynamically adapts to different types of complex lane inputs through ODConv, and combines Non-Local Level and Feature Flip Fusion Level to obtain richer contextual information using the horizontal symmetry of the lane. While using the self-attention mechanism of the Transformer encoder, a cross-attention mechanism between the feature map and the lane request is added in the decoder stage, and a feature weight generation mechanism based on Transformer is designed, which can intuitively capture the correlation between the feature sequence and the lane prior sequence and reduce the network complexity. The vanishing point detection module is introduced to assist the lane detection task, and a weighted loss function consisting of bipartite matching loss and vanishing point detection is designed. The model is trained together with the vanishing point detection to share weights, which enhances the generalization ability of the model. The performance of OTVPA-Net on the OpenLane dataset is 6.4% higher than that of the second-ranked model. At the same time, the detection speed of OTVPA-Net reached 103 FPS, the computing amount was only 14.2 GFlops, and the real-time performance was greatly improved. The performance and design of the model were verified through a series of comparative experiments and ablation experiments. However, the vanishing point detection module relies on the geometric structure of the image plane and has poor robustness in low-visibility scenarios such as extreme rainstorms/heavy fog, as well as in severely occluded environments. Furthermore, the model only outputs two-dimensional lane line pixel positions and lacks three-dimensional geometric information such as lane line height and curvature radius, making it unable to meet the requirements for lane spatial topology (such as uphill/curved curvature) in actual driving. Training relies on the OpenLane/CurveLanes dataset and does not cover unstructured roads. Therefore, in order to address the shortcomings of this article, in the future we will integrate radar/lidar point cloud data to compensate for the lack of vision in low visibility, refer to mainstream 3D lane line detection and BEV lane maps for 3D perception, and compare with the RSUD20K dataset to verify the generalization ability of roads in developing countries (such as narrow streets in Bangladesh and mixed rickshaw traffic).</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>All authors contributed to the study conception and design. Material preparation, data collection, and analysis were performed by Y.W. and Z.Z. The first draft of the manuscript was written by Y.W., L.J. and H.C. participated in the revision of the paper and provided many pertinent suggestions. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available upon request from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05475"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Son</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yoo</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sohn</surname><given-names>K.</given-names></name></person-group><article-title>Real-time illumination invariant lane detection for lane departure warning system</article-title><source>Expert Syst. Appl.</source><year>2015</year><volume>42</volume><fpage>1816</fpage><lpage>1824</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2014.10.024</pub-id></element-citation></ref><ref id="B2-sensors-25-05475"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Aly</surname><given-names>M.</given-names></name></person-group><article-title>Real time detection of lane markers in urban streets</article-title><source>Proceedings of the 2008 IEEE Intelligent Vehicles Symposium</source><conf-loc>Eindhoven, The Netherlands</conf-loc><conf-date>4&#8211;6 June 2008</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2008</year><fpage>7</fpage><lpage>12</lpage></element-citation></ref><ref id="B3-sensors-25-05475"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yoon</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Shin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bailo</surname><given-names>O.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>N.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>T.-H.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>H.S.</given-names></name><name name-style="western"><surname>Han</surname><given-names>S.-H.</given-names></name><name name-style="western"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>1947</fpage><lpage>1955</lpage></element-citation></ref><ref id="B4-sensors-25-05475"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abualsaud</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>D.B.</given-names></name><name name-style="western"><surname>Situ</surname><given-names>K.</given-names></name><name name-style="western"><surname>Rangesh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Trivedi</surname><given-names>M.M.</given-names></name></person-group><article-title>Laneaf: Robust multi-lane detection with affinity fields</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>6</volume><fpage>7477</fpage><lpage>7484</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3098066</pub-id></element-citation></ref><ref id="B5-sensors-25-05475"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yue</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name></person-group><article-title>Robust lane detection from continuous driving scenes using deep neural networks</article-title><source>IEEE Trans. Veh. Technol.</source><year>2019</year><volume>69</volume><fpage>41</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1109/TVT.2019.2949603</pub-id></element-citation></ref><ref id="B6-sensors-25-05475"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>Line-cnn: End-to-end traffic line detection with line proposal unit</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2019</year><volume>21</volume><fpage>248</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1109/TITS.2019.2890870</pub-id></element-citation></ref><ref id="B7-sensors-25-05475"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Lian</surname><given-names>C.</given-names></name></person-group><article-title>Pointlanenet: Efficient end-to-end cnns for accurate real-time lane detection</article-title><source>Proceedings of the 2019 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Paris, France</conf-loc><conf-date>9&#8211;12 June 2019</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2019</year><fpage>2563</fpage><lpage>2568</lpage></element-citation></ref><ref id="B8-sensors-25-05475"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>P.</given-names></name></person-group><article-title>Condlanenet: A top-to-down lane detection framework based on conditional convolution</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>3773</fpage><lpage>3782</lpage></element-citation></ref><ref id="B9-sensors-25-05475"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Van Gansbeke</surname><given-names>W.</given-names></name><name name-style="western"><surname>De Brabandere</surname><given-names>B.</given-names></name><name name-style="western"><surname>Neven</surname><given-names>D.</given-names></name><name name-style="western"><surname>Proesmans</surname><given-names>M.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>End-to-end lane detection through differentiable least-squares fitting</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27&#8211;28 October 2019</conf-date></element-citation></ref><ref id="B10-sensors-25-05475"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Polynomial regression network for variable-number lane detection</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2020: 16th European Conference, Part XVIII 16</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>719</fpage><lpage>734</lpage></element-citation></ref><ref id="B11-sensors-25-05475"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Z.</given-names></name></person-group><article-title>End-to-end lane shape prediction with transformers</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2021</conf-date><fpage>3694</fpage><lpage>3702</lpage></element-citation></ref><ref id="B12-sensors-25-05475"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>P.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title>Spatial as deep: Spatial cnn for traffic scene understanding</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>2&#8211;7 February 2018</conf-date><volume>Volume 32</volume><comment>No. 1</comment></element-citation></ref><ref id="B13-sensors-25-05475"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name></person-group><article-title>Learning lightweight lane detection cnns by self attention distillation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>1013</fpage><lpage>1021</lpage></element-citation></ref><ref id="B14-sensors-25-05475"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ni</surname><given-names>B.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>Z.</given-names></name></person-group><article-title>Geometric constrained joint lane segmentation and lane boundary detection</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>486</fpage><lpage>502</lpage></element-citation></ref><ref id="B15-sensors-25-05475"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Neven</surname><given-names>D.</given-names></name><name name-style="western"><surname>De Brabandere</surname><given-names>B.</given-names></name><name name-style="western"><surname>Georgoulis</surname><given-names>S.</given-names></name><name name-style="western"><surname>Proesmans</surname><given-names>M.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>Towards end-to-end lane detection: An instance segmentation approach</article-title><source>Proceedings of the 2018 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Changshu, China</conf-loc><conf-date>26&#8211;30 June 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2018</year><fpage>286</fpage><lpage>291</lpage></element-citation></ref><ref id="B16-sensors-25-05475"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hershey</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Le Roux</surname><given-names>J.</given-names></name><name name-style="western"><surname>Watanabe</surname><given-names>S.</given-names></name></person-group><article-title>Deep clustering: Discriminative embeddings for segmentation and separation</article-title><source>Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Shanghai, China</conf-loc><conf-date>20&#8211;25 March 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2016</year><fpage>31</fpage><lpage>35</lpage></element-citation></ref><ref id="B17-sensors-25-05475"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Fang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tong</surname><given-names>K.</given-names></name></person-group><article-title>Selective feature aggregation network with area-boundary constraints for polyp segmentation</article-title><source>Proceedings of the Medical Image Computing and Computer Assisted Intervention&#8212;MICCAI 2019: 22nd International Conference, Part I 22</source><conf-loc>Shenzhen, China</conf-loc><conf-date>13&#8211;17 October 2019</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>302</fpage><lpage>310</lpage></element-citation></ref><ref id="B18-sensors-25-05475"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>H.</given-names></name></person-group><article-title>FENet: Focusing Enhanced Network for Lane Detection</article-title><source>Proceedings of the 2024 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Niagara Falls, ON, Canada</conf-loc><conf-date>15&#8211;19 July 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B19-sensors-25-05475"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Grouplane: End-to-end 3d lane detection with channel-wise grouping</article-title><source>Proceedings of the IEEE Robotics and Automation Letters</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>13&#8211;17 May 2024</conf-date></element-citation></ref><ref id="B20-sensors-25-05475"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>W.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>J.</given-names></name></person-group><article-title>Decoupling the curve modeling and pavement regression for lane detection</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2309.10533</pub-id><pub-id pub-id-type="arxiv">2309.10533</pub-id></element-citation></ref><ref id="B21-sensors-25-05475"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hui</surname><given-names>T.W.</given-names></name><name name-style="western"><surname>Loy</surname><given-names>C.C.</given-names></name></person-group><article-title>Inter-region affinity distillation for road marking segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, Tennessee</conf-loc><conf-date>11&#8211;15 June 2020</conf-date><fpage>12486</fpage><lpage>12495</lpage></element-citation></ref><ref id="B22-sensors-25-05475"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Ultra fast structure-aware deep lane detection</article-title><source>Proceedings of the Computer Vision&#8211;ECCV 2020: 16th European Conference, Part XXIV 16</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>276</fpage><lpage>291</lpage></element-citation></ref><ref id="B23-sensors-25-05475"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name></person-group><article-title>Structure guided lane detection</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2105.05403</pub-id><pub-id pub-id-type="arxiv">2105.05403</pub-id></element-citation></ref><ref id="B24-sensors-25-05475"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nie</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Lanecorrect: Self-supervised lane detection</article-title><source>Int. J. Comput. Vis.</source><year>2025</year><volume>133</volume><fpage>4894</fpage><lpage>4908</lpage><pub-id pub-id-type="doi">10.1007/s11263-025-02417-3</pub-id></element-citation></ref><ref id="B25-sensors-25-05475"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tabelini</surname><given-names>L.</given-names></name><name name-style="western"><surname>Berriel</surname><given-names>R.</given-names></name><name name-style="western"><surname>Paixao</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Badue</surname><given-names>C.</given-names></name><name name-style="western"><surname>De Souza</surname><given-names>A.F.</given-names></name><name name-style="western"><surname>Oliveira-Santos</surname><given-names>T.</given-names></name></person-group><article-title>Polylanenet: Lane estimation via deep polynomial regression</article-title><source>Proceedings of the 2020 25th International Conference on PATTERN recognition (ICPR)</source><conf-loc>Milan, Italy</conf-loc><conf-date>10&#8211;15 January 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2021</year><fpage>6150</fpage><lpage>6156</lpage></element-citation></ref><ref id="B26-sensors-25-05475"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>L.</given-names></name></person-group><article-title>Rethinking efficient lane detection via curve modeling</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;24 June 2022</conf-date><fpage>17062</fpage><lpage>17070</lpage></element-citation></ref><ref id="B27-sensors-25-05475"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>D.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name></person-group><article-title>Clrnet: Cross layer refinement network for lane detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;24 June 2022</conf-date><fpage>898</fpage><lpage>907</lpage></element-citation></ref><ref id="B28-sensors-25-05475"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sima</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Geng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>Persformer: 3d lane detection via perspective transformer and the openlane benchmark</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>550</fpage><lpage>567</lpage></element-citation></ref><ref id="B29-sensors-25-05475"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Curvelane-nas: Unifying lane-sensitive architecture search and adaptive point blending</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>689</fpage><lpage>704</lpage></element-citation></ref><ref id="B30-sensors-25-05475"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>D.</given-names></name></person-group><article-title>RESA: Recurrent Feature-Shift Aggregator for Lane Detection</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Virtual</conf-loc><conf-date>2&#8211;9 February 2021</conf-date></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05475-f001" orientation="portrait"><label>Figure 1</label><caption><p>OTVLD-Net network structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05475-g001.jpg"/></fig><fig position="float" id="sensors-25-05475-f002" orientation="portrait"><label>Figure 2</label><caption><p>ODVT-Net network structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05475-g002.jpg"/></fig><fig position="float" id="sensors-25-05475-f003" orientation="portrait"><label>Figure 3</label><caption><p>Lane feature cross-attention decoder.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05475-g003.jpg"/></fig><fig position="float" id="sensors-25-05475-f004" orientation="portrait"><label>Figure 4</label><caption><p>Qualitative comparison result 1 on different challenging scenarios of the CurveLanes dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05475-g004.jpg"/></fig><fig position="float" id="sensors-25-05475-f005" orientation="portrait"><label>Figure 5</label><caption><p>Qualitative comparison result 2 on different challenging scenarios of the CurveLanes dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05475-g005.jpg"/></fig><table-wrap position="float" id="sensors-25-05475-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t001_Table 1</object-id><label>Table 1</label><caption><p>Performance evaluation results on the OpenLane dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Backbone</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GFlops</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG-16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.175</td><td align="center" valign="middle" rowspan="1" colspan="1">143</td><td align="center" valign="middle" rowspan="1" colspan="1">11.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Enet-SAD</td><td align="center" valign="middle" rowspan="1" colspan="1">ENet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.181</td><td align="center" valign="middle" rowspan="1" colspan="1">135</td><td align="center" valign="middle" rowspan="1" colspan="1">12.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">GoogleNet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.197</td><td align="center" valign="middle" rowspan="1" colspan="1">126</td><td align="center" valign="middle" rowspan="1" colspan="1">13.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LaneATT</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.282</td><td align="center" valign="middle" rowspan="1" colspan="1">151</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>9.5</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LaneATT</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.315</td><td align="center" valign="middle" rowspan="1" colspan="1">127</td><td align="center" valign="middle" rowspan="1" colspan="1">18.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PersFormer [<xref rid="B11-sensors-25-05475" ref-type="bibr">11</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">EfficientNet-B7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.418</td><td align="center" valign="middle" rowspan="1" colspan="1">106</td><td align="center" valign="middle" rowspan="1" colspan="1">20.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LSTR [<xref rid="B27-sensors-25-05475" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.406</td><td align="center" valign="middle" rowspan="1" colspan="1">103</td><td align="center" valign="middle" rowspan="1" colspan="1">18.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CLRNet [<xref rid="B29-sensors-25-05475" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.575</td><td align="center" valign="middle" rowspan="1" colspan="1">130</td><td align="center" valign="middle" rowspan="1" colspan="1">20.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-S</td><td align="center" valign="middle" rowspan="1" colspan="1">0.582</td><td align="center" valign="middle" rowspan="1" colspan="1">99</td><td align="center" valign="middle" rowspan="1" colspan="1">13.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-MM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.603</td><td align="center" valign="middle" rowspan="1" colspan="1">90</td><td align="center" valign="middle" rowspan="1" colspan="1">21.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-L</td><td align="center" valign="middle" rowspan="1" colspan="1">0.614</td><td align="center" valign="middle" rowspan="1" colspan="1">42</td><td align="center" valign="middle" rowspan="1" colspan="1">47.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.532</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>172</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">10.4</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.546</td><td align="center" valign="middle" rowspan="1" colspan="1">126</td><td align="center" valign="middle" rowspan="1" colspan="1">19.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.587</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">ODVT-Net-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.598</td><td align="center" valign="middle" rowspan="1" colspan="1">103</td><td align="center" valign="middle" rowspan="1" colspan="1">14.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">ODVT-Net-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.612</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td><td align="center" valign="middle" rowspan="1" colspan="1">23.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ODVT-Net-101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.625</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.4</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance evaluation results on different challenging lane scenarios in the OpenLane dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Backbone</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Uphill and Downhill</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bend</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Extreme Weather</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Night Road</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Intersection</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Convergence and Divergence</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG-16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.135</td><td align="center" valign="middle" rowspan="1" colspan="1">0.121</td><td align="center" valign="middle" rowspan="1" colspan="1">0.198</td><td align="center" valign="middle" rowspan="1" colspan="1">0.132</td><td align="center" valign="middle" rowspan="1" colspan="1">0.092</td><td align="center" valign="middle" rowspan="1" colspan="1">0.137</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Enet-SAD</td><td align="center" valign="middle" rowspan="1" colspan="1">ENet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.156</td><td align="center" valign="middle" rowspan="1" colspan="1">0.143</td><td align="center" valign="middle" rowspan="1" colspan="1">0.236</td><td align="center" valign="middle" rowspan="1" colspan="1">0.161</td><td align="center" valign="middle" rowspan="1" colspan="1">0.113</td><td align="center" valign="middle" rowspan="1" colspan="1">0.167</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">GoogLeNet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.189</td><td align="center" valign="middle" rowspan="1" colspan="1">0.217</td><td align="center" valign="middle" rowspan="1" colspan="1">0.282</td><td align="center" valign="middle" rowspan="1" colspan="1">0.193</td><td align="center" valign="middle" rowspan="1" colspan="1">0.129</td><td align="center" valign="middle" rowspan="1" colspan="1">0.203</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LaneATT</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.252</td><td align="center" valign="middle" rowspan="1" colspan="1">0.256</td><td align="center" valign="middle" rowspan="1" colspan="1">0.318</td><td align="center" valign="middle" rowspan="1" colspan="1">0.274</td><td align="center" valign="middle" rowspan="1" colspan="1">0.138</td><td align="center" valign="middle" rowspan="1" colspan="1">0.241</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LaneATT</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.281</td><td align="center" valign="middle" rowspan="1" colspan="1">0.272</td><td align="center" valign="middle" rowspan="1" colspan="1">0.345</td><td align="center" valign="middle" rowspan="1" colspan="1">0.319</td><td align="center" valign="middle" rowspan="1" colspan="1">0.168</td><td align="center" valign="middle" rowspan="1" colspan="1">0.263</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PersFormer</td><td align="center" valign="middle" rowspan="1" colspan="1">EfficientNet-B7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.405</td><td align="center" valign="middle" rowspan="1" colspan="1">0.461</td><td align="center" valign="middle" rowspan="1" colspan="1">0.435</td><td align="center" valign="middle" rowspan="1" colspan="1">0.359</td><td align="center" valign="middle" rowspan="1" colspan="1">0.287</td><td align="center" valign="middle" rowspan="1" colspan="1">0.410</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LSTR</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.392</td><td align="center" valign="middle" rowspan="1" colspan="1">0.439</td><td align="center" valign="middle" rowspan="1" colspan="1">0.421</td><td align="center" valign="middle" rowspan="1" colspan="1">0.313</td><td align="center" valign="middle" rowspan="1" colspan="1">0.265</td><td align="center" valign="middle" rowspan="1" colspan="1">0.407</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CLRNet</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.611</td><td align="center" valign="middle" rowspan="1" colspan="1">0.625</td><td align="center" valign="middle" rowspan="1" colspan="1">0.533</td><td align="center" valign="middle" rowspan="1" colspan="1">0.492</td><td align="center" valign="middle" rowspan="1" colspan="1">0.536</td><td align="center" valign="middle" rowspan="1" colspan="1">0.519</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-S</td><td align="center" valign="middle" rowspan="1" colspan="1">0.521</td><td align="center" valign="middle" rowspan="1" colspan="1">0.597</td><td align="center" valign="middle" rowspan="1" colspan="1">0.492</td><td align="center" valign="middle" rowspan="1" colspan="1">0.483</td><td align="center" valign="middle" rowspan="1" colspan="1">0.517</td><td align="center" valign="middle" rowspan="1" colspan="1">0.606</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-MM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.577</td><td align="center" valign="middle" rowspan="1" colspan="1">0.634</td><td align="center" valign="middle" rowspan="1" colspan="1">0.511</td><td align="center" valign="middle" rowspan="1" colspan="1">0.509</td><td align="center" valign="middle" rowspan="1" colspan="1">0.549</td><td align="center" valign="middle" rowspan="1" colspan="1">0.614</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-L</td><td align="center" valign="middle" rowspan="1" colspan="1">0.602</td><td align="center" valign="middle" rowspan="1" colspan="1">0.669</td><td align="center" valign="middle" rowspan="1" colspan="1">0.525</td><td align="center" valign="middle" rowspan="1" colspan="1">0.543</td><td align="center" valign="middle" rowspan="1" colspan="1">0.561</td><td align="center" valign="middle" rowspan="1" colspan="1">0.635</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.552</td><td align="center" valign="middle" rowspan="1" colspan="1">0.573</td><td align="center" valign="middle" rowspan="1" colspan="1">0.456</td><td align="center" valign="middle" rowspan="1" colspan="1">0.464</td><td align="center" valign="middle" rowspan="1" colspan="1">0.482</td><td align="center" valign="middle" rowspan="1" colspan="1">0.453</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.583</td><td align="center" valign="middle" rowspan="1" colspan="1">0.591</td><td align="center" valign="middle" rowspan="1" colspan="1">0.490</td><td align="center" valign="middle" rowspan="1" colspan="1">0.484</td><td align="center" valign="middle" rowspan="1" colspan="1">0.505</td><td align="center" valign="middle" rowspan="1" colspan="1">0.476</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.619</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.627</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.545</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.508</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.555</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.521</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">ODVT-Net-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.560</td><td align="center" valign="middle" rowspan="1" colspan="1">0.637</td><td align="center" valign="middle" rowspan="1" colspan="1">0.513</td><td align="center" valign="middle" rowspan="1" colspan="1">0.509</td><td align="center" valign="middle" rowspan="1" colspan="1">0.543</td><td align="center" valign="middle" rowspan="1" colspan="1">0.623</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">ODVT-Net-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.589</td><td align="center" valign="middle" rowspan="1" colspan="1">0.652</td><td align="center" valign="middle" rowspan="1" colspan="1">0.532</td><td align="center" valign="middle" rowspan="1" colspan="1">0.539</td><td align="center" valign="middle" rowspan="1" colspan="1">0.571</td><td align="center" valign="middle" rowspan="1" colspan="1">0.630</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ODVT-Net-101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.620</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.688</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.549</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.571</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.583</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.656</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t003_Table 3</object-id><label>Table 3</label><caption><p>Performance evaluation results on the CurveLanes dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Backbone</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG-16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6496</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7621</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5661</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Enet-SAD</td><td align="center" valign="middle" rowspan="1" colspan="1">Enet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5017</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6348</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4148</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">GoogleNet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7893</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8621</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7279</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LaneATT</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7982</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8683</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7305</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LaneATT</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8089</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8725</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7481</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PersFormer</td><td align="center" valign="middle" rowspan="1" colspan="1">EfficientNet-B7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8132</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8753</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8115</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LSTR</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8212</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8791</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8226</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CLRNet</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8289</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8832</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8297</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-S</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8099</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.9345</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7146</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-MM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8167</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9336</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7258</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CurveLane</td><td align="center" valign="middle" rowspan="1" colspan="1">Searched-L</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8219</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9104</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7490</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8496</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8762</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8245</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8565</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8816</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8328</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CondLaneNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8597</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8885</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8327</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">ODVT-Net-18</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8786</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9077</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8514</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">ODVT-Net-34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8816</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9112</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8539</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>OTVLD-Net</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ODVT-Net-101</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8835</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9120</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8568</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t004_Table 4</object-id><label>Table 4</label><caption><p>Quantitative evaluation of whether to use the feature flipping fusion module on the CurveLanes dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Whether to Add Feature Flip Fusion Module</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8452</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8977</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8458</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8819</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9087</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8514</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t005_Table 5</object-id><label>Table 5</label><caption><p>Quantitative evaluation of feature extraction networks on the CurveLanes dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Backbone</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">VGG-16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6728</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7984</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5841</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SENet</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7243</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8465</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6428</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EfficientNet-B7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7756</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8926</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7085</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7924</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9033</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7859</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ODVT-Net-34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8816</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9112</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8539</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t006_Table 6</object-id><label>Table 6</label><caption><p>Quantitative evaluation of the number of lane queries S on the CurveLanes dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Lane Query Number</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8725</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8917</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8541</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8746</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8976</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8532</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">80</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8789</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9078</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8514</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8787</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9108</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8497</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t007_Table 7</object-id><label>Table 7</label><caption><p>Quantitative evaluation of the number of decoders on the CurveLanes dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Decoders</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8671</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8963</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8396</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8710</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8997</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8439</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8788</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9076</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8513</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8804</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9105</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8531</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t008_Table 8</object-id><label>Table 8</label><caption><p>Quantitatively evaluate whether to use the vanishing point prediction module on the CurveLanes dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Whether to Add VPP Module</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8478</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8926</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8473</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8823</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.9032</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8554</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t009_Table 9</object-id><label>Table 9</label><caption><p>Quantitatively evaluate whether to use the lane detection module on the CurveLanes dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Whether to Add Lane Detection Module</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8361</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8872</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8353</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8809</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8997</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8523</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05475-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05475-t010_Table 10</object-id><label>Table 10</label><caption><p>The impact of model components on model performance on the CurveLanes dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Module Components</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Params/M</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time/ms</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18+RESA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7471</td><td align="center" valign="middle" rowspan="1" colspan="1">16.25</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18+SCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6942</td><td align="center" valign="middle" rowspan="1" colspan="1">14.83</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-18+TGFA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7519</td><td align="center" valign="middle" rowspan="1" colspan="1">13.12</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ODVT-Net-34+RESA</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8593</td><td align="center" valign="middle" rowspan="1" colspan="1">12.9</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ODVT-Net-34+SCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8436</td><td align="center" valign="middle" rowspan="1" colspan="1">11.24</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>7</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ODVT-Net-34+TGFA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8816</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>10.06</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>