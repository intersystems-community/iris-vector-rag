<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431104</article-id><article-id pub-id-type="pmcid-ver">PMC12431104.1</article-id><article-id pub-id-type="pmcaid">12431104</article-id><article-id pub-id-type="pmcaiid">12431104</article-id><article-id pub-id-type="doi">10.3390/s25175355</article-id><article-id pub-id-type="publisher-id">sensors-25-05355</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Efficient Navigable Area Computation for Underground Autonomous Vehicles via Ground Feature and Boundary Processing</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Yu</surname><given-names initials="M">Miao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05355" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Du</surname><given-names initials="Y">Yibo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05355" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="X">Xi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05355" ref-type="aff">1</xref><xref rid="c1-sensors-25-05355" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ma</surname><given-names initials="Z">Ziyan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05355" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="Z">Zhifeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af2-sensors-25-05355" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Baghdadi</surname><given-names initials="N">Nicolas</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05355"><label>1</label>School of Mechanical and Electrical Engineering, China University of Mining and Technology-Beijing, Beijing 100083, China; <email>bqt2000401011@student.cumtb.edu.cn</email></aff><aff id="af2-sensors-25-05355"><label>2</label>Coal Mining Research Institute, China Coal Technology Engineering Group, Beijing 100013, China; <email>duyibo@tdkcsj.com</email> (Y.D.); <email>maziyan@tdkcsj.com</email> (Z.M.); <email>kdbjwzf@tdkcsj.com</email> (Z.W.)</aff><author-notes><corresp id="c1-sensors-25-05355"><label>*</label>Correspondence: <email>zhangx@cumtb.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>29</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5355</elocation-id><history><date date-type="received"><day>10</day><month>2</month><year>2025</year></date><date date-type="rev-recd"><day>18</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>26</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>29</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05355.pdf"/><abstract><p>Accurate boundary detection is critical for autonomous trackless rubber-wheeled vehicles in underground coal mines, as it prevents lateral collisions with tunnel walls. Unlike open-road environments, underground tunnels suffer from poor illumination, water mist, and dust, which degrade visual imaging. To address these challenges, this paper proposes a navigable area computation for underground autonomous vehicles via ground feature and boundary processing, consisting of three core steps. First, a real-time point cloud correction process via pre-correction and dynamic update aligns ground point clouds with the LiDAR coordinate system to ensure parallelism. Second, corrected point clouds are projected onto a 2D grid map using a grid-based method, effectively mitigating the impact of ground unevenness on boundary extraction; third, an adaptive boundary completion method is designed to resolve boundary discontinuities in junctions and shunting chambers. Additionally, the method emphasizes continuous extraction of boundaries over extended periods by integrating temporal context, ensuring the continuity of boundary detection during vehicle operation. Experiments on real underground vehicle data validate that the method achieves accurate detection and consistent tracking of dual-sided boundaries across straight tunnels, curves, intersections, and shunting chambers, meeting the requirements of underground autonomous driving. This work provides a rule-based, real-time solution feasible under limited computing power, offering critical safety redundancy when deep learning methods fail in harsh underground environments.</p></abstract><kwd-group><kwd>autonomous driving</kwd><kwd>underground mines</kwd><kwd>3D point cloud</kwd><kwd>boundary detection</kwd><kwd>direction extraction</kwd></kwd-group><funding-group><award-group><funding-source>National Key Research and Development Program of China</funding-source><award-id>2022YFB4703603</award-id></award-group><funding-statement>This research was funded by the National Key Research and Development Program of China grant number 2022YFB4703603.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05355"><title>1. Introduction</title><p>With the growing global demand for coal, mining activities are increasingly shifting from open-pit to underground operations. However, underground mining faces inherent challenges, including lower operational efficiency and heightened safety risks due to the complex subsurface environment. The adoption of autonomous transportation systems in underground mines offers a transformative solution: it not only enhances operational efficiency but also minimizes human involvement, thereby significantly improving worker safety [<xref rid="B1-sensors-25-05355" ref-type="bibr">1</xref>]. Consequently, advancing autonomous transportation technologies tailored for underground mining environments has become a critical research priority with substantial practical and academic value.</p><p>Tunnel boundary detection stands as a cornerstone technology for autonomous underground transportation. By accurately identifying tunnel boundaries, autonomous vehicles can maintain precise positioning [<xref rid="B2-sensors-25-05355" ref-type="bibr">2</xref>] relative to the tunnel structure, thereby preventing catastrophic lateral collisions with tunnel walls. Existing boundary detection methods are broadly categorized into image-based and LiDAR-based approaches [<xref rid="B3-sensors-25-05355" ref-type="bibr">3</xref>]. Image-based methods typically leverage deep convolutional neural networks trained on manually annotated datasets to perform semantic segmentation of road regions [<xref rid="B4-sensors-25-05355" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05355" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05355" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05355" ref-type="bibr">7</xref>]. While images provide rich semantic details and high-resolution visual information, their reliability degrades severely in underground environments due to poor illumination, water mist, and airborne dust&#8212;factors that introduce noise and obscure critical features, undermining boundary detection accuracy [<xref rid="B8-sensors-25-05355" ref-type="bibr">8</xref>]. Furthermore, 2D images inherently lack depth information, making it challenging to quantify the precise distance between the vehicle and tunnel boundaries, a limitation that compromises collision avoidance efficacy.</p><p>In contrast, LiDAR systems, equipped with active light sources, are impervious to ambient lighting conditions. Their laser beams can penetrate water mist and dust particles, enabling the acquisition of high-fidelity 3D point clouds even in harsh underground environments [<xref rid="B9-sensors-25-05355" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05355" ref-type="bibr">10</xref>]. Previous studies have explored LiDAR-based boundary detection. Li et al. proposed a structured road boundary detection algorithm that generates super-voxels from point cloud attributes and extracts 3D boundaries using <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula>-shape and energy minimization techniques [<xref rid="B11-sensors-25-05355" ref-type="bibr">11</xref>]. Mi et al. developed a voxel-based method to identify candidate curbs, which are then clustered, classified, and fitted for boundary tracking&#8212;demonstrating efficacy on sloped roads but failing to account for ground unevenness, a prevalent issue in underground settings [<xref rid="B12-sensors-25-05355" ref-type="bibr">12</xref>]. Yu et al. addressed particle degeneration in sparse point cloud scenarios by introducing lateral particle shifting in particle filtering, though their validation was limited to low-traffic urban expressways [<xref rid="B13-sensors-25-05355" ref-type="bibr">13</xref>]. For unstructured environments like open-pit mines, Lu et al. proposed a real-time method that extracts boundary points from elevated ground point clouds using spatial and angular features [<xref rid="B14-sensors-25-05355" ref-type="bibr">14</xref>]. However, these approaches are primarily designed for open or structured environments and overlook the unique complexities of underground tunnels, such as irregular terrain, dynamic width variations, and discontinuous boundaries in junctions and shunting chambers.</p><p>Underground tunnels pose a constellation of distinct challenges that render conventional boundary detection methods inadequate, necessitating the development of specialized technical solutions. Foremost among these is the profound impact of topographic irregularities: uneven surfaces and abrupt slopes introduce significant distortions in point cloud data, complicating the accurate extraction of bilateral boundaries by disrupting the geometric continuity critical for reliable edge detection. Compounding this issue is the dynamic variability of tunnel morphology&#8212;frequent width fluctuations, coupled with the presence of chambers and occluded regions, often lead to partial or complete loss of boundary features in point cloud data, creating discontinuities that traditional extraction algorithms struggle to resolve. Further exacerbating these challenges is the need for robust performance under dynamic operational conditions, where vehicle motion artifacts and environmental disturbances introduce temporal instability, demanding not just one-time detection but persistent, high-fidelity tracking of boundary features to ensure continuous safety.</p><p>The main contributions of this paper are as follows:<list list-type="order"><list-item><p>A rapid ground orientation extraction method is developed to enable efficient initialization of spatial relationships between LiDAR data and tunnel terrain, laying a foundational coordinate reference for subsequent processing;</p></list-item><list-item><p>A real-time coordinate correction framework is proposed, which achieves alignment between the LiDAR coordinate system and the ground plane through a two-stage mechanism of pre-calibration and dynamic feedback adjustment;</p></list-item><list-item><p>An adaptive boundary completion algorithm is designed to address boundary discontinuities in complex scenarios, ensuring topological integrity of extracted boundaries;</p></list-item><list-item><p>A method for continuously extracting boundaries over duration is proposed. By fusing temporal context information, the continuity of boundary detection is maintained in a dynamic environment.</p></list-item></list></p></sec><sec id="sec2-sensors-25-05355"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05355"><title>2.1. Image-Based Road Boundary Detection</title><p>Image-based algorithms typically rely on edge detection operators or deep neural network models to identify road boundaries. Maddiralla et al. proposed a road detection method based on a convolutional attention mechanism, designed specifically for curved roads, roads with broken lanes, or roads without lanes [<xref rid="B15-sensors-25-05355" ref-type="bibr">15</xref>]. This method consists of three main components: an encoder, an enhanced convolutional attention module, and a decoder. The encoder extracts features from the input image, while the enhanced convolutional attention module refines these features for improved quality. Finally, the decoder generates the road detection results, accurately identifying road boundaries in complex scenarios. Swain et al. developed a lane detection approach utilizing the YOLOv5 segmentation model with a ResNet50 backbone network [<xref rid="B16-sensors-25-05355" ref-type="bibr">16</xref>]. ResNet50 effectively extracts high-level features from images, while the YOLOv5 model enhances nonlinearity and refines spatial information for better accuracy. Furthermore, the method incorporates data augmentation during preprocessing to improve the model&#8217;s robustness and adaptability. Zou et al. proposed a hybrid deep learning architecture for lane detection by leveraging consecutive multi-frame images [<xref rid="B17-sensors-25-05355" ref-type="bibr">17</xref>]. The method combines convolutional neural networks (CNNs) and recurrent neural networks (RNNs), where the CNN extracts features from individual image frames, and the RNN processes these features sequentially to establish temporal associations and predict lane positions with improved accuracy. Huang et al. proposed an algorithm for lane extraction on unstructured straight and curved roads [<xref rid="B18-sensors-25-05355" ref-type="bibr">18</xref>]. The process begins by converting RGB images into grayscale, followed by smoothing the image using Gaussian and mean filters. For straight lanes, edge detection combined with the Hough Transform is employed, while curved lanes are extracted using an enhanced region growing method and the least squares technique, ensuring accurate lane detection in diverse scenarios.</p><p>Although images can provide high-resolution environmental information, the low lighting conditions, water mist, and dust in underground tunnels significantly hinder the performance of image-based road boundary detection algorithms, making it challenging to achieve reliable results.</p></sec><sec id="sec2dot2-sensors-25-05355"><title>2.2. LiDAR-Based Road Boundary Detection</title><p>LiDAR is a high-precision active sensor that operates independently of light and shadows, making it ideal for enabling autonomous driving in complex environments. LiDAR-based road boundary detection methods can be broadly divided into scan-line-based methods and region-of-interest-based methods. The first method involves analyzing the point cloud of each layer in the horizontal direction, identifying road boundary points based on geometric features, and generating the drivable area. The second method extracts regions of interest from the global point cloud and detects the road boundaries.</p><p>Han et al. [<xref rid="B19-sensors-25-05355" ref-type="bibr">19</xref>] put forward an algorithm for road boundary detection using a single-line LiDAR mounted diagonally downward. It should be noted that single-line LiDAR, due to its structural characteristics, can only capture a single layer of point cloud data, which leads to a lack of height constraints and limits the acquisition of elevation information. This algorithm starts with feature extraction and classification of point cloud data to identify points that belong to the road boundary. Subsequently, the detected road boundaries are tracked using a joint probabilistic data association filter, with vehicle motion information incorporated to adjust detection performance. Wijesoma et al. [<xref rid="B20-sensors-25-05355" ref-type="bibr">20</xref>] employed a single-line LiDAR combined with an extended Kalman filter algorithm for road edge detection and tracking. Similarly, the single-line LiDAR here, being constrained by its single-layer detection capability, suffers from insufficient height constraints and cannot provide adequate elevation information, and the integration of the filter forms a notable aspect of their approach.</p><p>Liu et al. [<xref rid="B21-sensors-25-05355" ref-type="bibr">21</xref>] proposed a method for detecting road drivable areas by fusing images and point clouds. The specific process is as follows: first, the image is divided into superpixels, and the point cloud is projected onto the image. Within each superpixel block, the point cloud is triangulated, and the average of the normal vectors of all triangles adjacent to a single LiDAR point is computed as the normal vector of that point. Then, the angle between this normal vector and the ground is used to determine whether the point is an obstacle. Finally, the drivable area is generated by starting from the pixels in the middle and bottom of the image and extending outward to both sides; the extension stops when an obstacle boundary is encountered, thus obtaining the final drivable area. Rato et al. [<xref rid="B22-sensors-25-05355" ref-type="bibr">22</xref>] defined a region of interest with a certain range in front of the vehicle and to its sides. They then converted the point cloud within this region into a 2D grid map and adopted an image-based method to detect road boundaries.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05355"><title>3. Methods</title><p>To address the unique challenges of underground tunnel boundary detection&#8212;including topographic irregularities, boundary discontinuities, and dynamic tracking requirements&#8212;this paper proposes a LiDAR-based detection and tracking framework that integrates spatial correction, grid-based projection, and temporal context fusion. <xref rid="sensors-25-05355-f001" ref-type="fig">Figure 1</xref> illustrates the core workflow of the proposed underground tunnel boundary detection method, which consists of three interconnected modules: preprocessing, tunnel boundary fitting, and boundary continuously extracting. This step isolates ground point clouds from non-ground features through a fast segmentation strategy, laying the groundwork for subsequent processing. The extracted ground point clouds then undergo further correction&#8212;via pre-calibration and dynamic updates&#8212;to align with the LiDAR coordinate system, ensuring spatial parallelism. Following this, the corrected ground point clouds are projected to generate a 2D gridded point cloud, structuring the spatial information to facilitate robust boundary extraction. Subsequently, the gridded point cloud is fed into the boundary extraction module to identify initial tunnel boundaries. A scene analysis step is then incorporated to detect the presence of curves, intersections, or shunting chambers. In cases where such complex structures are absent, the process advances directly to boundary fitting. Conversely, if curves, intersections, or shunting chambers are detected, a compensation mechanism is activated to supplement missing point cloud data in occluded regions prior to boundary fitting. This adaptive strategy ensures the continuity and accuracy of boundary information even in scenarios with partial occlusions, such as curves, intersections, and shunting chambers. Finally, the fitted boundary is subjected to tracking via the Kalman filter algorithm, enhancing the stability of tunnel boundary detection results under dynamic conditions.</p><sec id="sec3dot1-sensors-25-05355"><title>3.1. Preprocessing Module</title><p>The quality of the preprocessing module is critical for ensuring accurate subsequent analysis and processing. To enhance the precision of boundary fitting in the acquired underground tunnel point cloud data, this paper incorporates horizontal correction and 2D gridding as key preprocessing steps.</p><sec id="sec3dot1dot1-sensors-25-05355"><title>3.1.1. Horizontal Correction</title><p>To capture point cloud data covering a larger perception range, the LiDAR sensor must be mounted at a certain height above the ground with a downward tilt relative to the horizontal plane. However, due to the uneven nature of underground tunnels, this tilt angle continuously changes as the sensor moves. To ensure accurate mapping of the point cloud data onto a 2D grid, this paper introduces a real-time horizontal correction method that adapts to the varying tilt angle. The proposed method comprises three steps: pre-correction, plane model fitting, and plane transformation.</p><p>The coordinate systems mainly involve the definition of the LiDAR coordinate system (<inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), horizontal rotation coordinate system (<inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), vehicle coordinate system <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math></inline-formula>, and ground coordinate system <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>X</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math></inline-formula>, as shown in <xref rid="sensors-25-05355-f002" ref-type="fig">Figure 2</xref>. The ground coordinate system takes the local ground map as the origin, with the vehicle&#8217;s forward direction as the <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>-axis and the left side as the <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>-axis. The vehicle coordinate system takes the center of the rear axle as the origin, with the vehicle&#8217;s forward direction as the <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>-axis and the left side as the <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>-axis. The horizontal rotation coordinate system is a coordinate system obtained by rotating the <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> plane along the <italic toggle="yes">Y</italic>-axis with the origin of the LiDAR coordinate system as the rotation center so that it is parallel to the <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> plane of the vehicle coordinate system.</p><p>Horizontal rotation:</p><p>The purpose of horizontal rotation is to initially align the <italic toggle="yes">X</italic>-<italic toggle="yes">Y</italic> plane around the <italic toggle="yes">Y</italic>-<italic toggle="yes">axis</italic> of the LiDAR coordinate system as closely as possible to the horizontal plane of the vehicle coordinate system, as illustrated in <xref rid="sensors-25-05355-f002" ref-type="fig">Figure 2</xref>. This step provides a better starting point for the subsequent iterative optimization of the plane model, reducing the likelihood of convergence to local optima. As illustrated in <xref rid="sensors-25-05355-f003" ref-type="fig">Figure 3</xref>, the coordinates of point cloud A in the LiDAR coordinate system are denoted as <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The angle <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> between the LiDAR and the horizontal plane is determined through manual measurement, while the yaw and roll angles of the LiDAR are disregarded. Using this information, the rotation matrix <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is derived, as shown in Equation (<xref rid="FD1-sensors-25-05355" ref-type="disp-formula">1</xref>).<disp-formula id="FD1-sensors-25-05355"><label>(1)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">sin</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">sin</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Subsequently, the coordinates of point cloud A are transformed using the rotation matrix <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, as shown in Equation (<xref rid="FD2-sensors-25-05355" ref-type="disp-formula">2</xref>). This transformation yields the horizontal rotation coordinates of point cloud A, denoted as <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The transformation process is carried out as follows:<disp-formula id="FD2-sensors-25-05355"><label>(2)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">sin</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">sin</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mi>&#952;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Rapid Ground Model Fitting via Adaptive Planar Segment Analysis:</p><p>Before conducting plane model fitting, an area-based filtering algorithm is applied to reduce the volume of point cloud data and improve runtime efficiency. The filtered point cloud, representing the current ground points, is denoted as set <italic toggle="yes">Q</italic>.</p><p>The extraction of the ground model establishes a spatial reference computation of the navigable area in underground tunnels. This framework extracts ground-oriented planar features from LiDAR point clouds, optimized for limited computing power, integrating adaptive neighborhood search, geometric consistency validation, and efficient normal vector propagation.</p><p>Local surface patches are first extracted by dynamically adjusting the neighborhood search range for each target point <italic toggle="yes">P</italic> (with distance <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from the sensor) via a logarithmic model: <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>&#183;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mo>(</mml:mo><mml:mi>b</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Here, <italic toggle="yes">a</italic> scales the range to match tunnel width, and <italic toggle="yes">b</italic> adjusts the growth rate to avoid over-expansion for distant points, both calibrated via offline tests. A valid patch includes the target point plus four cardinal-direction neighbors within <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to filter noise, capturing local depth variations.</p><p>Planar patches are filtered using horizontal&#8211;vertical collinearity checks. For each patch, three key points are evaluated: a central reference point <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and two adjacent points (<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> horizontally; <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> vertically). Horizontal consistency uses depth differences and lateral distances from <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, with a compensation factor for minor sidewall inclination. Vertical consistency follows a similar logic, with a factor to offset small ground undulations. Patches with both metrics below a threshold are classified as stable planar.</p><p>Normal vectors of these stable patches are derived via covariance matrix eigen decomposition (using the smallest eigenvalue&#8217;s eigenvector) and propagated to adjacent patches, reducing redundant calculations. Conflicting norms in overlapping areas trigger re-evaluation of the intersection. This framework prioritizes planar features critical for navigable areas, balancing accuracy and efficiency in resource-constrained underground environments. The results of plane extraction in four typical scenarios are shown in the <xref rid="sensors-25-05355-f004" ref-type="fig">Figure 4</xref>.</p><p>Plane transformation: The plane transformation method proposed transforms the original LiDAR point cloud from a horizontal rotation coordinate system (<inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) into a plane transformation coordinate system (<inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>), which transforms the <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>-<inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> plane around the <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>-axis of the horizontal rotation coordinate system as closely as possible to the horizontal plane aligned with the real-time ground plane, achieved by dynamically fitting the actual road surface conditions.</p><p>After processing the point cloud set <italic toggle="yes">Q</italic> with the plane model, a plane normal vector <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is obtained. Vector <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the normal vector of the real-time ground coordinate system. With <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, the pitch angle <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> and the rotation axis <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> between the <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-axis and the real-time fitted ground can be calculated, as illustrated in <xref rid="sensors-25-05355-f005" ref-type="fig">Figure 5</xref>.</p><p>The calculation process is outlined as follows:<disp-formula id="FD3-sensors-25-05355"><label>(3)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arccos</mml:mo><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>&#183;</mml:mo><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>)</mml:mo><mml:mspace width="-6.0pt"/></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05355"><label>(4)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>l</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>&#215;</mml:mo><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>According to <italic toggle="yes">Rodrigues&#8217;</italic> rotation formula, the rotation matrix <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> can be derived as:<disp-formula id="FD5-sensors-25-05355"><label>(5)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>A</mml:mi><mml:mi>B</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>A</mml:mi><mml:mo form="prefix">sin</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>A</mml:mi><mml:mi>B</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>B</mml:mi><mml:mo form="prefix">sin</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>A</mml:mi><mml:mo form="prefix">sin</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>B</mml:mi><mml:mo form="prefix">sin</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo form="prefix">cos</mml:mo><mml:mi>&#945;</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mspace width="-3.0pt"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The coordinates of the horizontal rotated point cloud <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in the horizontal rotation coordinate system <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are then transformed using the rotation matrix <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The transformation process is shown in Equation (<xref rid="FD6-sensors-25-05355" ref-type="disp-formula">6</xref>), resulting in the s plane transformed point cloud coordinates <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in the plane transformation coordinate system <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD6-sensors-25-05355"><label>(6)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The advantage of this approach is that it reliably transforms 3D point cloud data into a coordinate system consistent with the actual ground plane, even when the vehicle is driving on uneven surfaces, experiencing changes in posture, or encountering significant slopes.</p></sec><sec id="sec3dot1dot2-sensors-25-05355"><title>3.1.2. Two-Dimensional Gridding</title><p>To reduce computational complexity and enhance overall efficiency, this paper employs 2D gridding to process the corrected point cloud, as illustrated in <xref rid="sensors-25-05355-f006" ref-type="fig">Figure 6</xref>.</p><p>Since this paper focuses on boundary detection, a pass-through filter is applied prior to the gridding process to eliminate points outside the critical area, further reducing computational load. The specific operation of the pass-through filter is as follows: First, a filter object is created, and the filter fields and ranges, denoted as E, are defined. The filter then checks whether the corrected point cloud <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> falls within the range <italic toggle="yes">E</italic>, as shown in Equation (<xref rid="FD7-sensors-25-05355" ref-type="disp-formula">7</xref>):<disp-formula id="FD7-sensors-25-05355"><label>(7)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the minimum and maximum thresholds of <italic toggle="yes">E</italic> along the <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-axis in the LiDAR coordinate system; <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the corresponding thresholds along the Y-axis; and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are the thresholds along the <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-axis. Finally, a filtering operation is performed to retain the points within the specified range, resulting in the point cloud set <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo><mml:mo>&#8834;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For the boundary extraction task, consideration must be given to positions on the horizontal plane. Thus, data along the <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-axis is retained up to the maximum width of general tunnel designs, with pass-through filtering applied exclusively to the <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-axis and <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-axis. For the <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-value (distance), focus is placed on LiDAR point cloud data within the first 0-20 m in front of the vehicle. For the <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-value (height), account is taken of the vehicle height (<inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m) and the LiDAR&#8217;s installation position (<inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m above ground). Using the LiDAR coordinate system as the origin, attention is restricted to point cloud data corresponding to tunnel boundaries within the vehicle&#8217;s height range. To ensure safety, additional buffer zones are introduced in the height range: a <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m safety margin is added for uneven ground or downward slopes, while a <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m safety margin is applied for upward slopes or to account for vehicle vibrations.</p><p>In practical underground roadway environments, vehicles and pedestrians may interfere with boundary detection results. Therefore, based on actual measurements and point cloud data analysis, the experiment adopted an appropriate filtering range. Specific settings are provided in <xref rid="sensors-25-05355-t001" ref-type="table">Table 1</xref>.</p><p>After obtaining the point cloud <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> that has undergone pass-through filtering, the process of rasterization is initiated. The main steps are as follows: First, a grid map <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:msup><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:msup><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is initialized, where <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>,<inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are the predefined dimensions of the grid map. Each cell in the grid map is rectangular, with a uniform length and width of <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, as illustrated in <xref rid="sensors-25-05355-f007" ref-type="fig">Figure 7</xref>.</p><p>Next, each point <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in the filtered point cloud is traversed, and its two-dimensional index within the grid map is calculated. The specific calculation process is shown in Equations (<xref rid="FD8-sensors-25-05355" ref-type="disp-formula">8</xref>) and (<xref rid="FD9-sensors-25-05355" ref-type="disp-formula">9</xref>):<disp-formula id="FD8-sensors-25-05355"><label>(8)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05355"><label>(9)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8722;</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mspace width="-3.0pt"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>It should be noted that <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the mapped position of <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> from the 3D point cloud onto the grid map, and <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the mapped position of <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> from the 3D point cloud onto the grid map.</p><p>Finally, it is checked whether <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are within the valid range of the grid map, i.e., if <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>&#8722;</mml:mo></mml:mfenced><mml:mo>,</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>&#8722;</mml:mo></mml:mfenced><mml:mo>,</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> are both true, the point will be stored in the corresponding cell. When all points in the point cloud <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> have been traversed, the 2D rasterization is complete.</p><p>The preprocessing module used in this paper improves data quality, reduces data volume, enhances feature extraction, and increases processing efficiency, facilitating subsequent analysis and processing. After normalization, the point cloud data occupies a fixed and finite coordinate range, enhancing computational efficiency and reducing algorithmic complexity, thus improving real-time performance. Furthermore, normalization ensures the algorithm&#8217;s insensitivity to variations in tunnel dimensions; regardless of the actual tunnel size, the point cloud data is consistently mapped into the same scale, ensuring the generality and robustness of subsequent boundary detection algorithms.</p></sec></sec><sec id="sec3dot2-sensors-25-05355"><title>3.2. Tunnel Boundary Detection</title><p>After the original point cloud has undergone the preprocessing module, a representative 2D rasterized matrix is obtained. In this section, the fitting operation begins. The tunnel boundary detection module mainly consists of three main steps: boundary extraction, adaptive boundary optimization and completion, and boundary fitting.</p><sec id="sec3dot2dot1-sensors-25-05355"><title>3.2.1. Boundary Extraction</title><p>The primary goal of boundary extraction is to extract key points that form the region boundaries from the rasterized data for subsequent precise fitting. This process is divided into two parts: candidate boundary point extraction and outlier removal. Candidate boundary point extraction is undertaken to speed up the subsequent boundary fitting. By rasterizing the point cloud data <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, we obtain the grid map <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where each cell contains several points, and the left boundary point cloud falls within the column with the smallest non-empty cell index in each row, while the right boundary point cloud falls within the column with the largest non-empty cell index in each row. According to this pattern, the following extraction procedure is adopted:<list list-type="order"><list-item><p>Initialize the left and right boundary point clouds as L and R.</p></list-item><list-item><p>Traverse each row <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and initialize the minimum and maximum X coordinates <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and their indices.</p></list-item><list-item><p>Traverse each column within the row to find the points corresponding to the minimum and maximum X coordinates in the non-empty cells. After finding the points, compare them with <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, then update <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and their indices accordingly.</p></list-item><list-item><p>Store the leftmost point into the left boundary point cloud L and the rightmost point into the right boundary point cloud R.</p></list-item><list-item><p>Finally, after traversing all rows, the left boundary point cloud set <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the right boundary point cloud set <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained, where <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>,&#8230;, <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the 1st, 2nd, 3rd,..., wth clusters of the left boundary, and <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>,&#8230;, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the 1st, 2nd, 3rd,..., vth clusters of the right boundary.</p></list-item></list></p><p>Outlier removal: The main purpose of this step is to further improve the quality of the candidate boundary points and reduce the computational complexity for the subsequent fitting process. In this paper, outliers include outliers and aberrant points in the point cloud. The specific process is as follows (using the left boundary as an example):<list list-type="order"><list-item><p>For each point <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">L</italic> is the left boundary point cloud set, find the <italic toggle="yes">K</italic> nearest neighbor points of <italic toggle="yes">P</italic>.</p></list-item><list-item><p>Calculate the average distance <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> from P to these K points.</p></list-item><list-item><p>Determine if the distance from <italic toggle="yes">P</italic> to the <italic toggle="yes">K</italic> points is greater than <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. If not, it is not considered an outlier.</p></list-item><list-item><p>Check if the difference between the X coordinate of <italic toggle="yes">P</italic> and the last point in the point cloud L is less than 1. If yes, it is not an aberrant point.</p></list-item><list-item><p>Add points <italic toggle="yes">P</italic> that are determined to be neither outliers nor aberrant points to the output point cloud <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which is the left boundary point cloud set after removing outliers. Similarly, obtain the right boundary point cloud set <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> after removing outliers.</p></list-item></list></p><p>The point cloud data processed by the boundary extraction module provides high-quality input for further boundary fitting and other subsequent processing, enhancing the efficiency and effectiveness of the point cloud processing system.</p></sec><sec id="sec3dot2dot2-sensors-25-05355"><title>3.2.2. Adaptive Boundary Optimization</title><p>In underground tunnels, there may be intersections and shunting chambers. When such road conditions occur, the extracted left or right boundary may be interrupted, affecting the quality of subsequent fitting operations. Therefore, this paper adopts an adaptive boundary completion method to optimize the issue of discontinuous boundary points and reduce errors in subsequent fitting or tracking.</p><p>After obtaining the critical boundary points, it is determined whether adaptive boundary optimization is needed. The specific process is as follows: first, traverse and compare each point <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of the left and right boundary point clouds <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> obtained during boundary extraction; then, calculate the lateral differences for each point and set a threshold. If the difference is greater than the threshold, it is recorded. Finally, if the number of points with lateral differences greater than the threshold exceeds five after traversal, it is recognized that boundary completion is needed.</p><p>Adaptive boundary optimization is divided into two parts: foreground boundary extraction and boundary adaptive completion.</p><p>Foreground boundary extraction: Foreground boundary extraction is based on the already extracted left and right boundary point clouds. Its main role is to downsample these boundary point clouds and remove points within the road boundary threshold, thereby extracting the portion of the road boundary point cloud that needs to be completed. The specific implementation process is as follows:<list list-type="order"><list-item><p>Perform 2D rasterization on the <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> coordinates of each point in the left boundary point cloud <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and store them in <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; then, perform 2D rasterization on the <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> coordinates of each point in the right boundary point cloud <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and store them in <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Traverse each point <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in the input point cloud, calculate <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> using Equations (<xref rid="FD8-sensors-25-05355" ref-type="disp-formula">8</xref>) and (<xref rid="FD9-sensors-25-05355" ref-type="disp-formula">9</xref>), where <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the mapped position of <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> from the 3D point cloud onto the grid map, and <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the mapped position of <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> from the 3D point cloud onto the grid map. If the conditions of Equations (<xref rid="FD10-sensors-25-05355" ref-type="disp-formula">10</xref>) and (<xref rid="FD11-sensors-25-05355" ref-type="disp-formula">11</xref>) are met, add the point to the output point cloud.</p></list-item></list><disp-formula id="FD10-sensors-25-05355"><label>(10)</label><mml:math id="mm128" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-05355"><label>(11)</label><mml:math id="mm129" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The final output point cloud is the extracted discontinuous point cloud, with the left and right boundaries marked as <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Boundary adaptive completion: After obtaining the discontinuous boundary points, the completion process is started to ensure the continuity and accuracy of the boundaries. The main procedure is as follows:<list list-type="order"><list-item><p>Taking the case of a right boundary interruption as an example, select appropriate parameters for boundary fitting on the right boundary <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> extracted in <xref rid="sec3dot2dot1-sensors-25-05355" ref-type="sec">Section 3.2.1</xref> (the specific fitting process will be detailed in <xref rid="sec3dot2dot3-sensors-25-05355" ref-type="sec">Section 3.2.3</xref>), resulting in a preliminary fitted right boundary.</p></list-item><list-item><p>In <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, for the ith point, connect the (i &#8722; 1)th point with the (i + 1)th point, and shift the point along a direction perpendicular to this line by a distance equal to the width of the roadway.</p></list-item><list-item><p>Add the preliminarily fitted right boundary and the shifted <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> together to form the final right boundary. If the left boundary is interrupted, the same approach is used for completion.</p></list-item></list></p></sec><sec id="sec3dot2dot3-sensors-25-05355"><title>3.2.3. Boundary Fitting</title><p>After acquiring continuous key boundary points, the boundary fitting process is initiated to convert discrete feature points into a continuous geometric representation. To ensure the fitting results support stable and accurate subsequent tracking, the algorithm adopts an iterative optimization strategy that integrates spatial constraints with statistical fitting.</p><p>Specifically, the algorithm first determines the effective range of boundary points by calculating the maximum and minimum values of their X and Y coordinates, thereby establishing spatial constraints based on tunnel structural characteristics. It then performs polynomial curve fitting on randomly sampled subsets of key points, evaluates the fitting residuals against a predefined threshold, and dynamically updates the optimal model by retaining the fitting result with the highest number of inliers. This iterative refinement mechanism effectively filters out noise interference, ensuring the final fitted boundary is both continuous and morphologically consistent with the actual tunnel structure.</p><p>Through the fitting operation, a discrete set of key boundary points can be transformed into a continuous and smooth curve or line. In the next section, we will make predictions based on the fitted boundaries.</p></sec></sec><sec id="sec3dot3-sensors-25-05355"><title>3.3. Continuously Extracting Tunnel Boundaries</title><p>To maintain the continuity of boundary extraction in dynamic environments, temporal context information is fused to establish spatiotemporal correlations between successive frames. Given the irregular terrain and dynamic factors inherent in underground tunnels, a state estimation mechanism is employed to ensure the consistency of boundary features across time sequences&#8212;this mechanism, while leveraging recursive update logic, primarily serves to reinforce the continuity of boundary extraction rather than emphasizing independent tracking functionality. Prior to implementing this mechanism, the state vector <italic toggle="yes">u</italic> and observation vector <italic toggle="yes">z</italic> at the current timestamp are first derived to provide a foundational basis for maintaining temporal consistency of boundary features. The definition of the state vector <italic toggle="yes">u</italic> is shown in Equation (<xref rid="FD12-sensors-25-05355" ref-type="disp-formula">12</xref>):<disp-formula id="FD12-sensors-25-05355"><label>(12)</label><mml:math id="mm135" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mspace width="-3.0pt"/></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are the parameters of the left boundary fitting curve, and <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the rates of change of <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> within a unit time <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#916;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Since the fitting curve information of each frame&#8217;s boundary is obtained, the observation vector <italic toggle="yes">z</italic> is as shown in Equation (<xref rid="FD13-sensors-25-05355" ref-type="disp-formula">13</xref>):<disp-formula id="FD13-sensors-25-05355"><label>(13)</label><mml:math id="mm146" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mspace width="-3.0pt"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This state estimation mechanism operates through two interconnected phases: temporal prediction and measurement update, collectively ensuring the continuity of boundary features across successive frames. In the temporal prediction phase, the state of boundary features at time <italic toggle="yes">t</italic> is inferred from the state at <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, leveraging the temporal correlation of tunnel structures. The prediction model is formulated as:<disp-formula id="FD14-sensors-25-05355"><label>(14)</label><mml:math id="mm148" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mspace width="4pt"/><mml:msub><mml:mi>P</mml:mi><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:msub><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:mspace width="-3.0pt"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math></inline-formula> denotes the predicted state vector of boundary features at time <italic toggle="yes">t</italic>; <italic toggle="yes">F</italic> represents the transition matrix encoding the spatiotemporal relationship between consecutive frames; <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the estimated state vector at <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the prior and posterior covariance matrices, respectively; and <italic toggle="yes">Q</italic> accounts for system noise. Given the constrained morphological variations of underground tunnels, a constant-velocity model is adopted to characterize boundary evolution, leading to the transition matrix:<disp-formula id="FD15-sensors-25-05355"><label>(15)</label><mml:math id="mm154" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the time interval between consecutive point cloud frames. The measurement update phase refines the predicted state using real-time observations to maintain consistency with actual boundary features. The update process follows:<disp-formula id="FD16-sensors-25-05355"><label>(16)</label><mml:math id="mm156" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:msub><mml:msup><mml:mi>H</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:msub><mml:msup><mml:mi>H</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>H</mml:mi><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mspace width="-3.0pt"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the weight coefficient balancing prediction and measurement; <italic toggle="yes">H</italic> is the observation matrix projecting the state vector to the measurement space; <italic toggle="yes">G</italic> represents observation noise; and <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the observed boundary feature vector at time <italic toggle="yes">t</italic>. To ensure dimensional consistency between predicted and observed quantities, <italic toggle="yes">H</italic> is defined as:<disp-formula id="FD17-sensors-25-05355"><label>(17)</label><mml:math id="mm159" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="-3.0pt"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This iterative prediction-update process continuously adjusts boundary feature states, enhancing the temporal coherence of extracted boundaries and their adaptability to subtle environmental variations.</p></sec></sec><sec id="sec4-sensors-25-05355"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05355"><title>4.1. Experimental Platform Description</title><p>The experiments were conducted on a high-performance desktop workstation. This carefully configured platform ensured three critical advantages for underground road scenario analysis: (1) efficient parallel computation through GPU-accelerated tensor operations; (2) real-time processing capabilities for high-resolution sensor data streams; and (3) reproducible experimental conditions with version-controlled dependencies. The combination of hardware specifications and optimized software stack provided sufficient computational headroom for complex tasks while maintaining thermal stability during prolonged training sessions, making it particularly suitable for autonomous driving applications in challenging subterranean environments, as shown in <xref rid="sensors-25-05355-t002" ref-type="table">Table 2</xref>.</p></sec><sec id="sec4dot2-sensors-25-05355"><title>4.2. Data Source and Evaluation Metrics</title><p>The data used in this study were collected from a commonly used transportation road in an underground coal mine production area. The data collection platform was a specialized underground vehicle equipped with an Ouster OS1-64 LiDAR sensor, as shown in <xref rid="sensors-25-05355-f008" ref-type="fig">Figure 8</xref>. Data collection was conducted on a pre-selected, enclosed underground mine road that included four scenarios: straight road, curved road, intersection, and chamber. A total of 8 h of driving data were collected, from which 1200 frames were selected for testing, with 300 frames allocated to each scenario.</p><p>The visualization results of the point clouds for the four scenarios are shown in <xref rid="sensors-25-05355-f009" ref-type="fig">Figure 9</xref>. In the straight road scenario, the point cloud appears relatively uniform and continuous. In the curved road scenario, the point cloud distinctly exhibits curved features. For the intersection scenario, the data distribution becomes more complex, displaying multiple branching directions. In the chamber scenario, the point cloud density is significantly higher, reflecting the enclosed and irregular characteristics of the space. These scenarios reveal notable differences in the point clouds, with road boundary detection becoming increasingly challenging as the complexity of the scenarios grows.</p><p>To better evaluate the accuracy of underground tunnel boundary fitting, this paper uses manually annotated boundary point clouds as the ground truth. Point clouds within a 2 cm radius of the ground truth are defined as true positive (TP) point clouds, while those outside this range are considered false positive (FP) point clouds. The evaluation metric employed is precision, calculated as shown in Equation (<xref rid="FD18-sensors-25-05355" ref-type="disp-formula">18</xref>), where TP represents the total number of true positive point clouds, and FP represents the total number of false positive point clouds:<disp-formula id="FD18-sensors-25-05355"><label>(18)</label><mml:math id="mm160" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec4dot3-sensors-25-05355"><title>4.3. Underground Tunnel Boundary Detection Results</title><p>In this subsection, we primarily analyze the boundary fitting results in four scenarios: straight tunnels, curved tunnels, intersections, and shunting chambers. The experimental results for the straight road scenario are illustrated in <xref rid="sensors-25-05355-f010" ref-type="fig">Figure 10</xref>. <xref rid="sensors-25-05355-f010" ref-type="fig">Figure 10</xref>a shows the original point cloud. To clearly demonstrate the subsequent boundary fitting results, the experiment zooms in on the first half of the original point cloud by a factor of two, with the local magnification result presented in <xref rid="sensors-25-05355-f010" ref-type="fig">Figure 10</xref>b. <xref rid="sensors-25-05355-f010" ref-type="fig">Figure 10</xref>c displays the result after horizontal correction. Since it is challenging to observe the effect of the correction from a top view, <xref rid="sensors-25-05355-f010" ref-type="fig">Figure 10</xref>d and <xref rid="sensors-25-05355-f010" ref-type="fig">Figure 10</xref>e, respectively, show the before and after correction results from the side view. After the horizontal correction, the detected boundaries have an inclination angle that is essentially 0 with respect to the horizontal plane, which effectively aids in the subsequent rasterization process and boundary extraction.</p><p><xref rid="sensors-25-05355-f011" ref-type="fig">Figure 11</xref>a shows the extracted key boundary points, which are primarily aligned along a single straight line. <xref rid="sensors-25-05355-f011" ref-type="fig">Figure 11</xref>b,c illustrate the boundary fitting results and the boundary ground truth, respectively. In the straight road scenario, the boundary fitting results closely align with the ground truth, demonstrating high accuracy. <xref rid="sensors-25-05355-f011" ref-type="fig">Figure 11</xref>d presents the boundary tracking results after fitting. To provide a clearer view of the tracking effect, the experiment displays the results at a 2&#215; reduced scale.</p><p>The experimental results for the curved road scenario are illustrated in <xref rid="sensors-25-05355-f012" ref-type="fig">Figure 12</xref>. Similar to the straight road scenario, <xref rid="sensors-25-05355-f012" ref-type="fig">Figure 12</xref>a shows the original point cloud, while <xref rid="sensors-25-05355-f012" ref-type="fig">Figure 12</xref>b presents the horizontal correction result of the point cloud in the yellow dotted line area. <xref rid="sensors-25-05355-f012" ref-type="fig">Figure 12</xref>c highlights the key boundary points extracted in the curved tunnel scenario, which collectively form a curve consistent with the characteristics of a curved tunnel. <xref rid="sensors-25-05355-f012" ref-type="fig">Figure 12</xref>d,e depict the boundary fitting results and the ground truth, respectively, demonstrating that the fitting results closely align with the ground truth. <xref rid="sensors-25-05355-f012" ref-type="fig">Figure 12</xref>f illustrates the boundary tracking results, showing that in the curved tunnel scenario, the tracked boundaries accurately follow the actual tunnel boundaries.</p><p>The experimental results for the intersection scenario are illustrated in <xref rid="sensors-25-05355-f013" ref-type="fig">Figure 13</xref>. <xref rid="sensors-25-05355-f013" ref-type="fig">Figure 13</xref>a,b display the original point cloud and the result after horizontal correction, respectively. <xref rid="sensors-25-05355-f013" ref-type="fig">Figure 13</xref>c presents the extracted key boundary points in the intersection scenario, revealing a significant interruption on the left boundary. This interruption occurs due to a branch merging into the main path, resulting in a more complex road structure compared to straight and curved tunnels. This complexity makes it challenging to fully extract the boundary points, thereby impacting subsequent boundary fitting.</p><p>To address this, the interruption detection step will complete the boundaries in this scenario. <xref rid="sensors-25-05355-f013" ref-type="fig">Figure 13</xref>d shows the foreground of the target point cloud, and <xref rid="sensors-25-05355-f013" ref-type="fig">Figure 13</xref>e presents the boundary completion result, where the completion process ensures continuity of the boundaries on both sides. <xref rid="sensors-25-05355-f013" ref-type="fig">Figure 13</xref>f,g illustrate the boundary detection result and the boundary ground truth, respectively. While the right boundary fitting result closely matches the ground truth, the left boundary fitting shows some deviation due to the intersection on the left side of the road. <xref rid="sensors-25-05355-f013" ref-type="fig">Figure 13</xref>h illustrates the result of boundary tracking, revealing some initial errors in the left-side boundary. However, these errors are quickly corrected, aligning the tracking with the correct road boundary.</p><p>The experimental results for the shunting chamber scenario are illustrated in <xref rid="sensors-25-05355-f014" ref-type="fig">Figure 14</xref>. <xref rid="sensors-25-05355-f014" ref-type="fig">Figure 14</xref>a shows the original point cloud, while <xref rid="sensors-25-05355-f014" ref-type="fig">Figure 14</xref>b presents the data after horizontal correction. In the shunting chamber scenario, the presence of enclosed rooms alongside the tunnel causes interruptions in the extracted key boundary points, as shown in <xref rid="sensors-25-05355-f014" ref-type="fig">Figure 14</xref>c. Consequently, boundary completion operations were also required for the shunting chamber scenario. The process of adaptive boundary completion is depicted in <xref rid="sensors-25-05355-f014" ref-type="fig">Figure 14</xref>d,e. After adaptive completion, the boundary points on both sides become continuous, with the completed results shown in <xref rid="sensors-25-05355-f014" ref-type="fig">Figure 14</xref>f. <xref rid="sensors-25-05355-f014" ref-type="fig">Figure 14</xref>g,h illustrate the boundary fitting results and the manually annotated ground truth, respectively. The fitted boundaries after completion align closely with the ground truth, demonstrating the effectiveness of the boundary completion and fitting processes in this complex environment. <xref rid="sensors-25-05355-f014" ref-type="fig">Figure 14</xref>h presents the boundary tracking results, demonstrating a strong alignment between the tracked boundary and the ground truth.</p><p><xref rid="sensors-25-05355-f015" ref-type="fig">Figure 15</xref> shows a comparison between the ground truth and calculated values under four scenarios. The evaluation results of the boundary fitting compared to manually annotated ground truth boundaries for the four scenarios are summarized in <xref rid="sensors-25-05355-t002" ref-type="table">Table 2</xref>. The straight road scenario achieved the highest precision at 97.5% and required the shortest processing time of 29 ms. The intersection scenario exhibited the lowest precision at 85.0% and took the longest to process, requiring 45 ms. The curved tunnel and shunting chamber scenarios achieved precisions of 93.2% and 88.3%, with processing times of 30 ms and 45 ms, respectively.</p><p>The three formulas are presented as follows, with their syntax checked and confirmed to be correct:<disp-formula id="FD19-sensors-25-05355"><label>(19)</label><mml:math id="mm161" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD20-sensors-25-05355"><label>(20)</label><mml:math id="mm162" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Recall</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD21-sensors-25-05355"><label>(21)</label><mml:math id="mm163" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>Recall</mml:mi></mml:mrow><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>+</mml:mo><mml:mi>Recall</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here are the explanations of names and symbols in the formulas: MSE (Mean Squared Error) is a commonly used metric to evaluate the accuracy of a regression model; in the context of this study, it quantifies the average of the squared differences between the actual boundary positions (<inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) and the predicted boundary positions (<inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) for <italic toggle="yes">n</italic> sample points, with a smaller MSE value indicating a better fit between the predicted and actual boundaries. <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the actual measured value of the boundary position for the <italic toggle="yes">i</italic>-th sample point, while <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the predicted value of the boundary position for the <italic toggle="yes">i</italic>-th sample point obtained using the proposed method, and <italic toggle="yes">n</italic> is the total number of sample points used for error calculation. Recall (Sensitivity or True Positive Rate) is a metric used to evaluate the performance of a classification model in identifying positive instances; in this study, it reflects the proportion of actual boundary points that are correctly detected as boundary points, where TP (True Positive) is the number of actual boundary points that are correctly identified as boundary points by the method, and FN (False Negative) is the number of actual boundary points that are incorrectly identified as non-boundary points by the method. F1 score is a comprehensive metric that combines precision and recall to evaluate the overall performance of a classification model, balancing the trade-off between them and providing a single value to assess the model&#8217;s effectiveness, with a higher F1 score indicating better overall performance in detecting boundary points. Precision (Positive Predictive Value) is another important metric for classification models, representing the proportion of predicted boundary points that are actually boundary points. The results in four scenarios are summarized in <xref rid="sensors-25-05355-t003" ref-type="table">Table 3</xref>.</p><p>In summary, the method proposed has demonstrated outstanding performance across various complex scenarios. In the straight and curved tunnel scenarios, the results show that the proposed method can accurately fit and effectively track tunnel boundaries when no interruptions are present. In the intersection scenario, the increased complexity and diversity of the point cloud data introduce challenges, causing some interference with the left boundary. However, the overall results still exhibit high accuracy. In the shunting chamber scenario, despite boundary interruptions, the proposed method achieves relatively accurate boundary fitting through adaptive boundary completion operations. These experimental results highlight the robustness and high feasibility of the method for underground tunnel boundary detection.</p></sec><sec id="sec4dot4-sensors-25-05355"><title>4.4. Comparative Experiments and Analysis</title><p>The straight tunnels, curved tunnels, intersections, and chamber scenes in underground auxiliary transportation roads are similar to the straight roads, curved roads, and obstacle-blocked roads described in Reference [<xref rid="B23-sensors-25-05355" ref-type="bibr">23</xref>]. To further evaluate the effectiveness of the proposed road boundary detection method for underground coal mines, a comparison was conducted with the curb detection method outlined in Reference [<xref rid="B23-sensors-25-05355" ref-type="bibr">23</xref>]. The comparative results are detailed in <xref rid="sensors-25-05355-t004" ref-type="table">Table 4</xref>.</p><p>From <xref rid="sensors-25-05355-t004" ref-type="table">Table 4</xref>, the proposed method exhibits superior performance over comparative approaches across all four operational scenarios, with distinct advantages in curved tunnels, intersections, and shunting chambers. In curved tunnel environments, comparative methods rely predominantly on spatial features such as point cloud height differentials for boundary extraction. However, the inherent surface irregularities and steep gradients in underground settings induce frequent fluctuations in ground point cloud elevations, leading to heightened false detection rates when relying solely on height-based cues. In contrast, the proposed method incorporates domain-specific characteristics of underground tunnels and dynamic road surface variations affecting boundary extraction. Through continuous point cloud updating, it maintains the ground point cloud in parallel alignment with the LiDAR coordinate system&#8217;s horizontal plane. Moreover, the algorithm employs a grid-based projection strategy to map 3D point clouds onto a 2D grid map, effectively mitigating the impact of height variations caused by surface unevenness and enhancing boundary extraction robustness.</p><p>When addressing boundary discontinuities in intersections and shunting chambers, the proposed method achieves more accurate tunnel boundary fitting via adaptive boundary completion. Notably, the average processing time per frame of point cloud data is approximately 35 ms, representing a 10.3% reduction compared to the comparative method with 39 ms, thereby demonstrating competitive real-time performance.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05355"><title>5. Conclusions</title><p>This study proposes a method for detecting bilateral tunnel boundaries in underground environments, tailored to address the unique challenges of harsh subsurface conditions. In the preprocessing module, a real-time point cloud calibration mechanism&#8212;integrating horizontal correction and dynamic fine-tuning&#8212;achieves horizontal alignment of ground point clouds, effectively mitigating boundary misdetection caused by steep slopes and uneven surfaces. Subsequently, a grid-based approach, combined with a dedicated boundary extraction module, enables robust extraction of boundary point clouds. To tackle the issue of boundary discontinuities in intersections and shunting chambers, an adaptive boundary completion method is introduced, ensuring the integrity of boundary features in complex scenarios. The method designs a continuous boundary fitting mechanism, deeply integrating temporal context information to maintain long-term consistency of boundary detection results, ensuring sustained stability under dynamic operating conditions. This mechanism breaks free from the reliance of traditional methods on single-moment data; through dynamic correlation and trend prediction of historical boundary features, it effectively resists the impact of instantaneous noise interference and local feature loss, significantly enhancing the continuity of bilateral boundaries in complex scenarios.</p><p>Experimental results validate that the proposed method delivers accurate and reliable bilateral boundary detection across diverse unstructured underground tunnel scenarios&#8212;including straight sections, curves, intersections, and shunting chambers&#8212;while maintaining excellent real-time performance, meeting the safety requirements of underground autonomous driving. Future research will focus on enhancing the method&#8217;s adaptability to extreme environments, such as highly fragmented tunnels or sudden structural changes, exploring synergies with lightweight deep learning techniques to further optimize detection precision and processing efficiency, and validating its performance in larger-scale long-term mining operations. Additionally, we will investigate the potential integration of supplementary sensor technologies (e.g., inertial measurement units for improved pose estimation) and explore compatibility with broader autonomous driving systems to enhance scalability. These efforts aim to provide more robust technical support for underground autonomous transportation systems.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>M.Y. conceptualized the research framework and designed the overall methodology. Y.D. performed the experiments, analyzed the data, and drafted the initial manuscript. X.Z. contributed to the development of the adaptive boundary completion algorithm and revised the technical sections. Z.M. optimized the ground feature extraction module and validated the computational efficiency. Z.W. supervised the research, provided critical feedback on the manuscript, and secured funding for the project. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The full dataset used in this study cannot be publicly shared. However, to facilitate the understanding and verification of the proposed method, some sample test data can be obtained by contacting the first author (Miao Yu). Researchers interested in accessing the sample data are kindly requested to reach out via the corresponding contact information provided in the manuscript.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Authors Yibo Du, Ziyan Ma and ZhifengWang were employed by the company China Coal Technology Engineering Group. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05355"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>Cooperative Camera-LiDAR Extrinsic Calibration for Vehicle-Infrastructure Systems in Urban Intersections</article-title><source>IEEE Internet Things J.</source><year>2025</year><fpage>1</fpage><pub-id pub-id-type="doi">10.1109/JIOT.2025.3567973</pub-id></element-citation></ref><ref id="B2-sensors-25-05355"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><article-title>GF-SLAM: A Novel Hybrid Localization Method Incorporating Global and Arc Features</article-title><source>IEEE Trans. Autom. Sci. Eng.</source><year>2025</year><volume>22</volume><fpage>6653</fpage><lpage>6663</lpage><pub-id pub-id-type="doi">10.1109/TASE.2024.3451297</pub-id></element-citation></ref><ref id="B3-sensors-25-05355"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bar Hillel</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lerner</surname><given-names>R.</given-names></name><name name-style="western"><surname>Levi</surname><given-names>D.</given-names></name><name name-style="western"><surname>Raz</surname><given-names>G.</given-names></name></person-group><article-title>Recent progress in road and lane detection: A survey</article-title><source>Mach. Vis. Appl.</source><year>2014</year><volume>25</volume><fpage>727</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1007/s00138-011-0404-2</pub-id></element-citation></ref><ref id="B4-sensors-25-05355"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Du</surname><given-names>B.</given-names></name></person-group><article-title>Global context based automatic road segmentation via dilated convolutional neural network</article-title><source>Inf. Sci.</source><year>2020</year><volume>535</volume><fpage>156</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.ins.2020.05.062</pub-id></element-citation></ref><ref id="B5-sensors-25-05355"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Muthu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Maken</surname><given-names>F.A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Homography Guided Temporal Fusion for Road Line and Marking Segmentation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>1&#8211;6 October 2023</conf-date><fpage>1075</fpage><lpage>1085</lpage></element-citation></ref><ref id="B6-sensors-25-05355"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>MCANet: A joint semantic segmentation framework of optical and SAR images for land use classification</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2022</year><volume>106</volume><fpage>102638</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2021.102638</pub-id></element-citation></ref><ref id="B7-sensors-25-05355"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.J.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>M.M.</given-names></name></person-group><article-title>CoANet: Connectivity attention network for road extraction from satellite imagery</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>8540</fpage><lpage>8552</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3117076</pub-id><pub-id pub-id-type="pmid">34618672</pub-id></element-citation></ref><ref id="B8-sensors-25-05355"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>D.</given-names></name><name name-style="western"><surname>Long</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>K.</given-names></name></person-group><article-title>Real-time semantic segmentation for underground mine tunnel</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>133</volume><fpage>108269</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.108269</pub-id></element-citation></ref><ref id="B9-sensors-25-05355"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>H.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>Y.</given-names></name></person-group><article-title>Location estimation of autonomous driving robot and 3D tunnel mapping in underground mines using pattern matched LiDAR sequential images</article-title><source>Int. J. Min. Sci. Technol.</source><year>2021</year><volume>31</volume><fpage>779</fpage><lpage>788</lpage><pub-id pub-id-type="doi">10.1016/j.ijmst.2021.07.007</pub-id></element-citation></ref><ref id="B10-sensors-25-05355"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>N.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name></person-group><article-title>A Point Cloud Segmentation Method for Dim and Cluttered Underground Tunnel Scenes Based on the Segment Anything Model</article-title><source>Remote Sens.</source><year>2023</year><volume>16</volume><elocation-id>97</elocation-id><pub-id pub-id-type="doi">10.3390/rs16010097</pub-id></element-citation></ref><ref id="B11-sensors-25-05355"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name></person-group><article-title>Research on Unstructured Road Boundary Detection</article-title><source>Proceedings of the 2021 IEEE International Conference on Unmanned Systems (ICUS)</source><conf-loc>Beijing, China</conf-loc><conf-date>15&#8211;17 October 2021</conf-date><fpage>614</fpage><lpage>617</lpage></element-citation></ref><ref id="B12-sensors-25-05355"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name></person-group><article-title>Automated 3D road boundary extraction and vectorization using MLS point clouds</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2021</year><volume>23</volume><fpage>5287</fpage><lpage>5297</lpage><pub-id pub-id-type="doi">10.1109/TITS.2021.3052882</pub-id></element-citation></ref><ref id="B13-sensors-25-05355"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name></person-group><article-title>Ego-lane index estimation based on lane-level map and LiDAR road boundary detection</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>7118</elocation-id><pub-id pub-id-type="doi">10.3390/s21217118</pub-id><pub-id pub-id-type="pmid">34770426</pub-id><pub-id pub-id-type="pmcid">PMC8587028</pub-id></element-citation></ref><ref id="B14-sensors-25-05355"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>B.</given-names></name></person-group><article-title>Real-time mine road boundary detection and tracking for autonomous truck</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>1121</elocation-id><pub-id pub-id-type="doi">10.3390/s20041121</pub-id><pub-id pub-id-type="pmid">32085668</pub-id><pub-id pub-id-type="pmcid">PMC7070413</pub-id></element-citation></ref><ref id="B15-sensors-25-05355"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maddiralla</surname><given-names>V.</given-names></name><name name-style="western"><surname>Subramanian</surname><given-names>S.</given-names></name></person-group><article-title>Effective lane detection on complex roads with convolutional attention mechanism in autonomous vehicles</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>19193</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-70116-z</pub-id><pub-id pub-id-type="pmid">39160343</pub-id><pub-id pub-id-type="pmcid">PMC11333608</pub-id></element-citation></ref><ref id="B16-sensors-25-05355"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Swain</surname><given-names>S.</given-names></name><name name-style="western"><surname>Tripathy</surname><given-names>A.K.</given-names></name></person-group><article-title>Real-time lane detection for autonomous vehicles using YOLOV5 Segmentation Model</article-title><source>Int. J. Sustain. Eng.</source><year>2024</year><volume>17</volume><fpage>718</fpage><lpage>728</lpage><pub-id pub-id-type="doi">10.1080/19397038.2024.2400965</pub-id></element-citation></ref><ref id="B17-sensors-25-05355"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yue</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name></person-group><article-title>Robust lane detection from continuous driving scenes using deep neural networks</article-title><source>IEEE Trans. Veh. Technol.</source><year>2019</year><volume>69</volume><fpage>41</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1109/TVT.2019.2949603</pub-id></element-citation></ref><ref id="B18-sensors-25-05355"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name></person-group><article-title>Unstructured lane identification based on hough transform and improved region growing</article-title><source>Proceedings of the 2019 Chinese Control Conference (CCC)</source><conf-loc>Guangzhou, China</conf-loc><conf-date>27&#8211;30 July 2019</conf-date><fpage>7612</fpage><lpage>7617</lpage></element-citation></ref><ref id="B19-sensors-25-05355"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sunwoo</surname><given-names>M.</given-names></name></person-group><article-title>Enhanced road boundary and obstacle detection using a downward-looking LIDAR sensor</article-title><source>IEEE Trans. Veh. Technol.</source><year>2012</year><volume>61</volume><fpage>971</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.1109/TVT.2012.2182785</pub-id></element-citation></ref><ref id="B20-sensors-25-05355"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wijesoma</surname><given-names>W.S.</given-names></name><name name-style="western"><surname>Kodagoda</surname><given-names>K.S.</given-names></name><name name-style="western"><surname>Balasuriya</surname><given-names>A.P.</given-names></name></person-group><article-title>Road-boundary detection and tracking using ladar sensing</article-title><source>IEEE Trans. Robot. Autom.</source><year>2004</year><volume>20</volume><fpage>456</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1109/TRA.2004.825269</pub-id></element-citation></ref><ref id="B21-sensors-25-05355"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>N.</given-names></name></person-group><article-title>A co-point mapping-based approach to drivable area detection for self-driving cars</article-title><source>Engineering</source><year>2018</year><volume>4</volume><fpage>479</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.eng.2018.07.010</pub-id></element-citation></ref><ref id="B22-sensors-25-05355"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rato</surname><given-names>D.</given-names></name><name name-style="western"><surname>Santos</surname><given-names>V.</given-names></name></person-group><article-title>LIDAR based detection of road boundaries using the density of accumulated point clouds and their gradients</article-title><source>Robot. Auton. Syst.</source><year>2021</year><volume>138</volume><fpage>103714</fpage><pub-id pub-id-type="doi">10.1016/j.robot.2020.103714</pub-id></element-citation></ref><ref id="B23-sensors-25-05355"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>Research on Curb Detection and Tracking Method Based on Adaptive Multi-Feature Fusion</article-title><source>Automot. Eng.</source><year>2021</year><volume>43</volume><fpage>1762</fpage><lpage>1770</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05355-f001" orientation="portrait"><label>Figure 1</label><caption><p>Overall workflow for underground tunnel boundary detection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g001.jpg"/></fig><fig position="float" id="sensors-25-05355-f002" orientation="portrait"><label>Figure 2</label><caption><p>The LiDAR coordinate system <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of point clouds is horizontally rotated into the horizontal rotation coordinate system <inline-formula><mml:math id="mm169" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g002.jpg"/></fig><fig position="float" id="sensors-25-05355-f003" orientation="portrait"><label>Figure 3</label><caption><p>The angle <inline-formula><mml:math id="mm170" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> between the LiDAR coordinate system <inline-formula><mml:math id="mm171" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the horizontal rotation coordinate system <inline-formula><mml:math id="mm172" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g003.jpg"/></fig><fig position="float" id="sensors-25-05355-f004" orientation="portrait"><label>Figure 4</label><caption><p>Plane extraction results under four typical scenarios. (<bold>a</bold>,<bold>d</bold>,<bold>g</bold>,<bold>j</bold>) show the original point clouds of straight tunnel, curved tunnel, intersection, and shunting chambers scenarios, respectively. The subsequent two figures in each row represent the top view (<bold>b</bold>,<bold>e</bold>,<bold>h</bold>,<bold>k</bold>) and side view of the extracted ground (<bold>c</bold>,<bold>f</bold>,<bold>i</bold>,<bold>l</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g004.jpg"/></fig><fig position="float" id="sensors-25-05355-f005" orientation="portrait"><label>Figure 5</label><caption><p>The angle <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> between the plane normal vector <inline-formula><mml:math id="mm174" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and the normal vector of the real-time ground <inline-formula><mml:math id="mm175" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. White indicates the plane transformation coordinate system, while yellow indicates the horizontal rotation coordinate system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g005.jpg"/></fig><fig position="float" id="sensors-25-05355-f006" orientation="portrait"><label>Figure 6</label><caption><p>Three-dimensional limits of the point cloud of interest in the transformed plane coordinate system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g006.jpg"/></fig><fig position="float" id="sensors-25-05355-f007" orientation="portrait"><label>Figure 7</label><caption><p>Schematic diagram of the grid map.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g007.jpg"/></fig><fig position="float" id="sensors-25-05355-f008" orientation="portrait"><label>Figure 8</label><caption><p>Data collection environment.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g008.jpg"/></fig><fig position="float" id="sensors-25-05355-f009" orientation="portrait"><label>Figure 9</label><caption><p>Data scenarios.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g009.jpg"/></fig><fig position="float" id="sensors-25-05355-f010" orientation="portrait"><label>Figure 10</label><caption><p>Point cloud horizontal correction result. The area within the yellow dashed line is the calculation zone in front of the vehicle, and the red dashed line represents the reference line for the boundary of the drivable area.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g010.jpg"/></fig><fig position="float" id="sensors-25-05355-f011" orientation="portrait"><label>Figure 11</label><caption><p>Boundary detection results in a straight tunnel segment. (<bold>a</bold>) Key point extraction; (<bold>b</bold>) boundary fitting; (<bold>c</bold>) ground truth; (<bold>d</bold>) temporal consistency maintenance of boundaries.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g011.jpg"/></fig><fig position="float" id="sensors-25-05355-f012" orientation="portrait"><label>Figure 12</label><caption><p>Boundary detection result in the curved road scenario.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g012.jpg"/></fig><fig position="float" id="sensors-25-05355-f013" orientation="portrait"><label>Figure 13</label><caption><p>Boundary detection result in the intersection scenario.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g013.jpg"/></fig><fig position="float" id="sensors-25-05355-f014" orientation="portrait"><label>Figure 14</label><caption><p>Boundary detection result in the chamber scenario. The true values of boundary fitting are represented in pink, while Boundary fitting is shown in white.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g014.jpg"/></fig><fig position="float" id="sensors-25-05355-f015" orientation="portrait"><label>Figure 15</label><caption><p>Comparison of different types of roads detected by different methods. From left to right, they are on the straight tunnel, curved tunnel, intersection, and shunting chambers.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05355-g015.jpg"/></fig><table-wrap position="float" id="sensors-25-05355-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05355-t001_Table 1</object-id><label>Table 1</label><caption><p>Pass-through filter parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Filter Field</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Filter Range</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Inverted</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-axis</td><td align="center" valign="middle" rowspan="1" colspan="1">(0, 20)</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm177" overflow="scroll"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>&#8243;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>-axis</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(&#8722;2, 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05355-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05355-t002_Table 2</object-id><label>Table 2</label><caption><p>Experimental platform description.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Category</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Details</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Operating System</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ubuntu 20.04 LTS</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">8-core Arm&#174;Cortex&#174;-A78AE v8.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64-bit CPU 2MB L2 + 4MB L3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA (Santa Clara, CA, USA) Ampere architecture with</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1792 NVIDIA CUDA&#174;cores and 56 tensor cores</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Memory</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32GB 256-bit LPDDR5 204.8 GB/s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CUDA Version</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CUDA 11.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">cuDNN Version</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">cuDNN 8.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Python Version</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Python 3.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Development Environment</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PyCharm 2023.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Other Libraries</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NumPy 1.26.4, OpenCV 3.4.9.31, PyTorch 2.3.1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05355-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05355-t003_Table 3</object-id><label>Table 3</label><caption><p>Metrics for the method in four typical scenarios: precision, recall, F1-score, Mean Squared Error (MSE), time (in milliseconds).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Scenarios</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MSE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Time (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Straight road</td><td align="center" valign="middle" rowspan="1" colspan="1">97.5%</td><td align="center" valign="middle" rowspan="1" colspan="1">96.9%</td><td align="center" valign="middle" rowspan="1" colspan="1">97.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.015</td><td align="center" valign="middle" rowspan="1" colspan="1">29</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Curved road</td><td align="center" valign="middle" rowspan="1" colspan="1">93.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">91.0%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.1%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.062</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Intersection</td><td align="center" valign="middle" rowspan="1" colspan="1">85.0%</td><td align="center" valign="middle" rowspan="1" colspan="1">81.8%</td><td align="center" valign="middle" rowspan="1" colspan="1">83.0%</td><td align="center" valign="middle" rowspan="1" colspan="1">0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">45</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shunting chambers</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.11%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05355-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05355-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of detection performance across different methods: precision metrics in straight tunnels, curved tunnels, intersections, and shunting chambers, with average processing time per frame (ms).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Methods</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Precision</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Average Time <break/> per Frame (ms)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Straight Tunnel</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Curved Tunnel</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Intersection</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Shunting Chambers</bold>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Proposed method</td><td align="center" valign="middle" rowspan="1" colspan="1">97.5%</td><td align="center" valign="middle" rowspan="1" colspan="1">93.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">85.0%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.3%</td><td align="center" valign="middle" rowspan="1" colspan="1">35 ms</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Comparison method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.0%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.6%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.8%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39 ms</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>