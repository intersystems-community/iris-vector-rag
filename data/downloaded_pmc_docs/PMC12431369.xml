<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431369</article-id><article-id pub-id-type="pmcid-ver">PMC12431369.1</article-id><article-id pub-id-type="pmcaid">12431369</article-id><article-id pub-id-type="pmcaiid">12431369</article-id><article-id pub-id-type="doi">10.3390/s25175241</article-id><article-id pub-id-type="publisher-id">sensors-25-05241</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Stereo Vision-Based Underground Muck Pile Detection for Autonomous LHD Bucket Loading</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-2346-3897</contrib-id><name name-style="western"><surname>Hennen</surname><given-names initials="E">Emilia</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="c1-sensors-25-05241" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-8990-5072</contrib-id><name name-style="western"><surname>Pekarski</surname><given-names initials="A">Adam</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><name name-style="western"><surname>Storoschewich</surname><given-names initials="V">Violetta</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4171-9604</contrib-id><name name-style="western"><surname>Clausen</surname><given-names initials="E">Elisabeth</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Friedrich</surname><given-names initials="CM">Christoph M.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05241">Institute for Advanced Mining Technologies (AMT), RWTH Aachen University, 52062 Aachen, Germany; <email>apekarski@amt.rwth-aachen.de</email> (A.P.); <email>vstoroschewich@amt.rwth-aachen.de</email> (V.S.); <email>eclausen@amt.rwth-aachen.de</email> (E.C.)</aff><author-notes><corresp id="c1-sensors-25-05241"><label>*</label>Correspondence: <email>ehennen@amt.rwth-aachen.de</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5241</elocation-id><history><date date-type="received"><day>11</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>08</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>21</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>23</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 13:25:28.783"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05241.pdf"/><abstract><p>To increase the safety and efficiency of underground mining processes, it is important to advance automation. An important part of that is to achieve autonomous material loading using load&#8211;haul&#8211;dump (LHD) machines. To be able to autonomously load material from a muck pile, it is crucial to first detect and characterize it in terms of spatial configuration and geometry. Currently, the technologies available on the market that do not require an operator at the stope are only applicable in specific mine layouts or use 2D camera images of the surroundings that can be observed from a control room for teleoperation. However, due to missing depth information, estimating distances is difficult. This work presents a novel approach to muck pile detection developed as part of the EU-funded Next Generation Carbon Neutral Pilots for Smart Intelligent Mining Systems (NEXGEN SIMS) project. It uses a stereo camera mounted on an LHD to gather three-dimensional data of the surroundings. By applying a topological algorithm, a muck pile can be located and its overall shape determined. This system can detect and segment muck piles while driving towards them at full speed. The detected position and shape of the muck pile can then be used to determine an optimal attack point for the machine. This sensor solution was then integrated into a complete system for autonomous loading with an LHD. In two different underground mines, it was tested and demonstrated that the machines were able to reliably load material without human intervention.</p></abstract><kwd-group><kwd>stereo vision</kwd><kwd>automation</kwd><kwd>underground mining</kwd><kwd>environmental perception</kwd><kwd>LHD</kwd></kwd-group><funding-group><award-group><funding-source>NEXGEN SIMS: EU H2020 program</funding-source><award-id>101003591</award-id></award-group><funding-statement>NEXGEN SIMS: EU H2020 program, Grant Agreement No. 101003591.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05241"><title>1. Introduction</title><p>Load&#8211;haul&#8211;dump (LHD) machines are commonly used in underground mines for short-distance material transport. These rubber-tired, articulated machines load material from muck piles in working faces and haul it to dumping locations. The first type of this machine was invented by Wagner at the end of the 1950s [<xref rid="B1-sensors-25-05241" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05241" ref-type="bibr">2</xref>]. According to Tatiya [<xref rid="B3-sensors-25-05241" ref-type="bibr">3</xref>], nowadays, over 75% of the world&#8217;s underground metal mines utilise LHDs. As every mine is unique, to address the specific customer needs, LHDs are available on the market with various bucket sizes, designs, and features.</p><p>Automation plays a major role in increasing operator safety and boosting productivity. There are already solutions with modern LHDs that offer autonomous hauling and dumping [<xref rid="B2-sensors-25-05241" ref-type="bibr">2</xref>] at fixed locations, such as an orepass. The LHD is monitored by an operator from a remote control room, who can take over control if necessary. A bottleneck is the bucket-loading process, which is, in general, not fully automated and, therefore, carried out remotely by an operator. But LHD manufacturers are working on changing this. For example, in June 2023, Sandvik from Sweden launched a new feature called AutoMine<sup>&#174;</sup> AutoLoad 2.0 that enables operators to instruct LHDs on different loading profiles tailored to every drawpoint, thereby eliminating the necessity of an operator&#8217;s assistance when loading from fixed drawpoints [<xref rid="B4-sensors-25-05241" ref-type="bibr">4</xref>]. However, this approach requires the LHD to be trained first, or existing loading profiles must be selected by the operator before loading from a new drawpoint/location. In addition, it is limited to fixed drawpoints that usually have a constant point of attack or at least not much room to vary the attack point.</p><p>To reach fully autonomous bucket-loading, the first challenge to be tackled is autonomously detecting the muck pile to be loaded and its relevant characteristics. The information about the position and shape of the muck pile can then be used to determine the point of attack. This eliminates the need to teach the machine different loading profiles, and a higher level of autonomy can be achieved.</p><p>For sensor-based muck pile detection, the following requirements must be fulfilled:<list list-type="bullet"><list-item><p>The decision of which part of the muck pile to load is made during the drive. Therefore, the detection must take place while driving, as well as from a considerable distance. If the detection were performed from a close distance, the LHD would need to reverse to be able to drive into the muck pile. The resulting loss of time and, consequently, the loss of productivity must be prevented. In the specific application considered in this work, a minimum distance of 15 m between the LHD and the muck pile was specified by LHD manufacturer Epiroc.</p></list-item><list-item><p>The muck pile needs to be differentiated from the drift walls and floor, which consist of the same material. Otherwise, there is a risk of collision and damage to the machine.</p></list-item><list-item><p>Changes in the size and shape of the muck pile, as well as rock fragmentation, must be possible. Moreover, the system must be capable of detecting an empty working face to stop the loading operation and avoid machine damage.</p></list-item><list-item><p>Detection must be possible under varying, even harsh operational (vibration and shocks) and environmental conditions (dust, humidity, and a wide temperature range).</p></list-item><list-item><p>Muck pile detection must be guaranteed without additional light sources in the working faces.</p></list-item></list></p><p>This paper presents a solution for stereo vision-based muck pile detection for autonomous LHD bucket loading, which was developed as part of the Next Generation Carbon Neutral Pilots for Smart Intelligent Mining Systems (NEXGEN SIMS) project funded by the European Union&#8217;s Horizon 2020 research and innovation program. One focus area of the NEXGEN SIMS project was the autonomous material handling, comprising the autonomous loading of the muck pile with an LHD and unloading into an underground mine truck without the intervention of humans [<xref rid="B5-sensors-25-05241" ref-type="bibr">5</xref>]. Firstly, related work on muck pile detection in a mining environment is addressed. The novel stereo-based approach is then described, followed by test applications in two different underground mines. The paper ends with an outlook on future work.</p></sec><sec id="sec2-sensors-25-05241"><title>2. Related Work</title><p><xref rid="sensors-25-05241-t001" ref-type="table">Table 1</xref> shows an overview of previous works tackling the problem of muck pile detection in a mining environment. While many of these works also present methods beyond this particular problem, to look at the loading process more broadly here, only the muck pile detection itself is considered, as this is the most relevant to this work.</p><p>Back in the early 2000s, Whitehorn et al. [<xref rid="B6-sensors-25-05241" ref-type="bibr">6</xref>] utilised a stereo camera to perceive the environment around an LHD in an underground mine. The resulting point clouds were integrated over time to build a digital elevation model of the area and localize the LHD inside this map. However, no detection of specific structures like muck piles took place.</p><p>Sarata et al. [<xref rid="B7-sensors-25-05241" ref-type="bibr">7</xref>] used a stereo camera to observe the environment and build a column model from the resulting point cloud. The pile edges were assumed to be where there are columns in a specified height range above ground. This implicitly assumed a free-standing muck pile on flat ground and was applied above ground. Its precise edge was determined with a laser range finder.</p><p>Magnusson and Almqvist [<xref rid="B8-sensors-25-05241" ref-type="bibr">8</xref>] obtained a point cloud of the environment with an actuated lidar. The system then first determined the eigenvectors and eigenvalues of the points around a given point. To classify this point as belonging to a pile, the surface had to be relatively flat (meaning there had to be one dominant eigenvalue) and the right inclination. Then, nearby points were clustered together and segmented to obtain the final pile detection. The method was tested on simulated data and on a gravel pile, which was stationarily scanned from different positions. It is unclear how well this method would work when applied from a moving vehicle, especially as the complete computation time for a single scan was not given.</p><p>Backman et al. [<xref rid="B9-sensors-25-05241" ref-type="bibr">9</xref>] simulated a narrow drift to perform reinforcement learning in order to determine an optimal attack point. The simulated sensor is a depth camera installed in the stope. Explicit pile detection was not performed, but it had to occur implicitly through the machine learning model to determine an attack point. An attack point was always generated, so the question of whether a muck pile is even visible is sidestepped in this setup.</p><p>Tampier et al. [<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>] combined scans from two 2D laser scanners installed on an LHD machine and used odometry to merge single frames into a complete 3D point cloud. The pile inside this point cloud was then detected based on the normal vectors of the local surface around the points: when the normal vectors point along the y-axis, the point is assumed to belong to the walls. After some additional filters were applied, the slope of the remaining pile was determined, also based on normal vectors, and the pile was considered detected when it was neither too flat nor too steep to be safely loaded. The pile was localized, but the geometric form was not considered here, as the system was specifically designed for narrow drifts typical in sublevel stoping, for which determining an attack point does not make much sense, as the LHD does not have enough room to navigate. The goal was simply to hit the pile while avoiding collisions with the surrounding walls. If the muck pile detection was tested separately, it is unclear from the work.</p><p>Cardenas et al. [<xref rid="B11-sensors-25-05241" ref-type="bibr">11</xref>] built on the work of [<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>] by applying the system to a room-and-pillar mine and modifying it so that it could handle a new environment with wider tunnels where attack points have to be selected to load from a pile evenly. Among other things, the pile detection had been modified to reliably determine its geometric shape and contrast it from the ground. Also, the tunnel geometry was considered to determine whether the machine had enough space to maneuver. From the description, it is unclear whether the muck pile detection is performed while driving at normal speed towards the muck pile or whether the LHD first comes to a stop so that the muck pile detection can be performed from a stationary viewpoint. The distance from which a muck pile is usually detected is also unclear from the experimental results.</p><p>Jari and Lauri [<xref rid="B12-sensors-25-05241" ref-type="bibr">12</xref>] presented AutoLoad 2.0, a system for automatic loading with an LHD. It uses predefined routes on which the LHD navigates. Lidar sensors are used to avoid collisions with the walls. Entering into a muck pile is detected via clutch slip. With this setup designed for fixed drawpoints and narrow drifts, no explicit muck pile detection is needed. This autonomous loading solution, called AutoMine<sup>&#174;</sup> AutoLoad2.0, was launched by Sandvik on the market in 2023 [<xref rid="B4-sensors-25-05241" ref-type="bibr">4</xref>].</p><p>It is unclear whether a previous work performed muck pile detection while driving at normal speed and from large enough distances that the machine can drive towards the attack point without slowing down or even reversing. Achieving this goal would be an important step towards an efficient autonomous loading process. Beyond that, to the best of our knowledge, no previous work used a stereo camera for underground muck pile detection. Compared to a lidar sensor, it provides visual information linked with the computed 3D point cloud, which can be useful for subsequent detection tasks down the line and which makes achieving muck pile detection with a stereo camera a necessary requirement.</p></sec><sec id="sec3-sensors-25-05241"><title>3. Novel Approach to Muck Pile Detection</title><p>Our proposed method is based on a stereo camera mounted on an LHD. It continuously creates images and computes a point cloud from them. In the next step, this point cloud is further processed using a topological algorithm to decide whether it contains a muck pile and extract it. This information is sent to an internal software system controlling the LHD. This system will use the information to determine a good attack point and navigate to this point. The main components of the setup are illustrated in <xref rid="sensors-25-05241-f001" ref-type="fig">Figure 1</xref>.</p><sec id="sec3dot1-sensors-25-05241"><title>3.1. Stereo Vision from an LHD</title><p>For the stereo camera system, two monochrome cameras were chosen. Monochrome cameras produce grayscale images. As each pixel captures the entire incident light, they have a greater sensitivity to light compared to color cameras [<xref rid="B13-sensors-25-05241" ref-type="bibr">13</xref>]. In poor lighting conditions, such as those found in underground mines, better image results can be achieved. Color information is unimportant for muck pile detection, as the surrounding area usually consists of the same material. Also, higher frame rates and shorter processing times can be realized with monochrome cameras [<xref rid="B13-sensors-25-05241" ref-type="bibr">13</xref>].</p><p>The stereo camera system used consists of two monochrome cameras mounted on a profile with a baseline distance of approx. 1 m. The stereo camera was then mounted on a sensor platform on top of the LHD, around 5 m behind the bucket. The relatively large baseline allows accurate measurements from a greater distance. Usually, such a large baseline involves the drawback of objects close to the cameras only being seen through one camera, and therefore, no depth can be estimated. In our case, however, this is actually an advantage, as the first few meters before the cameras will mostly show the LHD itself, which is irrelevant for detection tasks. Keeping parts of the LHD out of the point cloud reduces clutter and increases focus on the more relevant parts of the camera images.</p><p>To assist the cameras, additional headlights were installed on the machine. Our experiments showed that, while they are not necessary to detect the muck piles, the additional lights both improve the detection quality and enable the system to work from greater distances, which in turn gives the LHD more time to navigate to the selected attack point.</p><p>The cameras were placed in industrial enclosures (IP66/67) to protect them from dirt and water (both powerful water jets and immersion up to 1 m). In order to secure the cameras from vibrations and shocks during operation, vibration dampers were attached to the mount.</p><p>The camera installation can be seen in <xref rid="sensors-25-05241-f002" ref-type="fig">Figure 2</xref>.</p><p>Even with additional headlights, an underground mining environment will always be relatively dark and, therefore, challenging for cameras. To mitigate this problem, attention was paid to the optical parameters controlling the brightness of the images. In particular, exposure time and gain were considered for this aspect. Exposure time denotes the time during which the optical sensors collect light before they combine it into an image. Gain is an electronic amplification of the optical signal and will linearly increase the brightness of the final image. To adapt to changing light conditions during operation, both exposure time and gain were automatically controlled. Every third frame, these parameters were set so that the upper half of the image achieved, on average, half intensity. The parameters were set only according to the top half to avoid including the bucket that is illuminated by the headlights and thus much brighter than the environment. Including it in determining the average would lead to overall much darker images, in which the environment would be harder to recognize. <xref rid="sensors-25-05241-f003" ref-type="fig">Figure 3</xref> shows an exemplary image in which the area used to determine image brightness is marked with a red rectangle.</p><p>As the cameras are freely mounted on a profile, the system has to be calibrated so that the exact optical parameters of the two cameras and their orientation with respect to one another are known. As long as the cameras stay fixed on the profile, the calibration data does not change. Both the calibration and stereo processing were performed using the Nerian SceneScanPro system, which also provides time synchronization between the cameras, so there will always be two corresponding images taken at the same time.</p><p>The first step is image rectification, during which radial distortions from the cameras are removed, and the two images are projected into a common image plane. Then comes the stereo matching itself; for each pixel in the left image, the corresponding pixel in the right image has to be determined. This problem is simplified due to the image rectification, as the corresponding pixels will be at the same height in both images. To solve the matching problem, a variation in the semi-global matching algorithm [<xref rid="B14-sensors-25-05241" ref-type="bibr">14</xref>] is applied to the rectified images from both cameras. The important output of this algorithm is the disparity; that is, for every pixel in the left image, return the distance to the corresponding pixel in the right image. After that, several post-processing steps are applied to create a high-quality disparity map. These post-processing steps are shown in the following list:<list list-type="bullet"><list-item><p>Consistency check: ensure that the stereo matching has the same result, whether it is applied left-to-right or right-to-left.</p></list-item><list-item><p>Uniqueness check: ensure that the stereo matching has a single optimal solution.</p></list-item><list-item><p>Gap interpolation: fill small gaps in the disparity map using linear interpolation.</p></list-item><list-item><p>Noise reduction: remove outliers in the disparity map.</p></list-item><list-item><p>Speckle filter: remove small, isolated patches with a similar disparity.</p></list-item></list></p><p>In <xref rid="sensors-25-05241-t002" ref-type="table">Table 2</xref>, the different parameters of the cameras and the stereo processing are listed.</p><p>From the disparity map, based on the position of a pixel in the image and the known camera parameters, the position of the pixel in 3D space can be computed using the formula in Equation (<xref rid="FD1-sensors-25-05241" ref-type="disp-formula">1</xref>). By applying this formula to every pixel in the left image, a point cloud is created.<disp-formula id="FD1-sensors-25-05241"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>u</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfenced><mml:mo>&#183;</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mi>v</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mfenced><mml:mo>&#183;</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where the following applies: <list list-type="simple"><list-item><p><inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> = position of point in 3D space;</p></list-item><list-item><p><inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> = image coordinates of pixel;</p></list-item><list-item><p><inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> = image coordinates of principal point;</p></list-item><list-item><p><italic toggle="yes">b</italic> = baseline distance between the two cameras;</p></list-item><list-item><p><italic toggle="yes">d</italic> = disparity at the pixel position;</p></list-item><list-item><p><italic toggle="yes">f</italic> = focal length.</p></list-item></list>
</p></sec><sec id="sec3dot2-sensors-25-05241"><title>3.2. Muck Pile Detection in a Point Cloud</title><p>The interface between the stereo camera system and the computer processing the muck pile detection algorithm was handled via the Robot Operation System (ROS 2 Version Jazzy Jalisco) [<xref rid="B15-sensors-25-05241" ref-type="bibr">15</xref>]. The stereo camera system periodically provides the image from the left camera and a point cloud. The detection system continuously takes the newest data from the stereo camera (when new data is available) and runs the detection algorithm on it. This structure ensures that the detection algorithm will always work on the newest available data and that the system can work together smoothly, regardless if the time between two images from the cameras or the runtime of the detection algorithm is longer.</p><p>A general overview of the muck pile detection algorithm is shown in <xref rid="sensors-25-05241-f004" ref-type="fig">Figure 4</xref>.</p><p>The original point cloud shown in <xref rid="sensors-25-05241-f004" ref-type="fig">Figure 4</xref>A is too large to be processed efficiently. So, as a first step, the point cloud is downsampled using voxelization. Outliers are removed from the downsampled point cloud by removing points that have fewer than n points in a radius, r, around them. In the next step, the surface normals for each point are computed by fitting a plane through the point and its nearest neighbors. The surface normals are used to determine candidate points, i.e., points that could potentially be part of a muck pile, as proposed by [<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>]. Due to the structure of the mine, the floor and roof surface normals have a large vertical and a small horizontal component, while the wall surface normals have a small vertical and a large horizontal component. The surface normals of the muck pile point diagonally upwards and have small diagonal components. Candidate points are selected on the basis of these characteristics. To be more precise, if the horizontal component of the normal vector, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, has an absolute value smaller than 0.3, and the absolute value of the vertical component of the normal vector, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, lies between 0.05 and 0.95, a point is considered a candidate for belonging to a pile.</p><p>The candidate points found in this way are marked in green in <xref rid="sensors-25-05241-f004" ref-type="fig">Figure 4</xref>B. The figure shows that there are points that are part of the muck pile that have not yet been classified as candidate points. In addition, there are points outside the muck pile that have been incorrectly marked as candidate points.</p><p>These points are considered in the following processing step. A KDTree is used to determine the number of candidate points in a neighborhood of each non-candidate point. If there are more than n candidate points in a neighborhood of a non-candidate point, this point becomes a candidate point. To remove the points incorrectly marked as candidate points, the candidate points are clustered. Only the biggest cluster remains part of the candidate points. The result of the processing step is displayed in <xref rid="sensors-25-05241-f004" ref-type="fig">Figure 4</xref>C.</p><p>In the following filtering step, different filters are applied to ensure that the detected cluster of candidate points belongs to a muck pile. For example, the slope of the muck pile is estimated by fitting a plane through it using the Random Sample Consensus (RANSAC) algorithm. If it is too steep, it is assumed to indicate either a wall or a hanging muck pile where it would be too dangerous to load from. In the context of this work, no muck pile will be detected in both cases. In later work, it might make sense to further differentiate these two cases.</p><p>Another filter concerns the distance to the muck pile. This is estimated via the depth dimension of the closest point in the detected muck pile. Euclidean distance does not make much sense, as the actual reference should not be the single point where the camera is but the LHD as a whole, which is several meters wide, so the additional computational cost to compute it does not pay off for this filter. We define both a minimum and a maximum distance to the muck pile. The reasoning behind the minimal distance is that, beyond it, the stereo matching becomes less stable, and the geometric structure is harder to detect, as the shovel will occlude most of the muck pile, so the detection result will be unreliable. Beyond that, if we are actually that close to the muck pile, the detection result is not needed anymore, as there is not enough room to maneuver. The reasoning behind the maximal distance is that the detection results also become less reliable beyond a critical distance. While it is good to provide a result as soon as possible, in the current system, the LHD navigation will work based on the first detection and not accept any updates, so the first result should already yield a good segmentation of the muck pile. To assure that, results beyond a maximal distance are discarded to wait for a better view of the muck pile. Still, a relatively large maximum distance was chosen. Upon consultation with Epiroc, which handled the attack point calculation based on the muck pile segmentation, it was determined that an earlier, rough outline of the muck pile would be more useful than a later, better segmentation of it.</p><p>When corners are driven around, the muck pile will become visible bit by bit instead of all at once. It is important that only complete muck piles are sent to the LHD. Otherwise, the draw point cannot be calculated reliably. Two filters were implemented to solve this problem. The first filter checks whether the width of the muck pile candidate is in a certain range. The second filter checks whether the muck pile candidate is roughly centered in the image on the horizontal axis. Only if both filters pass will the candidate points be recognized as a muck pile.</p><p>As a final prevention against false positives due to noise, the result is sent to the LHD only after a muck pile is detected in three consecutive frames.</p><p>In <xref rid="sensors-25-05241-t003" ref-type="table">Table 3</xref>, the different parameters of the muck pile detection algorithm and their experimentally obtained values are collected. The computations on the point cloud were performed using the Open3D library (Version 0.19.0) [<xref rid="B16-sensors-25-05241" ref-type="bibr">16</xref>].</p><p>If the cluster of candidate points passes all the filters, it is considered a muck pile. In this case, the original resolution is restored. To achieve this, a voxel grid is formed around the detected cluster points. All points of the original point cloud that lie within this voxel grid are then selected as muck pile points. The result of this operation is displayed in <xref rid="sensors-25-05241-f004" ref-type="fig">Figure 4</xref>D. This information is then sent to the system in the LHD, which will compute the optimal attack point based on this data.</p><p>Communication with the LHD takes place with the help of gRPC (Version 1.73.0) [<xref rid="B17-sensors-25-05241" ref-type="bibr">17</xref>]. gRPC is a framework for the communication between devices based on remote procedure calls. When the LHD is approaching the stope from which it is about to load material, the loading process starts. From that point on, periodic queries are sent to the muck pile detection system, whether a muck pile can currently be detected or not, which is answered with a yes-or-no answer. This answer is based on the last completed detection to keep the time delay between query and answer as short as possible. If a muck pile is detected, a different query for its points is sent, which is answered with the detected muck pile points corresponding to the previous successful detection. These are then used for further processing beyond the scope of this work. Once a muck pile segmentation is sent, no further queries will be sent in the current loading process. This makes it important to only send a result when a segmentation of sufficiently high quality has been obtained. Only when the next loading process starts will the pile detection system start again to produce a fresh segmentation of the pile.</p><p>Another application of the gRPC interface was establishing time synchronization between the different systems. The computations took about 250 ms for a point cloud, which reduces the effective frame rate of the complete processing pipeline to 4 Hz and introduces a delay between the moment the cameras take an image and the moment a corresponding pile segmentation reaches the internal system in the LHD. During this delay, the LHD is moving and thus changing its position and orientation. Preliminary tests showed that, while the change in position is negligible, the change in orientation is not. When all systems are time-synchronized, the delay can easily be computed, and therefore, the discrepancy due to this delay can be compensated. This delay could be reduced with higher-grade hardware, including using a GPU for the point-cloud computations. However, as the successful experiments show, the system already works well with the current setup, which has a low power consumption due to the lower-grade hardware.</p></sec></sec><sec id="sec4-sensors-25-05241"><title>4. Test Setup and Results</title><p>The muck pile detection system was tested in two different underground mines. The first mine is the Kvarntorp mine near &#214;rebro in Sweden. It is a former room-and-pillar limestone mine. Today, it is no longer in production but is used as a test mine by Epiroc. The second mine is the Kittil&#228; mine in northern Finland operated by Agnico Eagle [<xref rid="B18-sensors-25-05241" ref-type="bibr">18</xref>]. The Kittil&#228; mine is the largest active gold mine in Europe, producing about 230,000 oz of gold per year using the cut-and-fill method.</p><p>In neither mine were external light sources employed at the pile, as they are unrealistic at an active stope.</p><p>For the tests and demonstration of the system, an Epiroc Scooptram ST14 (Epiroc, Stockholm, Sweden) was used. It has a loading capacity of 14 t [<xref rid="B19-sensors-25-05241" ref-type="bibr">19</xref>]. An image of the machine is shown in <xref rid="sensors-25-05241-f005" ref-type="fig">Figure 5</xref>. The muck pile detection system was integrated into a larger loading system, enabling the machine to complete the whole process autonomously without a driver while being supervised remotely.</p><p>At first, isolated tests of the stereo camera were performed in Kvarntorp to make sure it could accomplish the task. These tests provided insight into the specific mechanical requirements of the installation and also data that was used to tune the optical parameters and the parameters of the detection algorithm.</p><sec id="sec4dot1-sensors-25-05241"><title>4.1. Integration Test in Kvarntorp Mine</title><p>As the next step, integration tests were performed in Kvarntorp to make sure the system works in combination with the attack point analyzer to provide the necessary data for automatic loading. During the final integration test, 44 automatic loading attempts were performed on two different muck piles. The first pile was 11.4 m wide. Its stope could be entered in a straight line or around a curve, both of which were tested. The second pile was 10.7 m wide. Its stope could only enter around a curve. This pile was more challenging, as, due to repeated loading and unloading, the material became very fine. Since it also had high water content, in combination, it looked mostly like mud. This yielded less optical contrast inside the pile than would be expected from fragmented rock, which made the stereo matching more challenging. <xref rid="sensors-25-05241-f006" ref-type="fig">Figure 6</xref> shows a side-by-side comparison of the two piles.</p><p>Sixty-one percent of the trials were successful. Thirty percent failed due to navigation problems unrelated to the muck pile detection system. This includes failures to enter the stope, failure to successfully navigate to an attack point, and failure to exit the stope after the shovel was filled. Nine percent failed because, after the 36th trial, one of the cameras was hit with dirt, which disrupted the stereo matching. This happened only once, but it caused four trials to fail before the problem was identified and solved.</p><p>In the trials where a bucket could be filled, the amount was estimated both by volume optically and by tonnage using an integrated scale. While the tonnage could be measured much more accurately, it was systematically biased: the bucket was not large enough to accommodate the relatively low density of the material, so, in many test runs, the nominal load was far from being reached, although the bucket was already full. On average, over all applicable trials, the bucket was filled to 94% and carried a mass of 9.3 t.</p><p>Illustrative examples of the muck pile detection results from a single test run are shown in <xref rid="sensors-25-05241-f007" ref-type="fig">Figure 7</xref>. A rough outline of the muck pile is already detected at a distance of 30 m from the camera. The quality of the segmentation greatly improves at a distance of 25 m, which still leaves time to maneuver to an attack point.</p><p>The relationship between the distance to the muck pile and the time until the LHD will collide with the muck pile is illustrated with concrete values in <xref rid="sensors-25-05241-t004" ref-type="table">Table 4</xref>. These results are obtained by varying the maximum distance parameter described in <xref rid="sec3dot2-sensors-25-05241" ref-type="sec">Section 3.2</xref> in recordings of different loading sequences. This includes three loading sequences where the LHD entered the stope around a curve relatively close to the pile and five loading sequences where the LHD approached the pile in a straight line for more than 30 m. After each loading sequence, the loaded material was dumped back onto the respective pile, so it changed its shape without getting smaller. It should be noted that these distances are computed from the stereo camera. The tip of the shovel is around 5 m in front of the cameras. A sharp difference between a straight approach to the pile and an approach around a curve is apparent, as, in the latter case, the LHD might already be relatively close to the pile when it is around the curve enough that the pile is in the field of view of the cameras.</p><p>In <xref rid="sensors-25-05241-t005" ref-type="table">Table 5</xref>, the accuracy of the detection is evaluated via the estimated and true widths of the segmented pile at a distance of 25 m from the left camera. Again, the types of approach to a pile have to be differentiated: when approaching around a curve, the system often already detects and segments parts of the pile before it comes into view completely, which leads to vastly underestimating its width. With a straight approach, this problem does not occur, leading to much smaller errors.</p><p><xref rid="sensors-25-05241-t006" ref-type="table">Table 6</xref> compares these accuracies to the ones reported in [<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>] (the mean relative error was manually computed from values in <xref rid="sensors-25-05241-t006" ref-type="table">Table 6</xref> of the cited work). This comparison should be taken with a grain of salt, as the two systems were tested in different environments (the former room-and-pillar Kvarntorp mine in the case of this work and a sublevel stoping mine in Chile in the case of [<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>]). The mine in Chile has much narrower tunnels with a pile width between 3 m and 3.4 m compared to 10.6 m to 12.0 m in Kvarntorp. It is not quite clear how far from the pile the LHD was during the measurements (or, rather, at what range of distances, as several measurements were integrated over time). It seems to be a relatively straight approach to the pile there. All in all, this comparison should be treated as only a rough estimate, as several parameters either significantly differ from each other or are at least unclear.</p></sec><sec id="sec4dot2-sensors-25-05241"><title>4.2. System Demonstration in Kittil&#228; Mine</title><p>Finally, the complete system was demonstrated in front of the NEXGEN SIMS project consortium in the Kittil&#228; mine. A video of this demonstration was published under <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.youtube.com/watch?v=z9gNyciqV7Y">https://www.youtube.com/watch?v=z9gNyciqV7Y</uri> (accessed on 30 April 2025).</p><p>The final demonstration of the autonomous material handling took place at a depth of 750 m within Agnico Eagle&#8217;s Kittil&#228; gold mine, located in Finland. It included a fully autonomous material handling cycle utilizing a battery-electric LHD and mine truck from the manufacturer, Epiroc (Stockholm, Sweden). The demonstration was monitored from a nearby control room set up outside the Autonomous Operating Zone (AOZ), which was designated for the autonomous machine operation. The muck pile used for the demonstration of autonomous loading was about 11 m wide.</p><p>The cycle begins with the LHD autonomously entering the working face with the muck pile. As the LHD approaches the muck pile, it is detected via the installed stereo camera system. The detection happens while driving, so the information is used with internal software to determine the best attack point to enter. The LHD drives to the attack point to fill the bucket. After that, the material is hauled to the defined mine truck position. The LHD and the mine truck communicate with each other via a 5G network installed in the AOZ. As soon as the mine truck confirms that it is ready for loading, the bucket is unloaded into the mine truck. This process is repeated multiple times until the mine truck is filled. Every time the LHD returns to the muck pile, its shape has changed.</p><p>During this demonstration, the autonomous loading from the muck pile with a driverless LHD worked well, successfully proving the effectiveness of the complete system, including the component for muck pile detection. During preparation for this demonstration, the parameters optimized in the Kvarntorp mine still worked well in this new environment, indicating the robustness of the detection algorithm to different underground environments.</p></sec><sec id="sec4dot3-sensors-25-05241"><title>4.3. Limitations</title><p>An important limitation of the sensor setup at the current stage is that the cameras are missing a cleaning mechanism against dust and dirt hitting their case. As the whole system depends on matching two camera images, the quality is reduced drastically if parts of one image are blocked out due to dust. It might also make sense to try to at least detect the problem algorithmically, as the current system will just output nonsensical results in that case. During the experiments, it was fortunately only a minor problem, as the cameras on top of and in the middle of the LHD were not hit with much dust or dirt, but this cannot be guaranteed in a production environment.</p><p>Neither mine in which this system was tested featured large amounts of airborne dust, which would limit the visibility range of the cameras. Before the muck pile system is employed in such an environment, further experiments would be necessary to estimate the influence of airborne dust on the system.</p><p>An additional limitation is that the algorithm expects the muck pile to be consecutive and removes any parts not connected to the main muck pile during the clustering step described in <xref rid="sec3dot2-sensors-25-05241" ref-type="sec">Section 3.2</xref> as noise. While it is important not to have such noise in the detection, this means that smaller muck piles not directly connected to the main muck pile will be discarded, although they might be useful to consider when deciding how to approach the muck pile, as they may represent material lying apart from the main muck pile.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05241"><title>5. Conclusions and Future Work</title><sec sec-type="conclusions" id="sec5dot1-sensors-25-05241"><title>5.1. Conclusions</title><p>This work has presented a novel system for detecting muck piles in underground mining environments using a stereo camera. A 3D representation of the environment was constructed by processing the stereo images. Using geometric conditions, a muck pile could then be detected and segmented. Based on this segmentation, autonomous loading from a muck pile could be performed. This proves that a stereo camera is indeed a viable sensor to provide the three-dimensional data necessary for material loading with an LHD. The muck pile detection inside a point cloud from the stereo camera was performed using a geometry-based algorithm.</p><p>It was demonstrated to work reliably in two different mines as a component integrated into an autonomous loading system. The detection system both is robust against motion blur and can compute results fast enough so that the muck pile detection works from an LHD driving at regular speed. This means no time is lost on the LHD slowing down or stopping. Therefore, the system can be efficiently integrated into existing processes without increasing the cycle time.</p></sec><sec id="sec5dot2-sensors-25-05241"><title>5.2. Future Work</title><p>For long-term deployment in a production environment, a wiper should be added to the cameras to remove dust and dirt from them. It might also make sense to add a dirt detection routine by looking for portions of the image that are completely black over several frames.</p><p>The algorithm could be extended to cover more variants in the environment. One example from the experiments is smaller, unconnected muck piles in the same stope as the main muck pile. When the system is applied in a production setting over a longer period of time, more unforeseen situations like this will probably occur. Incorporating them into the algorithm would make it more robust and allow for a higher level of automation. In any case, it is critical to be aware of the assumptions behind the systems to recognize where it can be applied reliably.</p><p>Finally, there are still important issues beyond the scope of this work. For example, if a large enough boulder is in the muck pile, it needs to be recognized as such, as it requires special treatment and cannot just be loaded normally together with the surrounding rock. To achieve autonomous driving, the sensor systems on the machine also need to provide obstacle detection to prevent accidents.</p></sec></sec></body><back><ack><title>Acknowledgments</title><p>We would like to thank our project partners in NEXGEN SIMS for their support and great cooperation in our project work. The work was conducted within the NEXGEN SIMS project as part of work package 6, and it received funding from the EU H2020 program under No. 101003591.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, V.S. and E.C.; data curation, E.H. and A.P.; funding acquisition, E.C.; investigation, E.H., A.P. and V.S.; methodology, E.H. and V.S.; project administration, V.S.; software, E.H. and A.P.; supervision, E.C.; validation, E.H., A.P. and V.S.; visualization, E.H. and A.P.; writing&#8212;original draft, E.H., A.P. and V.S.; Writing&#8212;review and editing, E.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Restrictions apply to the availability of these data. Data were obtained from mines of Epiroc and Agnico Eagle Mines Ltd. and are available from the authors with the permission of the respective mine owner.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05241"><label>1.</label><element-citation publication-type="webpage"><article-title>International Mining Technology Hall of Fame 2015 Underground Production: Eddie Wagner</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://im-mining.com/site/wp-content/uploads/2020/06/UG-PRODUCTION-Eddie-Wagner-low-res.pdf" ext-link-type="uri">https://im-mining.com/site/wp-content/uploads/2020/06/UG-PRODUCTION-Eddie-Wagner-low-res.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-12">(accessed on 12 March 2025)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-05241"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tryggvesson</surname><given-names>O.</given-names></name></person-group><source>Automated Underground Loader</source><publisher-name>Swedish Rock Engineering Association and Authors</publisher-name><publisher-loc>Stockholm, Sweden</publisher-loc><year>2010</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://svbergteknik.se/wp-content/uploads/2024/11/BK2010.001.pdf" ext-link-type="uri">https://svbergteknik.se/wp-content/uploads/2024/11/BK2010.001.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-12">(accessed on 12 March 2025)</date-in-citation></element-citation></ref><ref id="B3-sensors-25-05241"><label>3.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tatiya</surname><given-names>R.R.</given-names></name></person-group><source>Surface and Underground Excavations</source><edition>2nd ed.</edition><publisher-name>CRC Press</publisher-name><publisher-loc>Leiden, The Netherlands</publisher-loc><year>2013</year><fpage>181</fpage></element-citation></ref><ref id="B4-sensors-25-05241"><label>4.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Koppanen</surname><given-names>J.</given-names></name></person-group><article-title>Sandvik Introduces AutoMine<sup>&#174;</sup> AutoLoad 2.0 for Improved Autonomous Bucket Loading</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.rocktechnology.sandvik/en/news-and-media/news-archive/2023/06/sandvik-introduces-automine-autoload-2.0-for-improved-autonomous-bucket-loading" ext-link-type="uri">https://www.rocktechnology.sandvik/en/news-and-media/news-archive/2023/06/sandvik-introduces-automine-autoload-2.0-for-improved-autonomous-bucket-loading</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-12">(accessed on 12 March 2025)</date-in-citation></element-citation></ref><ref id="B5-sensors-25-05241"><label>5.</label><element-citation publication-type="webpage"><article-title>NEXGEN SIMS Autonomous Material Handling</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.nexgensims.eu/autonomous-material-handling/" ext-link-type="uri">https://www.nexgensims.eu/autonomous-material-handling/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-14">(accessed on 14 March 2025)</date-in-citation></element-citation></ref><ref id="B6-sensors-25-05241"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Whitehorn</surname><given-names>M.</given-names></name><name name-style="western"><surname>Vincent</surname><given-names>T.</given-names></name><name name-style="western"><surname>Debrunner</surname><given-names>C.</given-names></name><name name-style="western"><surname>Steele</surname><given-names>J.</given-names></name></person-group><article-title>Stereo vision in LHD automation</article-title><source>IEEE Trans. Ind. Appl.</source><year>2003</year><volume>39</volume><fpage>21</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1109/TIA.2002.807245</pub-id></element-citation></ref><ref id="B7-sensors-25-05241"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sarata</surname><given-names>S.</given-names></name><name name-style="western"><surname>Koyachi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sugawara</surname><given-names>K.</given-names></name></person-group><article-title>Field test of autonomous loading operation by wheel loader</article-title><source>Proceedings of the 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>Nice, France</conf-loc><conf-date>22&#8211;26 September 2008</conf-date><fpage>2661</fpage><lpage>2666</lpage></element-citation></ref><ref id="B8-sensors-25-05241"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Magnusson</surname><given-names>M.</given-names></name><name name-style="western"><surname>Almqvist</surname><given-names>H.</given-names></name></person-group><article-title>Consistent pile-shape quantification for autonomous wheel loaders</article-title><source>Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>25&#8211;30 September 2011</conf-date><fpage>4078</fpage><lpage>4083</lpage></element-citation></ref><ref id="B9-sensors-25-05241"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Backman</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lindmark</surname><given-names>D.</given-names></name><name name-style="western"><surname>Bodin</surname><given-names>K.</given-names></name><name name-style="western"><surname>Servin</surname><given-names>M.</given-names></name><name name-style="western"><surname>M&#246;rk</surname><given-names>J.</given-names></name><name name-style="western"><surname>L&#246;fgren</surname><given-names>H.</given-names></name></person-group><article-title>Continuous control of an underground loader using deep reinforcement learning</article-title><source>Machines</source><year>2021</year><volume>9</volume><elocation-id>216</elocation-id><pub-id pub-id-type="doi">10.3390/machines9100216</pub-id></element-citation></ref><ref id="B10-sensors-25-05241"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tampier</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mascar&#243;</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ruiz-Del-solar</surname><given-names>J.</given-names></name></person-group><article-title>Autonomous loading system for load-haul-dump (Lhd) machines used in underground mining</article-title><source>Appl. Sci.</source><year>2021</year><volume>11</volume><elocation-id>8718</elocation-id><pub-id pub-id-type="doi">10.3390/app11188718</pub-id></element-citation></ref><ref id="B11-sensors-25-05241"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cardenas</surname><given-names>D.</given-names></name><name name-style="western"><surname>Loncomilla</surname><given-names>P.</given-names></name><name name-style="western"><surname>Inostroza</surname><given-names>F.</given-names></name><name name-style="western"><surname>Parra-Tsunekawa</surname><given-names>I.</given-names></name><name name-style="western"><surname>Ruiz-del Solar</surname><given-names>J.</given-names></name></person-group><article-title>Autonomous detection and loading of ore piles with load&#8211;haul&#8211;dump machines in Room &amp; Pillar mines</article-title><source>J. Field Robot.</source><year>2023</year><volume>40</volume><fpage>1424</fpage><lpage>1443</lpage></element-citation></ref><ref id="B12-sensors-25-05241"><label>12.</label><element-citation publication-type="patent"><person-group person-group-type="author"><name name-style="western"><surname>Jari</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lauri</surname><given-names>S.</given-names></name></person-group><article-title>Autonomous Mining Vehicle Control</article-title><source>Patent</source><patent>EP4177405A1</patent><day>10</day><month>May</month><year>2023</year></element-citation></ref><ref id="B13-sensors-25-05241"><label>13.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>P.</given-names></name></person-group><article-title>A Short Guide to Why Monochrome Cameras Have the Edge Over Color Cameras</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.e-consystems.com/blog/camera/technology/a-short-guide-to-why-monochrome-cameras-have-the-edge-over-color-cameras/" ext-link-type="uri">https://www.e-consystems.com/blog/camera/technology/a-short-guide-to-why-monochrome-cameras-have-the-edge-over-color-cameras/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-14">(accessed on 14 March 2025)</date-in-citation></element-citation></ref><ref id="B14-sensors-25-05241"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hirschmuller</surname><given-names>H.</given-names></name></person-group><article-title>Accurate and efficient stereo processing by semi-global matching and mutual information</article-title><source>Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#8217;05)</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>20&#8211;25 June 2005</conf-date><volume>Volume 2</volume><fpage>807</fpage><lpage>814</lpage></element-citation></ref><ref id="B15-sensors-25-05241"><label>15.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Stanford Artificial Intelligence Laboratory</collab></person-group><article-title>Robotic Operating System. Version Humble Hawksbill, Released on 5 March 2022</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.ros.org" ext-link-type="uri">https://www.ros.org</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-02-03">(accessed on 3 February 2025)</date-in-citation></element-citation></ref><ref id="B16-sensors-25-05241"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Q.Y.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Koltun</surname><given-names>V.</given-names></name></person-group><article-title>Open3D: A Modern Library for 3D Data Processing</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1801.09847</pub-id><pub-id pub-id-type="arxiv">1801.09847</pub-id></element-citation></ref><ref id="B17-sensors-25-05241"><label>17.</label><element-citation publication-type="webpage"><article-title>gRPC Remote Procedure Calls</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://grpc.io/" ext-link-type="uri">https://grpc.io/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-06">(accessed on 6 December 2024)</date-in-citation></element-citation></ref><ref id="B18-sensors-25-05241"><label>18.</label><element-citation publication-type="webpage"><article-title>Kittil&#228; Mine</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.agnicoeagle.com/English/operations/operations/kittila/default.aspx" ext-link-type="uri">https://www.agnicoeagle.com/English/operations/operations/kittila/default.aspx</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-01-31">(accessed on 31 January 2025)</date-in-citation></element-citation></ref><ref id="B19-sensors-25-05241"><label>19.</label><element-citation publication-type="webpage"><article-title>Scooptram ST14 SG</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.epiroc.com/content/dam/epiroc/underground-mining-and-tunneling/lhd/electric-scooptram/scooptram-st14-sg/technical-specification/9869%200237%2001b%20Scooptram%2014%20SG%20Technical%20Specification_digital.pdf" ext-link-type="uri">https://www.epiroc.com/content/dam/epiroc/underground-mining-and-tunneling/lhd/electric-scooptram/scooptram-st14-sg/technical-specification/9869%200237%2001b%20Scooptram%2014%20SG%20Technical%20Specification_digital.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-03-25">(accessed on 25 March 2025)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05241-f001" orientation="portrait"><label>Figure 1</label><caption><p>Components of the muck pile detection system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05241-g001.jpg"/></fig><fig position="float" id="sensors-25-05241-f002" orientation="portrait"><label>Figure 2</label><caption><p>Camera installation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05241-g002.jpg"/></fig><fig position="float" id="sensors-25-05241-f003" orientation="portrait"><label>Figure 3</label><caption><p>Example image taken from the left camera. The red rectangle shows the area that is set to half intensity on average by controlling the exposure time and the gain. The bright bucket is overexposed due to this optimization, but the environment is easily recognizable.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05241-g003.jpg"/></fig><fig position="float" id="sensors-25-05241-f004" orientation="portrait"><label>Figure 4</label><caption><p>General overview of the operations that the proposed algorithm performs (<bold>left</bold>). Display of different stages during the segmentation (<bold>right</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05241-g004.jpg"/></fig><fig position="float" id="sensors-25-05241-f005" orientation="portrait"><label>Figure 5</label><caption><p>ST14 used for testing the muck pile detection system.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05241-g005.jpg"/></fig><fig position="float" id="sensors-25-05241-f006" orientation="portrait"><label>Figure 6</label><caption><p>The two piles in the Kvarntorp mine on which automated loading was tested.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05241-g006.jpg"/></fig><fig position="float" id="sensors-25-05241-f007" orientation="portrait"><label>Figure 7</label><caption><p>Muck pile detection results from a single loading sequence at a distance of 30 m (<bold>a</bold>), 25 m (<bold>b</bold>) and 20 m (<bold>c</bold>) from the camera respectively. On the left, the image from the left camera is shown, while the right shows the computed point cloud using the color information from the left camera with the detected muck pile points marked in green. The red square has a size of <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m to illustrate the scale. The pile is 12.1 m wide and 3.8 m high. It was estimated as 12.5 m wide and 2.92 m high (<bold>a</bold>), 12.1 m wide and 3.46 m high (<bold>b</bold>) and 12.7 m wide and 3.42 m high (<bold>c</bold>) respectively. Even though this sequence represents a challenging case where the stereo matching suffers under low optical contrast in the pile, relatively good segmentation can still be achieved at a distance of 25 m from the camera.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05241-g007.jpg"/></fig><table-wrap position="float" id="sensors-25-05241-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05241-t001_Table 1</object-id><label>Table 1</label><caption><p>Previous works on muck pile detection in mining environments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Title</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Author</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensor Technology</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Environment</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stereo vision in LHD automation&#160;[<xref rid="B6-sensors-25-05241" ref-type="bibr">6</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mark Whitehorn, Tyrone Vincent, Christian Debrunner, and John Steele</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2003</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stereo camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Edgar Experimental Mine</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Field test of autonomous loading operation by wheel loader&#160;[<xref rid="B7-sensors-25-05241" ref-type="bibr">7</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shigeru Sarata, Noriho Koyachi, and Kazuhiro Sugawara</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2008</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stereo camera, laser range finder</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Free-standing crushed sandstone pile above ground in a field test site in Tsukuba</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Consistent pile-shape quantification for autonomous wheel loaders&#160;[<xref rid="B8-sensors-25-05241" ref-type="bibr">8</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Martin Magnusson and H&#229;kan Almqvist</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2011</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Actuated lidar</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gravel pile above ground</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Continuous control of an underground loader using deep reinforcement learning&#160;[<xref rid="B9-sensors-25-05241" ref-type="bibr">9</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sofi Backman, Daniel Lindmark, Kenneth Bodin, Martin Servin, Joakim M&#246;rk, and H&#229;kan L&#246;fgren</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Depth camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Simulated narrow underground drift with muck pile</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Autonomous Loading System for Load-Haul-Dump (LHD) Machines Used in Underground Mining&#160;[<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Carlos Tampier, Mauricio Mascar&#243;, and Javier Ruiz-Del-solar</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2D lidars</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sublevel stoping mine</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Autonomous detection and loading of ore piles with load&#8211;haul&#8211;dump machines in room-and-pillar mines&#160;[<xref rid="B11-sensors-25-05241" ref-type="bibr">11</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Daniel Cardenas, Patricio Loncomilla, Felipe Inostroza, Isao Parra-Tsunekawa, and Javier Ruiz-Del-solar</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2D and 3D lidars</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Werra Potash Mine (room and pillar)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Autonomous Mining Vehicle Control&#160;[<xref rid="B4-sensors-25-05241" ref-type="bibr">4</xref>,<xref rid="B12-sensors-25-05241" ref-type="bibr">12</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Jasu Jari and Siivonen Lauri</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lidars</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mines with narrow drifts and a fixed drawpoint</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05241-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05241-t002_Table 2</object-id><label>Table 2</label><caption><p>Relevant parameters of the camera setup. These include (<bold>a</bold>) the optical parameters of the cameras and (<bold>b</bold>) the parameters of the stereo matching.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<bold>a</bold>) Optical Parameters</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Resolution</td><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1024</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Focal length</td><td align="left" valign="middle" rowspan="1" colspan="1">6 mm</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Focal ratio</td><td align="left" valign="middle" rowspan="1" colspan="1">f/2.4</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Framerate</td><td align="left" valign="middle" rowspan="1" colspan="1">32 Hz</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Exposure time</td><td align="left" valign="middle" rowspan="1" colspan="1">Set automatically below 30 ms</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gain</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set automatically below 17 dB</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<bold>b</bold>) Stereo Parameters</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Baseline length</td><td align="left" valign="middle" rowspan="1" colspan="1">1 m</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Maximal disparity</td><td align="left" valign="middle" rowspan="1" colspan="1">256 Pixels</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Resolution of disparities</td><td align="left" valign="middle" rowspan="1" colspan="1">1/16th pixel</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Penalty for disparity change on image edge</td><td align="left" valign="middle" rowspan="1" colspan="1">3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Penalty for disparity change without image edge</td><td align="left" valign="middle" rowspan="1" colspan="1">14</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Penalty for disparity discontinuity on image edge</td><td align="left" valign="middle" rowspan="1" colspan="1">22</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Penalty for disparity discontinuity without image edge</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05241-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05241-t003_Table 3</object-id><label>Table 3</label><caption><p>Parameters of the muck pile detection algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameter</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Voxel size for downsampling</td><td align="left" valign="middle" rowspan="1" colspan="1">0.5 m</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Outlier removal</td><td align="left" valign="middle" rowspan="1" colspan="1">Min. 10 neighboring points in a radius of 1&#160;m</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Normal computation neighbors</td><td align="left" valign="middle" rowspan="1" colspan="1">10 nearest neighbors considered</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Normal criteria for muck pile</td><td align="left" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.05</mml:mn><mml:mo>&lt;</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Neighborhood criteria for muck pile</td><td align="left" valign="middle" rowspan="1" colspan="1">5 points in a radius of 1 m belong to pile</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Minimum muck pile width</td><td align="left" valign="middle" rowspan="1" colspan="1">5 m</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Minimum distance from cameras to muck pile</td><td align="left" valign="middle" rowspan="1" colspan="1">5 m</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum distance from cameras to muck pile</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30 m</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05241-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05241-t004_Table 4</object-id><label>Table 4</label><caption><p>Average time from the moment of successful muck pile detection until collision of the LHD with the muck pile for different minimum distances at which the detection is considered. For these averages, five loading sequences in which the pile was approached in a straight line and three loading sequences in which the pile was approached from a curve were examined. In an approach around a curve, more time passes before the pile comes into the view of the cameras, and by then, it is often already relatively close. That is why the overall times are lower, and there is only a slight relationship to the minimum distance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Minimum Distance from Stereo Camera to Muck Pile</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average Time Until Muck Pile Collision (Straight Line)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average Time Until Muck Pile Collision (Curve)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">30 m</td><td align="left" valign="middle" rowspan="1" colspan="1">14.5 s</td><td align="left" valign="middle" rowspan="1" colspan="1">6.1 s</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">25 m</td><td align="left" valign="middle" rowspan="1" colspan="1">11.7 s</td><td align="left" valign="middle" rowspan="1" colspan="1">6.1 s</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20 m</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.6 s</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.5 s</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05241-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05241-t005_Table 5</object-id><label>Table 5</label><caption><p>The accuracy of the estimated pile width for different loading cycles. The accuracy is a lot lower when the pile is approached around a curve, as in that case, segmentation will already be sent before the pile is in full view.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Approach</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">True Pile Width</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Estimated Pile Width</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Absolute Error</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Relative Error</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Straight</td><td align="left" valign="middle" rowspan="1" colspan="1">11.5 m</td><td align="left" valign="middle" rowspan="1" colspan="1">11.2 m</td><td align="left" valign="middle" rowspan="1" colspan="1">0.3 m</td><td align="left" valign="middle" rowspan="1" colspan="1">3%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Curve</td><td align="left" valign="middle" rowspan="1" colspan="1">10.6 m</td><td align="left" valign="middle" rowspan="1" colspan="1">4.94 m</td><td align="left" valign="middle" rowspan="1" colspan="1">5.66 m</td><td align="left" valign="middle" rowspan="1" colspan="1">53%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Curve</td><td align="left" valign="middle" rowspan="1" colspan="1">10.6 m</td><td align="left" valign="middle" rowspan="1" colspan="1">6.79 m</td><td align="left" valign="middle" rowspan="1" colspan="1">3.81 m</td><td align="left" valign="middle" rowspan="1" colspan="1">36%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Curve</td><td align="left" valign="middle" rowspan="1" colspan="1">10.5 m</td><td align="left" valign="middle" rowspan="1" colspan="1">5.75 m</td><td align="left" valign="middle" rowspan="1" colspan="1">4.75 m</td><td align="left" valign="middle" rowspan="1" colspan="1">45%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Straight</td><td align="left" valign="middle" rowspan="1" colspan="1">11.9 m</td><td align="left" valign="middle" rowspan="1" colspan="1">12.4 m</td><td align="left" valign="middle" rowspan="1" colspan="1">0.5 m</td><td align="left" valign="middle" rowspan="1" colspan="1">4%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Straight</td><td align="left" valign="middle" rowspan="1" colspan="1">12.0 m</td><td align="left" valign="middle" rowspan="1" colspan="1">12.2 m</td><td align="left" valign="middle" rowspan="1" colspan="1">0.2 m</td><td align="left" valign="middle" rowspan="1" colspan="1">2%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Straight</td><td align="left" valign="middle" rowspan="1" colspan="1">11.5 m</td><td align="left" valign="middle" rowspan="1" colspan="1">12.6 m</td><td align="left" valign="middle" rowspan="1" colspan="1">1.1 m</td><td align="left" valign="middle" rowspan="1" colspan="1">10%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Straight</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.5 m</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.8 m</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3 m</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05241-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05241-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of the accuracy of the pile detection in [<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>] and in this work. Ref. [<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>] is probably more comparable to a straight approach in this work, although the conditions still differ significantly. The lowest error is marked with italics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Work</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Mean Absolute Error</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Mean Relative Error</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Autonomous Loading System for Load-Haul-Dump (LHD) Machines Used in Underground Mining&#160;[<xref rid="B10-sensors-25-05241" ref-type="bibr">10</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">0.58 m</td><td align="left" valign="middle" rowspan="1" colspan="1">18%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Stereo vision-based underground muck pile detection for
autonomous LHD bucket loading (straight approach)</td><td align="left" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">0.48 m</italic>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">4%</italic>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stereo vision-based underground muck pile detection for
autonomous LHD bucket loading (curved approach)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.7 m</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45%</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>