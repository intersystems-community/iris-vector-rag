<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430888</article-id><article-id pub-id-type="pmcid-ver">PMC12430888.1</article-id><article-id pub-id-type="pmcaid">12430888</article-id><article-id pub-id-type="pmcaiid">12430888</article-id><article-id pub-id-type="doi">10.3390/s25175301</article-id><article-id pub-id-type="publisher-id">sensors-25-05301</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Intraoperative Computed Tomography, Ultrasound, and Augmented Reality in Mesial Temporal Lobe Epilepsy Surgery&#8212;A Retrospective Cohort Study</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Neumann</surname><given-names initials="F">Franziska</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05301" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6545-0622</contrib-id><name name-style="western"><surname>Grote</surname><given-names initials="A">Alexander</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05301" ref-type="aff">1</xref><xref rid="c1-sensors-25-05301" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8641-2919</contrib-id><name name-style="western"><surname>Gjorgjevski</surname><given-names initials="M">Marko</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05301" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3661-9908</contrib-id><name name-style="western"><surname>Carl</surname><given-names initials="B">Barbara</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05301" ref-type="aff">1</xref><xref rid="af2-sensors-25-05301" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Knake</surname><given-names initials="S">Susanne</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af3-sensors-25-05301" ref-type="aff">3</xref><xref rid="af4-sensors-25-05301" ref-type="aff">4</xref><xref rid="af5-sensors-25-05301" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8289-480X</contrib-id><name name-style="western"><surname>Menzler</surname><given-names initials="K">Katja</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af3-sensors-25-05301" ref-type="aff">3</xref><xref rid="af4-sensors-25-05301" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8216-9410</contrib-id><name name-style="western"><surname>Nimsky</surname><given-names initials="C">Christopher</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05301" ref-type="aff">1</xref><xref rid="af4-sensors-25-05301" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1574-7572</contrib-id><name name-style="western"><surname>Bopp</surname><given-names initials="MHA">Miriam H. A.</given-names></name><xref rid="af1-sensors-25-05301" ref-type="aff">1</xref><xref rid="af4-sensors-25-05301" ref-type="aff">4</xref><xref rid="c1-sensors-25-05301" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Iordachita</surname><given-names initials="II">Iulian I.</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Elmi Terander</surname><given-names initials="A">Adrian</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05301"><label>1</label>Department of Neurosurgery, University Hospital Marburg, Philipps University Marburg, Baldingerstrasse, 35043 Marburg, Germany; <email>franziska.neumann@uk-gm.de</email> (F.N.); <email>marko.gjorgjevski@uk-gm.de</email> (M.G.); <email>barbara.carl@helios-gesundheit.de</email> (B.C.); <email>nimsky@med.uni-marburg.de</email> (C.N.)</aff><aff id="af2-sensors-25-05301"><label>2</label>Department of Neurosurgery, Helios Dr. Horst Schmidt Kliniken, Ludwig-Erhard-Stra&#223;e 100, 65199 Wiesbaden, Germany</aff><aff id="af3-sensors-25-05301"><label>3</label>Epilepsy Center Hesse, Department for Neurology, University Hospital Marburg, Philipps University Marburg, Baldingerstrasse, 35043 Marburg, Germany; <email>knake@med.uni-marburg.de</email> (S.K.); <email>katja.menzler@med.uni-marburg.de</email> (K.M.)</aff><aff id="af4-sensors-25-05301"><label>4</label>Center for Mind, Brain and Behavior (CMBB), 35043 Marburg, Germany</aff><aff id="af5-sensors-25-05301"><label>5</label>LOEWE-Research-Cluster for Advanced Medical Physics in Imaging and Therapy (ADMIT), TH Mittelhessen University of Applied Sciences, 35390 Giessen, Germany</aff><author-notes><corresp id="c1-sensors-25-05301"><label>*</label>Correspondence: <email>alexander.grote@uni-marburg.de</email> (A.G.); <email>bauermi@med.uni-marburg.de</email> (M.H.A.B.)</corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5301</elocation-id><history><date date-type="received"><day>27</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>13</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>24</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>26</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05301.pdf"/><abstract><p>Mesial temporal lobe epilepsy (mTLE) surgery, particularly selective amygdalohippocampectomy (sAHE), is a recognized treatment for pharmacoresistant temporal lobe epilepsy (TLE). Accurate intraoperative orientation is crucial for complete resection while maintaining functional integrity. This study evaluated the usability and effectiveness of multimodal neuronavigation and microscope-based augmented reality (AR) with intraoperative computed tomography (iCT) and navigated intraoperative ultrasound (iUS) in 28 patients undergoing resective surgery. Automatic iCT-based registration provided high initial navigation accuracy. Navigated iUS was utilized to verify navigational accuracy and assess the extent of resection during the procedure. AR support was successfully implemented in all cases, enhancing surgical orientation, surgeon comfort, and patient safety, while also aiding training and education. At one-year follow-up, 60.7% of patients achieved complete seizure freedom (ILAE Class 1), rising to 67.9% at the latest follow-up (median 4.6 years). Surgical complications were present in three cases (10.7%), but none resulted in permanent deficits. The integration of microscope-based AR with iCT and navigated iUS provides a precise and safe approach to resection in TLE surgery, additionally serving as valuable tool for neurosurgical training and education.</p></abstract><kwd-group><kwd>epilepsy surgery</kwd><kwd>navigation</kwd><kwd>augmented reality</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05301"><title>1. Introduction</title><p>Augmented reality (AR) is currently a widely used technique in neurosurgery, allowing surgeons to overlay virtually acquired preoperative information onto their view of the patient [<xref rid="B1-sensors-25-05301" ref-type="bibr">1</xref>]. This enhances and integrates standard image-guidance into the surgical field. The first introduction of virtual image overlays into the operating microscope&#8217;s optical view, as proposed by Kelly et al. [<xref rid="B2-sensors-25-05301" ref-type="bibr">2</xref>] and Roberts et al. [<xref rid="B3-sensors-25-05301" ref-type="bibr">3</xref>], laid a solid foundation for developing AR hardware in neurosurgical settings. Although initial AR hardware was limited, the emergence of head-up display (HUD) operating microscopes in the 1990s made AR support accessible to a broader neurosurgical audience [<xref rid="B4-sensors-25-05301" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05301" ref-type="bibr">5</xref>]. Beyond standard image-guidance, which remains the gold standard, microscope-based AR is now frequently used in brain tumor surgeries. It allows for real-time visualization of tumors and vital structural and functional risk areas [<xref rid="B6-sensors-25-05301" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05301" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05301" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05301" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05301" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05301" ref-type="bibr">11</xref>], thereby improving surgical orientation. Additionally, its availability has already been extended to other neurosurgical application areas such as skull base surgery [<xref rid="B12-sensors-25-05301" ref-type="bibr">12</xref>] or vascular surgery [<xref rid="B13-sensors-25-05301" ref-type="bibr">13</xref>].</p><p>Approximately 65 million individuals globally live with epilepsy, of which about one-third are classified as pharmacoresistant [<xref rid="B14-sensors-25-05301" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05301" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05301" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05301" ref-type="bibr">17</xref>]. Patients with pharmacoresistant epilepsy, particularly, most frequently exhibit temporal lobe epilepsy (TLE) [<xref rid="B18-sensors-25-05301" ref-type="bibr">18</xref>]. Today, for carefully selected patients within this group, resective surgery is a well-established treatment option that yields positive outcomes [<xref rid="B18-sensors-25-05301" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05301" ref-type="bibr">19</xref>].</p><p>In the majority of TLE cases, seizures originate in the mesio-basal temporal structures, including the hippocampus, amygdala, and parahippocampal gyrus [<xref rid="B20-sensors-25-05301" ref-type="bibr">20</xref>]. While the traditional approach has involved resecting the anterior temporal lobe (ATL) due to the critical function of the temporomesial structures in TLE, a more tailored method known as selective amygdalohippocampectomy (sAHE)&#8212;which preserves the temporal neocortex&#8212;has been introduced [<xref rid="B20-sensors-25-05301" ref-type="bibr">20</xref>], both before the advent of navigation technology. Various surgical approaches have been described, including the transcortical, transsylvian, and subtemporal techniques, each offering distinct advantages [<xref rid="B20-sensors-25-05301" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05301" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05301" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05301" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05301" ref-type="bibr">24</xref>]. However, as is generally the case in neurosurgery, the primary objective of resective surgery is to excise the targeted structure&#8212;in this case, the epileptogenic focus&#8212;without inducing new neurological deficits. Hence, comprehending the topography of the temporomesial target structures is crucial for the safe and effective selective resection of the amygdala, hippocampus, and parahippocampal gyrus [<xref rid="B20-sensors-25-05301" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05301" ref-type="bibr">21</xref>].</p><p>Intraoperative image-guidance has demonstrated its advantages in epilepsy surgery by correlating imaging data, information obtained from multimodal diagnostics, and the patient&#8217;s anatomical specifics, which benefits safe and successful individualized resection [<xref rid="B25-sensors-25-05301" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05301" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05301" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05301" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-05301" ref-type="bibr">29</xref>]. Expanding beyond conventional navigation, which is typically performed using separate navigation displays close to the surgical field along with dedicated navigation instruments (e.g., pointer), microscope-based augmented reality (AR) offers immediate mapping of image data and patient anatomy. This technology virtualizes the physical instruments&#8217; tip according to the microscope&#8217;s focal point, integrating all information into the surgical view. By eliminating the need to switch instruments or change viewing angles during surgery, microscope-based AR enhances the surgeon&#8217;s mental visualization of the imaging data, simplifying surgical orientation and the mapping of both imaging and anatomical data. This reduces attention shifts, thus improving surgeon comfort [<xref rid="B1-sensors-25-05301" ref-type="bibr">1</xref>,<xref rid="B30-sensors-25-05301" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05301" ref-type="bibr">31</xref>].</p><p>Tailored resection for mTLE such as sAHE with its different tissue saving approaches [<xref rid="B21-sensors-25-05301" ref-type="bibr">21</xref>,<xref rid="B32-sensors-25-05301" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-05301" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05301" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05301" ref-type="bibr">35</xref>] in contrast to ATL is vital for pinpointing target structures and preventing an excessive approach in selected cases. In this case, in particular, accurate image-guidance combined with microscope-based AR might be highly advantageous as AR allows for direct operation on the visualized target structure, thereby minimizing the size of the access path. Furthermore, it might significantly reduce the risk of unintended mesial deviations, especially with transsylvian and transcortical accesses. The use of microscope-based AR support in temporal lobe epilepsy surgery has rarely been studied thus far. A recent report [<xref rid="B36-sensors-25-05301" ref-type="bibr">36</xref>] investigated AR support in a pediatric epilepsy patient cohort with various entities and surgical strategies. However, despite showing the supportive effect of AR assistance, it did not elaborate on AR support in mTLE surgery, further tailoring resection and access paths, in clinical practice and education.</p><p>Consequently, this study seeks to report and evaluate the clinical experience, usability, practicality, and potential of microscope-based AR support in mTLE surgery, also for training and educational purposes. To date, reports focusing specifically on the use of neuronavigation and AR in relation to navigational accuracy essential for surgery and education in TLE surgery have been scarce.</p></sec><sec id="sec2-sensors-25-05301"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-25-05301"><title>2.1. Study Cohort</title><p>In this study, data from 28 patients with pharmacoresistant temporal lobe epilepsy who underwent resective surgery in the temporomesial lobe between September 2016 and December 2024 were analyzed. All patients underwent presurgical assessments following a standard protocol that included clinical, imaging, neuropsychological, and surface electroencephalography (EEG) data, as well as, in part, invasive EEG. This was followed by a surgical recommendation for selective temporomesial lobe resection by the interdisciplinary epilepsy surgery board (epileptology, neurosurgery, neuropsychology, neuroradiology).</p><p>Ethics approval for collecting routine clinical and technical data during the neurosurgical treatment of patients was obtained in accordance with the Declaration of Helsinki by the local ethics committee at the University of Marburg (No. 99/18); the analysis of the collected data was additionally permitted by the local ethics committee (24-214 RS). Written informed consent was provided by all included patients.</p></sec><sec id="sec2dot2-sensors-25-05301"><title>2.2. Preoperative Imaging and Planning</title><p>After initial diagnostic imaging and surgical recommendation, all patients underwent preoperative magnetic resonance imaging (MRI) using a 3T MRI system (Tim Trio, Siemens, Erlangen, Germany) equipped with a 12-channel head matrix Rx-coil. Data acquisition included a 3D T1-weighted, 3D T2-weighted, 3D Fluid Attenuated Inversion Recovery (FLAIR), time-of-flight (ToF) angiography data set, as well as a diffusion-weighted (DWI) single-shot echo-planar imaging (EPI) data set for fiber tractography with 30 non-collinear diffusion encoding gradients (high b-value 1000 s/mm<sup>2</sup>). If relevant, functional MRI (fMRI) data utilizing language tasks to localize Broca&#8217;s and Wernicke&#8217;s area were assessed to determine hemispheric lateralization.</p><p>Following rigid image co-registration of all data sets using the image fusion element (Brainlab, Munich, Germany), automatic segmentation of the amygdala, hippocampus, and brainstem, as well as the cerebrum, was performed using the anatomical mapping element (Brainlab, Munich, Germany) and manually refined if required. In addition, the lateral ventricle and vascular structures (middle cerebral artery) were segmented manually within the smart brush element (Brainlab, Munich, Germany).</p><p>Fiber tractography of the optic radiation and corticospinal tract was performed using the fiber tracking element (Brainlab, Munich, Germany) by that time based on the standard diffusion tensor imaging deterministic approach. If available, fMRI data sets were analyzed using SPM8/SPM12 in accordance with a standard processing protocol (without normalization). Resulting activation clusters (family-wise error corrected) were incorporated into the multimodal preoperative plan.</p></sec><sec id="sec2dot3-sensors-25-05301"><title>2.3. Operating Room Setup</title><p>All patients underwent microscope-based AR-supported resection of temporomesial structures (amygdala, hippocampus) using navigation and microscope technologies in cases of unilateral TLE. The operating room is equipped with an optical neuronavigation system (Curve Navigation, Brainlab, Munich, Germany), a mobile 32-slice intraoperative CT (iCT) system (AIRO<sup>&#174;</sup>, Brainlab, Munich, Germany) for registration and control scans, operating microscopes (Pentero 900 or Kinevo 900, Zeiss, Oberkochen, Germany), and ultrasound systems (FlexFocus 800/BK5000, BK Medical, Herlev, Denmark), both fully integrated into the navigation system with a convex craniotomy transducer, respectively.</p></sec><sec id="sec2dot4-sensors-25-05301"><title>2.4. Intraoperative Workflow</title><p>An overview of the overall workflow is provided in <xref rid="sensors-25-05301-f001" ref-type="fig">Figure 1</xref>, and a detailed description of all procedural steps can be found in the following.</p><p>Under general anesthesia, the patient&#8217;s head was fixated in a radiolucent carbon head clamp (DORO, Black Forest Medical Group, Freiburg, Germany) using three metallic pins with those pins placed outside the area of interest to prevent artifacts in a potential full-dose control scan at the end of resection. To allow for navigational support, a radiolucent patient reference geometry was mounted on the left side of the head clamp. To assess registration accuracy, three adhesive skin markers were attached to the patient&#8217;s head within the scan area.</p><p>A sequential low-dose iCT scan with limited scan length was performed for automatic intraoperative patient registration (7.1 mA, 120 kV, 1.92 s exposure time, 1 mm reconstructed slice thickness, 512 &#215; 512 matrix size, 33.3 cm<sup>2</sup> field of view, 6.2 cm scan length, resulting in a dose-length product of 17.8 mGy*cm). The target registration error (TRE) was estimated using the attached skin markers as offset between the physical pointer&#8217;s tooltip (Cranial Pointer, Brainlab, Munich, Germany) that was placed in the divot of the three skin markers and the tooltip in the virtual representation of the skin markers within the generated iCT data set. After assuring high patient registration using the attached skin markers, the preoperative data including the surgical plan was rigidly co-registered with the low-dose registration iCT scan, allowing instant navigation support.</p><p>Following a standardized skin incision, a navigation-supported tailored craniotomy was performed to assess the temporomesial target structures. Before durotomy, navigated ultrasound (iUS) imaging was performed using a trackable craniotomy transducer (8862/N13C5, BK Medical, Herlev, Denmark) with standard settings (imaging depth 63 mm/65 mm, B-mode) to (1) assess navigation accuracy by overlaying segmented outlines based on the preoperative MRI data onto the live-ultrasound view and (2) to identify the temporomesial target structures and landmarks within the ultrasound data for additional guidance and later intraoperative resection control. Therefore, in addition to the navigated live-ultrasound usage, a 3D iUS data set was acquired by uniformly sweeping the transducer across the accessible dural layer within the craniotomy. Further, a vascular 3D iUS data set (C-Mode) was acquired analogously to allow for visualization of relevant vascular structures.</p><p>Besides navigation support in terms of pointer-based navigation with navigation displays close by the surgical field with fused multimodal image data enriched with the outlined objects and structures, microscope-based navigation using augmented reality was enabled using the microscope navigation element (Brainlab, Munich, Germany). To enable this, the head-up displays (HUDs) of the operating microscopes Pentero 900/Kinevo 900 (Zeiss, Oberkochen, Germany), fully integrated into the navigational setup, were utilized. The microscope was tracked (Optical Tracking System, Brainlab, Munich, Germany) in the navigational space using an attached 4-sphere reference array to allow for AR support throughout the surgery. Before usage, the AR visualization was calibrated to reduce minor spatial initial misalignments using the reference array. Based on that, all outlined objects such as the hippocampus, the amygdala, the lateral ventricle, the brainstem, the vascular tree, the optic radiation, and the corticospinal tract could be visualized utilizing the HUD of the operating microscope within surgical view enabling microscope-based AR support throughout the whole microsurgical part of the procedure. The objects can be visualized using the HUD either in 2D (solid object outlines representing the diameter in the recent focal plan with dotted outlines displaying the maximal object extend beyond) or 3D fashion. However, to adapt to the surgeon&#8217;s needs and the surgical phase, all included object outlines can separately be switched on and off at any time to provide the best support.</p><p>After the dural opening, the navigation accuracy was reassessed using the outlined cortical profile. If there were any slight in-plane deviations (2D, no depth inaccuracies), the augmented overlay was adjusted by translating and rotating it to better fit the patient&#8217;s anatomy. Accurate neuronavigation and microscope-based AR allow for direct operation on the target structure. Once the middle temporal gyrus was located, the cortex was incised minimally based on image-guidance, and dissection was performed towards the temporal horn, which serves as a major anatomical landmark, with image-guidance reducing the risk of unintended mesial deviations. Upon entering the temporal horn, self-retaining brain spatulas were inserted to provide an optimal view. In the initial step, the parahippocampal gyrus was examined and dissected in a temporo-basal direction, ensuring that the medial arachnoidal border was preserved. After the subpial resection of the uncus, the resection was extended medially and posteriorly to remove the hippocampus, preferably en bloc or as anterior and posterior samples. Subsequently, the amygdala was resected, despite the absence of clear anatomical borders.</p><p>Following resection of the temporomesial target structures, navigated intraoperative ultrasound (iUS) was re-evaluated to (1) confirm and assess the extent of the resection while in surgery and (2) rule out any complications along the resection cavity. To achieve this, a 3D iUS data set was obtained by consistently sweeping the transducer over the saline-filled resection cavity.</p></sec><sec id="sec2dot5-sensors-25-05301"><title>2.5. Epileptogenic Outcome</title><p>Epileptogenic outcome was postoperatively assessed according to the International League Against Epilepsy (ILAE) outcome classification [<xref rid="B37-sensors-25-05301" ref-type="bibr">37</xref>] at on-site follow-up examinations one year after surgery and at the most recent follow-up. Complete seizure-freedom (ILAE 1) was considered an excellent epileptogenic outcome, while at latest follow-up, ILAE classes 1, 2, and 3 were considered a favorable outcome [<xref rid="B38-sensors-25-05301" ref-type="bibr">38</xref>].</p><p>Assessment of functions of the dominant and non-dominant mesial temporal lobe with the delayed recognition trial of the verbal learning and memory test (VLMT [<xref rid="B39-sensors-25-05301" ref-type="bibr">39</xref>]) assessing verbal declarative memory functions (encoding/decoding) as well as the delayed recall of the Rey&#8211;Osterrieth Complex Figure Test (ROCFT [<xref rid="B40-sensors-25-05301" ref-type="bibr">40</xref>]) for non-verbal memory function were performed preoperatively and during follow-up.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-05301"><title>3. Results</title><sec id="sec3dot1-sensors-25-05301"><title>3.1. Clinical and Demographic Information</title><p>In this study, data from 28 patients (mean age: 39.75 &#177; 15.49 years, male/female: 14/14) who underwent surgery for mTLE were analyzed. Twelve patients underwent selective amygdalohippocampectomy on the right side, and fourteen on the left side; in two additional cases, only resection of the amygdala was performed. Neuropathological examination of the resected tissue revealed hippocampal sclerosis (<italic toggle="yes">n</italic> = 22), heterotopia (<italic toggle="yes">n</italic> = 2), gliosis (<italic toggle="yes">n</italic> = 2), dysembryoplastic neuroepithelial tumor (<italic toggle="yes">n</italic> = 1), and glioma (<italic toggle="yes">n</italic> = 1).</p></sec><sec id="sec3dot2-sensors-25-05301"><title>3.2. Epileptogenic Outcome</title><p>At the one-year follow-up, 17 patients (60.71%) were categorized as ILAE class 1, with no seizures reported after resective surgery. Five patients were assigned to ILAE class 3 (17.86%), three to ILAE class 4 (10.71%), and two to ILAE class 5 (7.14%). One patient (3.57%) did not undergo any postoperative follow-up examination. In a long-term follow-up with varying intervals, 19 patients (67.86%) remained seizure-free at their most recent follow-up (median: 4.62 years, interquartile range: 2.93), classified as ILAE class 1. Two patients were classified as ILAE class 3 (one had deteriorated from ILAE class 1), while three were categorized as ILAE class 4 (two had deteriorated from class 3 and one from class 1), and three patients were termed ILAE class 5 (one had deteriorated from class 4, the others remained in class 5). At the latest follow-up, 75.00% of patients exhibited favorable outcomes, with 67.86% completely seizure-free. A slight reduction in the initial group classified in ILAE class 1 was noted (two patients), while four patients remained seizure-free during long-term follow-up despite having auras or seizures at the one-year mark; see <xref rid="sensors-25-05301-f002" ref-type="fig">Figure 2</xref>.</p><p>Neuropsychological and neurocognitive assessments were available pre- and postoperatively were available in 21 cases (seven patients did not participate in preoperative or follow-up assessments; one patient was excluded due to age (&lt;5 years)). For patients who underwent MTLE surgery on the dominant side (<italic toggle="yes">n</italic> = 11) results of the delayed recognition trial of the VLMT revealed means of 9.9 &#177; 4.0 vs. 8.0 &#177; 4.8 for preoperative and follow-up assessments. In case of patients with MTLE surgery on the non-dominant side (<italic toggle="yes">n</italic> = 10), results of the delayed recall of the ROCFT revealed means of 13.9 &#177; 5.2 vs. 17.5 &#177; 6.4 for preoperative and follow-up assessments.</p></sec><sec id="sec3dot3-sensors-25-05301"><title>3.3. Complications</title><p>Among the twenty-eight patients who underwent resection, four cases of surgical complications were observed. Two patients developed a subgaleal cerebrospinal fluid (CSF) fistula, which was managed with lumbar CSF drainage for five days. After the drainage was removed, no further intervention was required. Another patient presented with an abscess that necessitated surgical intervention. In the final case, the patient experienced a remote cerebellar hemorrhage, which was addressed with suboccipital decompression and external ventricular drainage. Following these procedures, no additional surgeries were necessary.</p><p>Five patients who had surgery in the dominant temporal lobe developed transient postoperative aphasia, which resolved completely by the time of discharge. One patient experienced postoperative oculomotor paralysis, which fully improved within four weeks after surgery.</p><p>Postoperative visual field deficits (VFDs) were observed in seven cases; all seven cases were confirmed to have quadrantanopia through computerized visual field perimetry. Nine patients did not exhibit any VFD, while in the remaining eleven patients, the examination could not be successfully performed due to compliance issues.</p></sec><sec id="sec3dot4-sensors-25-05301"><title>3.4. Navigation and Augmented Reality Support</title><p>Navigation and microscope-based AR support, enabled by automatic iCT-based registration and intraoperative ultrasound, was facilitated in all surgeries with a mean initial TRE of 0.74 &#177; 0.28 mm. In all cases, visualization of target structures and risk structures related to the surgical trajectory or in proximity of the surgical target was provided in the surgical plan and enhanced the microscopic view throughout the surgery. Provided visualizations included outlines of the brainstem (risk structure), amygdala and hippocampus (target structures), lateral ventricle (landmark), carotid arteria and media (risk structures/landmark), cortical vessels and cerebrum (verification/navigation update) and fiber tractography of the optic radiation (risk structure). To provide efficient AR support, tailored to the surgeon&#8217;s preferences and needs, all structures could be separately switched on and off. In this way, microscope-based AR support allowed for improved intraoperative orientation for the surgeon but also the assisting surgeon (residents) and the OR staff using the microscopic view displayed on the navigation screens close by, and therefore contributed to patient safety while in parallel increasing surgeon&#8217;s comfort.</p></sec><sec id="sec3dot5-sensors-25-05301"><title>3.5. Workflow Illustrations</title><p>The applicability of navigation and microscope-based AR support during the surgical procedure and educational purposes tremendously depends on the navigational accuracy. The presented setup allowed for different methods to gain, keep, and restore navigation accuracy throughout the procedure. In the recent setup, automatic intraoperative CT-based registration is used for mapping of image and patient data showing a high registration accuracy with very low TREs estimated using artificial landmarks. Navigation accuracy can be verified and quantified in different ways throughout the procedure, e.g., using artificial landmarks not used for the registration procedure assessed with a pointer (see <xref rid="sensors-25-05301-f003" ref-type="fig">Figure 3</xref>A) or using microscope-based AR with segmented outlines of the landmarks (see <xref rid="sensors-25-05301-f003" ref-type="fig">Figure 3</xref>B).</p><p>Furthermore, during the surgical procedure, AR can also be utilized for accuracy checks and navigation updates to reassure high navigational accuracy throughout the intervention using anatomical landmarks, such as cortical vascular structures eligible for in-plane (2D, no depth inaccuracies) corrections of spatial inaccuracies (for further details see [<xref rid="B41-sensors-25-05301" ref-type="bibr">41</xref>]) as well as 3D reconstruction of the cortical profile (see <xref rid="sensors-25-05301-f004" ref-type="fig">Figure 4</xref>).</p><p>In addition, as described in the workflow, navigated intraoperative ultrasound can be investigated to verify navigation accuracy (see <xref rid="sensors-25-05301-f005" ref-type="fig">Figure 5</xref>) before resection using the live view or acquiring a 3D iUS data set, as well as to estimate intraoperative extent of resection and exclusion of intraoperative complications (see <xref rid="sensors-25-05301-f006" ref-type="fig">Figure 6</xref>).</p><p>In the course of resection, image-guidance is provided on a screen close-by with the microscope&#8217;s focal point used as virtual pointer. In parallel, the outlines of all segmented structures (e.g., hippocampus, temporal horn, amygdala, brainstem) are visualized within the microscopic view, allowing for an immediate transfer of imaging data into the surgical situs (see <xref rid="sensors-25-05301-f007" ref-type="fig">Figure 7</xref>).</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05301"><title>4. Discussion</title><p>This study aimed to showcase both the clinical benefits and the educational potential of neuronavigation, particularly microscope-based augmented reality (AR) support, in surgery for mTLE. In all cases, microscope-based AR assistance was employed, enhancing intraoperative surgical orientation, patient safety, and surgeon comfort. Consequently, the findings of this study bolster the hypothesis regarding the educational benefits and clinical advantages of utilizing microscope-based AR support in treating patients with temporal lobe epilepsy.</p><p>Resective surgery is a well-established treatment option for carefully selected patients with pharmacoresistant epilepsy, primarily those with TLE [<xref rid="B18-sensors-25-05301" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05301" ref-type="bibr">19</xref>]. Surgical treatment, particularly for mTLE, is regarded as safe and standardized, offering advantages over conservative approaches [<xref rid="B18-sensors-25-05301" ref-type="bibr">18</xref>]. A crucial factor for successful surgical intervention is the precise identification of the epileptogenic zone [<xref rid="B38-sensors-25-05301" ref-type="bibr">38</xref>]. In most cases of TLE, seizure origins are located in the mesio-basal temporal structures, such as the hippocampus, amygdala, and parahippocampal gyrus, which are typically resected using a selective approach (sAHE) that spares the temporal neocortex [<xref rid="B20-sensors-25-05301" ref-type="bibr">20</xref>]. While various techniques exist, including transcortical, transsylvian, and subtemporal approaches, a deep understanding of the anatomy of the temporomesial target structures, surrounding risk areas, and intraoperative landmarks is essential for the safe and successful selective resection of the amygdala, hippocampus, and parahippocampal gyrus [<xref rid="B20-sensors-25-05301" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05301" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05301" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05301" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05301" ref-type="bibr">24</xref>].</p><p>While having extensive knowledge of the topography and significant landmarks can guide surgical procedures without the need for neuronavigation, the technology remains a valuable addition. It aids in pinpointing the cortical incision&#8217;s location, directing dissection toward the temporal horn, and identifying key target structures for intraoperative orientation. This helps to avoid unintended trajectories that are too mesial [<xref rid="B20-sensors-25-05301" ref-type="bibr">20</xref>,<xref rid="B35-sensors-25-05301" ref-type="bibr">35</xref>]. Additionally, neuronavigation support allows surgeons to accurately assess the extent of resection during the operation [<xref rid="B35-sensors-25-05301" ref-type="bibr">35</xref>], as demonstrated in this study, which utilized both standard navigation and real-time imaging techniques like navigated ultrasound.</p><p>Nevertheless, comprehensive surgical planning is essential, particularly in understanding the spatial relationships between target structures and nearby risk areas along the surgical route. This understanding not only enhances preoperative and intraoperative decision-making but also assists in surgical orientation and contributes to more radical resections while improving patient safety [<xref rid="B42-sensors-25-05301" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-05301" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-05301" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05301" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-05301" ref-type="bibr">46</xref>,<xref rid="B47-sensors-25-05301" ref-type="bibr">47</xref>]. Consequently, neuronavigation, along with microscope-based AR support, has been integrated into functional and epilepsy surgeries for treatment planning, invasive diagnostics, and the resection of epileptogenic zones [<xref rid="B25-sensors-25-05301" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05301" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05301" ref-type="bibr">27</xref>,<xref rid="B29-sensors-25-05301" ref-type="bibr">29</xref>,<xref rid="B48-sensors-25-05301" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-05301" ref-type="bibr">49</xref>,<xref rid="B50-sensors-25-05301" ref-type="bibr">50</xref>].</p><p>Standard navigation typically relies on dedicated instruments and separate displays situated near the surgical field. This arrangement necessitates frequent switching of instruments and changing viewing angles between the patient and display during surgery [<xref rid="B31-sensors-25-05301" ref-type="bibr">31</xref>], often requiring the surgeon to mentally synchronize the navigation data with their understanding of the surgical site, which heavily relies on their experience. Microscope-based AR enhances mental visualization by integrating navigational tooltips into the microscope&#8217;s focal point, combining all navigational information into the surgical view. This approach improves orientation, reduces the need for attention shifts, and increases the comfort of the surgeon [<xref rid="B1-sensors-25-05301" ref-type="bibr">1</xref>,<xref rid="B30-sensors-25-05301" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05301" ref-type="bibr">31</xref>]. It also offers opportunities to enhance mapping image data and understand the patient&#8217;s anatomy, facilitating the identification of key landmarks for intraoperative awareness and spatial relationships between targets and risk structures relevant to specific surgical approaches. Early versions of microscope-based AR displayed manually outlined objects using dashed and solid lines in the current focal plane through the microscope&#8217;s HUD [<xref rid="B10-sensors-25-05301" ref-type="bibr">10</xref>,<xref rid="B51-sensors-25-05301" ref-type="bibr">51</xref>,<xref rid="B52-sensors-25-05301" ref-type="bibr">52</xref>]. Although 2D representations may limit depth perception [<xref rid="B53-sensors-25-05301" ref-type="bibr">53</xref>], the latest state-of-the-art implementations enable better 3D perception of outlined structures overlaid onto the surgical view. Enhanced HUD resolution, the incorporation of multiple colors for object differentiation, and smooth real-time visualization&#8212;made possible by significant advances in computing power&#8212;support a more intuitive use of microscope-based AR. The complexity of the visualization and the variety of visualizable objects can be individually toggled to prevent clutter in the surgical view, adapting to each surgeon&#8217;s needs and the current surgical phase. Thus, microscope-based AR is viewed as an addition, complementing standard navigation, as it can provide contextual information beyond what is currently visualized [<xref rid="B1-sensors-25-05301" ref-type="bibr">1</xref>,<xref rid="B31-sensors-25-05301" ref-type="bibr">31</xref>].</p><p>While augmented reality based on navigation and microscopy offers a distinct advantage for intraoperative surgical orientation and educational purposes&#8212;by aligning in-situ surgical landmarks with enhanced imaging data&#8212;its effectiveness relies heavily on accurately matching patient and imaging spaces; misalignment can lead to misleading and potentially dangerous situations if one solely depends on this technology. Consequently, achieving high navigational and overall accuracy is essential. Overall accuracy is a multifaceted term, influenced by four key domains: imaging accuracy, technical accuracy, registration accuracy, and the impact of intraoperative events, all contributing to application accuracy [<xref rid="B43-sensors-25-05301" ref-type="bibr">43</xref>,<xref rid="B54-sensors-25-05301" ref-type="bibr">54</xref>].</p><p>Imaging accuracy pertains to the specific imaging modality, like MRI, which has inherent benefits, such as tissue contrast, and drawbacks, including non-linear geometric distortions that complicate image fusion with non-distorted imaging data like CT. This is particularly significant during stereotactic procedures, such as electrode placement or biopsies [<xref rid="B46-sensors-25-05301" ref-type="bibr">46</xref>,<xref rid="B55-sensors-25-05301" ref-type="bibr">55</xref>,<xref rid="B56-sensors-25-05301" ref-type="bibr">56</xref>].</p><p>Technical accuracy, on the other hand, involves the intrinsic precision of the navigation system, including tracking technology and the geometric integrity of the various instruments used (e.g., microscope, ultrasound probe, pointer) [<xref rid="B57-sensors-25-05301" ref-type="bibr">57</xref>], which has demonstrated less than 3 mm accuracy in frameless setups [<xref rid="B54-sensors-25-05301" ref-type="bibr">54</xref>].</p><p>Registration accuracy is now recognized as a crucial factor affecting application accuracy, with various methods available [<xref rid="B58-sensors-25-05301" ref-type="bibr">58</xref>,<xref rid="B59-sensors-25-05301" ref-type="bibr">59</xref>,<xref rid="B60-sensors-25-05301" ref-type="bibr">60</xref>]. In frameless setups, patient registration is commonly conducted using either landmark-based or surface-based approaches. The landmark-based method utilizes anatomical landmarks, but it often employs artificial markers placed across the patient&#8217;s head to facilitate 3D paired point rigid registration between imaging and real-world markers [<xref rid="B54-sensors-25-05301" ref-type="bibr">54</xref>]. However, this method has demonstrated varying target registration errors (TREs) ranging from 1.8 mm to 5.0 mm [<xref rid="B58-sensors-25-05301" ref-type="bibr">58</xref>,<xref rid="B61-sensors-25-05301" ref-type="bibr">61</xref>], influenced by factors such as the number of markers, their positioning, spatial arrangement, skin shifts during imaging (e.g., caused by padding or protective headphones), and intraoperative conditions (e.g., skin displacement due to three-pin fixation or the pointer during registration), as well as the user [<xref rid="B54-sensors-25-05301" ref-type="bibr">54</xref>,<xref rid="B62-sensors-25-05301" ref-type="bibr">62</xref>,<xref rid="B63-sensors-25-05301" ref-type="bibr">63</xref>,<xref rid="B64-sensors-25-05301" ref-type="bibr">64</xref>,<xref rid="B65-sensors-25-05301" ref-type="bibr">65</xref>]. Similarly, surface-based methods integrate anatomical landmarks with laser surface matching to allow for a virtual sampling of the patient&#8217;s head for registration. Despite not being dependent on specific preoperative imaging with adhesive skin markers, its application is limited due to image quality and intraoperative patient positioning, with reports indicating even lower registration accuracy&#8212;mean TREs of 5.3 mm&#8212;depending on the imaging modality and quality [<xref rid="B59-sensors-25-05301" ref-type="bibr">59</xref>]. The advent of intraoperative CT and MRI imaging has enabled automatic and user-independent registration achieving TREs of around or less than 1 mm [<xref rid="B41-sensors-25-05301" ref-type="bibr">41</xref>,<xref rid="B48-sensors-25-05301" ref-type="bibr">48</xref>,<xref rid="B60-sensors-25-05301" ref-type="bibr">60</xref>,<xref rid="B62-sensors-25-05301" ref-type="bibr">62</xref>], as also seen in this study with a mean TRE of 0.74 &#177; 0.28 mm. This registration procedure, as a key component, significantly reduces errors introduced at this stage of surgery prior to skin incision, thereby enhancing overall clinical accuracy when compared to other widely used methods. It provides an ideal foundation for surgical orientation and educational purposes while aligning imaging and augmented reality (AR) data with the patient&#8217;s anatomy.</p><p>Surgical navigation accuracy is known to continuously diminish during procedures due to factors such as the application of surgical drapes, skin incisions, trepanation, craniotomy, and the length of surgery [<xref rid="B42-sensors-25-05301" ref-type="bibr">42</xref>,<xref rid="B58-sensors-25-05301" ref-type="bibr">58</xref>,<xref rid="B66-sensors-25-05301" ref-type="bibr">66</xref>]. This decline primarily results from changes in the spatial relationship between the patient&#8217;s head and the reference array. Although brain shift may be minimal before the dural opening, the non-linear deformations of the brain can occur in unpredictable ways due to factors like the use of brain retractors, loss of cerebrospinal fluid (CSF), gravitational effects, and ongoing tissue resection, all contributing to unavoidable errors. Consequently, intraoperative accuracy post-registration is compromised, hindering the direct and intuitive connection of virtual data to real-world surgical orientation and education, which in severe cases can render navigation ineffective. Thus, it is crucial to (1) recognize potential intrinsic and extrinsic inaccuracies that may arise during the procedure and (2) be equipped to identify and potentially correct these inaccuracies.</p><p>Multiple strategies have been established to tackle this challenge, including intraoperative imaging methods like MRI [<xref rid="B44-sensors-25-05301" ref-type="bibr">44</xref>,<xref rid="B45-sensors-25-05301" ref-type="bibr">45</xref>,<xref rid="B67-sensors-25-05301" ref-type="bibr">67</xref>,<xref rid="B68-sensors-25-05301" ref-type="bibr">68</xref>] or ultrasound [<xref rid="B69-sensors-25-05301" ref-type="bibr">69</xref>,<xref rid="B70-sensors-25-05301" ref-type="bibr">70</xref>,<xref rid="B71-sensors-25-05301" ref-type="bibr">71</xref>]. These techniques can either update planning and navigation data with intraoperative data or convert complex preoperative data to reflect the recent intraoperative situation in a non-linear manner. The use of iMRI and repetitive iMRI is restricted due to limited availability, structural demands, prolonged time requirements, and high costs. As a result, these methods are more often employed for resection control in neurooncology or for navigation updates later in surgery when significant brain shift occurs. In contrast, intraoperative ultrasound can be utilized at any point during the surgical procedure and can be repeated without noticeable time delays. It is cost-effective, especially when seamlessly integrated into the neuronavigation system. While non-linear image fusion using iMRI data remains a long-term research focus and is clinically available [<xref rid="B68-sensors-25-05301" ref-type="bibr">68</xref>], non-linear image registration based on intraoperative ultrasound is still in development. Recently, clinically established rigid image fusion of preoperative MRI and intraoperative ultrasound data has facilitated local navigation updates [<xref rid="B72-sensors-25-05301" ref-type="bibr">72</xref>]. Notably, in addition to the intraoperative imaging techniques discussed earlier and exemplified in this study, microscope-based AR offers a convenient way to monitor navigation accuracy by examining uniquely identifiable landmarks and structures&#8212;like cortical vessels, the cortex profile, bony landmarks when relevant, and deep vascular structures [<xref rid="B41-sensors-25-05301" ref-type="bibr">41</xref>]. It also compensates for in-plane registration inaccuracies, assuming high calibration precision of the microscope [<xref rid="B73-sensors-25-05301" ref-type="bibr">73</xref>]. However, current AR adjustments are still confined to translational and rotational transformations within the microscope&#8217;s focal plane (2D adjustment). Even though accuracy checks can be performed at various points in the surgical site, AR adjustments still have limitations; as recently implemented, AR base adjustments can only compensate for in-plane, 2D misalignments, rather than 3D inaccuracies or depth misalignments and are therefore not yet fully capable of overcoming all kinds of misalignments. Nevertheless, microscope-based AR support enables accuracy monitoring and navigation updates in subcortical areas, provided uniquely identifiable landmarks such as vascular structures are present [<xref rid="B13-sensors-25-05301" ref-type="bibr">13</xref>].</p><p>While microscope-based AR is commonly utilized in neurosurgery, the virtualization added to the microscopic view introduces a further element to the accuracy chain [<xref rid="B73-sensors-25-05301" ref-type="bibr">73</xref>]. The accuracy of AR entails both the inherent precision of the AR system, which cannot be modified by the user, and the user-dependent calibration of AR within the surgical environment. This approach ensures high levels of AR and navigation accuracy at this stage.</p><p>Alongside its clinical benefits of microscope-based AR by offering assistance during surgery, AR has also emerged as a valuable tool for neurosurgical education and training [<xref rid="B74-sensors-25-05301" ref-type="bibr">74</xref>]. In the educational setting, AR is most often used to enable simulations to create a virtual risk-free environment for neurosurgical trainees to develop and refine their surgical skills repetitively, and is seen as an important adjunct to training on real patients [<xref rid="B75-sensors-25-05301" ref-type="bibr">75</xref>,<xref rid="B76-sensors-25-05301" ref-type="bibr">76</xref>]. The first setups included training material for external ventricular drainages (EVD) and needle biopsies with printed skull phantoms, and AR demonstrated its ability to aid in EVD placement and needle biopsy insertion [<xref rid="B77-sensors-25-05301" ref-type="bibr">77</xref>]. Furthermore, AR has been utilized in a training setup for locating, navigating, and rating malignant brain lesions, e.g., for craniotomy planning and surgical trajectory training [<xref rid="B78-sensors-25-05301" ref-type="bibr">78</xref>] or delineation of tumor borders [<xref rid="B79-sensors-25-05301" ref-type="bibr">79</xref>]. However, in vitro training setups are somewhat limited and unable to fully simulate a real patient&#8217;s anatomy. Newer investigations are incorporating haptic technology into simulators to replicate a real surgical experience and synthetic tissue simulations [<xref rid="B80-sensors-25-05301" ref-type="bibr">80</xref>,<xref rid="B81-sensors-25-05301" ref-type="bibr">81</xref>]. Despite these tremendous developments, simulations still cannot provide an entirely realistic experience for trainees, as it would be of utmost importance for surgeons practicing medicine on real patients [<xref rid="B74-sensors-25-05301" ref-type="bibr">74</xref>]. Therefore, simulations must be designed in a way that creates a realistic visual and tactile experience to maximize the educational advantage.</p><p>In this way, training and education should be based on various adjuncts. Besides AR simulators for procedural training, AR can also be used, as shown in this study in terms of microscope-based AR with high navigational accuracy, to support education and training for residents in matching image data and real patient anatomy in a real surgical setting during surgery but also by using video recordings of navigation and in-parallel microscope data for offline training. This provides the potential to identify surgical landmarks and trajectories to support surgical orientation and mental representation of virtual image content connected to the patient&#8217;s anatomy, especially in cases of a limited number of patients with specific pathologies. Therefore, microscope-based AR represents a valuable adjunct to AR-based simulations, extending the surgical education toolbox.</p><p>The presented surgical workflow includes various technical instruments added to the standard clinical image-guided navigation approach, all supporting and contributing to its valuable use. One major prerequisite is the best possible mapping of imaging and patient data to ensure reliable and trustworthy image-guidance and AR support. As mentioned earlier, applying iCT for automatic patient registration allows for highly accurate, user-independent matching of imaging and patient data at the beginning of the procedure. Therefore, it serves as a highly accurate navigation initialization. To ensure high navigation accuracy before assessing the brain itself after dural operating, intraoperative imaging such as navigated iUS can be utilized to verify accuracy, estimate the amount of inaccuracy, or recently, also allowing for navigation updates by iUS-iMRI image fusion to overcome misalignments. With the latter, one might also overcome initial registration inaccuracies. Additionally, iUS can serve as a tool to determine the extent of resection intraoperatively and rule out surgical complications around the resection cavity. As shown here, microscope-based AR might also contribute to navigational accuracy by allowing for minor alignments based on AR and patient anatomy at various stages of surgery. In addition to accurate image-guidance supported by iCT and/or iUS, microscopes with AR enhance the surgical view with contextual information. This allows for immediate surgical orientation without the need to constantly mentally transfer 2D imaging data onto the surgical site. It provides further intuitive surgical guidance for tailored resection in close proximity to risk structures, given high navigational accuracy. However, achieving high navigation accuracy independently of image-guidance, with or without AR support, is a key prerequisite. This should be pursued using the technical tools available to make full use of the benefits of image-guidance with or without AR support.</p><p>The limitations of this study include its retrospective design and the moderate sample size, which results from the selection criteria that exclusively recruited TLE patients who underwent selective temporomesial resection. These criteria facilitate a more nuanced examination of microscope-based AR support within clinical and educational framework related to this particular procedure. However, while the sample size is reasonable for a technical study, it limits broader generalizability. Therefore, multicenter studies are required to confirm the results presented here. In addition, due to its retrospective nature, comparisons with standard techniques (e.g., image-guidance only) are lacking, necessitating future controlled trials to further elaborate on the additional value of AR assistance. Another limitation of the retrospective analysis is the lack of objective, quantifiable measures of the benefits in terms of surgical orientation and comfort, as well as a lack of prospectively collected trainee assessments to quantify the educational benefits further. This shall be overcome in future prospective analyses using trainee and surgeon assessments.</p><p>However, in light of the well-documented benefits of these techniques across the broader spectrum of neurosurgery, specifically in enhancing resection precision, increasing patient safety, and improving functional outcomes, as well as providing pedagogical guidance, there exists a critical need to engage in discussions regarding the implications of not adopting these methodologies.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05301"><title>5. Conclusions</title><p>Mesial temporal lobe epilepsy (MTLE) surgery, including selective amygdalohippocampectomy, is a well-established treatment for patients with drug-resistant TLE. However, precise intraoperative orientation is crucial for the safe and complete removal of epileptogenic structures while preserving functional integrity. Microscope-based AR support relying on high navigational accuracy enhances surgical orientation, even for experienced surgeons, and provides ergonomic comfort, ultimately increasing patient safety. Additionally, microscope-based AR serves as a valuable tool for neurosurgical training and education by creating a mental representation of the structural and functional anatomy of the patient and corresponding imaging data. This technology helps to identify key surgical and procedural landmarks. Despite the study&#8217;s retrospective design and moderate sample size, it highlights the importance of adopting such technologies to achieve surgical excellence and improve training in complex epilepsy procedures.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.G. and M.H.A.B.; methodology, F.N., A.G. and M.H.A.B.; validation, A.G. and M.H.A.B.; formal analysis, F.N., A.G. and M.H.A.B.; investigation, A.G. and M.H.A.B.; resources, A.G., C.N. and M.H.A.B.; data curation, A.G., M.G., B.C., C.N. and M.H.A.B.; writing&#8212;original draft preparation, A.G. and M.H.A.B.; writing&#8212;review and editing, F.N., A.G., M.G., B.C., S.K., K.M., C.N. and M.H.A.B.; visualization, A.G. and M.H.A.B.; supervision, A.G. and M.H.A.B.; project administration, A.G. and M.H.A.B.; All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>This study was conducted in accordance with the Declaration of Helsinki, and approved by the Local Ethics Committee of the University of Marburg (No. 99/18 and 24-214 RS, 26 July 2024).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data in this study are available on request from the corresponding author. The data are not publicly available due to privacy restrictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>C.N. is a scientific consultant for Brainlab, Zeiss and BK Medical (GE Healthcare); M.B. is a scientific consultant for Brainlab. F.N., B.C., S.K., K.M., and A.G. declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05301"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meola</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cutolo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Carbone</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cagnazzo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ferrari</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ferrari</surname><given-names>V.</given-names></name></person-group><article-title>Augmented reality in neurosurgery: A systematic review</article-title><source>Neurosurg. Rev.</source><year>2017</year><volume>40</volume><fpage>537</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1007/s10143-016-0732-9</pub-id><pub-id pub-id-type="pmid">27154018</pub-id><pub-id pub-id-type="pmcid">PMC6155988</pub-id></element-citation></ref><ref id="B2-sensors-25-05301"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kelly</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>Alker</surname><given-names>G.J.</given-names><suffix>Jr.</suffix></name><name name-style="western"><surname>Goerss</surname><given-names>S.</given-names></name></person-group><article-title>Computer-assisted stereotactic microsurgery for the treatment of intracranial neoplasms</article-title><source>Neurosurgery</source><year>1982</year><volume>10</volume><fpage>324</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1227/00006123-198203000-00005</pub-id><pub-id pub-id-type="pmid">7041004</pub-id></element-citation></ref><ref id="B3-sensors-25-05301"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roberts</surname><given-names>D.W.</given-names></name><name name-style="western"><surname>Strohbehn</surname><given-names>J.W.</given-names></name><name name-style="western"><surname>Hatch</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Murray</surname><given-names>W.</given-names></name><name name-style="western"><surname>Kettenberger</surname><given-names>H.</given-names></name></person-group><article-title>A frameless stereotaxic integration of computerized tomographic imaging and the operating microscope</article-title><source>J. Neurosurg.</source><year>1986</year><volume>65</volume><fpage>545</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.3171/jns.1986.65.4.0545</pub-id><pub-id pub-id-type="pmid">3531430</pub-id></element-citation></ref><ref id="B4-sensors-25-05301"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>King</surname><given-names>A.P.</given-names></name><name name-style="western"><surname>Edwards</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>Maurer</surname><given-names>C.R.</given-names><suffix>Jr.</suffix></name><name name-style="western"><surname>de Cunha</surname><given-names>D.A.</given-names></name><name name-style="western"><surname>Hawkes</surname><given-names>D.J.</given-names></name><name name-style="western"><surname>Hill</surname><given-names>D.L.</given-names></name><name name-style="western"><surname>Gaston</surname><given-names>R.P.</given-names></name><name name-style="western"><surname>Fenlon</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Strong</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Chandler</surname><given-names>C.L.</given-names></name><etal/></person-group><article-title>A system for microscope-assisted guided interventions</article-title><source>Stereotact. Funct. Neurosurg.</source><year>1999</year><volume>72</volume><fpage>107</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1159/000029708</pub-id><pub-id pub-id-type="pmid">10853060</pub-id></element-citation></ref><ref id="B5-sensors-25-05301"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kiya</surname><given-names>N.</given-names></name><name name-style="western"><surname>Dureza</surname><given-names>C.</given-names></name><name name-style="western"><surname>Fukushima</surname><given-names>T.</given-names></name><name name-style="western"><surname>Maroon</surname><given-names>J.C.</given-names></name></person-group><article-title>Computer navigational microscope for minimally invasive neurosurgery</article-title><source>Minim. Invasive Neurosurg.</source><year>1997</year><volume>40</volume><fpage>110</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1055/s-2008-1053429</pub-id><pub-id pub-id-type="pmid">9359091</pub-id></element-citation></ref><ref id="B6-sensors-25-05301"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cabrilo</surname><given-names>I.</given-names></name><name name-style="western"><surname>Bijlenga</surname><given-names>P.</given-names></name><name name-style="western"><surname>Schaller</surname><given-names>K.</given-names></name></person-group><article-title>Augmented reality in the surgery of cerebral arteriovenous malformations: Technique assessment and considerations</article-title><source>Acta Neurochir.</source><year>2014</year><volume>156</volume><fpage>1769</fpage><lpage>1774</lpage><pub-id pub-id-type="doi">10.1007/s00701-014-2183-9</pub-id><pub-id pub-id-type="pmid">25037466</pub-id></element-citation></ref><ref id="B7-sensors-25-05301"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cabrilo</surname><given-names>I.</given-names></name><name name-style="western"><surname>Bijlenga</surname><given-names>P.</given-names></name><name name-style="western"><surname>Schaller</surname><given-names>K.</given-names></name></person-group><article-title>Augmented reality in the surgery of cerebral aneurysms: A technical report</article-title><source>Neurosurgery</source><year>2014</year><volume>10</volume><issue>(Suppl. S2)</issue><fpage>252</fpage><lpage>260; discussion 260&#8211;261</lpage><pub-id pub-id-type="doi">10.1227/NEU.0000000000000328</pub-id><pub-id pub-id-type="pmid">24594927</pub-id></element-citation></ref><ref id="B8-sensors-25-05301"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cabrilo</surname><given-names>I.</given-names></name><name name-style="western"><surname>Schaller</surname><given-names>K.</given-names></name><name name-style="western"><surname>Bijlenga</surname><given-names>P.</given-names></name></person-group><article-title>Augmented reality-assisted bypass surgery: Embracing minimal invasiveness</article-title><source>World Neurosurg.</source><year>2015</year><volume>83</volume><fpage>596</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1016/j.wneu.2014.12.020</pub-id><pub-id pub-id-type="pmid">25527874</pub-id></element-citation></ref><ref id="B9-sensors-25-05301"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cannizzaro</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zaed</surname><given-names>I.</given-names></name><name name-style="western"><surname>Safa</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jelmoni</surname><given-names>A.J.M.</given-names></name><name name-style="western"><surname>Composto</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bisoglio</surname><given-names>A.</given-names></name><name name-style="western"><surname>Schmeizer</surname><given-names>K.</given-names></name><name name-style="western"><surname>Becker</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Pizzi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cardia</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Augmented Reality in Neurosurgery, State of Art and Future Projections. A Systematic Review</article-title><source>Front. Surg.</source><year>2022</year><volume>9</volume><elocation-id>864792</elocation-id><pub-id pub-id-type="doi">10.3389/fsurg.2022.864792</pub-id><pub-id pub-id-type="pmid">35360432</pub-id><pub-id pub-id-type="pmcid">PMC8961734</pub-id></element-citation></ref><ref id="B10-sensors-25-05301"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mascitelli</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Schlachter</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chartrain</surname><given-names>A.G.</given-names></name><name name-style="western"><surname>Oemke</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gilligan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Costa</surname><given-names>A.B.</given-names></name><name name-style="western"><surname>Shrivastava</surname><given-names>R.K.</given-names></name><name name-style="western"><surname>Bederson</surname><given-names>J.B.</given-names></name></person-group><article-title>Navigation-Linked Heads-Up Display in Intracranial Surgery: Early Experience</article-title><source>Oper. Neurosurg.</source><year>2018</year><volume>15</volume><fpage>184</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1093/ons/opx205</pub-id><pub-id pub-id-type="pmid">29040677</pub-id><pub-id pub-id-type="pmcid">PMC6047456</pub-id></element-citation></ref><ref id="B11-sensors-25-05301"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>G.C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.L.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>X.G.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>X.D.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.B.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>R.Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.N.</given-names></name></person-group><article-title>Impact of Virtual and Augmented Reality Based on Intraoperative Magnetic Resonance Imaging and Functional Neuronavigation in Glioma Surgery Involving Eloquent Areas</article-title><source>World Neurosurg.</source><year>2016</year><volume>96</volume><fpage>375</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1016/j.wneu.2016.07.107</pub-id><pub-id pub-id-type="pmid">27521727</pub-id></element-citation></ref><ref id="B12-sensors-25-05301"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cabrilo</surname><given-names>I.</given-names></name><name name-style="western"><surname>Sarrafzadeh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bijlenga</surname><given-names>P.</given-names></name><name name-style="western"><surname>Landis</surname><given-names>B.N.</given-names></name><name name-style="western"><surname>Schaller</surname><given-names>K.</given-names></name></person-group><article-title>Augmented reality-assisted skull base surgery</article-title><source>Neurochirurgie</source><year>2014</year><volume>60</volume><fpage>304</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/j.neuchi.2014.07.001</pub-id><pub-id pub-id-type="pmid">25245926</pub-id></element-citation></ref><ref id="B13-sensors-25-05301"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Carl</surname><given-names>B.</given-names></name><name name-style="western"><surname>Bopp</surname><given-names>M.</given-names></name><name name-style="western"><surname>Benescu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sass</surname><given-names>B.</given-names></name><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name></person-group><article-title>Indocyanine Green Angiography Visualized by Augmented Reality in Aneurysm Surgery</article-title><source>World Neurosurg.</source><year>2020</year><volume>142</volume><fpage>e307</fpage><lpage>e315</lpage><pub-id pub-id-type="doi">10.1016/j.wneu.2020.06.219</pub-id><pub-id pub-id-type="pmid">32640326</pub-id></element-citation></ref><ref id="B14-sensors-25-05301"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Devinsky</surname><given-names>O.</given-names></name><name name-style="western"><surname>Hesdorffer</surname><given-names>D.C.</given-names></name><name name-style="western"><surname>Thurman</surname><given-names>D.J.</given-names></name><name name-style="western"><surname>Lhatoo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Richerson</surname><given-names>G.</given-names></name></person-group><article-title>Sudden unexpected death in epilepsy: Epidemiology, mechanisms, and prevention</article-title><source>Lancet Neurol.</source><year>2016</year><volume>15</volume><fpage>1075</fpage><lpage>1088</lpage><pub-id pub-id-type="doi">10.1016/S1474-4422(16)30158-2</pub-id><pub-id pub-id-type="pmid">27571159</pub-id></element-citation></ref><ref id="B15-sensors-25-05301"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kwan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Brodie</surname><given-names>M.J.</given-names></name></person-group><article-title>Early identification of refractory epilepsy</article-title><source>N. Engl. J. Med.</source><year>2000</year><volume>342</volume><fpage>314</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1056/NEJM200002033420503</pub-id><pub-id pub-id-type="pmid">10660394</pub-id></element-citation></ref><ref id="B16-sensors-25-05301"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thurman</surname><given-names>D.J.</given-names></name><name name-style="western"><surname>Beghi</surname><given-names>E.</given-names></name><name name-style="western"><surname>Begley</surname><given-names>C.E.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.T.</given-names></name><name name-style="western"><surname>Buchhalter</surname><given-names>J.R.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hesdorffer</surname><given-names>D.C.</given-names></name><name name-style="western"><surname>Hauser</surname><given-names>W.A.</given-names></name><name name-style="western"><surname>Kazis</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kobau</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Standards for epidemiologic studies and surveillance of epilepsy</article-title><source>Epilepsia</source><year>2011</year><volume>52</volume><issue>(Suppl. S7)</issue><fpage>2</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1111/j.1528-1167.2011.03121.x</pub-id><pub-id pub-id-type="pmid">21899536</pub-id></element-citation></ref><ref id="B17-sensors-25-05301"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sultana</surname><given-names>B.</given-names></name><name name-style="western"><surname>Panzini</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Veilleux Carpentier</surname><given-names>A.</given-names></name><name name-style="western"><surname>Comtois</surname><given-names>J.</given-names></name><name name-style="western"><surname>Rioux</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gore</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bauer</surname><given-names>P.R.</given-names></name><name name-style="western"><surname>Kwon</surname><given-names>C.S.</given-names></name><name name-style="western"><surname>Jette</surname><given-names>N.</given-names></name><name name-style="western"><surname>Josephson</surname><given-names>C.B.</given-names></name><etal/></person-group><article-title>Incidence and Prevalence of Drug-Resistant Epilepsy: A Systematic Review and Meta-analysis</article-title><source>Neurology</source><year>2021</year><volume>96</volume><fpage>805</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1212/WNL.0000000000011839</pub-id><pub-id pub-id-type="pmid">33722992</pub-id></element-citation></ref><ref id="B18-sensors-25-05301"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wiebe</surname><given-names>S.</given-names></name><name name-style="western"><surname>Blume</surname><given-names>W.T.</given-names></name><name name-style="western"><surname>Girvin</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Eliasziw</surname><given-names>M.</given-names></name><collab>the Effectiveness and Efficiency of Surgery for Temporal Lobe Epilepsy Study Group</collab></person-group><article-title>A randomized, controlled trial of surgery for temporal-lobe epilepsy</article-title><source>N. Engl. J. Med.</source><year>2001</year><volume>345</volume><fpage>311</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1056/NEJM200108023450501</pub-id><pub-id pub-id-type="pmid">11484687</pub-id></element-citation></ref><ref id="B19-sensors-25-05301"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Delev</surname><given-names>D.</given-names></name><name name-style="western"><surname>Oehl</surname><given-names>B.</given-names></name><name name-style="western"><surname>Steinhoff</surname><given-names>B.J.</given-names></name><name name-style="western"><surname>Nakagawa</surname><given-names>J.</given-names></name><name name-style="western"><surname>Scheiwe</surname><given-names>C.</given-names></name><name name-style="western"><surname>Schulze-Bonhage</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zentner</surname><given-names>J.</given-names></name></person-group><article-title>Surgical Treatment of Extratemporal Epilepsy: Results and Prognostic Factors</article-title><source>Neurosurgery</source><year>2019</year><volume>84</volume><fpage>242</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1093/neuros/nyy099</pub-id><pub-id pub-id-type="pmid">29618099</pub-id></element-citation></ref><ref id="B20-sensors-25-05301"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spencer</surname><given-names>D.</given-names></name><name name-style="western"><surname>Burchiel</surname><given-names>K.</given-names></name></person-group><article-title>Selective amygdalohippocampectomy</article-title><source>Epilepsy Res. Treat.</source><year>2012</year><volume>2012</volume><fpage>382095</fpage><pub-id pub-id-type="doi">10.1155/2012/382095</pub-id><pub-id pub-id-type="pmid">22957229</pub-id><pub-id pub-id-type="pmcid">PMC3420672</pub-id></element-citation></ref><ref id="B21-sensors-25-05301"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Adada</surname><given-names>B.</given-names></name></person-group><article-title>Selective amygdalohippocampectomy via the transsylvian approach</article-title><source>Neurosurg. Focus.</source><year>2008</year><volume>25</volume><fpage>E5</fpage><pub-id pub-id-type="doi">10.3171/FOC/2008/25/9/E5</pub-id><pub-id pub-id-type="pmid">18759629</pub-id></element-citation></ref><ref id="B22-sensors-25-05301"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bozkurt</surname><given-names>B.</given-names></name><name name-style="western"><surname>da Silva Centeno</surname><given-names>R.</given-names></name><name name-style="western"><surname>Chaddad-Neto</surname><given-names>F.</given-names></name><name name-style="western"><surname>da Costa</surname><given-names>M.D.</given-names></name><name name-style="western"><surname>Goiri</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Karadag</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tugcu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ovalioglu</surname><given-names>T.C.</given-names></name><name name-style="western"><surname>Tanriover</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kaya</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Transcortical selective amygdalohippocampectomy technique through the middle temporal gyrus revisited: An anatomical study laboratory investigation</article-title><source>J. Clin. Neurosci.</source><year>2016</year><volume>34</volume><fpage>237</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/j.jocn.2016.05.035</pub-id><pub-id pub-id-type="pmid">27499121</pub-id></element-citation></ref><ref id="B23-sensors-25-05301"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Olivier</surname><given-names>A.</given-names></name></person-group><article-title>Transcortical selective amygdalohippocampectomy in temporal lobe epilepsy</article-title><source>Can. J. Neurol. Sci.</source><year>2000</year><volume>27</volume><issue>(Suppl. S1)</issue><fpage>S68</fpage><lpage>S76; discussion S92&#8211;S96</lpage><pub-id pub-id-type="doi">10.1017/S031716710000069X</pub-id><pub-id pub-id-type="pmid">10830331</pub-id></element-citation></ref><ref id="B24-sensors-25-05301"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hori</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yamane</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ochiai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kondo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shimizu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>K.</given-names></name><name name-style="western"><surname>Miyata</surname><given-names>H.</given-names></name></person-group><article-title>Selective subtemporal amygdalohippocampectomy for refractory temporal lobe epilepsy: Operative and neuropsychological outcomes</article-title><source>J. Neurosurg.</source><year>2007</year><volume>106</volume><fpage>134</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.3171/jns.2007.106.1.134</pub-id><pub-id pub-id-type="pmid">17236499</pub-id></element-citation></ref><ref id="B25-sensors-25-05301"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wurm</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ringler</surname><given-names>H.</given-names></name><name name-style="western"><surname>Knogler</surname><given-names>F.</given-names></name><name name-style="western"><surname>Schnizer</surname><given-names>M.</given-names></name></person-group><article-title>Evaluation of neuronavigation in lesional and non-lesional epilepsy surgery</article-title><source>Comput. Aided Surg.</source><year>2003</year><volume>8</volume><fpage>204</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.3109/10929080309146055</pub-id><pub-id pub-id-type="pmid">15360102</pub-id></element-citation></ref><ref id="B26-sensors-25-05301"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chamoun</surname><given-names>R.B.</given-names></name><name name-style="western"><surname>Nayar</surname><given-names>V.V.</given-names></name><name name-style="western"><surname>Yoshor</surname><given-names>D.</given-names></name></person-group><article-title>Neuronavigation applied to epilepsy monitoring with subdural electrodes</article-title><source>Neurosurg. Focus.</source><year>2008</year><volume>25</volume><fpage>E21</fpage><pub-id pub-id-type="doi">10.3171/FOC/2008/25/9/E21</pub-id><pub-id pub-id-type="pmid">18759623</pub-id></element-citation></ref><ref id="B27-sensors-25-05301"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kamida</surname><given-names>T.</given-names></name><name name-style="western"><surname>Anan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Shimotaka</surname><given-names>K.</given-names></name><name name-style="western"><surname>Abe</surname><given-names>T.</given-names></name><name name-style="western"><surname>Fujiki</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kobayashi</surname><given-names>H.</given-names></name></person-group><article-title>Visualization of subdural electrodes with fusion CT scan/MRI during neuronavigation-guided epilepsy surgery</article-title><source>J. Clin. Neurosci.</source><year>2010</year><volume>17</volume><fpage>511</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1016/j.jocn.2009.06.038</pub-id><pub-id pub-id-type="pmid">20122830</pub-id></element-citation></ref><ref id="B28-sensors-25-05301"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maslarova</surname><given-names>A.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Rosch</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dorfler</surname><given-names>A.</given-names></name><name name-style="western"><surname>Coras</surname><given-names>R.</given-names></name><name name-style="western"><surname>Blumcke</surname><given-names>I.</given-names></name><name name-style="western"><surname>Lang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Schmidt</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hamer</surname><given-names>H.M.</given-names></name><name name-style="western"><surname>Reindl</surname><given-names>C.</given-names></name><etal/></person-group><article-title>Surgical planning, histopathology findings and postoperative outcome in MR-negative extra-temporal epilepsy using intracranial EEG, functional imaging, magnetoencephalography, neuronavigation and intraoperative MRI</article-title><source>Clin. Neurol. Neurosurg.</source><year>2023</year><volume>226</volume><fpage>107603</fpage><pub-id pub-id-type="doi">10.1016/j.clineuro.2023.107603</pub-id><pub-id pub-id-type="pmid">36706680</pub-id></element-citation></ref><ref id="B29-sensors-25-05301"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name><name name-style="western"><surname>Buchfelder</surname><given-names>M.</given-names></name></person-group><article-title>Neuronavigation in epilepsy surgery</article-title><source>Arq. Neuropsiquiatr.</source><year>2003</year><volume>61</volume><issue>(Suppl. S1)</issue><fpage>109</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">15104400</pub-id></element-citation></ref><ref id="B30-sensors-25-05301"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leger</surname><given-names>E.</given-names></name><name name-style="western"><surname>Drouin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Collins</surname><given-names>D.L.</given-names></name><name name-style="western"><surname>Popa</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kersten-Oertel</surname><given-names>M.</given-names></name></person-group><article-title>Quantifying attention shifts in augmented reality image-guided neurosurgery</article-title><source>Healthc. Technol. Lett.</source><year>2017</year><volume>4</volume><fpage>188</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1049/htl.2017.0062</pub-id><pub-id pub-id-type="pmid">29184663</pub-id><pub-id pub-id-type="pmcid">PMC5683248</pub-id></element-citation></ref><ref id="B31-sensors-25-05301"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roethe</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Rosler</surname><given-names>J.</given-names></name><name name-style="western"><surname>Misch</surname><given-names>M.</given-names></name><name name-style="western"><surname>Vajkoczy</surname><given-names>P.</given-names></name><name name-style="western"><surname>Picht</surname><given-names>T.</given-names></name></person-group><article-title>Augmented reality visualization in brain lesions: A prospective randomized controlled evaluation of its potential and current limitations in navigated microneurosurgery</article-title><source>Acta Neurochir.</source><year>2022</year><volume>164</volume><fpage>3</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1007/s00701-021-05045-1</pub-id><pub-id pub-id-type="pmid">34904183</pub-id><pub-id pub-id-type="pmcid">PMC8761141</pub-id></element-citation></ref><ref id="B32-sensors-25-05301"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Delev</surname><given-names>D.</given-names></name><name name-style="western"><surname>Schramm</surname><given-names>J.</given-names></name><name name-style="western"><surname>Clusmann</surname><given-names>H.</given-names></name></person-group><article-title>How I do it&#8212;Selective amygdalohippocampectomy via a navigated temporobasal approach, when veins forbid elevation of the temporal lobe</article-title><source>Acta Neurochir.</source><year>2018</year><volume>160</volume><fpage>597</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1007/s00701-017-3386-7</pub-id><pub-id pub-id-type="pmid">29147777</pub-id></element-citation></ref><ref id="B33-sensors-25-05301"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mathon</surname><given-names>B.</given-names></name><name name-style="western"><surname>Clemenceau</surname><given-names>S.</given-names></name></person-group><article-title>Selective amygdalohippocampectomy via trans-superior temporal gyrus keyhole approach</article-title><source>Acta Neurochir.</source><year>2016</year><volume>158</volume><fpage>785</fpage><lpage>789</lpage><pub-id pub-id-type="doi">10.1007/s00701-016-2717-4</pub-id><pub-id pub-id-type="pmid">26852036</pub-id></element-citation></ref><ref id="B34-sensors-25-05301"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>P.F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Pei</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Mei</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.Q.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>Y.Z.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>Z.H.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Z.Y.</given-names></name></person-group><article-title>Keyhole epilepsy surgery: Corticoamygdalohippocampectomy for mesial temporal sclerosis</article-title><source>Neurosurg. Rev.</source><year>2016</year><volume>39</volume><fpage>99</fpage><lpage>108</lpage><page-range>99&#8211;108; discussion 108</page-range><pub-id pub-id-type="doi">10.1007/s10143-015-0657-8</pub-id><pub-id pub-id-type="pmid">26277790</pub-id></element-citation></ref><ref id="B35-sensors-25-05301"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wheatley</surname><given-names>B.M.</given-names></name></person-group><article-title>Selective amygdalohippocampectomy: The trans-middle temporal gyrus approach</article-title><source>Neurosurg. Focus.</source><year>2008</year><volume>25</volume><fpage>E4</fpage><pub-id pub-id-type="doi">10.3171/FOC/2008/25/9/E4</pub-id><pub-id pub-id-type="pmid">18759628</pub-id></element-citation></ref><ref id="B36-sensors-25-05301"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shawarba</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tomschik</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wais</surname><given-names>J.</given-names></name><name name-style="western"><surname>Winter</surname><given-names>F.</given-names></name><name name-style="western"><surname>Dorfer</surname><given-names>C.</given-names></name><name name-style="western"><surname>Mayer</surname><given-names>F.</given-names></name><name name-style="western"><surname>Feucht</surname><given-names>M.</given-names></name><name name-style="western"><surname>Roessler</surname><given-names>K.</given-names></name></person-group><article-title>Augmented reality (AR) in microsurgical multimodal image guided focal pediatric epilepsy surgery: Results of a retrospective feasibility study</article-title><source>Brain Spine</source><year>2025</year><volume>5</volume><fpage>104180</fpage><pub-id pub-id-type="doi">10.1016/j.bas.2024.104180</pub-id><pub-id pub-id-type="pmid">39898005</pub-id><pub-id pub-id-type="pmcid">PMC11786778</pub-id></element-citation></ref><ref id="B37-sensors-25-05301"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wieser</surname><given-names>H.G.</given-names></name><name name-style="western"><surname>Blume</surname><given-names>W.T.</given-names></name><name name-style="western"><surname>Fish</surname><given-names>D.</given-names></name><name name-style="western"><surname>Goldensohn</surname><given-names>E.</given-names></name><name name-style="western"><surname>Hufnagel</surname><given-names>A.</given-names></name><name name-style="western"><surname>King</surname><given-names>D.</given-names></name><name name-style="western"><surname>Sperling</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Luders</surname><given-names>H.</given-names></name><name name-style="western"><surname>Pedley</surname><given-names>T.A.</given-names></name><collab>Commission on Neurosurgery of the International League Against Epilepsy (ILAE)</collab></person-group><article-title>ILAE Commission Report. Proposal for a new classification of outcome with respect to epileptic seizures following epilepsy surgery</article-title><source>Epilepsia</source><year>2001</year><volume>42</volume><fpage>282</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1046/j.1528-1157.2001.4220282.x</pub-id><pub-id pub-id-type="pmid">11240604</pub-id></element-citation></ref><ref id="B38-sensors-25-05301"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Delev</surname><given-names>D.</given-names></name><name name-style="western"><surname>Taube</surname><given-names>J.</given-names></name><name name-style="western"><surname>Helmstaedter</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hakvoort</surname><given-names>K.</given-names></name><name name-style="western"><surname>Grote</surname><given-names>A.</given-names></name><name name-style="western"><surname>Clusmann</surname><given-names>H.</given-names></name><name name-style="western"><surname>von Lehe</surname><given-names>M.</given-names></name></person-group><article-title>Surgery for temporal lobe epilepsy in the elderly: Improving quality of life despite cognitive impairment</article-title><source>Seizure</source><year>2020</year><volume>79</volume><fpage>112</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/j.seizure.2020.05.003</pub-id><pub-id pub-id-type="pmid">32464533</pub-id></element-citation></ref><ref id="B39-sensors-25-05301"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Helmstaedter</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lendt</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lux</surname><given-names>S.</given-names></name></person-group><source>Verbaler Lern- und Merkfaehigkeitstest</source><publisher-name>Beltz-Test</publisher-name><publisher-loc>G&#246;ttingen, Germany</publisher-loc><year>2001</year></element-citation></ref><ref id="B40-sensors-25-05301"><label>40.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Meyers</surname><given-names>J.E.</given-names></name><name name-style="western"><surname>Meyers</surname><given-names>K.R.</given-names></name></person-group><source>Rey Complex Figure Test and Recognition Trial</source><publisher-name>PAR</publisher-name><publisher-loc>Lutz, FL, USA</publisher-loc><year>1995</year></element-citation></ref><ref id="B41-sensors-25-05301"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bopp</surname><given-names>M.H.A.</given-names></name><name name-style="western"><surname>Corr</surname><given-names>F.</given-names></name><name name-style="western"><surname>Sass</surname><given-names>B.</given-names></name><name name-style="western"><surname>Pojskic</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kemmling</surname><given-names>A.</given-names></name><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name></person-group><article-title>Augmented Reality to Compensate for Navigation Inaccuracies</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>9591</elocation-id><pub-id pub-id-type="doi">10.3390/s22249591</pub-id><pub-id pub-id-type="pmid">36559961</pub-id><pub-id pub-id-type="pmcid">PMC9787763</pub-id></element-citation></ref><ref id="B42-sensors-25-05301"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kantelhardt</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Gutenberg</surname><given-names>A.</given-names></name><name name-style="western"><surname>Neulen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Keric</surname><given-names>N.</given-names></name><name name-style="western"><surname>Renovanz</surname><given-names>M.</given-names></name><name name-style="western"><surname>Giese</surname><given-names>A.</given-names></name></person-group><article-title>Video-Assisted Navigation for Adjustment of Image-Guidance Accuracy to Slight Brain Shift</article-title><source>Oper. Neurosurg.</source><year>2015</year><volume>11</volume><fpage>504</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1227/NEU.0000000000000921</pub-id><pub-id pub-id-type="pmid">29506163</pub-id></element-citation></ref><ref id="B43-sensors-25-05301"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Steinmeier</surname><given-names>R.</given-names></name><name name-style="western"><surname>Rachinger</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kaus</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ganslandt</surname><given-names>O.</given-names></name><name name-style="western"><surname>Huk</surname><given-names>W.</given-names></name><name name-style="western"><surname>Fahlbusch</surname><given-names>R.</given-names></name></person-group><article-title>Factors influencing the application accuracy of neuronavigation systems</article-title><source>Stereotact. Funct. Neurosurg.</source><year>2000</year><volume>75</volume><fpage>188</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1159/000048404</pub-id><pub-id pub-id-type="pmid">11910212</pub-id></element-citation></ref><ref id="B44-sensors-25-05301"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ganslandt</surname><given-names>O.</given-names></name><name name-style="western"><surname>Cerny</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hastreiter</surname><given-names>P.</given-names></name><name name-style="western"><surname>Greiner</surname><given-names>G.</given-names></name><name name-style="western"><surname>Fahlbusch</surname><given-names>R.</given-names></name></person-group><article-title>Quantification of, visualization of, and compensation for brain shift using intraoperative magnetic resonance imaging</article-title><source>Neurosurgery</source><year>2000</year><volume>47</volume><fpage>1070</fpage><lpage>1079</lpage><page-range>1070&#8211;1079; discussion 1079&#8211;1080</page-range><pub-id pub-id-type="doi">10.1097/00006123-200011000-00008</pub-id><pub-id pub-id-type="pmid">11063099</pub-id></element-citation></ref><ref id="B45-sensors-25-05301"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ganslandt</surname><given-names>O.</given-names></name><name name-style="western"><surname>Hastreiter</surname><given-names>P.</given-names></name><name name-style="western"><surname>Fahlbusch</surname><given-names>R.</given-names></name></person-group><article-title>Intraoperative compensation for brain shift</article-title><source>Surg. Neurol.</source><year>2001</year><volume>56</volume><fpage>357</fpage><lpage>364</lpage><page-range>357&#8211;364; discussion 364&#8211;365</page-range><pub-id pub-id-type="doi">10.1016/S0090-3019(01)00628-0</pub-id><pub-id pub-id-type="pmid">11755962</pub-id></element-citation></ref><ref id="B46-sensors-25-05301"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Poggi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pallotta</surname><given-names>S.</given-names></name><name name-style="western"><surname>Russo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gallina</surname><given-names>P.</given-names></name><name name-style="western"><surname>Torresin</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bucciolini</surname><given-names>M.</given-names></name></person-group><article-title>Neuronavigation accuracy dependence on CT and MR imaging parameters: A phantom-based study</article-title><source>Phys. Med. Biol.</source><year>2003</year><volume>48</volume><fpage>2199</fpage><lpage>2216</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/48/14/311</pub-id><pub-id pub-id-type="pmid">12894979</pub-id></element-citation></ref><ref id="B47-sensors-25-05301"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hastreiter</surname><given-names>P.</given-names></name><name name-style="western"><surname>Rezk-Salama</surname><given-names>C.</given-names></name><name name-style="western"><surname>Soza</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bauer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Greiner</surname><given-names>G.</given-names></name><name name-style="western"><surname>Fahlbusch</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ganslandt</surname><given-names>O.</given-names></name><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name></person-group><article-title>Strategies for brain shift evaluation</article-title><source>Med. Image Anal.</source><year>2004</year><volume>8</volume><fpage>447</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.media.2004.02.001</pub-id><pub-id pub-id-type="pmid">15567708</pub-id></element-citation></ref><ref id="B48-sensors-25-05301"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grote</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gjorgjevski</surname><given-names>M.</given-names></name><name name-style="western"><surname>Carl</surname><given-names>B.</given-names></name><name name-style="western"><surname>Delev</surname><given-names>D.</given-names></name><name name-style="western"><surname>Knake</surname><given-names>S.</given-names></name><name name-style="western"><surname>Menzler</surname><given-names>K.</given-names></name><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bopp</surname><given-names>M.H.A.</given-names></name></person-group><article-title>Frameless Stereotaxy in Stereoelectroencephalography Using Intraoperative Computed Tomography</article-title><source>Brain Sci.</source><year>2025</year><volume>15</volume><elocation-id>184</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci15020184</pub-id><pub-id pub-id-type="pmid">40002517</pub-id><pub-id pub-id-type="pmcid">PMC11853342</pub-id></element-citation></ref><ref id="B49-sensors-25-05301"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grote</surname><given-names>A.</given-names></name><name name-style="western"><surname>Neumann</surname><given-names>F.</given-names></name><name name-style="western"><surname>Menzler</surname><given-names>K.</given-names></name><name name-style="western"><surname>Carl</surname><given-names>B.</given-names></name><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name><name name-style="western"><surname>Bopp</surname><given-names>M.H.A.</given-names></name></person-group><article-title>Augmented Reality in Extratemporal Lobe Epilepsy Surgery</article-title><source>J. Clin. Med.</source><year>2024</year><volume>13</volume><elocation-id>5692</elocation-id><pub-id pub-id-type="doi">10.3390/jcm13195692</pub-id><pub-id pub-id-type="pmid">39407752</pub-id><pub-id pub-id-type="pmcid">PMC11477171</pub-id></element-citation></ref><ref id="B50-sensors-25-05301"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Doyle</surname><given-names>W.K.</given-names></name></person-group><article-title>Low end interactive image-directed neurosurgery. Update on rudimentary augmented reality used in epilepsy surgery</article-title><source>Stud. Health Technol. Inform.</source><year>1996</year><volume>29</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">10163741</pub-id></element-citation></ref><ref id="B51-sensors-25-05301"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brinker</surname><given-names>T.</given-names></name><name name-style="western"><surname>Arango</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kaminsky</surname><given-names>J.</given-names></name><name name-style="western"><surname>Samii</surname><given-names>A.</given-names></name><name name-style="western"><surname>Thorns</surname><given-names>U.</given-names></name><name name-style="western"><surname>Vorkapic</surname><given-names>P.</given-names></name><name name-style="western"><surname>Samii</surname><given-names>M.</given-names></name></person-group><article-title>An experimental approach to image guided skull base surgery employing a microscope-based neuronavigation system</article-title><source>Acta Neurochir.</source><year>1998</year><volume>140</volume><fpage>883</fpage><lpage>889</lpage><pub-id pub-id-type="doi">10.1007/s007010050189</pub-id><pub-id pub-id-type="pmid">9842424</pub-id></element-citation></ref><ref id="B52-sensors-25-05301"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kajiwara</surname><given-names>K.</given-names></name><name name-style="western"><surname>Nishizaki</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ohmoto</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Nomura</surname><given-names>S.</given-names></name><name name-style="western"><surname>Suzuki</surname><given-names>M.</given-names></name></person-group><article-title>Image-guided transsphenoidal surgery for pituitary lesions using Mehrkoordinaten Manipulator (MKM) navigation system</article-title><source>Minim. Invasive Neurosurg.</source><year>2003</year><volume>46</volume><fpage>78</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1055/s-2003-39340</pub-id><pub-id pub-id-type="pmid">12761676</pub-id></element-citation></ref><ref id="B53-sensors-25-05301"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meola</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>S.D.</given-names></name></person-group><article-title>Letter: Navigation-Linked Heads-Up Display in Intracranial Surgery: Early Experience</article-title><source>Oper. Neurosurg.</source><year>2018</year><volume>14</volume><fpage>E71</fpage><lpage>E72</lpage><pub-id pub-id-type="doi">10.1093/ons/opy048</pub-id><pub-id pub-id-type="pmid">29590481</pub-id></element-citation></ref><ref id="B54-sensors-25-05301"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Watanabe</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fujii</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hayashi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kimura</surname><given-names>M.</given-names></name><name name-style="western"><surname>Murai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hata</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sugiura</surname><given-names>A.</given-names></name><name name-style="western"><surname>Tsuzaka</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wakabayashi</surname><given-names>T.</given-names></name></person-group><article-title>Evaluation of errors influencing accuracy in image-guided neurosurgery</article-title><source>Radiol. Phys. Technol.</source><year>2009</year><volume>2</volume><fpage>120</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1007/s12194-009-0053-6</pub-id><pub-id pub-id-type="pmid">20821109</pub-id></element-citation></ref><ref id="B55-sensors-25-05301"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fiegele</surname><given-names>T.</given-names></name><name name-style="western"><surname>Feuchtner</surname><given-names>G.</given-names></name><name name-style="western"><surname>Sohm</surname><given-names>F.</given-names></name><name name-style="western"><surname>Bauer</surname><given-names>R.</given-names></name><name name-style="western"><surname>Anton</surname><given-names>J.V.</given-names></name><name name-style="western"><surname>Gotwald</surname><given-names>T.</given-names></name><name name-style="western"><surname>Twerdy</surname><given-names>K.</given-names></name><name name-style="western"><surname>Eisner</surname><given-names>W.</given-names></name></person-group><article-title>Accuracy of stereotactic electrode placement in deep brain stimulation by intraoperative computed tomography</article-title><source>Parkinsonism Relat. Disord.</source><year>2008</year><volume>14</volume><fpage>595</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1016/j.parkreldis.2008.01.008</pub-id><pub-id pub-id-type="pmid">18328766</pub-id></element-citation></ref><ref id="B56-sensors-25-05301"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wolfsberger</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rossler</surname><given-names>K.</given-names></name><name name-style="western"><surname>Regatschnig</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ungersbock</surname><given-names>K.</given-names></name></person-group><article-title>Anatomical landmarks for image registration in frameless stereotactic neuronavigation</article-title><source>Neurosurg. Rev.</source><year>2002</year><volume>25</volume><fpage>68</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1007/s10143-001-0201-x</pub-id><pub-id pub-id-type="pmid">11954768</pub-id></element-citation></ref><ref id="B57-sensors-25-05301"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koivukangas</surname><given-names>T.</given-names></name><name name-style="western"><surname>Katisko</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Koivukangas</surname><given-names>J.P.</given-names></name></person-group><article-title>Technical accuracy of optical and the electromagnetic tracking systems</article-title><source>Springerplus</source><year>2013</year><volume>2</volume><fpage>90</fpage><pub-id pub-id-type="doi">10.1186/2193-1801-2-90</pub-id><pub-id pub-id-type="pmid">23586003</pub-id><pub-id pub-id-type="pmcid">PMC3622743</pub-id></element-citation></ref><ref id="B58-sensors-25-05301"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stieglitz</surname><given-names>L.H.</given-names></name><name name-style="western"><surname>Fichtner</surname><given-names>J.</given-names></name><name name-style="western"><surname>Andres</surname><given-names>R.</given-names></name><name name-style="western"><surname>Schucht</surname><given-names>P.</given-names></name><name name-style="western"><surname>Krahenbuhl</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Raabe</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beck</surname><given-names>J.</given-names></name></person-group><article-title>The silent loss of neuronavigation accuracy: A systematic retrospective analysis of factors influencing the mismatch of frameless stereotactic systems in cranial neurosurgery</article-title><source>Neurosurgery</source><year>2013</year><volume>72</volume><fpage>796</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1227/NEU.0b013e318287072d</pub-id><pub-id pub-id-type="pmid">23334280</pub-id></element-citation></ref><ref id="B59-sensors-25-05301"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mongen</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Willems</surname><given-names>P.W.A.</given-names></name></person-group><article-title>Current accuracy of surface matching compared to adhesive markers in patient-to-image registration</article-title><source>Acta Neurochir.</source><year>2019</year><volume>161</volume><fpage>865</fpage><lpage>870</lpage><pub-id pub-id-type="doi">10.1007/s00701-019-03867-8</pub-id><pub-id pub-id-type="pmid">30879130</pub-id><pub-id pub-id-type="pmcid">PMC6483968</pub-id></element-citation></ref><ref id="B60-sensors-25-05301"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Carl</surname><given-names>B.</given-names></name><name name-style="western"><surname>Bopp</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sass</surname><given-names>B.</given-names></name><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name></person-group><article-title>Intraoperative computed tomography as reliable navigation registration device in 200 cranial procedures</article-title><source>Acta Neurochir.</source><year>2018</year><volume>160</volume><fpage>1681</fpage><lpage>1689</lpage><pub-id pub-id-type="doi">10.1007/s00701-018-3641-6</pub-id><pub-id pub-id-type="pmid">30051160</pub-id></element-citation></ref><ref id="B61-sensors-25-05301"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pfisterer</surname><given-names>W.K.</given-names></name><name name-style="western"><surname>Papadopoulos</surname><given-names>S.</given-names></name><name name-style="western"><surname>Drumm</surname><given-names>D.A.</given-names></name><name name-style="western"><surname>Smith</surname><given-names>K.</given-names></name><name name-style="western"><surname>Preul</surname><given-names>M.C.</given-names></name></person-group><article-title>Fiducial versus nonfiducial neuronavigation registration assessment and considerations of accuracy</article-title><source>Neurosurgery</source><year>2008</year><volume>62</volume><fpage>201</fpage><lpage>207</lpage><page-range>201&#8211;207; discussion 207&#8211;208</page-range><pub-id pub-id-type="doi">10.1227/01.neu.0000317394.14303.99</pub-id><pub-id pub-id-type="pmid">18424987</pub-id></element-citation></ref><ref id="B62-sensors-25-05301"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rachinger</surname><given-names>J.</given-names></name><name name-style="western"><surname>von Keller</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ganslandt</surname><given-names>O.</given-names></name><name name-style="western"><surname>Fahlbusch</surname><given-names>R.</given-names></name><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name></person-group><article-title>Application accuracy of automatic registration in frameless stereotaxy</article-title><source>Stereotact. Funct. Neurosurg.</source><year>2006</year><volume>84</volume><fpage>109</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1159/000094462</pub-id><pub-id pub-id-type="pmid">16840821</pub-id></element-citation></ref><ref id="B63-sensors-25-05301"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name></person-group><article-title>Guidelines for the placement of fiducial points in image-guided neurosurgery</article-title><source>Int. J. Med. Robot.</source><year>2010</year><volume>6</volume><fpage>142</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1002/rcs.299</pub-id><pub-id pub-id-type="pmid">20131341</pub-id></element-citation></ref><ref id="B64-sensors-25-05301"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name></person-group><article-title>Distribution templates of the fiducial points in image-guided neurosurgery</article-title><source>Neurosurgery</source><year>2010</year><volume>66</volume><fpage>143</fpage><lpage>150</lpage><page-range>143&#8211;150; discussion 150&#8211;151</page-range><pub-id pub-id-type="doi">10.1227/01.NEU.0000365827.88888.80</pub-id><pub-id pub-id-type="pmid">20124925</pub-id></element-citation></ref><ref id="B65-sensors-25-05301"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name></person-group><article-title>Improving target registration accuracy in image-guided neurosurgery by optimizing the distribution of fiducial points</article-title><source>Int. J. Med. Robot.</source><year>2009</year><volume>5</volume><fpage>26</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1002/rcs.227</pub-id><pub-id pub-id-type="pmid">19107838</pub-id></element-citation></ref><ref id="B66-sensors-25-05301"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paraskevopoulos</surname><given-names>D.</given-names></name><name name-style="western"><surname>Unterberg</surname><given-names>A.</given-names></name><name name-style="western"><surname>Metzner</surname><given-names>R.</given-names></name><name name-style="western"><surname>Dreyhaupt</surname><given-names>J.</given-names></name><name name-style="western"><surname>Eggers</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wirtz</surname><given-names>C.R.</given-names></name></person-group><article-title>Comparative study of application accuracy of two frameless neuronavigation systems: Experimental error assessment quantifying registration methods and clinically influencing factors</article-title><source>Neurosurg. Rev.</source><year>2010</year><volume>34</volume><fpage>217</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1007/s10143-010-0302-5</pub-id><pub-id pub-id-type="pmid">21246391</pub-id></element-citation></ref><ref id="B67-sensors-25-05301"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nabavi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Black</surname><given-names>P.M.</given-names></name><name name-style="western"><surname>Gering</surname><given-names>D.T.</given-names></name><name name-style="western"><surname>Westin</surname><given-names>C.F.</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>V.</given-names></name><name name-style="western"><surname>Pergolizzi</surname><given-names>R.S.</given-names><suffix>Jr.</suffix></name><name name-style="western"><surname>Ferrant</surname><given-names>M.</given-names></name><name name-style="western"><surname>Warfield</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Hata</surname><given-names>N.</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>R.B.</given-names></name><etal/></person-group><article-title>Serial intraoperative magnetic resonance imaging of brain shift</article-title><source>Neurosurgery</source><year>2001</year><volume>48</volume><fpage>787</fpage><lpage>797</lpage><page-range>787&#8211;797; discussion 797&#8211;798</page-range><pub-id pub-id-type="doi">10.1097/00006123-200104000-00019</pub-id><pub-id pub-id-type="pmid">11322439</pub-id></element-citation></ref><ref id="B68-sensors-25-05301"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Negwer</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hiepe</surname><given-names>P.</given-names></name><name name-style="western"><surname>Meyer</surname><given-names>B.</given-names></name><name name-style="western"><surname>Krieg</surname><given-names>S.M.</given-names></name></person-group><article-title>Elastic Fusion Enables Fusion of Intraoperative Magnetic Resonance Imaging Data with Preoperative Neuronavigation Data</article-title><source>World Neurosurg.</source><year>2020</year><volume>142</volume><fpage>e223</fpage><lpage>e228</lpage><pub-id pub-id-type="doi">10.1016/j.wneu.2020.06.166</pub-id><pub-id pub-id-type="pmid">32599196</pub-id></element-citation></ref><ref id="B69-sensors-25-05301"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Letteboer</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Willems</surname><given-names>P.W.</given-names></name><name name-style="western"><surname>Viergever</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Niessen</surname><given-names>W.J.</given-names></name></person-group><article-title>Brain shift estimation in image-guided neurosurgery using 3-D ultrasound</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2005</year><volume>52</volume><fpage>268</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1109/TBME.2004.840186</pub-id><pub-id pub-id-type="pmid">15709664</pub-id></element-citation></ref><ref id="B70-sensors-25-05301"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arbel</surname><given-names>T.</given-names></name><name name-style="western"><surname>Morandi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Comeau</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Collins</surname><given-names>D.L.</given-names></name></person-group><article-title>Automatic non-linear MRI-ultrasound registration for the correction of intra-operative brain deformations</article-title><source>Comput. Aided Surg.</source><year>2004</year><volume>9</volume><fpage>123</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.3109/10929080500079248</pub-id><pub-id pub-id-type="pmid">16192052</pub-id></element-citation></ref><ref id="B71-sensors-25-05301"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Comeau</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Sadikot</surname><given-names>A.F.</given-names></name><name name-style="western"><surname>Fenster</surname><given-names>A.</given-names></name><name name-style="western"><surname>Peters</surname><given-names>T.M.</given-names></name></person-group><article-title>Intraoperative ultrasound for guidance and tissue shift correction in image-guided neurosurgery</article-title><source>Med. Phys.</source><year>2000</year><volume>27</volume><fpage>787</fpage><lpage>800</lpage><pub-id pub-id-type="doi">10.1118/1.598942</pub-id><pub-id pub-id-type="pmid">10798702</pub-id></element-citation></ref><ref id="B72-sensors-25-05301"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bopp</surname><given-names>M.H.A.</given-names></name><name name-style="western"><surname>Grote</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gjorgjevski</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pojskic</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sass</surname><given-names>B.</given-names></name><name name-style="western"><surname>Nimsky</surname><given-names>C.</given-names></name></person-group><article-title>Enabling Navigation and Augmented Reality in the Sitting Position in Posterior Fossa Surgery Using Intraoperative Ultrasound</article-title><source>Cancers</source><year>2024</year><volume>16</volume><elocation-id>1985</elocation-id><pub-id pub-id-type="doi">10.3390/cancers16111985</pub-id><pub-id pub-id-type="pmid">38893106</pub-id><pub-id pub-id-type="pmcid">PMC11171013</pub-id></element-citation></ref><ref id="B73-sensors-25-05301"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fick</surname><given-names>T.</given-names></name><name name-style="western"><surname>van Doormaal</surname><given-names>J.A.M.</given-names></name><name name-style="western"><surname>Hoving</surname><given-names>E.W.</given-names></name><name name-style="western"><surname>Willems</surname><given-names>P.W.A.</given-names></name><name name-style="western"><surname>van Doormaal</surname><given-names>T.P.C.</given-names></name></person-group><article-title>Current Accuracy of Augmented Reality Neuronavigation Systems: Systematic Review and Meta-Analysis</article-title><source>World Neurosurg.</source><year>2021</year><volume>146</volume><fpage>179</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.wneu.2020.11.029</pub-id><pub-id pub-id-type="pmid">33197631</pub-id></element-citation></ref><ref id="B74-sensors-25-05301"><label>74.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hey</surname><given-names>G.</given-names></name><name name-style="western"><surname>Guyot</surname><given-names>M.</given-names></name><name name-style="western"><surname>Carter</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lucke-Wold</surname><given-names>B.</given-names></name></person-group><article-title>Augmented Reality in Neurosurgery: A New Paradigm for Training</article-title><source>Medicina</source><year>2023</year><volume>59</volume><elocation-id>1721</elocation-id><pub-id pub-id-type="doi">10.3390/medicina59101721</pub-id><pub-id pub-id-type="pmid">37893439</pub-id><pub-id pub-id-type="pmcid">PMC10608758</pub-id></element-citation></ref><ref id="B75-sensors-25-05301"><label>75.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Malone</surname><given-names>H.R.</given-names></name><name name-style="western"><surname>Syed</surname><given-names>O.N.</given-names></name><name name-style="western"><surname>Downes</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>D&#8217;Ambrosio</surname><given-names>A.L.</given-names></name><name name-style="western"><surname>Quest</surname><given-names>D.O.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>M.G.</given-names></name></person-group><article-title>Simulation in neurosurgery: A review of computer-based simulation environments and their surgical applications</article-title><source>Neurosurgery</source><year>2010</year><volume>67</volume><fpage>1105</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1227/NEU.0b013e3181ee46d0</pub-id><pub-id pub-id-type="pmid">20881575</pub-id></element-citation></ref><ref id="B76-sensors-25-05301"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fargen</surname><given-names>K.M.</given-names></name><name name-style="western"><surname>Siddiqui</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Veznedaroglu</surname><given-names>E.</given-names></name><name name-style="western"><surname>Turner</surname><given-names>R.D.</given-names></name><name name-style="western"><surname>Ringer</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Mocco</surname><given-names>J.</given-names></name></person-group><article-title>Simulator based angiography education in neurosurgery: Results of a pilot educational program</article-title><source>J. Neurointerv Surg.</source><year>2012</year><volume>4</volume><fpage>438</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1136/neurintsurg-2011-010128</pub-id><pub-id pub-id-type="pmid">22015637</pub-id></element-citation></ref><ref id="B77-sensors-25-05301"><label>77.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Skyrman</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>M.</given-names></name><name name-style="western"><surname>Edstrom</surname><given-names>E.</given-names></name><name name-style="western"><surname>Burstrom</surname><given-names>G.</given-names></name><name name-style="western"><surname>Forander</surname><given-names>P.</given-names></name><name name-style="western"><surname>Homan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kor</surname><given-names>F.</given-names></name><name name-style="western"><surname>Holthuizen</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hendriks</surname><given-names>B.H.W.</given-names></name><name name-style="western"><surname>Persson</surname><given-names>O.</given-names></name><etal/></person-group><article-title>Augmented reality navigation for cranial biopsy and external ventricular drain insertion</article-title><source>Neurosurg. Focus.</source><year>2021</year><volume>51</volume><fpage>E7</fpage><pub-id pub-id-type="doi">10.3171/2021.5.FOCUS20813</pub-id><pub-id pub-id-type="pmid">34333469</pub-id></element-citation></ref><ref id="B78-sensors-25-05301"><label>78.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Montemurro</surname><given-names>N.</given-names></name><name name-style="western"><surname>Condino</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cattari</surname><given-names>N.</given-names></name><name name-style="western"><surname>D&#8217;Amato</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ferrari</surname><given-names>V.</given-names></name><name name-style="western"><surname>Cutolo</surname><given-names>F.</given-names></name></person-group><article-title>Augmented Reality-Assisted Craniotomy for Parasagittal and Convexity En Plaque Meningiomas and Custom-Made Cranio-Plasty: A Preliminary Laboratory Report</article-title><source>Int. J. Environ. Res. Public. Health</source><year>2021</year><volume>18</volume><elocation-id>9955</elocation-id><pub-id pub-id-type="doi">10.3390/ijerph18199955</pub-id><pub-id pub-id-type="pmid">34639256</pub-id><pub-id pub-id-type="pmcid">PMC8507881</pub-id></element-citation></ref><ref id="B79-sensors-25-05301"><label>79.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Gestel</surname><given-names>F.</given-names></name><name name-style="western"><surname>Frantz</surname><given-names>T.</given-names></name><name name-style="western"><surname>Buyck</surname><given-names>F.</given-names></name><name name-style="western"><surname>Geens</surname><given-names>W.</given-names></name><name name-style="western"><surname>Neuville</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Bruneau</surname><given-names>M.</given-names></name><name name-style="western"><surname>Jansen</surname><given-names>B.</given-names></name><name name-style="western"><surname>Scheerlinck</surname><given-names>T.</given-names></name><name name-style="western"><surname>Vandemeulebroucke</surname><given-names>J.</given-names></name><name name-style="western"><surname>Duerinck</surname><given-names>J.</given-names></name></person-group><article-title>Neuro-oncological augmented reality planning for intracranial tumor resection</article-title><source>Front. Neurol.</source><year>2023</year><volume>14</volume><elocation-id>1104571</elocation-id><pub-id pub-id-type="doi">10.3389/fneur.2023.1104571</pub-id><pub-id pub-id-type="pmid">36998774</pub-id><pub-id pub-id-type="pmcid">PMC10043492</pub-id></element-citation></ref><ref id="B80-sensors-25-05301"><label>80.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mishra</surname><given-names>R.</given-names></name><name name-style="western"><surname>Narayanan</surname><given-names>M.D.K.</given-names></name><name name-style="western"><surname>Umana</surname><given-names>G.E.</given-names></name><name name-style="western"><surname>Montemurro</surname><given-names>N.</given-names></name><name name-style="western"><surname>Chaurasia</surname><given-names>B.</given-names></name><name name-style="western"><surname>Deora</surname><given-names>H.</given-names></name></person-group><article-title>Virtual Reality in Neurosurgery: Beyond Neurosurgical Planning</article-title><source>Int. J. Environ. Res. Public. Health</source><year>2022</year><volume>19</volume><elocation-id>1719</elocation-id><pub-id pub-id-type="doi">10.3390/ijerph19031719</pub-id><pub-id pub-id-type="pmid">35162742</pub-id><pub-id pub-id-type="pmcid">PMC8835688</pub-id></element-citation></ref><ref id="B81-sensors-25-05301"><label>81.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alaraj</surname><given-names>A.</given-names></name><name name-style="western"><surname>Charbel</surname><given-names>F.T.</given-names></name><name name-style="western"><surname>Birk</surname><given-names>D.</given-names></name><name name-style="western"><surname>Tobin</surname><given-names>M.</given-names></name><name name-style="western"><surname>Luciano</surname><given-names>C.</given-names></name><name name-style="western"><surname>Banerjee</surname><given-names>P.P.</given-names></name><name name-style="western"><surname>Rizzi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sorenson</surname><given-names>J.</given-names></name><name name-style="western"><surname>Foley</surname><given-names>K.</given-names></name><name name-style="western"><surname>Slavin</surname><given-names>K.</given-names></name><etal/></person-group><article-title>Role of cranial and spinal virtual and augmented reality simulation using immersive touch modules in neurosurgical training</article-title><source>Neurosurgery</source><year>2013</year><volume>72</volume><issue>(Suppl. S1)</issue><fpage>115</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1227/NEU.0b013e3182753093</pub-id><pub-id pub-id-type="pmid">23254799</pub-id><pub-id pub-id-type="pmcid">PMC3676942</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05301-f001" orientation="portrait"><label>Figure 1</label><caption><p>Overall workflow of the presented approach to MTLE surgery investigating intraoperative computed tomography (iCT), navigated intraoperative ultrasound (iUS), and augmented reality (AR) support.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05301-g001.jpg"/></fig><fig position="float" id="sensors-25-05301-f002" orientation="portrait"><label>Figure 2</label><caption><p>Epileptogenic outcome after mTLE surgery after 1 year (<bold>left</bold>) and at latest available follow-up (<bold>right</bold>) classified according to ILAE (ILAE class 1 to ILAE class 5).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05301-g002.jpg"/></fig><fig position="float" id="sensors-25-05301-f003" orientation="portrait"><label>Figure 3</label><caption><p>Accuracy checks using artificial landmarks either assessed with a pointer (<bold>A</bold>) or the virtualized pointer using the operative microscope (<bold>B</bold>) with segmented outlines of the artificial marker visualized in the operating field using microscope-based AR.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05301-g003.jpg"/></fig><fig position="float" id="sensors-25-05301-f004" orientation="portrait"><label>Figure 4</label><caption><p>Accuracy checks using a 3D reconstruction of the cortical profile with the microscope&#8217;s focal plane being stepwise moved along the viewing trajectory.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05301-g004.jpg"/></fig><fig position="float" id="sensors-25-05301-f005" orientation="portrait"><label>Figure 5</label><caption><p>Navigated intraoperative ultrasound allowing for accuracy checks with outlined MRI-based objects (orange: amygdala, blue: hippocampus, green: brainstem, light blue: ventricle) and fiber tractography of the corticospinal tracts and optic radiation in axial (<bold>A</bold>) and coronar (<bold>B</bold>) view.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05301-g005.jpg"/></fig><fig position="float" id="sensors-25-05301-f006" orientation="portrait"><label>Figure 6</label><caption><p>Navigated intraoperative ultrasound after resection of the amygdala (orange) and hippocampus (blue) visualizing the extent of resection and surgical trajectory in axial views (<bold>A</bold>,<bold>B</bold>) and oblique view (<bold>C</bold>).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05301-g006.jpg"/></fig><fig position="float" id="sensors-25-05301-f007" orientation="portrait"><label>Figure 7</label><caption><p>Microscope-based augmented reality and image-guidance in the course of surgery while assessing the temporal horn/hippocampus (<bold>A</bold>), the hippocampus (<bold>B</bold>), parts of the amygdala (<bold>C</bold>), and finally, showing the arachnoid level after resection of the amygdala (<bold>D</bold>) with preoperative MRI data (<bold>A</bold>&#8211;<bold>C</bold>) as well as intraoperative US data after resection (<bold>D</bold>) in the navigation panel below the microscopic view in axial, coronal, and sagittal orientation, as well as a probe&#8217;s eye view (left upper corner) of preoperative MRI data according the recent microscope&#8217;s focal plane, allowing for immediate matching of imaging data and patient&#8217;s anatomy (blue: hippocampus, orange: amygdala, green: brainstem, light blue: temporal horn, dark blue: vessels).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05301-g007.jpg"/></fig></floats-group></article></pmc-articleset>