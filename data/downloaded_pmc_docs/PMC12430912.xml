<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430912</article-id><article-id pub-id-type="pmcid-ver">PMC12430912.1</article-id><article-id pub-id-type="pmcaid">12430912</article-id><article-id pub-id-type="pmcaiid">12430912</article-id><article-id pub-id-type="doi">10.3390/s25175261</article-id><article-id pub-id-type="publisher-id">sensors-25-05261</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>ADBM: Adversarial Diffusion Bridge Model for Denoising of 3D Point Cloud Data</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-6879-4562</contrib-id><name name-style="western"><surname>Nam</surname><given-names initials="C">Changwoo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9312-6299</contrib-id><name name-style="western"><surname>Lee</surname><given-names initials="SJ">Sang Jun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="c1-sensors-25-05261" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Piano</surname><given-names initials="S">Samanta</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Khameneifar</surname><given-names initials="F">Farbod</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05261">Division of Electronic Engineering, Jeonbuk National University, 567 Baekje-daero, Deokjin-gu, Jeonju 54896, Republic of Korea; <email>cw.nam@jbnu.ac.kr</email></aff><author-notes><corresp id="c1-sensors-25-05261"><label>*</label>Correspondence: <email>sj.lee@jbnu.ac.kr</email>; Tel.: +82-63-270-2463</corresp></author-notes><pub-date pub-type="epub"><day>24</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5261</elocation-id><history><date date-type="received"><day>21</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>11</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>22</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>24</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 11:25:14.803"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05261.pdf"/><abstract><p>We address the task of point cloud denoising by leveraging a diffusion-based generative framework augmented with adversarial training. While recent diffusion models have demonstrated strong capabilities in learning complex data distributions, their effectiveness in recovering fine geometric details remains limited, especially under severe noise conditions. To mitigate this, we propose the Adversarial Diffusion Bridge Model (ADBM), a novel approach for denoising 3D point cloud data by integrating a diffusion bridge model with adversarial learning. ADBM incorporates a lightweight discriminator that guides the denoising process through adversarial supervision, encouraging sharper and more faithful reconstructions. The denoiser is trained using a denoising diffusion objective based on a Schr&#246;dinger Bridge, while the discriminator distinguishes between real, clean point clouds and generated outputs, promoting perceptual realism. Experiments are conducted on the PU-Net and PC-Net datasets, with performance evaluation employing the Chamfer distance and Point-to-Mesh metrics. The qualitative and quantitative results both highlight the effectiveness of adversarial supervision in enhancing local detail reconstruction, making our approach a promising direction for robust point cloud restoration.</p></abstract><kwd-group><kwd>deep learning</kwd><kwd>diffusion model</kwd><kwd>adversarial training</kwd><kwd>generative model</kwd><kwd>3D point cloud denoising</kwd></kwd-group><funding-group><award-group><funding-source>Korea government</funding-source><award-id>IITP-2025-RS-2024-00439292</award-id></award-group><award-group><funding-source>National Research Foundation of Korea</funding-source></award-group><funding-statement>This work was partly supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation(IITP)-Innovative Human Resource Development for Local Intellectualization program grant funded by the Korea government(MSIT)(IITP-2025-RS-2024-00439292) and the Regional Innovation System &amp; Education (RISE) initiative funded by the Ministry of Education and administered by the National Research Foundation of Korea (NRF).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05261"><title>1. Introduction</title><p>Point cloud denoising is critical for enhancing data quality in applications where accurate spatial representation directly impacts system performance and user accessibility. Point clouds acquired via LiDAR, depth sensors, or photogrammetry frequently contain noise from environmental interference, sensor limitations, or motion artifacts. This degradation is especially critical in accessibility applications, such as assistive navigation, where noisy inputs cause errors in object detection&#160;[<xref rid="B1-sensors-25-05261" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05261" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05261" ref-type="bibr">3</xref>] and scene reconstruction&#160;[<xref rid="B4-sensors-25-05261" ref-type="bibr">4</xref>]. Also, the presence of noise can obscure fine geometric details and lead to inaccurate shape representations, which are especially problematic for applications requiring high-precision measurements. As the reliance on 3D point cloud data also continues to grow across diverse fields such as robotics&#160;[<xref rid="B5-sensors-25-05261" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05261" ref-type="bibr">6</xref>], urban mapping&#160;[<xref rid="B7-sensors-25-05261" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05261" ref-type="bibr">8</xref>], and medical imaging&#160;[<xref rid="B9-sensors-25-05261" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05261" ref-type="bibr">10</xref>], the demand for robust and effective denoising techniques is becoming increasingly important.</p><p>Traditional 3D point cloud denoising approaches&#160;[<xref rid="B11-sensors-25-05261" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05261" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05261" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05261" ref-type="bibr">14</xref>] have mainly relied on geometric priors and statistical optimization. These approaches demonstrated measurable denoising efficacy under controlled conditions, particularly for Gaussian-type noise distributions. However, they consistently struggled with structural oversimplification in real-world scenarios, where rigid smoothing operators erode fine features like edges and corners, degrading geometric fidelity. Also, non-Gaussian noise from LiDAR or other sensors caused performance collapse, while iterative optimization hindered real-world deployment. These limitations have prompted a shift toward learning-based denoising approaches to adaptively model complex noise patterns while maintaining geometric&#160;fidelity.</p><p>Recent years have seen generative models, particularly diffusion models, emerge as powerful tools for 3D point cloud data synthesis and restoration&#160;[<xref rid="B15-sensors-25-05261" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05261" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05261" ref-type="bibr">17</xref>]. By iteratively refining their understanding of complex data distributions, these models achieve high-fidelity reconstruction of noisy inputs through the structured denoising process. However, traditional diffusion approaches suffer from slow sampling speeds, sampling trajectory design inefficiencies, and instability when handling complex noise distributions. Diffusion bridges&#160;[<xref rid="B18-sensors-25-05261" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05261" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05261" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05261" ref-type="bibr">21</xref>] address these gaps by predicting a direct probabilistic pathway between noisy and clean data distributions, through mitigating the constraints on the prior distribution. While the direct pathway offers improved sampling efficiency and stability, achieving optimal denoising performance, particularly against complex and unknown noise patterns, necessitates a more adaptive and self-improving mechanism.</p><p>Inspired by the success of adversarial learning in generative models&#160;[<xref rid="B22-sensors-25-05261" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05261" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05261" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05261" ref-type="bibr">25</xref>], we propose the Adversarial Diffusion Bridge Model (ADBM), which integrates adversarial supervision into the diffusion bridge framework to enhance 3D point cloud denoising. Specifically, a lightweight discriminator is incorporated into the training pipeline to compel the diffusion bridge model to generate outputs that are not only distributionally close to clean data but also perceptually realistic. As shown in <xref rid="sensors-25-05261-f001" ref-type="fig">Figure 1</xref>, ADBM effectively restores clean shapes from severely noisy inputs across various object categories. The adversarial signal complements the original diffusion bridge objective, providing an additional learning signal that facilitates the recovery of fine geometric details, particularly under complex or non-Gaussian noise conditions. We validate ADBM on PC-Net&#160;[<xref rid="B15-sensors-25-05261" ref-type="bibr">15</xref>] and PU-Net&#160;[<xref rid="B26-sensors-25-05261" ref-type="bibr">26</xref>], 3D object-level point cloud datasets. The experimental results demonstrate that ADBM consistently outperforms existing state-of-the-art denoising methods in terms of both fidelity and generalization. In summary, the main contributions of this paper are as follows:<list list-type="bullet"><list-item><p>We propose ADBM, a novel denoising framework that integrates adversarial learning into a diffusion bridge model, enhancing robustness and generation quality for 3D point cloud restoration.</p></list-item><list-item><p>We design an adversarial training objective specifically formulated for diffusion-based point cloud denoising, which reconstructs fine-grained geometric details of the 3D point cloud.</p></list-item><list-item><p>We perform comparative evaluations on the PU-Net and PC-Net datasets, using the latter solely for testing, and demonstrate that ADBM achieves state-of-the-art denoising performance with strong generalization across unseen objects categories and varying resolutions.</p></list-item></list></p><p>The remainder of this paper is organized as follows: <xref rid="sec2-sensors-25-05261" ref-type="sec">Section 2</xref> reviews relevant literature on point cloud denoising. <xref rid="sec3-sensors-25-05261" ref-type="sec">Section 3</xref> introduces the proposed method. <xref rid="sec4-sensors-25-05261" ref-type="sec">Section 4</xref> and <xref rid="sec5-sensors-25-05261" ref-type="sec">Section 5</xref> present the experimental results and conclusions, respectively.</p></sec><sec id="sec2-sensors-25-05261"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-05261"><title>2.1. Traditional Denoising Methods</title><p>Traditional methods for 3D point cloud denoising mainly leverage geometric priors and local statistics to suppress noise while preserving structural features. Han et al.&#160;[<xref rid="B11-sensors-25-05261" ref-type="bibr">11</xref>] proposed a position-guided linear filter for 3D point cloud denoising that significantly improves computational efficiency while preserving geometric features. To preserve sharp features in noisy point clouds, Zheng et al.&#160;[<xref rid="B12-sensors-25-05261" ref-type="bibr">12</xref>] proposed a guided filter extension that assigns multiple normals to feature points via k-medial skeleton extraction and k-means clustering. To enhance the quality of noisy point sets, Yadav et al.&#160;[<xref rid="B13-sensors-25-05261" ref-type="bibr">13</xref>] introduced a constraint-based denoising method utilizing a vertex-based normal voting tensor and binary eigenvalue optimization. Their approach iteratively filters vertex normals and updates positions with feature-aware constraints, enabling effective noise removal while preserving geometric sharpness. To address the trade-off between noise removal and feature preservation, Liu at al.&#160;[<xref rid="B14-sensors-25-05261" ref-type="bibr">14</xref>] developed a two-stage point cloud denoising method that decouples normal filtering from position updating. Their optimization-based framework maintains the underlying geometric structures, achieving high-quality denoising without oversmoothing sharp edges.</p></sec><sec id="sec2dot2-sensors-25-05261"><title>2.2. Deep Learning-Based Methods</title><p>To overcome the limitations of traditional denoising approaches, recent research has shifted toward learning-based methods that leverage neural networks to model complex noise patterns in point clouds. PointCleanNet&#160;[<xref rid="B15-sensors-25-05261" ref-type="bibr">15</xref>] introduced supervised frameworks that learn mappings from noisy to clean point clouds using regression-based losses. They employ an architecture that explicitly encodes spatial features while incorporating a two-step denoising mechanism to refine predictions iteratively. Another notable approach is score-based point cloud denoising&#160;[<xref rid="B16-sensors-25-05261" ref-type="bibr">16</xref>], which introduces a probabilistic generative framework based on score matching and Langevin dynamics. By learning a score function that estimates the gradient of the data distribution, this method can denoise corrupted point clouds through iterative updates. However, the stochastic nature and high iteration cost of score-based sampling remain key challenges. More recently, the P2P-Bridge&#160;[<xref rid="B17-sensors-25-05261" ref-type="bibr">17</xref>] framework proposes a diffusion bridge-based model that constructs a direct probabilistic path between noisy and clean point clouds via a Schr&#246;dinger Bridge formulation&#160;[<xref rid="B19-sensors-25-05261" ref-type="bibr">19</xref>]. This method utilizes a learnable forward diffusion and reverse denoising to generate geometrically consistent reconstructions, offering improved sample efficiency and generation quality.</p><p>While P2P-Bridge demonstrates strong performance, it remains limited in adaptively learning discriminative features for real-world noise, due to the absence of an explicit adversarial signal. In this work, we integrate adversarial learning on the diffusion bridge model based on P2P-Bridge to further enhance robustness against diverse noise types.</p></sec><sec id="sec2dot3-sensors-25-05261"><title>2.3. Adversarial Training Approaches</title><p>Recent studies have explored adversarial training to improve the quality and realism of diffusion-based generative models. Ko et al.&#160;[<xref rid="B22-sensors-25-05261" ref-type="bibr">22</xref>] introduces dual discriminators in the time and frequency domains to enhance speech fidelity in multi-speaker TTS tasks. Zeng et al.&#160;[<xref rid="B23-sensors-25-05261" ref-type="bibr">23</xref>] leverages semantic priors and adversarial loss for self-supervised shadow removal, enabling structure-preserving generation without paired labels. Liu et al.&#160;[<xref rid="B24-sensors-25-05261" ref-type="bibr">24</xref>] combines adversarial learning approach with torsion angle priors to ensure biologically valid backbones in protein structure generation. A structure-guided discriminator&#160;[<xref rid="B25-sensors-25-05261" ref-type="bibr">25</xref>] has also been proposed to fine-tune diffusion models under layout constraints, improving both semantic consistency and image quality. These approaches demonstrate the effectiveness of adversarial signals in guiding diffusion models toward more realistic and task-aligned&#160;outputs.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05261"><title>3. Methods</title><p>We propose ADBM, an adversarial diffusion bridge model based on P2P-Bridge&#160;[<xref rid="B17-sensors-25-05261" ref-type="bibr">17</xref>], which formulates point cloud denoising as a Schr&#246;dinger Bridge problem between clean and noisy distributions. This approach enables efficient sampling of intermediate states without numerically solving stochastic differential equations, by leveraging a Gaussian approximation under a paired data boundary condition. By predicting the underlying noise component, the model iteratively refines the input through a learned reverse process. To improve the perceptual quality of the denoised outputs, we further incorporate an adversarial training objective. A lightweight discriminator is trained to distinguish real clean point clouds from generated samples, providing an additional supervisory signal to guide the denoising network. <xref rid="sensors-25-05261-f002" ref-type="fig">Figure 2</xref> presents the overall framework.</p><sec id="sec3dot1-sensors-25-05261"><title>3.1. Diffusion Bridge Training</title><p>We formulate point cloud denoising as a Schr&#246;dinger Bridge problem, which seeks a stochastic process that interpolates between two marginal distributions: the clean data distribution <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>data</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the noisy prior distribution <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>prior</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The goal is to find a path measure <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> that minimizes the Kullback&#8211;Leibler divergence from a reference process <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>ref</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> while satisfying the boundary conditions:<disp-formula id="FD1-sensors-25-05261"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>data</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>prior</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Following the formulation proposed in P2P-Bridge, the optimal diffusion path is modeled by a pair of forward and backward stochastic differential equations (SDEs), given, respectively, by<disp-formula id="FD2-sensors-25-05261"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8711;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8711;</mml:mo><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a vector-valued drift function, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a scalar-valued diffusion coefficient controlling the noise, and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are independent standard Wiener processes. <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are potential functions associated with the forward and backward processes and these two processes are coupled as follows:<disp-formula id="FD3-sensors-25-05261"><label>(3)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>data</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>prior</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mi mathvariant="sans-serif">&#936;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This structure ensures that the marginal density <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> interpolates the clean data distribution at <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and the noisy prior at <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, forming a time-consistent probabilistic bridge between the two distributions.</p><p>However, directly solving the system of Equation&#160;(<xref rid="FD2-sensors-25-05261" ref-type="disp-formula">2</xref>) is not practicable for high-dimensional data. To address this, recent works approximate this bridge under a paired data assumption <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>data</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>prior</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and assume linear drift with zero external force, i.e., <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, yielding a tractable Gaussian posterior. Under the assumption of a linear drift <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and a known diffusion schedule <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the posterior of the latent process <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> conditioned on the endpoints <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be written in closed form as a Gaussian&#160;distribution:<disp-formula id="FD4-sensors-25-05261"><label>(4)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mo>&#931;</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where the mean <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the covariance <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#931;</mml:mo><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are given by<disp-formula id="FD5-sensors-25-05261"><label>(5)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msubsup><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mo>&#931;</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle><mml:mspace width="0.166667em"/><mml:mi>I</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#963;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent the accumulated forward and backward variances up to time <italic toggle="yes">t</italic>, respectively. This analytic form enables efficient sampling of intermediate states <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> without requiring numerical integration of the SDE. During training, we sample <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and define the target noise as the residual between the noisy sample and the clean sample as follows:<disp-formula id="FD6-sensors-25-05261"><label>(6)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The denoiser network <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is trained to predict this noise using MSE loss:<disp-formula id="FD7-sensors-25-05261"><label>(7)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>MSE</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>&#1013;</mml:mi></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This training objective is conceptually aligned with denoising diffusion probabilistic models, but is distinct in that the noise is conditioned on paired clean and noisy samples, following the diffusion bridge model.</p></sec><sec id="sec3dot2-sensors-25-05261"><title>3.2. Adversarial Training Method</title><p>While the diffusion bridge framework optimizes a noise prediction loss based on the Schr&#246;dinger Bridge formulation, we further enhance the denoising performance by incorporating an adversarial learning objective. Inspired by GAN-based training schemes&#160;[<xref rid="B27-sensors-25-05261" ref-type="bibr">27</xref>], we introduce a discriminator network that encourages the generation of samples which are indistinguishable from clean point clouds. Specifically, let <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>pred</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the model-generated clean sample obtained via reverse diffusion, and let <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>gt</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the corresponding ground-truth clean point cloud. We define a discriminator <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that learns to assign high scores to real samples and low scores to generated samples. During each training step, we first sample <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and use the denoising network <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to estimate <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>pred</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. We then obtain <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>pred</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> via reverse sampling. The discriminator is trained to distinguish real clean point clouds from those synthesized by the denoising model. Following the typical GAN formulation, the discriminator loss is defined as<disp-formula id="FD8-sensors-25-05261"><label>(8)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>gt</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>data</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:mo form="prefix">log</mml:mo><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>gt</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>pred</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>&#952;</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:mo form="prefix">log</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>pred</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mfenced></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The generator (i.e., the diffusion bridge model) is trained not only to minimize the original noise prediction loss <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>MSE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, but also to fool the discriminator by maximizing its predicted score. This adversarial objective for the generator is defined as<disp-formula id="FD9-sensors-25-05261"><label>(9)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>adv</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>pred</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>&#952;</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:mo form="prefix">log</mml:mo><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>pred</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
which encourages the generator to maximize the discriminator&#8217;s belief that <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>pred</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a real sample. The adversarial signal thus acts as an additional supervisory signal, particularly effective in recovering complex geometric features that are difficult to optimize solely through point-wise regression. To balance the reconstruction and adversarial objectives, we define the final generator loss as a weighted sum:<disp-formula id="FD10-sensors-25-05261"><label>(10)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>MSE</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>adv</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> controls the influence of the adversarial signal. This adversarial extension encourages the generator to produce denoised point clouds that not only minimize numerical reconstruction error but also align with the distribution of real clean point clouds.</p><p>The procedure of adversarial diffusion bridge training, including noise prediction, adversarial loss computation, and alternating updates of the generator and discriminator is summarized in Algorithm 1. In the training procedure, we employ an <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of 0.7 to balance the MSE loss and adversarial objectives.</p></sec><sec id="sec3dot3-sensors-25-05261"><title>3.3. Implementation</title><p>In this work, we adopt the point cloud denoiser network proposed in P2P-Bridge&#160;[<xref rid="B17-sensors-25-05261" ref-type="bibr">17</xref>] as our backbone denoiser architecture. The model is designed to predict the drift vector field between clean and noisy point clouds, following the Schr&#246;dinger Bridge formulation. The denoiser network follows the encoder&#8211;decoder structure of PointNet++&#160;[<xref rid="B28-sensors-25-05261" ref-type="bibr">28</xref>], consisting of multi-scale set abstraction modules and feature propagation modules.</p><p>To facilitate adversarial learning, we introduce a lightweight discriminator network, which is designed to distinguish between ground-truth clean point clouds and denoised samples generated by the diffusion bridge model. It is important to note that the discriminator is only involved during the training phase to provide adversarial feedback to the generator. During inference, the discriminator is removed entirely, and thus the inference time and latency of ADBM are identical to those of the baseline model. The architecture of the discriminator first applies a point-wise encoder composed of two linear layers with ReLU activation and layer normalization, transforming each point into a latent feature. The resulting latent features are then aggregated via average pooling across the point dimension, yielding a global feature vector for each sample. This global representation is further processed by a two-layer MLP to produce a scalar output indicating the realism of the input. Overall, the discriminator contains only 0.07 million parameters, indicating that it is lightweight and adds minimal overhead to the model.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1:</bold> Training of Adversarial Diffusion Bridge Model</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="sensors-25-05261-i001.jpg"/></td></tr></tbody></array>
</p></sec></sec><sec id="sec4-sensors-25-05261"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-05261"><title>4.1. Datasets</title><p>We evaluate our method on two benchmark datasets: PU-Net [<xref rid="B26-sensors-25-05261" ref-type="bibr">26</xref>] and PC-Net [<xref rid="B15-sensors-25-05261" ref-type="bibr">15</xref>]. The PU-Net dataset contains 40 object categories for training and 20 categories for testing. For each object, ground-truth point clouds are provided at three resolutions: 10,000, 30,000, and 50,000 points. To standardize the training input size, we apply farthest-point sampling [<xref rid="B28-sensors-25-05261" ref-type="bibr">28</xref>] to extract 2048 points from each noisy input, regardless of its original resolution. This allows the model to be trained on a fixed-size representation while leveraging geometric information from diverse scales. The PC-Net dataset is used solely for testing to assess the generalization ability of the model. It consists of 10 object categories, each provided at three resolutions, totaling 30 test samples. During evaluation, the model outputs a 2048-point cloud, which is then compared to the ground truth using alignment techniques and point-wise distance metrics. This setup allows us to evaluate the denoising performance of the model on both seen and unseen object distributions across varying resolutions.</p></sec><sec id="sec4dot2-sensors-25-05261"><title>4.2. Evaluation Measure</title><p>To quantitatively assess the quality of the denoised point clouds, we adopt two widely used metrics: the Chamfer distance (CD) and Point-to-Mesh (P2M) distance. The CD evaluates the average bidirectional proximity between predicted and ground-truth point sets. It penalizes both missing and redundant points, promoting accurate reconstruction and uniform coverage. Formally, it is defined as<disp-formula id="FD11-sensors-25-05261"><label>(11)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>CD</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant="script">P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>NN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="script">P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>NN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:math></inline-formula> denote the predicted and reference point clouds, and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>NN</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> returns the nearest neighbor. To evaluate the geometric consistency with the underlying surface, we also compute the P2M distance. This metric compares points to a mesh surface, taking into account both the distance from points to the mesh and vice versa. It is defined as<disp-formula id="FD12-sensors-25-05261"><label>(12)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant="script">M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:munder><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>|</mml:mo><mml:mi mathvariant="script">M</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mover accent="true"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:munder><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></inline-formula> denotes the ground-truth mesh, and <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> measures the shortest distance between a point and a mesh face. The first term captures how well the predicted points lie on the mesh surface, while the second encourages surface coverage. All point clouds and meshes are normalized to the unit sphere before evaluation to ensure scale invariance.</p></sec><sec id="sec4dot3-sensors-25-05261"><title>4.3. Training Details</title><p>Training is conducted on a single NVIDIA H100 GPU 80 GB with an Intel(R) Xeon(R) Platinum 8480+ CPU, running Ubuntu 22.04.2 LTS. The model is trained for a total of 650,000 iterations with a batch size of 32. Automatic mixed precision is enabled for memory and computing efficiency, and gradient clipping with a maximum norm of 1.0 is applied to stabilize training. Both the denoiser network and the discriminator of ADBM are trained using the AdamW optimizer. The denoiser network training uses a constant learning rate of 0.0003, while the discriminator is trained with a learning rate of 0.0001. The exponential moving average of the denoiser network parameters is maintained with a decay factor of 0.999. We use 10 reverse diffusion steps during both adversarial training and evaluation to generate denoised point clouds.</p></sec><sec id="sec4dot4-sensors-25-05261"><title>4.4. Experimental Results</title><p>We evaluate our method, ADBM, on the PU-Net and PC-Net datasets under varying Gaussian noise levels and point cloud resolutions. <xref rid="sensors-25-05261-t001" ref-type="table">Table 1</xref> presents the quantitative comparison of the denoising performance based on Chamfer distance and Point-to-Mesh distance, where lower values indicate better denoising performance. On the PU-Net dataset with 10 k input points, ADBM consistently outperforms all baselines across all noise levels. At 1% noise, ADBM records a CD of 2.18 and a P2M of 0.34, outperforming P2P-Bridge which achieves 2.45 for CD and 0.39 for P2M. When the noise level increases to 2%, ADBM achieves 3.15 for CD and 0.77 for P2M, showing improvements over P2P-Bridge&#8217;s 3.27 and 0.86, respectively. At the highest noise level of 3%, ADBM achieves 3.98 for CD and 1.40 for P2M, compared to 4.07 and 1.47 by P2P-Bridge. For the high-resolution setting with 50 k points, ADBM continues to outperform the baselines. At 1% noise, ADBM achieves a CD of 0.57 and P2M of 0.08, showing improvements over P2P-Bridge&#8217;s values of 0.60 and 0.09. At 2% noise, the CD and P2M values achieved by ADBM are 0.90 and 0.32, respectively, whereas P2P-Bridge achieves 0.95 and 0.35. At 3% noise, ADBM yields 1.61 for CD and 0.88 for P2M, outperforming P2P-Bridge&#8217;s values of 1.63 and 0.90. ADBM shows average relative improvements of 5.63% for CD and 9.35% for P2M in the 10 k point setting, and 3.83% and 7.30% in the 50 k point setting compared to P2P-Bridge.</p><p>On the PC-Net dataset, which is used to evaluate generalization to unseen shapes, our method, ADBM, also shows robust performance. At 10 k input points and 1% noise, ADBM records a CD of 2.82 and a P2M of 0.59, slightly improving upon P2P-Bridge&#8217;s results of 2.87 and 0.63. For 2% noise, ADBM achieves 4.43 for CD and 0.86 for P2M, again outperforming P2P-Bridge, which reports 4.52 and 0.92. At 3% noise, ADBM shows a clear advantage with a CD of 5.57 and a P2M of 1.27, while P2P-Bridge reports 5.65 and 1.34. For the 50 k-point resolution, the same trend holds. At 1% noise, ADBM achieves a CD of 0.90 and a P2M of 0.11, whereas P2P-Bridge reports 0.92 and 0.12. With 2% noise, ADBM records 1.37 for CD and 0.25 for P2M, improving upon P2P-Bridge&#8217;s 1.39 and 0.26. At 3% noise, ADBM achieves 2.14 for CD and 0.49 for P2M, while P2P-Bridge results in 2.17 and 0.51. The improvements are smaller but consistent, with 1.72% for CD and 6.03% for P2M for 10 k points, and 1.66% and 5.37% for 50 k points.</p><p>These comprehensive results demonstrate that our proposed method not only consistently outperforms the existing baselines across all noise levels and resolutions, but also generalizes effectively to unseen object categories, yielding the best performance in terms of both point-wise accuracy and surface-level fidelity. Previous denoising methods, such as ScoreDenoise [<xref rid="B16-sensors-25-05261" ref-type="bibr">16</xref>], primarily rely on loss functions focused on noise prediction, which emphasize overall noise suppression rather than fine-grained geometric reconstruction. In contrast, the proposed method incorporates a discriminator-based adversarial loss, which explicitly enforces structural fidelity by distinguishing between clean and denoised point clouds. From a training robustness perspective, this adversarial term acts as a regularizer, guiding the model to preserve sharp edges and recover challenging geometric patterns. As a result, the proposed method demonstrates improved performance in scenarios with diverse noise levels and intricate structural shapes, where conventional score-based approaches may struggle.</p><p>To further investigate the generalization behavior of the proposed method, we conducted a class-wise performance analysis on both the PU-Net and PC-Net datasets. <xref rid="sensors-25-05261-f003" ref-type="fig">Figure 3</xref> presents the CD and P2M metrics for each class under the 10 k point and 1% Gaussian noise setting. The results reveal that reconstruction difficulty varies substantially across object categories, with geometrically complex structures (e.g., chair, elk) exhibiting higher error values. In contrast, objects with smoother or more compact surfaces tend to yield lower reconstruction errors, reflecting the relative ease of recovering their geometric details. Notably, the model maintains competitive performance across unseen PC-Net shapes, indicating robust generalization to novel object geometries. These observations indicate that the model effectively captures transferable structural priors rather than overfitting to the training distribution.</p><p>To qualitatively evaluate the denoising performance, <xref rid="sensors-25-05261-f004" ref-type="fig">Figure 4</xref> presents visual comparisons across various object categories. The first row shows the ground-truth clean point clouds, uniformly sampled with 10 k points per object. To generate the noisy inputs shown in the second row, Gaussian noise with a standard deviation of 1% unit sphere radius is added to the clean shapes. These noisy point clouds exhibit substantial structural distortion and irregular point distribution, particularly around thin or intricate regions such as the camel&#8217;s legs, the chair&#8217;s backrest, and the curvature of the duck shape. The third row shows the outputs produced by the P2P-Bridge baseline without adversarial learning. While the overall shapes are recovered to some extent, the results often suffer from blurring or loss of fine details. For instance, the legs of animal models appear less distinct, and the duck&#8217;s bill lacks geometric sharpness and continuity. In comparison, the proposed method, in the fourth row, restores both global structure and fine-grained geometric details. The denoised results exhibit more faithful alignment with the ground truth, better preserving object-specific characteristics and surface continuity. Moreover, the point distribution appears more uniform and natural, indicating improved surface coverage and sampling quality. These qualitative observations are consistent with the quantitative results, highlighting the superior denoising capability and structural fidelity of our method across diverse shapes.</p><p><xref rid="sensors-25-05261-f005" ref-type="fig">Figure 5</xref> shows per-point error heatmaps between the denoised outputs and the ground-truth shapes, where the color represents the Euclidean distance to the corresponding ground-truth point. All samples consist of 10 k points, and the input noise follows a Gaussian distribution with a standard deviation of 1% of the unit sphere. Overall, our method achieves low reconstruction errors across most surface regions, especially in smooth and planar areas such as the camel&#8217;s torso and the cow&#8217;s flank. These regions are predominantly rendered in blue, indicating accurate point-wise recovery. However, increased reconstruction errors are observed in geometrically complex areas, including thin structures and high-curvature boundaries, such as for the camel&#8217;s legs, the edges of the chair&#8217;s backrest, and the tail of the horse. These failure cases typically arise due to the local sparsity or overlapping noise in the input, which can distort fine geometric cues during denoising. To mitigate these localized failures, future work may focus on stabilizing the adversarial training process and improving the loss function to better capture fine-grained geometric discrepancies. In particular, incorporating region-aware weighting schemes or multi-scale structural constraints into the training objective could enhance the model&#8217;s sensitivity to delicate features. These improvements may lead to more faithful reconstructions in challenging regions.</p></sec><sec id="sec4dot5-sensors-25-05261"><title>4.5. Ablation Results</title><p>We conducted an ablation study to investigate the effect of the adversarial loss weight <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on the denoising performance, as summarized in <xref rid="sensors-25-05261-t002" ref-type="table">Table 2</xref>. Across different Gaussian noise levels and point counts, the proposed method consistently improved results compared to the base model without adversarial learning. Among the tested values, <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> yielded the best overall performance, achieving the lowest CD and P2M errors for most settings. While <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> sometimes produced competitive results, especially with 50 k points and the 3% Gaussian noise setting, its performance degraded under other scenarios. <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> showed no clear advantage and in some cases slightly worsened the results, suggesting training instability of the adversarial component. Based on the results, we selected <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> as the optimal trade-off between shape fidelity and adversarial guidance, leading to robust denoising performance across diverse noise levels and point densities.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05261"><title>5. Conclusions</title><p>In this paper, we proposed an adversarial diffusion bridge training method for 3D point cloud denoising. Building on the Schr&#246;dinger Bridge formulation, our method models the interpolation between noisy and clean point clouds, enabling effective restoration of fine-grained geometry. To further improve the perceptual quality and fidelity of denoised outputs, we introduced an adversarial learning scheme, where a lightweight discriminator is trained to guide the generator toward producing samples indistinguishable from real clean point clouds. The proposed method achieves superior reconstruction fidelity, showing strong generalization performance across diverse object categories. However, as shown in <xref rid="sensors-25-05261-f005" ref-type="fig">Figure 5</xref>, denoising performance in highly corrupted or geometrically complex regions remains challenging. These cases highlight the need for further refinement of the adversarial component. In future work, we aim to explore improved training stability through adversarial loss regularization and conduct systematic studies on how varying the weighting parameter (e.g., <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) influences the denoising quality and convergence behavior.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, C.N.; methodology, C.N.; software, C.N.; validation, C.N.; writing&#8212;original draft preparation, C.N. and S.J.L.; writing&#8212;review and editing, C.N. and S.J.L.; visualization, C.N. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Not applicable.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05261"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Behera</surname><given-names>S.</given-names></name><name name-style="western"><surname>Anand</surname><given-names>B.</given-names></name><name name-style="western"><surname>Rajalakshmi</surname><given-names>P.</given-names></name></person-group><article-title>YoloV8 Based Novel Approach for Object Detection on LiDAR Point Cloud</article-title><source>Proceedings of the 2024 IEEE 99th Vehicular Technology Conference (VTC2024-Spring)</source><conf-loc>Singapore</conf-loc><conf-date>24&#8211;27 June 2024</conf-date><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/VTC2024-Spring62846.2024.10683316</pub-id></element-citation></ref><ref id="B2-sensors-25-05261"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tomizuka</surname><given-names>M.</given-names></name><name name-style="western"><surname>Keutzer</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name></person-group><article-title>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation</article-title><source>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>18&#8211;22 June 2023</conf-date><fpage>1190</fpage><lpage>1199</lpage><pub-id pub-id-type="doi">10.1109/CVPR52729.2023.00121</pub-id></element-citation></ref><ref id="B3-sensors-25-05261"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Litany</surname><given-names>O.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.</given-names></name></person-group><article-title>Deep Hough Voting for 3D Object Detection in Point Clouds</article-title><source>Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>9276</fpage><lpage>9285</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2019.00937</pub-id></element-citation></ref><ref id="B4-sensors-25-05261"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nagavarapu</surname><given-names>S.C.</given-names></name><name name-style="western"><surname>Abraham</surname><given-names>A.</given-names></name><name name-style="western"><surname>Selvaraj</surname><given-names>N.M.</given-names></name><name name-style="western"><surname>Dauwels</surname><given-names>J.</given-names></name></person-group><article-title>A Dynamic Object Removal and Reconstruction Algorithm for Point Clouds</article-title><source>Proceedings of the 2023 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)</source><conf-loc>Singapore</conf-loc><conf-date>11&#8211;13 December 2023</conf-date><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/SOLI60636.2023.10425733</pub-id></element-citation></ref><ref id="B5-sensors-25-05261"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>P.</given-names></name><name name-style="western"><surname>Mian</surname><given-names>A.</given-names></name></person-group><article-title>3D point cloud-based place recognition: A survey</article-title><source>Artif. Intell. Rev.</source><year>2024</year><volume>57</volume><fpage>83</fpage><pub-id pub-id-type="doi">10.1007/s10462-024-10713-6</pub-id></element-citation></ref><ref id="B6-sensors-25-05261"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Z.</given-names></name></person-group><article-title>Recent Advances and Perspectives in Deep Learning Techniques for 3D Point Cloud Data Processing</article-title><source>Robotics</source><year>2023</year><volume>12</volume><elocation-id>100</elocation-id><pub-id pub-id-type="doi">10.3390/robotics12040100</pub-id></element-citation></ref><ref id="B7-sensors-25-05261"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shamim</surname><given-names>S.</given-names></name><name name-style="western"><surname>un Nabi Jafri</surname><given-names>S.R.</given-names></name></person-group><article-title>Enhanced vehicle localization with low-cost sensor fusion for urban 3D mapping</article-title><source>PLoS ONE</source><year>2025</year><volume>20</volume><elocation-id>e0318710</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0318710</pub-id><pub-id pub-id-type="pmid">40315232</pub-id><pub-id pub-id-type="pmcid">PMC12047803</pub-id></element-citation></ref><ref id="B8-sensors-25-05261"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sang</surname><given-names>H.</given-names></name></person-group><article-title>Application of UAV-based 3D modeling and visualization technology in urban planning</article-title><source>Adv. Eng. Technol. Res.</source><year>2024</year><volume>12</volume><fpage>912</fpage><pub-id pub-id-type="doi">10.56028/aetr.12.1.912.2024</pub-id></element-citation></ref><ref id="B9-sensors-25-05261"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>P.X.</given-names></name></person-group><article-title>A morphing-Based 3D point cloud reconstruction framework for medical image processing</article-title><source>Comput. Methods Programs Biomed.</source><year>2020</year><volume>193</volume><elocation-id>105495</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105495</pub-id><pub-id pub-id-type="pmid">32311509</pub-id></element-citation></ref><ref id="B10-sensors-25-05261"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Beetz</surname><given-names>M.</given-names></name><name name-style="western"><surname>Banerjee</surname><given-names>A.</given-names></name><name name-style="western"><surname>Grau</surname><given-names>V.</given-names></name></person-group><source>Point2Mesh-Net: Combining Point Cloud and Mesh-Based Deep Learning for Cardiac Shape Reconstruction</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>280</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-23443-9_26</pub-id></element-citation></ref><ref id="B11-sensors-25-05261"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>X.F.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>W.</given-names></name></person-group><article-title>Guided 3D point cloud filtering</article-title><source>Multimed. Tools Appl.</source><year>2018</year><volume>77</volume><fpage>17397</fpage><lpage>17411</lpage><pub-id pub-id-type="doi">10.1007/s11042-017-5310-9</pub-id></element-citation></ref><ref id="B12-sensors-25-05261"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name></person-group><article-title>Guided point cloud denoising via sharp feature skeletons</article-title><source>Vis. Comput.</source><year>2017</year><volume>33</volume><fpage>857</fpage><lpage>867</lpage><pub-id pub-id-type="doi">10.1007/s00371-017-1391-8</pub-id></element-citation></ref><ref id="B13-sensors-25-05261"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yadav</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Reitebuch</surname><given-names>U.</given-names></name><name name-style="western"><surname>Skrodzki</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zimmermann</surname><given-names>E.</given-names></name><name name-style="western"><surname>Polthier</surname><given-names>K.</given-names></name></person-group><article-title>Constraint-based point set denoising using normal voting tensor and restricted quadratic error metrics</article-title><source>Comput. Graph.</source><year>2018</year><volume>74</volume><fpage>234</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.cag.2018.05.014</pub-id></element-citation></ref><ref id="B14-sensors-25-05261"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Z.</given-names></name></person-group><article-title>A feature-preserving framework for point cloud denoising</article-title><source>Comput.-Aided Des.</source><year>2020</year><volume>127</volume><fpage>102857</fpage><pub-id pub-id-type="doi">10.1016/j.cad.2020.102857</pub-id></element-citation></ref><ref id="B15-sensors-25-05261"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rakotosaona</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barbera</surname><given-names>V.L.</given-names></name><name name-style="western"><surname>Guerrero</surname><given-names>P.</given-names></name><name name-style="western"><surname>Mitra</surname><given-names>N.J.</given-names></name><name name-style="western"><surname>Ovsjanikov</surname><given-names>M.</given-names></name></person-group><article-title>PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point Clouds</article-title><source>Comput. Graph. Forum</source><year>2020</year><volume>39</volume><fpage>185</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1111/cgf.13753</pub-id></element-citation></ref><ref id="B16-sensors-25-05261"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>W.</given-names></name></person-group><article-title>Score-Based Point Cloud Denoising</article-title><source>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>4563</fpage><lpage>4572</lpage><pub-id pub-id-type="doi">10.1109/ICCV48922.2021.00454</pub-id></element-citation></ref><ref id="B17-sensors-25-05261"><label>17.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vogel</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tateno</surname><given-names>K.</given-names></name><name name-style="western"><surname>Pollefeys</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tombari</surname><given-names>F.</given-names></name><name name-style="western"><surname>Rakotosaona</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Engelmann</surname><given-names>F.</given-names></name></person-group><source>P2P-Bridge: Diffusion Bridges for 3D Point Cloud Denoising</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2025</year><fpage>184</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-72627-9_11</pub-id></element-citation></ref><ref id="B18-sensors-25-05261"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Jiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name></person-group><article-title>Deep Generative Learning via Schr&#246;dinger Bridge</article-title><source>Proceedings of the 38th International Conference on Machine Learning</source><conf-loc>Virtual</conf-loc><conf-date>18&#8211;24 July 2021</conf-date><volume>Volume 139</volume><fpage>10794</fpage><lpage>10804</lpage></element-citation></ref><ref id="B19-sensors-25-05261"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>De Bortoli</surname><given-names>V.</given-names></name><name name-style="western"><surname>Thornton</surname><given-names>J.</given-names></name><name name-style="western"><surname>Heng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Doucet</surname><given-names>A.</given-names></name></person-group><article-title>Diffusion Schr&#246;dinger Bridge with Applications to Score-Based Generative Modeling</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>17695</fpage><lpage>17709</lpage></element-citation></ref><ref id="B20-sensors-25-05261"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>De Bortoli</surname><given-names>V.</given-names></name><name name-style="western"><surname>Deligiannidis</surname><given-names>G.</given-names></name><name name-style="western"><surname>Doucet</surname><given-names>A.</given-names></name></person-group><article-title>Conditional simulation using diffusion Schr&#246;dinger bridges</article-title><source>Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence</source><conf-loc>Eindhoven, The Netherlands</conf-loc><conf-date>1&#8211;5 August 2022</conf-date><volume>Volume 180</volume><fpage>1792</fpage><lpage>1802</lpage></element-citation></ref><ref id="B21-sensors-25-05261"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tong</surname><given-names>A.</given-names></name><name name-style="western"><surname>Malkin</surname><given-names>N.</given-names></name><name name-style="western"><surname>Fatras</surname><given-names>K.</given-names></name><name name-style="western"><surname>Atanackovic</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huguet</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wolf</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Simulation-free Schr&#246;dinger bridges via score and flow matching</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2307.03672</pub-id><pub-id pub-id-type="arxiv">2307.03672</pub-id></element-citation></ref><ref id="B22-sensors-25-05261"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ko</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>E.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>Y.H.</given-names></name></person-group><article-title>Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS</article-title><source>IEEE Open J. Signal Process.</source><year>2024</year><volume>5</volume><fpage>577</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1109/OJSP.2024.3386495</pub-id></element-citation></ref><ref id="B23-sensors-25-05261"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>W.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>C.</given-names></name></person-group><article-title>Semantic-guided Adversarial Diffusion Model for Self-supervised Shadow Removal</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2407.01104</pub-id></element-citation></ref><ref id="B24-sensors-25-05261"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name></person-group><article-title>De novo protein backbone generation based on diffusion with structured priors and adversarial training</article-title><source>bioRxiv</source><year>2022</year><pub-id pub-id-type="doi">10.1101/2022.12.17.520847</pub-id></element-citation></ref><ref id="B25-sensors-25-05261"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>B.</given-names></name></person-group><article-title>Structure-Guided Adversarial Training of Diffusion Models</article-title><source>Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>17&#8211;21 June 2024</conf-date><fpage>7256</fpage><lpage>7266</lpage><pub-id pub-id-type="doi">10.1109/CVPR52733.2024.00693</pub-id></element-citation></ref><ref id="B26-sensors-25-05261"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Cohen-Or</surname><given-names>D.</given-names></name><name name-style="western"><surname>Heng</surname><given-names>P.A.</given-names></name></person-group><article-title>PU-Net: Point Cloud Upsampling Network</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>2790</fpage><lpage>2799</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00295</pub-id></element-citation></ref><ref id="B27-sensors-25-05261"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goodfellow</surname><given-names>I.J.</given-names></name><name name-style="western"><surname>Pouget-Abadie</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mirza</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Warde-Farley</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ozair</surname><given-names>S.</given-names></name><name name-style="western"><surname>Courville</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Generative Adversarial Nets</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2014</year><volume>27</volume><fpage>2672</fpage><lpage>2680</lpage></element-citation></ref><ref id="B28-sensors-25-05261"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>30</volume><fpage>5099</fpage><lpage>5108</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05261-f001" orientation="portrait"><label>Figure 1</label><caption><p>Visual examples of point cloud denoising results using the proposed method, ADBM.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05261-g001.jpg"/></fig><fig position="float" id="sensors-25-05261-f002" orientation="portrait"><label>Figure 2</label><caption><p>Overview of the proposed adversarial diffusion bridge model (ADBM) training pipeline.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05261-g002.jpg"/></fig><fig position="float" id="sensors-25-05261-f003" orientation="portrait"><label>Figure 3</label><caption><p>Class-wise denoising performance (CD&#8595; / P2M&#8595;) on PU-Net and PC-Net datasets with 10 k points and 1% Gaussian noise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05261-g003.jpg"/></fig><fig position="float" id="sensors-25-05261-f004" orientation="portrait"><label>Figure 4</label><caption><p>Qualitative comparison of point cloud denoising results with close-up views of key object regions (e.g., legs, backs, and bills).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05261-g004.jpg"/></fig><fig position="float" id="sensors-25-05261-f005" orientation="portrait"><label>Figure 5</label><caption><p>Visualization of per-point Euclidean errors between the denoised outputs and ground-truth point clouds.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05261-g005.jpg"/></fig><table-wrap position="float" id="sensors-25-05261-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05261-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of denoising performance (CD&#8595; / P2M&#8595;) under different Gaussian noise levels and point counts. The best results are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Dataset</th><th rowspan="3" align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Number of Points Gaussian Noise Level Method/Metric</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1"><inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>10</mml:mn><mml:mo>&#183;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> Points</th><th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1"><inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>50</mml:mn><mml:mo>&#183;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> Points</th></tr><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>1%</bold>
</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>2%</bold>
</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>3%</bold>
</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>1%</bold>
</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>2%</bold>
</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>3%</bold>
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</th></tr></thead><tbody><tr><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">PU-Net&#160;[<xref rid="B26-sensors-25-05261" ref-type="bibr">26</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">PC-Net&#160;[<xref rid="B15-sensors-25-05261" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.52</td><td align="center" valign="middle" rowspan="1" colspan="1">1.15</td><td align="center" valign="middle" rowspan="1" colspan="1">7.47</td><td align="center" valign="middle" rowspan="1" colspan="1">3.97</td><td align="center" valign="middle" rowspan="1" colspan="1">13.1</td><td align="center" valign="middle" rowspan="1" colspan="1">8.74</td><td align="center" valign="middle" rowspan="1" colspan="1">1.05</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">1.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" rowspan="1" colspan="1">2.29</td><td align="center" valign="middle" rowspan="1" colspan="1">1.29</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ScoreDenoise&#160;[<xref rid="B16-sensors-25-05261" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.52</td><td align="center" valign="middle" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" rowspan="1" colspan="1">3.69</td><td align="center" valign="middle" rowspan="1" colspan="1">1.07</td><td align="center" valign="middle" rowspan="1" colspan="1">4.71</td><td align="center" valign="middle" rowspan="1" colspan="1">1.94</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72</td><td align="center" valign="middle" rowspan="1" colspan="1">0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">1.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.57</td><td align="center" valign="middle" rowspan="1" colspan="1">1.93</td><td align="center" valign="middle" rowspan="1" colspan="1">1.04</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">P2P-Bridge&#160;[<xref rid="B17-sensors-25-05261" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.45</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">3.27</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">4.07</td><td align="center" valign="middle" rowspan="1" colspan="1">1.47</td><td align="center" valign="middle" rowspan="1" colspan="1">0.60</td><td align="center" valign="middle" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">1.63</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ADBM (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.18</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.34</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>3.15</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.77</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>3.98</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.40</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.57</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.08</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.90</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.32</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.61</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.88</bold>
</td></tr><tr><td rowspan="4" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">PC-Net&#160;[<xref rid="B15-sensors-25-05261" ref-type="bibr">15</xref>]</td><td align="left" valign="middle" rowspan="1" colspan="1">PC-Net&#160;[<xref rid="B15-sensors-25-05261" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.85</td><td align="center" valign="middle" rowspan="1" colspan="1">1.22</td><td align="center" valign="middle" rowspan="1" colspan="1">6.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1.45</td><td align="center" valign="middle" rowspan="1" colspan="1">5.87</td><td align="center" valign="middle" rowspan="1" colspan="1">1.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.29</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">0.51</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" rowspan="1" colspan="1">3.25</td><td align="center" valign="middle" rowspan="1" colspan="1">1.08</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ScoreDenoise&#160;[<xref rid="B16-sensors-25-05261" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">3.37</td><td align="center" valign="middle" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" rowspan="1" colspan="1">4.52</td><td align="center" valign="middle" rowspan="1" colspan="1">1.16</td><td align="center" valign="middle" rowspan="1" colspan="1">6.78</td><td align="center" valign="middle" rowspan="1" colspan="1">1.94</td><td align="center" valign="middle" rowspan="1" colspan="1">1.07</td><td align="center" valign="middle" rowspan="1" colspan="1">0.17</td><td align="center" valign="middle" rowspan="1" colspan="1">1.66</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">2.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">P2P-Bridge&#160;[<xref rid="B17-sensors-25-05261" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.87</td><td align="center" valign="middle" rowspan="1" colspan="1">0.63</td><td align="center" valign="middle" rowspan="1" colspan="1">4.52</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92</td><td align="center" valign="middle" rowspan="1" colspan="1">5.65</td><td align="center" valign="middle" rowspan="1" colspan="1">1.34</td><td align="center" valign="middle" rowspan="1" colspan="1">0.92</td><td align="center" valign="middle" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">1.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.26</td><td align="center" valign="middle" rowspan="1" colspan="1">2.17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.51</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ADBM (ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.82</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.59</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.43</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.86</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>5.57</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.27</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.90</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.11</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.37</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.25</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.14</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.49</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05261-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05261-t002_Table 2</object-id><label>Table 2</label><caption><p>Ablation study on the adversarial loss weight <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>adv</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for the PU-Net dataset under varying Gaussian noise levels and numbers of points. The best results are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of Points</th><th colspan="6" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1"><inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>10</mml:mn><mml:mo>&#183;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> Points</th><th colspan="6" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1"><inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>50</mml:mn><mml:mo>&#183;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> Points</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Gaussian Noise Level</bold>
</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>1%</bold>
</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>2%</bold>
</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>3%</bold>
</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>1%</bold>
</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>2%</bold>
</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
<bold>3%</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm90" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">&#955;</mml:mi><mml:mi mathvariant="bold">adv</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
<bold>/Metric</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CD</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>P2M</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Base (w/o ADBM)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.90</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.54</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.82</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.18</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.34</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>3.15</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.77</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>3.98</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.40</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.57</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.08</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.90</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.32</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.88</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>