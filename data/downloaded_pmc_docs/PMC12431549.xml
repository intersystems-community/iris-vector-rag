<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431549</article-id><article-id pub-id-type="pmcid-ver">PMC12431549.1</article-id><article-id pub-id-type="pmcaid">12431549</article-id><article-id pub-id-type="pmcaiid">12431549</article-id><article-id pub-id-type="doi">10.3390/s25175244</article-id><article-id pub-id-type="publisher-id">sensors-25-05244</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Efficient Deep Learning-Based Arrhythmia Detection Using Smartwatch ECG Electrocardiograms</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9385-7940</contrib-id><name name-style="western"><surname>Baca</surname><given-names initials="HAH">Herwin Alayn Huillcen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="c1-sensors-25-05244" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4638-5762</contrib-id><name name-style="western"><surname>Valdivia</surname><given-names initials="FDLP">Flor de Luz Palomino</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Angrisani</surname><given-names initials="L">Leopoldo</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Maria</surname><given-names initials="R">Romano</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Tedesco</surname><given-names initials="A">Annarita</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05244">Faculty of Engineering, Academic Department of Engineering and Information Technology, Jose Maria Arguedas National University, Andahuaylas 03701, Peru; <email>fpalomino@unajma.edu.pe</email></aff><author-notes><corresp id="c1-sensors-25-05244"><label>*</label>Correspondence: <email>hhuillcen@unajma.edu.pe</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5244</elocation-id><history><date date-type="received"><day>15</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>14</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>22</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>23</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05244.pdf"/><abstract><p>According to the World Health Organization, cardiovascular diseases, including cardiac arrhythmias, are the leading cause of death worldwide due to their silent, asymptomatic nature. To address this problem, early and accurate diagnosis is crucial. Although this task is typically performed by a cardiologist, diagnosing arrhythmias can be imprecise due to the subjectivity of reading and interpreting electrocardiograms (ECGs), and electrocardiograms are often subject to noise and interference. Deep learning-based approaches present methods for automatically detecting arrhythmias and are positioned as an alternative to support cardiologists&#8217; diagnoses. However, these methods are trained and tested only on open datasets of electrocardiograms from Holter devices, whose results aim to improve the accuracy of the state of the art, neglecting the efficiency of the model and its application in a practical clinical context. In this work, we propose an efficient model based on a 1D CNN architecture to detect arrhythmias from smartwatch ECGs, for subsequent deployment in a practical scenario for the monitoring and early detection of arrhythmias. Two datasets were used: UMass Medical School Simband for a binary arrhythmia detection model to evaluate its efficiency and effectiveness, and the MIT-BIH arrhythmia database to validate the multiclass model and compare it with state-of-the-art models. The results of the binary model achieved an accuracy of 64.81%, a sensitivity of 89.47%, and a specificity of 6.25%, demonstrating the model&#8217;s reliability, especially in specificity. Furthermore, the computational complexity was 1.2 million parameters and 68.48 MFlops, demonstrating the efficiency of the model. Finally, the results of the multiclass model achieved an accuracy of 99.57%, a sensitivity of 99.57%, and a specificity of 99.47%, making it one of the best state-of-the-art proposals and also reconfirming the reliability of the model.</p></abstract><kwd-group><kwd>arrhythmia detection</kwd><kwd>deep learning</kwd><kwd>smartwatch</kwd><kwd>electrocardiogram</kwd><kwd>ECG</kwd><kwd>heart rate</kwd><kwd>CNN</kwd><kwd>cardiovascular diseases</kwd><kwd>PPG</kwd></kwd-group><funding-group><award-group><funding-source>Jose Maria Arguedas National University</funding-source><award-id>230-2024-UNAJMA-CU</award-id></award-group><funding-statement>This research was funded by Jose Maria Arguedas National University, Peru, as part of the 2024 research project competition, approved with Resolution No. 230-2024-UNAJMA-CU.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05244"><title>1. Introduction</title><p>Cardiovascular diseases (CVDs) are the leading cause of death worldwide, accounting for 17.9 million deaths per year [<xref rid="B1-sensors-25-05244" ref-type="bibr">1</xref>]. Among these diseases, arrhythmias are associated with cardiovascular events, heart failure, and sudden death [<xref rid="B2-sensors-25-05244" ref-type="bibr">2</xref>]. Therefore, accurate diagnosis and early detection are crucial for receiving appropriate treatment, preventing subsequent complications, and even saving lives [<xref rid="B3-sensors-25-05244" ref-type="bibr">3</xref>].</p><p>The standard diagnosis of an arrhythmia is performed by a cardiologist through the interpretation of an electrocardiogram (ECG). However, this diagnosis can be cumbersome and complex, leading to inaccuracies, as it is subject to the subjectivity of the cardiologist&#8217;s visual observation [<xref rid="B4-sensors-25-05244" ref-type="bibr">4</xref>]. Additionally, the electrocardiograph is often affected by noise and interference, resulting in inaccurate ECGs and subsequent diagnostic errors [<xref rid="B5-sensors-25-05244" ref-type="bibr">5</xref>]. Therefore, it is essential to provide reliable tools to support cardiologists in making accurate and timely diagnoses.</p><p>In recent years, deep learning techniques have achieved excellent results in automating arrhythmia diagnosis based on the information contained in ECG signals. Deep learning methods are continually refined, improving the accuracy and efficiency of arrhythmia detection [<xref rid="B6-sensors-25-05244" ref-type="bibr">6</xref>].</p><p>Convolutional neural networks (CNNs) have been employed with great success in extracting local features from ECG signals and improving arrhythmia detection accuracy, offering the advantage of low-complexity techniques [<xref rid="B5-sensors-25-05244" ref-type="bibr">5</xref>]. The state of the art also includes techniques that combine CNNs with others, such as LSTM and hybrid models [<xref rid="B7-sensors-25-05244" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05244" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05244" ref-type="bibr">9</xref>], which improve accuracy but increase model complexity. Recently, Transformer models, with their attention mechanisms, have been proposed to enhance accuracy [<xref rid="B5-sensors-25-05244" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-05244" ref-type="bibr">6</xref>]. Despite their high computational complexity, they have achieved cutting-edge results. However, most deep learning-based proposals train and test their models on open ECG signal datasets from Holter devices, which limits the practical context of their results, especially for early arrhythmia detection.</p><p>Fortunately, technology has enabled the widespread use of smart wearable devices [<xref rid="B10-sensors-25-05244" ref-type="bibr">10</xref>], such as smartwatches, which feature photoplethysmography (PPG) sensors that can record patients&#8217; electrocardiograms (ECGs), making smartwatches a crucial tool for arrhythmia monitoring [<xref rid="B11-sensors-25-05244" ref-type="bibr">11</xref>]. Numerous studies have proposed arrhythmia detection models based on ECGs from smartwatches [<xref rid="B12-sensors-25-05244" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05244" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05244" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05244" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05244" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05244" ref-type="bibr">17</xref>], achieving good accuracy results. However, reliable arrhythmia detection results have not yet been achieved, nor are they oriented toward a practical clinical context for deploying the model on a smartwatch, as the focus is on accuracy, without considering that the model must be computationally lightweight.</p><p>In this context, we propose an efficient model for detecting arrhythmias based on ECG signals from smartwatches. The model is based on a one-dimensional (1D) convolutional neural network (CNN) architecture and utilizes two datasets: the UMass Medical School Simband dataset [<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>] for a binary arrhythmia detection model in smartwatches, to validate its efficiency and effectiveness, and the MIT-BIH arrhythmia database [<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>] for a multiclass arrhythmia detection model, to validate its results and compare them with the state of the art.</p><p>The model is designed for subsequent deployment on a smartwatch to monitor and detect arrhythmias as an early warning system, for which low computational complexity and FLOPS are crucial. It also aims to be a reliable model for arrhythmia detection, requiring cutting-edge results in accuracy, sensitivity, and specificity.</p></sec><sec id="sec2-sensors-25-05244"><title>2. Related&#160;Work</title><p>Artificial intelligence-based ECG arrhythmia detection is a field of great interest to the scientific community, with numerous proposals and, above all, review articles published each year [<xref rid="B21-sensors-25-05244" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05244" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05244" ref-type="bibr">23</xref>]. Therefore, related works are identified from these studies, as well as the most innovative proposals with cutting-edge results.</p><sec id="sec2dot1-sensors-25-05244"><title>2.1. ECG Arrhythmia Detection in Classic&#160;Datasets</title><p>To analyze the results and methods of the most recent work, we used the MIT-BIH Arrhythmia dataset [<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>], a benchmark dataset that has become the standard for this type of work. The best and most recent proposals validate their methods on this dataset. In our work, we also use this dataset as a means of validating our proposal and comparing results with the state of the art.</p><p>Deep learning-based approaches are positioned as the most prominent and promising for detecting ECG arrhythmias. These models consist of single- or multi-layer structures that take ECG recordings as input. Each layer acts as a feature extractor, extracting the best classification or recognition patterns [<xref rid="B24-sensors-25-05244" ref-type="bibr">24</xref>]. These approaches can be grouped according to the methods used, including works based on CNNs; combinations of CNNs, RNN, LSTM, and hybrids; and works based on Transformers [<xref rid="B21-sensors-25-05244" ref-type="bibr">21</xref>].</p><p>Xia et al. [<xref rid="B25-sensors-25-05244" ref-type="bibr">25</xref>] use a 1D CNN to detect atrial fibrillation. It utilizes the stationary wavelet transform (SWT) to convert the input into a 2D structure, which serves as input to the CNN. Ullah et al. [<xref rid="B26-sensors-25-05244" ref-type="bibr">26</xref>] propose a 2D convolutional neural network (CNN) to classify ECG signals into eight beat classes. The 1D ECG signals are transformed into 2D spectrograms using the short-time Fourier transform (STFT). There is also the work of Zubair et al. [<xref rid="B27-sensors-25-05244" ref-type="bibr">27</xref>] who present a temporal transition module with convolutional layers of different kernel sizes to capture short- and long-term patterns. They present a cost-sensitive loss function that adjusts class weights and solves the problem of data imbalance.</p><p>Some works modify CNN architecture with similar results. For instance, Rizqyawan et al. [<xref rid="B28-sensors-25-05244" ref-type="bibr">28</xref>] propose a deep convolutional neural network (DNN) to classify arrhythmia using ECG signals without preprocessing. To address class imbalance, it employs a weighted loss function that adjusts the weights accordingly. Ojha et al. [<xref rid="B29-sensors-25-05244" ref-type="bibr">29</xref>] present a 1D CNN model and an auto-encoder ACN to extract optimal features from ECG heartbeat windows; the extracted features are classified using SVM. Then, Jamil and Rahman [<xref rid="B30-sensors-25-05244" ref-type="bibr">30</xref>] use a CNN with continuous wavelet transform (CWT)-based preprocessing to convert the signals into 2D signals for processing in the CNN. This approach also includes an attention block to extract a spatial feature vector. These approaches, utilizing a CNN architecture, have the advantage of low complexity and computational cost, in addition to achieving state-of-the-art results and being consolidated as the approach for our proposal.</p><p>Among the approaches that combine CNN and LSTM, there is the work of Chen et al. [<xref rid="B7-sensors-25-05244" ref-type="bibr">7</xref>], who propose a model to classify six types of ECG signals in the MIT-BIH arrhythmia database. It employs a multi-input structure to process 10-second ECG segments, along with their corresponding RR intervals. Then, the work of Hassan et al. [<xref rid="B8-sensors-25-05244" ref-type="bibr">8</xref>] proposes a deep learning model that combines a CNN with a bidirectional short-term and long-term memory network (BiLSTM) to classify five types of ECG signals. The model processes ECG signals without specifying additional preprocessing. Midani et al. [<xref rid="B31-sensors-25-05244" ref-type="bibr">31</xref>] propose a deep learning model called &#8220;DeepArr,&#8221; which uses a sequential fusion method that combines deep feed-forward and recurrent neural networks to extract feature representations. In addition, Alamatsaz et al. [<xref rid="B9-sensors-25-05244" ref-type="bibr">9</xref>] propose a deep learning model based on a convolutional neural network (CNN) with an LSTM attention block to classify five types of arrhythmias. ECG signals are transformed into 2D images using a continuous wavelet transform (CWT), allowing for the capture of time&#8211;frequency features. These approaches yield similar results to CNNs, but their computational complexity and costs are higher due to the use of LSTM.</p><p>We cannot fail to mention Transformer-based works. They are more recent but not necessarily state of the art. For example, Akan et al. [<xref rid="B32-sensors-25-05244" ref-type="bibr">32</xref>] present a Transformer architecture for classifying arrhythmias in ECG signals. The technique uses multiple multi-head attention layers to capture complex temporal and spatial relationships in ECG data, along with positional encoding to maintain the sequential order of the signals. Another study is presented by Islam et al. [<xref rid="B4-sensors-25-05244" ref-type="bibr">4</xref>], who propose CAT-Net, a model that combines multi-head attention layers and a Transformer encoder to classify arrhythmias. The convolutional layers capture local morphological features of the heartbeats, while multi-head attention and the Transformer extract global contextual information.</p><p>El-Ghaish et al. [<xref rid="B6-sensors-25-05244" ref-type="bibr">6</xref>] propose ECGTransForm, a Bidirectional Transformer (BiTrans)-based model that captures temporal dependencies of previous and subsequent contexts and integrates multi-scale convolutions to extract spatial features at different granularities, and a channel recalibration module to improve feature salience. In addition, the work of Kim et al. [<xref rid="B5-sensors-25-05244" ref-type="bibr">5</xref>] uses the Stockwell transform to convert time-to-frequency signals and extract features. A CNN captures local patterns, while a Transformer architecture models long-term dependencies without requiring peak detection. <xref rid="sensors-25-05244-t001" ref-type="table">Table 1</xref> presents a summary of the results of the different works tested on the MIT-BIH Arrhythmia dataset [<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>].</p><p>Recently, TinyML and Transformer-lite architectures optimized for wearable devices have been developed, aiming to achieve high accuracy while maintaining minimal computation and memory requirements. Busia et al. [<xref rid="B33-sensors-25-05244" ref-type="bibr">33</xref>] propose a tiny Transformer for ECG classification on the MIT-BIH arrhythmia database, with 98.97% accuracy, optimized for wearables, achieving 4.28 ms of inference and 0.09 mJ on the GAP9 processor. Kim et al. [<xref rid="B34-sensors-25-05244" ref-type="bibr">34</xref>] propose TinyCES, a TinyML-based ECG classification system that processes data directly on the device, using CNNs on the MIT-BIH arrhythmia database and PTB Diagnostic ECG Database, achieving 97% accuracy on an Arduino prototype. Alvarado et al. [<xref rid="B35-sensors-25-05244" ref-type="bibr">35</xref>] developed a TinyML-based wearable cardiac monitoring system to detect arrhythmias in real time using the PTB-XL dataset, achieving 95% accuracy and an 88% reduction in false positives.</p><p>These works demonstrate the potential of combining wearable efficiency with optimization for detecting and monitoring arrhythmias. By contrast, our CNN-based approach focuses on maximizing classification performance while maintaining an efficient Conv1D architecture and allows for future adaptation for deployment on wearable platforms.</p></sec><sec id="sec2dot2-sensors-25-05244"><title>2.2. ECG Arrhythmia Detection in&#160;Smartwatches</title><p>Recently, there have been significant technological advances in wearable devices, especially smartwatches, for arrhythmia detection. Despite being consumer-grade devices, advanced smartwatches can measure health data comparable to electrocardiographs. A notable advantage of these devices is their ability to provide continuous, noninvasive monitoring. These features are crucial for identifying asymptomatic arrhythmias and achieving the early detection of arrhythmias [<xref rid="B36-sensors-25-05244" ref-type="bibr">36</xref>].</p><p>Smartwatches utilize photoplethysmography (PPG) technology to detect irregularities that may indicate underlying arrhythmias. PPG is a noninvasive technology for monitoring heart rate by detecting changes in blood volume through the skin. The device uses a light source directed at the skin. The light penetrates the skin and reflects off blood vessels. A photodetector measures the amount of reflected light, which varies with blood volume with each heartbeat [<xref rid="B37-sensors-25-05244" ref-type="bibr">37</xref>].</p><p>We compiled the most recent studies seeking to detect ECG arrhythmias using smartwatches. We searched for reviews and some more recent studies [<xref rid="B38-sensors-25-05244" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05244" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05244" ref-type="bibr">40</xref>]. We carefully selected those with measurable results, recent ones, and, above all, those based on smartwatches. These studies serve as a benchmark against which to compare the results of our proposal.</p><p>Avran et al. [<xref rid="B12-sensors-25-05244" ref-type="bibr">12</xref>] evaluated atrial fibrillation (AF) detection using an Apple Watch Series 4 smartwatch in 204 patients, comparing its performance with a 12-lead Holter monitor. They used a machine learning-based algorithm to identify AF. The results showed a sensitivity of 88% and a specificity of 98%. Subsequently, Ploux et al. [<xref rid="B13-sensors-25-05244" ref-type="bibr">13</xref>] evaluated atrial fibrillation (AF) detection using an Apple Watch Series 4 smartwatch in 260 patients. A deep neural network-based algorithm was used to analyze PPG signals and classify AF. The results reported an accuracy of 92%, a sensitivity of 91%, and a specificity of 94%.</p><p>We also identified the work of Ford et al. [<xref rid="B14-sensors-25-05244" ref-type="bibr">14</xref>], who utilized an Apple Watch Series 4 smartwatch to detect arrhythmias using PPG signals and compare the results with those from an electrocardiograph. They utilized a machine learning algorithm, achieving an accuracy of 87%, a sensitivity of 68%, and a specificity of 93%. A study with 200 patients by Abu-Alrub et al. [<xref rid="B15-sensors-25-05244" ref-type="bibr">15</xref>] used a Samsung Galaxy Watch 3, achieving 88% sensitivity and 81% specificity. This research highlights the usability of the smartwatch for the continuous monitoring of arrhythmia episodes.</p><p>More recent work with cutting-edge results is found in Wasserlauf et al. [<xref rid="B16-sensors-25-05244" ref-type="bibr">16</xref>], who treated 250 patients using an Apple Watch 5. The approach utilized deep learning to process and classify arrhythmias, achieving a sensitivity of 25% and a specificity of 99%. Its low sensitivity indicated limitations in detecting false positives. Mannhart et al. [<xref rid="B17-sensors-25-05244" ref-type="bibr">17</xref>] presentaron otro estudio, en el que probaron varios modelos de relojes inteligentes comerciales. Utilizando los resultados del Galaxy Watch 4 en 201 pacientes, los resultados no fueron &#243;ptimos, con una sensibilidad del 58% y una especificidad del 75%. El trabajo sugiere que los profesionales sanitarios deber&#237;an diagnosticar posteriormente cualquier problema de salud detectado mediante relojes inteligentes. <xref rid="sensors-25-05244-t002" ref-type="table">Table 2</xref> presents a summary of the results of the aforementioned works.</p></sec></sec><sec id="sec3-sensors-25-05244"><title>3. Materials and Methods</title><list list-type="bullet"><list-item><p>Datasets: UMass Medical School Simband Dataset [<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>], and MIT-BIH Arrhythmia Database [<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>].</p></list-item><list-item><p>Anaconda Navigator (Version 2.6.3).</p></list-item><list-item><p>Jupyter Notebook (version 7.3.2).</p></list-item><list-item><p>Python (Version 3.12.19).</p></list-item><list-item><p>Tensorflow (Version: 2.19.0).</p></list-item><list-item><p>Keras (Version: 3.10.0).</p></list-item><list-item><p>Pandas (Version: 2.2.3).</p></list-item><list-item><p>Scikit-learn (Version: 1.7.1).</p></list-item><list-item><p>Numpy (Version: 2.0.1).</p></list-item><list-item><p>PyWavelets (Version: 1.8.0).</p></list-item><list-item><p>Matplotlib (Version: 3.10.1).</p></list-item><list-item><p>Scipy (Version: 1.15.2).</p></list-item></list><sec id="sec3dot1-sensors-25-05244"><title>3.1. Proposed&#160;Model</title><p>A model is proposed for detecting arrhythmias from electrocardiogram (ECG) data from smartwatches. When deployed on a smartwatch, the model monitors and displays early warnings about the possible detection of arrhythmias. Therefore, given the low computational power of smartwatches, the deployed model must be lightweight; that is, the model architecture should be simple and efficient.</p><p>Under this approach, we propose an architecture based on a one-dimensional neural network (1D CNN), which takes electrocardiogram data as input, performs multi-stage preprocessing, trains and tests the model to extract features, evaluates the model by extracting metrics, and finally validates the model by detecting arrhythmias. <xref rid="sensors-25-05244-f001" ref-type="fig">Figure 1</xref> shows the pipeline of the proposed model.</p></sec><sec id="sec3dot2-sensors-25-05244"><title>3.2. ECG Data&#160;Input</title><p>Two datasets were used as input.</p><sec id="sec3dot2dot1-sensors-25-05244"><title>3.2.1. MIT-BIH Arrhythmia&#160;Database</title><p>The dataset corresponds to electrocardiogram (ECG) signals from Holter devices, compiled by the Massachusetts Institute of Technology (MIT) and Beth Israel Deaconess Medical Center in Boston [<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>]. It includes ECG recordings from 118 normal patients and 48 patients with arrhythmias (N: normal, L: left bundle branch block, R: right bundle branch block, A: atrial premature beat, V: ventricular premature beat). The ECG signals were recorded at a sampling rate of 128 Hz using an Ampex VR-66/IVA digital tape recorder. It records twelve 30 min ECG recordings at a sampling rate of 128 Hz.</p><p>Although these electrocardiogram data do not come from smartwatches but from Holter monitors, we used this dataset as a basis to validate the reliability of our model and compare our results with the state of the art, as it is the reference dataset used by the most innovative proposals.</p></sec><sec id="sec3dot2dot2-sensors-25-05244"><title>3.2.2. UMass Medical School Simband&#160;Dataset</title><p>Data were taken from 37 patients (28 men and 9 women) with and without cardiac arrhythmia, ranging in age from 50 to 91 years. Patients simultaneously wore smartwatches and Holter monitors and were monitored in the outpatient cardiovascular clinic at the University of Massachusetts Medical Center (UMMC) [<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>]. This dataset contains data from normal (NSR) patients and patients with arrhythmias, including atrial fibrillation (AF), premature atrial contractions (PACs), and premature ventricular contractions (PVCs). ECG and smartwatch data were measured simultaneously on the chest and wrist using a 7-lead Holter monitor and a Samsung Simband 2 smartwatch. The smartwatch data consisted of 8-channel PPG signals, three-axis accelerometers, and a single-lead ECG. The PPG signals from the smartwatch were sampled at 128 Hz. All signals were segmented into 30 s lengths. Single-lead ECG data were taken for our study.</p><p>Although the data are divided into arrhythmia classes (NSR, AF, PAC, and VS), data specifically corresponding to smartwatch ECG recordings were not present in all patients, nor were there recordings of the specific arrhythmia type. Therefore, arrhythmia classes were grouped into two classes, making our model binary in detection: (N) normal; (A) arrhythmia. We used this dataset as the basis for our results. Access had to be requested through requests to the authors (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/Cassey2016/UMass_Simband_Dataset">https://github.com/Cassey2016/UMass_Simband_Dataset</uri>, accessed on 13 July 2025), as, to our knowledge, there is no publicly available ECG dataset from smartwatches.</p></sec></sec><sec id="sec3dot3-sensors-25-05244"><title>3.3. Preprocessing</title><p>The following steps were used when processing both datasets.</p><sec id="sec3dot3dot1-sensors-25-05244"><title>3.3.1. Baseline Drift&#160;Removal</title><p>A specific procedure was implemented to eliminate baseline drift, which is primarily due to low-frequency artifacts generated by the patient&#8217;s breathing or movement. This distortion can affect the correct identification of ECG signals and the model&#8217;s classification results. To correct this, a digital high-pass filter with a cutoff frequency of 0.5 Hz was applied, designed using a 4th-order Butterworth filter. This filter attenuates very low-frequency components, preserving the morphology of the ECG signal. The process was performed on each signal segment before applying the subsequent steps of denoising, z-score normalization, and segmentation.</p></sec><sec id="sec3dot3dot2-sensors-25-05244"><title>3.3.2. Denoising</title><p>Denoising is crucial in the preprocessing of electrocardiograms (ECGs), particularly in smartwatch signals, such as the Simband dataset, where noise can amplify detection errors. The goal is to minimize artifacts that could distort heartbeat morphologies, thereby enhancing segmentation accuracy and feature extraction for the CNN.</p><p>A discrete wavelet transform (sym4) was applied to each window of 1000 samples extracted from the ECG segments (see <xref rid="sensors-25-05244-f002" ref-type="fig">Figure 2</xref>). The signal is decomposed to the maximum allowable level, and the detail coefficients are filtered with a fixed threshold (0.04 times the maximum value of each level). ECG signals were obtained from a random patient from the MIT-BIH arrhythmia database in a window of 1000 samples.</p></sec><sec id="sec3dot3dot3-sensors-25-05244"><title>3.3.3. Z-Score&#160;Normalization</title><p>This stage standardizes the voltage values to have a mean of 0 and a standard deviation of 1. This is a common practice in CNN preprocessing. The idea is to standardize the amplitudes so that the CNN processes consistent signals, regardless of variations in the voltage scale, such as those caused by the use of different equipment. This improves training convergence.</p></sec><sec id="sec3dot3dot4-sensors-25-05244"><title>3.3.4. Segmentation</title><p>Each patient&#8217;s ECG recordings were divided into sets of 1000 records, called segments, which serve as input for the CNN, ensuring that each segment contains complete beats in a temporal context. The idea is to ensure that R peaks are captured in each segment and that the relevant morphologies are obtained.</p><p>Each ECG segment is divided into windows of 1000 samples, each approximately 7.81 s long, since the sampling frequency in both datasets is 128 Hz. Windows are extracted if they have exactly 1000 samples and are not labeled as noise.</p></sec><sec id="sec3dot3dot5-sensors-25-05244"><title>3.3.5. Class&#160;Coding</title><p>Specific labels are assigned to each segment, allowing the model to learn specific patterns for each class. One-hot encoding is used, which is compatible with the categorical loss used in training.</p><p>In the MIT-BIH arrhythmia database [<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>], each segment is labeled according to the spike class R (N, L, R, A, and V) obtained from the class annotations. The labels are converted to one-hot format (vectors of length 5). In the Simband dataset [<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>], each segment initially has a label (0.0 for NSR, 1.0 for AF, 2.0 for PAC, and 3.0 for PVC). Labels 1.0, 2.0, and 3.0 are grouped as Arrhythmia (binary label 1), and 0.0 as Normal (label 0). The labels are encoded in one-hot format (vectors of length 2).</p></sec></sec><sec id="sec3dot4-sensors-25-05244"><title>3.4. Model Training and&#160;Testing</title><sec id="sec3dot4dot1-sensors-25-05244"><title>3.4.1. Data&#160;Split</title><p>In both datasets, the segments with 1000-sample windows and their one-hot labels were divided into training and test sets, with 80% allocated to training and 20% to testing. To achieve this, a patient-level partition was performed instead of the classic random partitioning, ensuring that</p><list list-type="bullet"><list-item><p>Each patient was uniquely included in either the training set or the test set.</p></list-item><list-item><p>All ECG windows from the same patient remained in the same set.</p></list-item></list><p>Therefore, we applied a Leave-One-Subject-Out Cross-Validation (LOSO) validation scheme, in which each patient is excluded from the training set and used as a test set in a separate iteration. This approach eliminates the possibility of information leakage between sets and allows for a more realistic assessment of the model&#8217;s ability to generalize to new patients.</p></sec><sec id="sec3dot4dot2-sensors-25-05244"><title>3.4.2. Model&#160;Architecture</title><p>The model architecture for multiclass arrhythmia detection in the MIT-BIH arrhythmia database [<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>] is based on a 1D CNN, which receives preprocessed windows of 1000 samples. The network consists of four convolutional layers (Conv1D) with 16, 32, 64, and 128 filters, and kernel sizes of 11, 13, 15, and 17, respectively. All layers use ReLU activation and the &#8216;same&#8217; padding to maintain the input length. Each convolutional layer is followed by a max pooling layer (pool size 2), progressively reducing the length from 1000 to 999, 499, 249, and 124 samples, extracting temporal features.</p><p>After the convolutional layers, the output of (124, 128) is flattened into a 15,872-element vector using a Flatten layer, followed by a Dropout layer to eliminate overfitting. Then, a dense layer with 35 neurons (ReLU) combines the extracted features, and a final dense layer with five neurons combines classes N, L, R, A, and V. A softmax layer converts these logits into probabilities, achieving multiclass classification. <xref rid="sensors-25-05244-f003" ref-type="fig">Figure 3</xref> graphically shows this architecture.</p><p>The model architecture for arrhythmia detection on the UMass Medical School Simband dataset [<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>] is also based on a 1D CNN, with an input of a 1000-sample segment. The network consists of four convolutional layers (Conv1D) followed by MaxPooling1D layers, each with filters of 16, 32, 64, and 128. The first convolutional layer applies 16 filters with a kernel size of 13, followed by a MaxPooling1D layer that reduces the length from 1000 to 499. The following layers use kernels of sizes 11, 13, 15, and 17, with 32, 64, and 128 filters, respectively, and each is followed by a MaxPooling1D that reduces the dimensions to 249, 124, and 61. The output is then flattened into a 7808-element vector by a Flatten layer, followed by a Dropout layer. Finally, two dense layers are used: the first with 128 units and ReLU activation, and the second with 2 units for a binary classification of (N) normal and (A) arrhythmia. <xref rid="sensors-25-05244-f004" ref-type="fig">Figure 4</xref> graphically shows this architecture.</p><p>Although both architectures share a similar structure, comprising four Conv1D and four MaxPooling1D, Flatten, Dropout, and Dense layers, with kernel sizes of 11, 13, 15, and 17 and numbers of filters of 16, 32, 64, and 128, the architecture for the Simband dataset [<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>] presents several differences. First, the number of output classes is two versus five. Furthermore, the intermediate dense layer in Simband has 128 units, compared to 35. Another difference is the output of the Flatten layer. In Simband, it produces a vector of 7,808 elements because the last MaxPooling1D reduces the dimension to 61 with 128 filters, while in MIT-BIH, it produces 15,872 elements.</p></sec></sec></sec><sec id="sec4-sensors-25-05244"><title>4. Experiments and&#160;Results</title><p>The proposal was implemented and evaluated using Python version 3.12.9, as a Jupyter notebook on the Anaconda platform. A GPU was not required, as the model&#8217;s low complexity meant training only took 20 to 30 minutes in both datasets. The following training parameters were used:<list list-type="bullet"><list-item><p>Epochs: 50;</p></list-item><list-item><p>Batch size: 32;</p></list-item><list-item><p>Loss function: Categorical cross-entropy;</p></list-item><list-item><p>Optimizer: Adam.</p></list-item></list></p><p>After training and generating the arrhythmia detection model, the model was evaluated using the following metrics:<list list-type="bullet"><list-item><p>Accuracy: Measures the model&#8217;s precision, i.e., the proportion of correct predictions out of the total samples.</p></list-item><list-item><p>Sensitivity: Measures the proportion of positive cases correctly classified per class.</p></list-item><list-item><p>Specificity: Measures the proportion of negative cases correctly identified per class.</p></list-item><list-item><p>F1-score: Measures the proportion of how well it correctly identifies positives without generating many false positives or negatives.</p></list-item><list-item><p>AUROC: Measures the ability of a classification model to distinguish between classes.</p></list-item></list></p><sec sec-type="results" id="sec4dot1-sensors-25-05244"><title>4.1. Results in UMass Medical School Simband Dataset&#160;[<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>]&#8212;Effectiveness and&#160;Efficiency</title><p>The model converged in the first 20 epochs, achieving the following effectiveness global results:<list list-type="bullet"><list-item><p>Accuracy: 64.81%;</p></list-item><list-item><p>Sensitivity: 89.47%;</p></list-item><list-item><p>Specificity: 6.25%;</p></list-item><list-item><p>F1-score: 78.16%;</p></list-item><list-item><p>AUROC: 0.1978;</p></list-item><list-item><p>AUROC 95% bootstrap CI: 0.1446&#8211;0.2485;</p></list-item><list-item><p>F1-score 95% bootstrap CI: 0.7445&#8211;0.8189.</p></list-item></list></p><p>To calculate the efficiency of the binary model, the computational complexity was measured, reaching 1.2 million parameters and 68.48 MFlops. Additionally, <xref rid="sensors-25-05244-t003" ref-type="table">Table 3</xref> shows the results of accuracy, sensitivity, and specificity by class. Finally, <xref rid="sensors-25-05244-f005" ref-type="fig">Figure 5</xref> shows the ROC Curve, Precision&#8211;Recall Curve, and Confusion Matrix.</p></sec><sec sec-type="results" id="sec4dot2-sensors-25-05244"><title>4.2. Results in MIT-BIH Arrhythmia Database&#160;[<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>]</title><p>In this dataset, convergence was also short. <xref rid="sensors-25-05244-f006" ref-type="fig">Figure 6</xref> shows that the model stabilizes within the first 15 epochs, after which it achieves good results.</p><list list-type="bullet"><list-item><p>Accuracy: 99.57%;</p></list-item><list-item><p>Sensitivity: 99.57%;</p></list-item><list-item><p>Specificity: 99.47%;</p></list-item><list-item><p>F1-score: 0.9957;</p></list-item><list-item><p>AUROC: 0.9958;</p></list-item><list-item><p>AUROC 95% bootstrap CI: 0.9943&#8211;0.9972;</p></list-item><list-item><p>F1-score 95% bootstrap CI: 0.9946&#8211;0.9966.</p></list-item></list><p>Additionally, <xref rid="sensors-25-05244-t004" ref-type="table">Table 4</xref> shows the results of accuracy, sensitivity, and specificity by class (N, L, R, A, and V). Finally, <xref rid="sensors-25-05244-f007" ref-type="fig">Figure 7</xref> shows the Precision&#8211;Recall Curve and Confusion Matrix.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-05244"><title>5. Discussion</title><sec sec-type="discussion" id="sec5dot1-sensors-25-05244"><title>5.1. Discussion of Results from UMass Medical School Simband Dataset&#160;[<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>]&#8212;Effectiveness and&#160;Efficiency</title><p>The overall results of our proposal on the Simband dataset are acceptable, achieving an overall accuracy of 64.81%. This result suggests that our model could be deployed on smartwatches.</p><p>In a clinical setting, sensitivity, or true positives, is the most relevant metric, as it is important to understand how well it detects patients with arrhythmia. Our model achieved 89.47%. This is a crucial result for the objectives of our work, as it confirms that our model is effective in detecting arrhythmias when processing ECGs from smartwatches. On the other hand, the specificity results, or how well it detects patients without arrhythmia, reached 6.25%. This result suggests that our model is also ineffective in detecting patients without arrhythmia.</p><p>It should be noted that these results respond to the specific characteristics of electrocardiograms (ECGs) from Samsung&#8217;s Simband 2 smartwatches only, which raises the possibility of overfitting and model specialization. Therefore, generalizing the model to ECGs from other smartwatches remains unproven. It would be necessary to test it with such data. Unfortunately, to our knowledge, there are no open ECG data from smartwatches of other models and brands.</p><p>However, considering the small size of the Simband dataset (37 patients), to avoid overfitting, we used data partitioning using a Leave-One-Subject-Out Cross-Validation (LOSO) scheme, ensuring that each patient is only in the training or test set, and ECG windows from the same patient remain in the same set.</p><p>This scheme better simulates a real-life clinical application, where the model must detect arrhythmias in patients not seen during training. This eliminates the possibility of information leakage between sets and allows for a more realistic evaluation of the model for generalization to new patients. Furthermore, the Simband dataset is imbalanced (29.63% Normal and 73.7% Arrhythmia), and using resampling affects the minority class and skews the results (as analyzed in sect:Ablation Study). Therefore, our proposal processes the data without resampling, yielding more realistic and unbiased results.</p><p>On the other hand, our model is binary on the Simband dataset, meaning it only has two classes: (N) Normal and (A) Arrhythmia. This fact could undermine our model&#8217;s merit. However, in a subsequent deployment of our proposal in a smartwatch app, the detection of specific arrhythmia classes is not yet reliable, as the reliability of smartwatch PPG technology is not yet comparable to that of electrocardiographs or Holter devices. Therefore, it is more reliable to detect only arrhythmia in general and subsequently diagnose, classify, and treat it using healthcare professionals.</p><p>To further validate our model, our results were compared with the most important and innovative state-of-the-art proposals (see <xref rid="sensors-25-05244-t005" ref-type="table">Table 5</xref>). Our accuracy, sensitivity, and specificity results, 68.41%, 89.47%, and 6.25%, respectively, are below those of most other studies. However, our proposal almost surpasses the state of the art in sensitivity (89.47%) and confirms that our model is effective in detecting arrhythmias when processing ECGs from smartwatches.</p><p>Regarding efficiency results, computational complexity was measured, reaching 1.2 million parameters and 68.48 MFlops. To our knowledge, there are no review articles on the efficiency of models for detecting arrhythmias in electrocardiogram data, nor did we find efficiency results in related studies, and we have no references against which to compare the results. However, we can demonstrate that our 1D CNN architecture is more efficient considering both the nature of the data and the computational complexity and cost:<list list-type="bullet"><list-item><p>ECG signals are one-dimensional time series. One-dimensional CNNs directly process this structure, avoiding conversions to two-dimensional representations that involve more preprocessing, storage, and the potential loss of fine-grained temporal information.</p></list-item><list-item><p>Lower parameter count. A typical 1D CNN for ECG (four Conv1D layers with small filters) typically has between <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> parameters, while 2D-CNN, LSTM, or Transformer-based architectures for the same task can easily exceed <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> parameters.</p></list-item><list-item><p>Lower FLOPs. Such a 1D CNN can require on the order of <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> FLOPs, compared to <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> or more for equivalent 2D or LSTM/Transformer networks, resulting in faster inference times.</p></list-item><list-item><p>Suitability for resource-constrained devices. The lower number of parameters and FLOPs reduces memory usage and allows execution on microcontrollers, embedded hardware, or smartwatches without sacrificing accuracy.</p></list-item></list></p></sec><sec sec-type="discussion" id="sec5dot2-sensors-25-05244"><title>5.2. Discussion of Results from MIT-BIH Arrhythmia Database&#160;[<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>]</title><p>The evaluation and comparison of our binary model with state-of-the-art proposals lack a standard dataset for equal comparisons. MIT-BIH is the benchmark dataset for multiclass arrhythmia detection. There are many results from proposals using this dataset. For this reason, we chose to use this dataset to test our model in a multiclass scenario and compare its results with the state-of-the-art datasets. This allows us to validate our proposal&#8217;s architecture, even though the data are not from smartwatches but from Holter devices. Our overall accuracy, sensitivity, and specificity results&#8212;99.57%, 99.57%, and 99.47%, respectively&#8212;place us among the best state-of-the-art proposals (see <xref rid="sensors-25-05244-t006" ref-type="table">Table 6</xref>). These results confirm that our model is reliable and can also be used as a benchmark for detecting multiclass arrhythmias with Holter device data. Another of our goals was to make our model lightweight in terms of complexity. As can be seen in <xref rid="sensors-25-05244-t006" ref-type="table">Table 6</xref>, the most recent proposals use Transformer-Attention as their primary method. These proposals have high computational complexity and also fail to outperform our method. The same is true for proposals that combine CNN and LSTM. Thus, our 1D CNN-based model achieves the best results and the lowest complexity.</p></sec></sec><sec id="sec6-sensors-25-05244"><title>6. Ablation&#160;Study</title><sec id="sec6dot1-sensors-25-05244"><title>6.1. An Ablation Study of the Model Tested on the UMass Medical School Simband Dataset&#160;[<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>]&#8212;Effectiveness and&#160;Efficiency</title><p>As clarified in sect:Discussion, our proposed model uses the Leave-One-Subject-Out Cross-Validation (LOSO) validation technique and does not resample the dataset, thus achieving accuracy, sensitivity, specificity, and AUROC values of 64.81, 89.47, 6.25, and 62.5%, respectively.</p><p>However, resampling the data and ensuring that each class (Normal and Arrhythmia) accounts for 50% of the variance improve the metrics, achieving values of 66.05, 98.72, 52.1, and 65.8%, respectively. Given the highly imbalanced Simband dataset (29.63% Normal and 70.37% Arrhythmia), this variant is not realistic, as it affects the metrics of the less dominant class.</p><p>Furthermore, if Leave-One-Subject-Out Cross-Validation (LOSO) is eliminated after resampling, replacing it with a random 80/20 data partition, excellent results of 98.73, 98.73, 97.52, and 97.7% are achieved. These results are almost perfect and surpass the state of the art (see <xref rid="sensors-25-05244-t007" ref-type="table">Table 7</xref>), but this variant is also unrealistic. It can introduce data from the same patient in both training and validation, artificially inflating model performance due to the morphological specificity of the ECG between subjects. Therefore, it is discarded.</p><p>Tests were also run by reducing the four convolutional layers to three, and the metrics decreased considerably. Finally, the kernel sizes of the four convolutional layers (11, 13, 15, 17) were incremented to (13, 15, 17, 19), and the metrics also decreased (see <xref rid="sensors-25-05244-t007" ref-type="table">Table 7</xref>).</p><p>To determine which regions of the input ECG signals were most important for our 1D CNN model to make a decision, the Grad-CAM explanation technique was applied to the first input sample (see <xref rid="sensors-25-05244-f008" ref-type="fig">Figure 8</xref>). Since a typical smartwatch signal is noisy and highly variable, the exact activation segment cannot be identified. However, it can be seen that it involves the R peak and the PQR segments. Therefore, the PQR segment is the most important region for arrhythmia detection in smartwatches.</p></sec><sec id="sec6dot2-sensors-25-05244"><title>6.2. An Ablation Study of the Model Tested on the MIT-BIH Arrhythmia Database&#160;[<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>]</title><p>As with our binary model, the proposed multiclass model uses the Leave-One-Subject-Out Cross-Validation (LOSO) validation technique and does not resample the dataset, achieving accuracy, sensitivity, specificity, and AUROC values of 99.57%, 99.57%, 99.47%, and 99.58%, respectively.</p><p>Upon resampling the data and ensuring that each class (N, L, R, A, and V) accounts for 20% of the sample size, the metrics slightly decrease, reaching values of 98.69, 98.69, 98.38, and 98.71%, respectively. Given the imbalanced nature of the MIT-BIH dataset, this variation is unrealistic, as it affects the metrics of the less dominant classes.</p><p>If, after resampling, Leave-One-Subject-Out Cross-Validation (LOSO) is eliminated, replacing it with a random 80/20 data partition, the results are increased to 96.64, 99.64, 99.9, and 99.71%. These results are almost perfect and surpass the state of the art (see <xref rid="sensors-25-05244-t008" ref-type="table">Table 8</xref>), but this variant is also unrealistic. It can introduce data from the same patient in both training and validation, artificially inflating the model&#8217;s performance due to the morphological specificity of the ECG between subjects. Therefore, it is discarded.</p><p>Tests were also run by reducing the four convolutional layers to three, and the metrics decreased slightly. Finally, the kernel sizes of the four convolutional layers (11, 13, 15, and 17) were incremented to (13, 15, 17, and 19), and the metrics continued to decrease (see <xref rid="sensors-25-05244-t008" ref-type="table">Table 8</xref>).</p><p>The input regions of the ECG signals that were most important for our multiclass 1D CNN model to make a decision were identified by applying the Grad-CAM explanation technique to the first input sample (see <xref rid="sensors-25-05244-f009" ref-type="fig">Figure 9</xref>). The most important region is the R peak.</p></sec></sec><sec sec-type="conclusions" id="sec7-sensors-25-05244"><title>7. Conclusions</title><p>This study proposes a model for arrhythmia detection using smartwatch ECGs. A 1D CNN architecture was used to train and test our binary model on two datasets: the UMass Medical School Simband dataset, which was used to evaluate the effectiveness and efficiency of our binary model on smartwatch data, and the MIT-BIH arrhythmia database, which was used to compare the results of our multiclass model with the most recent state-of-the-art proposals.</p><p>A four-layer 1D CNN architecture was proposed, making the model lightweight and of low complexity and ensuring its subsequent deployment on smartwatches, as smartwatches currently have limited computational power.</p><p>The effectiveness results of the binary model on the UMass Medical School Simband dataset achieved an accuracy of 64.81%, sensitivity of 89.47%, and specificity of 6.25%. The results were acceptable. Since sensitivity is the metric that evaluates true positives and is most relevant in a clinical context, the result (89.47%) suggests high clinical reliability.</p><p>The efficiency results of the binary model reached 1.2 million parameters and 68.48 MFlops. This result demonstrates the model&#8217;s efficiency, which is also because the 1D CNN method used has lower computational complexity than state-of-the-art methods: 2D CNN, LSTM, hybrids, and Transformers.</p><p>The MIT-BIH arrhythmia database is a commonly used dataset in recent proposals for multiclass arrhythmia detection in Holter ECGs. Our multiclass model was tested on this dataset and achieved an accuracy of 99.57%, sensitivity of 99.57%, and specificity of 99.47%. These results make it a cutting-edge proposal and demonstrate that our model is also effective in detecting multiclass arrhythmias by processing electrocardiogram data from Holter devices.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, H.A.H.B. and F.d.L.P.V.; methodology, H.A.H.B.; software, H.A.H.B.; validation, H.A.H.B. and F.d.L.P.V.; formal analysis, H.A.H.B.; investigation, H.A.H.B.; resources, F.d.L.P.V.; data curation, F.d.L.P.V.; writing&#8212;original draft preparation, H.A.H.B.; writing&#8212;review and editing, H.A.H.B.; visualization, F.d.L.P.V.; supervision, H.A.H.B.; project administration, H.A.H.B.; funding acquisition, F.d.L.P.V. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available in Synapse at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.synapse.org/Synapse:syn23565056/wiki/608635">https://www.synapse.org/Synapse:syn23565056/wiki/608635</uri> (accessed on 13 July 2025), reference number [<xref rid="B18-sensors-25-05244" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05244" ref-type="bibr">19</xref>]. And PhysioNet at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.physionet.org/content/mitdb/1.0.0/">https://www.physionet.org/content/mitdb/1.0.0/</uri> (accessed on 13 July 2025), reference number [<xref rid="B20-sensors-25-05244" ref-type="bibr">20</xref>].</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare that there are no conflicts of interest between authors.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05244"><label>1.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>World Health Organization</collab></person-group><article-title>Cardiovascular Diseases (CVDs)&#8212;who.int</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)" ext-link-type="uri">https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-05244"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kirchhof</surname><given-names>P.</given-names></name><name name-style="western"><surname>Benussi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kotecha</surname><given-names>D.</given-names></name></person-group><article-title>ESC Guidelines for the diagnosis and management of atrial fibrillation</article-title><source>Eur. Heart J.</source><year>2020</year><volume>42</volume><fpage>373</fpage><lpage>498</lpage></element-citation></ref><ref id="B3-sensors-25-05244"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Berkaya</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Uysal</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Gunal</surname><given-names>E.S.</given-names></name><name name-style="western"><surname>Ergin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gunal</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gulmezoglu</surname><given-names>M.B.</given-names></name></person-group><article-title>A survey on ECG analysis</article-title><source>Biomed. Signal Process. Control.</source><year>2018</year><volume>43</volume><fpage>216</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.bspc.2018.03.003</pub-id></element-citation></ref><ref id="B4-sensors-25-05244"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Qaraqe</surname><given-names>M.</given-names></name><name name-style="western"><surname>Qaraqe</surname><given-names>K.</given-names></name><name name-style="western"><surname>Serpedin</surname><given-names>E.</given-names></name></person-group><article-title>Cat-net: Convolution, attention, and transformer based network for single-lead ecg arrhythmia classification</article-title><source>Biomed. Signal Process. Control.</source><year>2024</year><volume>93</volume><elocation-id>106211</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2024.106211</pub-id></element-citation></ref><ref id="B5-sensors-25-05244"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>K.R.</given-names></name><name name-style="western"><surname>Lim</surname><given-names>D.S.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>K.H.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>D.Y.</given-names></name><name name-style="western"><surname>Sohn</surname><given-names>C.B.</given-names></name></person-group><article-title>A novel hybrid CNN-transformer model for arrhythmia detection without R-peak identification using stockwell transform</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><elocation-id>7817</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-025-92582-9</pub-id><pub-id pub-id-type="pmid">40050678</pub-id><pub-id pub-id-type="pmcid">PMC11885558</pub-id></element-citation></ref><ref id="B6-sensors-25-05244"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>El-Ghaish</surname><given-names>H.</given-names></name><name name-style="western"><surname>Eldele</surname><given-names>E.</given-names></name></person-group><article-title>ECGTransForm: Empowering adaptive ECG arrhythmia classification framework with bidirectional transformer</article-title><source>Biomed. Signal Process. Control.</source><year>2024</year><volume>89</volume><elocation-id>105714</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2023.105714</pub-id></element-citation></ref><ref id="B7-sensors-25-05244"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>W.</given-names></name></person-group><article-title>Automated arrhythmia classification based on a combination network of CNN and LSTM</article-title><source>Biomed. Signal Process. Control.</source><year>2020</year><volume>57</volume><elocation-id>101819</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2019.101819</pub-id></element-citation></ref><ref id="B8-sensors-25-05244"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hassan</surname><given-names>S.U.</given-names></name><name name-style="western"><surname>Mohd Zahid</surname><given-names>M.S.</given-names></name><name name-style="western"><surname>Abdullah</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Husain</surname><given-names>K.</given-names></name></person-group><article-title>Classification of cardiac arrhythmia using a convolutional neural network and bi-directional long short-term memory</article-title><source>Digit. Health</source><year>2022</year><volume>8</volume><fpage>20552076221102766</fpage><pub-id pub-id-type="doi">10.1177/20552076221102766</pub-id><pub-id pub-id-type="pmid">35656286</pub-id><pub-id pub-id-type="pmcid">PMC9152186</pub-id></element-citation></ref><ref id="B9-sensors-25-05244"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alamatsaz</surname><given-names>N.</given-names></name><name name-style="western"><surname>Tabatabaei</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yazdchi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Payan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Alamatsaz</surname><given-names>N.</given-names></name><name name-style="western"><surname>Nasimi</surname><given-names>F.</given-names></name></person-group><article-title>A lightweight hybrid CNN-LSTM explainable model for ECG-based arrhythmia detection</article-title><source>Biomed. Signal Process. Control.</source><year>2024</year><volume>90</volume><elocation-id>105884</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2023.105884</pub-id></element-citation></ref><ref id="B10-sensors-25-05244"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>L.</given-names></name></person-group><article-title>Smart wearable technologies for continuous and proactive blood pressure monitoring</article-title><source>Innov. Mater.</source><year>2023</year><volume>1</volume><fpage>100035</fpage><pub-id pub-id-type="doi">10.59717/j.xinn-mater.2023.100035</pub-id></element-citation></ref><ref id="B11-sensors-25-05244"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abdelrazik</surname><given-names>A.</given-names></name><name name-style="western"><surname>Eldesouky</surname><given-names>M.</given-names></name><name name-style="western"><surname>Antoun</surname><given-names>I.</given-names></name><name name-style="western"><surname>Lau</surname><given-names>E.Y.</given-names></name><name name-style="western"><surname>Koya</surname><given-names>A.</given-names></name><name name-style="western"><surname>Vali</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Suleman</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>Donaldson</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ng</surname><given-names>G.A.</given-names></name></person-group><article-title>Wearable devices for arrhythmia detection: Advancements and clinical implications</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>2848</elocation-id><pub-id pub-id-type="doi">10.3390/s25092848</pub-id><pub-id pub-id-type="pmid">40363284</pub-id><pub-id pub-id-type="pmcid">PMC12074175</pub-id></element-citation></ref><ref id="B12-sensors-25-05244"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Avram</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ramsis</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cristal</surname><given-names>A.D.</given-names></name><name name-style="western"><surname>Nathan</surname><given-names>V.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kuang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Vittinghoff</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rohdin-Bibby</surname><given-names>L.</given-names></name><etal/></person-group><article-title>Validation of an algorithm for continuous monitoring of atrial fibrillation using a consumer smartwatch</article-title><source>Heart Rhythm</source><year>2021</year><volume>18</volume><fpage>1482</fpage><lpage>1490</lpage><pub-id pub-id-type="doi">10.1016/j.hrthm.2021.03.044</pub-id><pub-id pub-id-type="pmid">33838317</pub-id></element-citation></ref><ref id="B13-sensors-25-05244"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ploux</surname><given-names>S.</given-names></name><name name-style="western"><surname>Strik</surname><given-names>M.</given-names></name><name name-style="western"><surname>Caillol</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ramirez</surname><given-names>F.D.</given-names></name><name name-style="western"><surname>Abu-Alrub</surname><given-names>S.</given-names></name><name name-style="western"><surname>Marchand</surname><given-names>H.</given-names></name><name name-style="western"><surname>Buliard</surname><given-names>S.</given-names></name><name name-style="western"><surname>Haissaguerre</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bordachar</surname><given-names>P.</given-names></name></person-group><article-title>Beyond the wrist: Using a smartwatch electrocardiogram to detect electrocardiographic abnormalities</article-title><source>Arch. Cardiovasc. Dis.</source><year>2022</year><volume>115</volume><fpage>29</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.acvd.2021.11.003</pub-id><pub-id pub-id-type="pmid">34953753</pub-id></element-citation></ref><ref id="B14-sensors-25-05244"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ford</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>C.X.</given-names></name><name name-style="western"><surname>Low</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rajakariar</surname><given-names>K.</given-names></name><name name-style="western"><surname>Koshy</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Sajeev</surname><given-names>J.K.</given-names></name><name name-style="western"><surname>Roberts</surname><given-names>L.</given-names></name><name name-style="western"><surname>Pathik</surname><given-names>B.</given-names></name><name name-style="western"><surname>Teh</surname><given-names>A.W.</given-names></name></person-group><article-title>Comparison of 2 smart watch algorithms for detection of atrial fibrillation and the benefit of clinician interpretation: SMART WARS study</article-title><source>Clin. Electrophysiol.</source><year>2022</year><volume>8</volume><fpage>782</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1016/j.jacep.2022.02.013</pub-id><pub-id pub-id-type="pmid">35738855</pub-id></element-citation></ref><ref id="B15-sensors-25-05244"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abu-Alrub</surname><given-names>S.</given-names></name><name name-style="western"><surname>Strik</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ramirez</surname><given-names>F.D.</given-names></name><name name-style="western"><surname>Moussaoui</surname><given-names>N.</given-names></name><name name-style="western"><surname>Racine</surname><given-names>H.P.</given-names></name><name name-style="western"><surname>Marchand</surname><given-names>H.</given-names></name><name name-style="western"><surname>Buliard</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ha&#239;ssaguerre</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ploux</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bordachar</surname><given-names>P.</given-names></name></person-group><article-title>Smartwatch electrocardiograms for automated and manual diagnosis of atrial fibrillation: A comparative analysis of three models</article-title><source>Front. Cardiovasc. Med.</source><year>2022</year><volume>9</volume><elocation-id>836375</elocation-id><pub-id pub-id-type="doi">10.3389/fcvm.2022.836375</pub-id><pub-id pub-id-type="pmid">35187135</pub-id><pub-id pub-id-type="pmcid">PMC8854369</pub-id></element-citation></ref><ref id="B16-sensors-25-05244"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wasserlauf</surname><given-names>J.</given-names></name><name name-style="western"><surname>Vogel</surname><given-names>K.</given-names></name><name name-style="western"><surname>Whisler</surname><given-names>C.</given-names></name><name name-style="western"><surname>Benjamin</surname><given-names>E.</given-names></name><name name-style="western"><surname>Helm</surname><given-names>R.</given-names></name><name name-style="western"><surname>Steinhaus</surname><given-names>D.A.</given-names></name><name name-style="western"><surname>Yousuf</surname><given-names>O.</given-names></name><name name-style="western"><surname>Passman</surname><given-names>R.S.</given-names></name></person-group><article-title>Accuracy of the Apple watch for detection of AF: A multicenter experience</article-title><source>J. Cardiovasc. Electrophysiol.</source><year>2023</year><volume>34</volume><fpage>1103</fpage><lpage>1107</lpage><pub-id pub-id-type="doi">10.1111/jce.15892</pub-id><pub-id pub-id-type="pmid">36942773</pub-id><pub-id pub-id-type="pmcid">PMC11694482</pub-id></element-citation></ref><ref id="B17-sensors-25-05244"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mannhart</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lischer</surname><given-names>M.</given-names></name><name name-style="western"><surname>Knecht</surname><given-names>S.</given-names></name><name name-style="western"><surname>du Fay de Lavallaz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Strebel</surname><given-names>I.</given-names></name><name name-style="western"><surname>Serban</surname><given-names>T.</given-names></name><name name-style="western"><surname>V&#246;geli</surname><given-names>D.</given-names></name><name name-style="western"><surname>Schaer</surname><given-names>B.</given-names></name><name name-style="western"><surname>Osswald</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mueller</surname><given-names>C.</given-names></name><etal/></person-group><article-title>Clinical validation of 5 direct-to-consumer wearable smart devices to detect atrial fibrillation: BASEL wearable study</article-title><source>Clin. Electrophysiol.</source><year>2023</year><volume>9</volume><fpage>232</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1016/j.jacep.2022.09.011</pub-id><pub-id pub-id-type="pmid">36858690</pub-id></element-citation></ref><ref id="B18-sensors-25-05244"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>D.</given-names></name><name name-style="western"><surname>Han</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mercado-D&#237;az</surname><given-names>L.R.</given-names></name><name name-style="western"><surname>Moon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chon</surname><given-names>K.H.</given-names></name></person-group><article-title>Smartwatch Photoplethysmogram-Based Atrial Fibrillation Detection with Premature Atrial and Ventricular Contraction Differentiation Using Densely Connected Convolutional Neural Networks</article-title><source>Proceedings of the 2024 IEEE 20th International Conference on Body Sensor Networks (BSN)</source><conf-loc>Chicago, IL, USA</conf-loc><conf-date>15&#8211;17 October 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B19-sensors-25-05244"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bashar</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Han</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hajeb-Mohammadalipour</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>E.</given-names></name><name name-style="western"><surname>Whitcomb</surname><given-names>C.</given-names></name><name name-style="western"><surname>McManus</surname><given-names>D.D.</given-names></name><name name-style="western"><surname>Chon</surname><given-names>K.H.</given-names></name></person-group><article-title>Atrial fibrillation detection from wrist photoplethysmography signals using smartwatches</article-title><source>Sci. Rep.</source><year>2019</year><volume>9</volume><elocation-id>15054</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-49092-2</pub-id><pub-id pub-id-type="pmid">31636284</pub-id><pub-id pub-id-type="pmcid">PMC6803677</pub-id></element-citation></ref><ref id="B20-sensors-25-05244"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moody</surname><given-names>G.B.</given-names></name><name name-style="western"><surname>Mark</surname><given-names>R.G.</given-names></name></person-group><article-title>The impact of the MIT-BIH arrhythmia database</article-title><source>IEEE Eng. Med. Biol. Mag.</source><year>2001</year><volume>20</volume><fpage>45</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1109/51.932724</pub-id><pub-id pub-id-type="pmid">11446209</pub-id></element-citation></ref><ref id="B21-sensors-25-05244"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>K.</given-names></name><name name-style="western"><surname>Mokhtar</surname><given-names>S.A.</given-names></name><name name-style="western"><surname>Ismail</surname><given-names>I.</given-names></name><name name-style="western"><surname>Pauzi</surname><given-names>A.L.b.M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Lim</surname><given-names>P.Y.</given-names></name></person-group><article-title>Deep learning-based ECG arrhythmia classification: A systematic review</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>4964</elocation-id><pub-id pub-id-type="doi">10.3390/app13084964</pub-id></element-citation></ref><ref id="B22-sensors-25-05244"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ansari</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mourad</surname><given-names>O.</given-names></name><name name-style="western"><surname>Qaraqe</surname><given-names>K.</given-names></name><name name-style="western"><surname>Serpedin</surname><given-names>E.</given-names></name></person-group><article-title>Deep learning for ECG Arrhythmia detection and classification: An overview of progress for period 2017&#8211;2023</article-title><source>Front. Physiol.</source><year>2023</year><volume>14</volume><elocation-id>1246746</elocation-id><pub-id pub-id-type="doi">10.3389/fphys.2023.1246746</pub-id><pub-id pub-id-type="pmid">37791347</pub-id><pub-id pub-id-type="pmcid">PMC10542398</pub-id></element-citation></ref><ref id="B23-sensors-25-05244"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lichaee</surname><given-names>F.K.</given-names></name><name name-style="western"><surname>Salari</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jalili</surname><given-names>J.</given-names></name><name name-style="western"><surname>Dalivand</surname><given-names>S.B.</given-names></name><name name-style="western"><surname>Rad</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Mojarad</surname><given-names>M.</given-names></name></person-group><article-title>Advancements in artificial intelligence for ECG signal analysis and arrhythmia detection: A review</article-title><source>Int. J. Cardiovasc. Pract.</source><year>2023</year><volume>8</volume><fpage>e143437</fpage></element-citation></ref><ref id="B24-sensors-25-05244"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huillcen Baca</surname><given-names>H.A.</given-names></name><name name-style="western"><surname>Mu&#241;oz Del Carpio Toia</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sulla Torres</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Cusirramos Montesinos</surname><given-names>R.</given-names></name><name name-style="western"><surname>Contreras Salas</surname><given-names>L.A.</given-names></name><name name-style="western"><surname>Correa Herrera</surname><given-names>S.C.</given-names></name></person-group><article-title>Detection of Arrhythmias Using Heart Rate Signals from Smartwatches</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>7233</elocation-id><pub-id pub-id-type="doi">10.3390/app14167233</pub-id></element-citation></ref><ref id="B25-sensors-25-05244"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wulan</surname><given-names>N.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>Atrial fibrillation detection using stationary wavelet transform and deep learning</article-title><source>Proceedings of the 2017 Computing in Cardiology (CinC)</source><conf-loc>Rennes, France</conf-loc><conf-date>24&#8211;27 September 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2017</year><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B26-sensors-25-05244"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ullah</surname><given-names>A.</given-names></name><name name-style="western"><surname>Anwar</surname><given-names>S.M.</given-names></name><name name-style="western"><surname>Bilal</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mehmood</surname><given-names>R.M.</given-names></name></person-group><article-title>Classification of arrhythmia by using deep learning with 2-D ECG spectral image representation</article-title><source>Remote. Sens.</source><year>2020</year><volume>12</volume><elocation-id>1685</elocation-id><pub-id pub-id-type="doi">10.3390/rs12101685</pub-id></element-citation></ref><ref id="B27-sensors-25-05244"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zubair</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yoon</surname><given-names>C.</given-names></name></person-group><article-title>Cost-sensitive learning for anomaly detection in imbalanced ECG data using convolutional neural networks</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>4075</elocation-id><pub-id pub-id-type="doi">10.3390/s22114075</pub-id><pub-id pub-id-type="pmid">35684694</pub-id><pub-id pub-id-type="pmcid">PMC9185309</pub-id></element-citation></ref><ref id="B28-sensors-25-05244"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rizqyawan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Siradj</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Amri</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pratondo</surname><given-names>A.</given-names></name></person-group><article-title>Re-implementation of convolutional neural network for arrhythmia detection</article-title><source>Int. J. Adv. Sci. Eng. Inf. Technol.</source><year>2022</year><volume>12</volume><fpage>1319</fpage><lpage>1326</lpage><pub-id pub-id-type="doi">10.18517/ijaseit.12.4.13435</pub-id></element-citation></ref><ref id="B29-sensors-25-05244"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ojha</surname><given-names>M.K.</given-names></name><name name-style="western"><surname>Wadhwani</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wadhwani</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Shukla</surname><given-names>A.</given-names></name></person-group><article-title>Automatic detection of arrhythmias from an ECG signal using an auto-encoder and SVM classifier</article-title><source>Phys. Eng. Sci. Med.</source><year>2022</year><volume>45</volume><fpage>665</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1007/s13246-022-01119-1</pub-id><pub-id pub-id-type="pmid">35304901</pub-id></element-citation></ref><ref id="B30-sensors-25-05244"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jamil</surname><given-names>S.</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>M.</given-names></name></person-group><article-title>A novel deep-learning-based framework for the classification of cardiac arrhythmia</article-title><source>J. Imaging</source><year>2022</year><volume>8</volume><elocation-id>70</elocation-id><pub-id pub-id-type="doi">10.3390/jimaging8030070</pub-id><pub-id pub-id-type="pmid">35324625</pub-id><pub-id pub-id-type="pmcid">PMC8949672</pub-id></element-citation></ref><ref id="B31-sensors-25-05244"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Midani</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ouarda</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ayed</surname><given-names>M.B.</given-names></name></person-group><article-title>DeepArr: An investigative tool for arrhythmia detection using a contextual deep neural network from electrocardiograms (ECG) signals</article-title><source>Biomed. Signal Process. Control.</source><year>2023</year><volume>85</volume><elocation-id>104954</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2023.104954</pub-id></element-citation></ref><ref id="B32-sensors-25-05244"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Akan</surname><given-names>T.</given-names></name><name name-style="western"><surname>Alp</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bhuiyan</surname><given-names>M.A.N.</given-names></name></person-group><article-title>ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification</article-title><source>Proceedings of the 2023 International Conference on Computational Science and Computational Intelligence (CSCI)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>13&#8211;15 December 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2023</year><fpage>1412</fpage><lpage>1417</lpage></element-citation></ref><ref id="B33-sensors-25-05244"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Busia</surname><given-names>P.</given-names></name><name name-style="western"><surname>Scrugli</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>V.J.B.</given-names></name><name name-style="western"><surname>Benini</surname><given-names>L.</given-names></name><name name-style="western"><surname>Meloni</surname><given-names>P.</given-names></name></person-group><article-title>A tiny transformer for low-power arrhythmia classification on microcontrollers</article-title><source>IEEE Trans. Biomed. Circuits Syst.</source><year>2024</year><volume>19</volume><fpage>142</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1109/TBCAS.2024.3401858</pub-id><pub-id pub-id-type="pmid">40031438</pub-id></element-citation></ref><ref id="B34-sensors-25-05244"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ko</surname><given-names>H.</given-names></name><name name-style="western"><surname>Kyung</surname><given-names>Y.</given-names></name></person-group><article-title>TinyML-based classification in an ECG monitoring embedded system</article-title><source>Comput. Mater. Contin.</source><year>2023</year><volume>75</volume><fpage>1751</fpage><lpage>1764</lpage><pub-id pub-id-type="doi">10.32604/cmc.2023.031663</pub-id></element-citation></ref><ref id="B35-sensors-25-05244"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Alvarado Barros</surname><given-names>X.A.</given-names></name><name name-style="western"><surname>Sabagay Vizueta</surname><given-names>G.S.</given-names></name><name name-style="western"><surname>Criollo Bonilla</surname><given-names>R.R.</given-names></name></person-group><article-title>Desarrollo de un Sistema de Monitoreo Card&#237;aco Inteligente de bajo Costo Utilizando TinyML</article-title><source>Ph.D. Thesis</source><publisher-name>ESPOL, FIEC</publisher-name><publisher-loc>Guayaquil, Ecuador</publisher-loc><year>2025</year></element-citation></ref><ref id="B36-sensors-25-05244"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheung</surname><given-names>C.C.</given-names></name><name name-style="western"><surname>Krahn</surname><given-names>A.D.</given-names></name><name name-style="western"><surname>Andrade</surname><given-names>J.G.</given-names></name></person-group><article-title>The emerging role of wearable technologies in detection of arrhythmia</article-title><source>Can. J. Cardiol.</source><year>2018</year><volume>34</volume><fpage>1083</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1016/j.cjca.2018.05.003</pub-id><pub-id pub-id-type="pmid">30049358</pub-id></element-citation></ref><ref id="B37-sensors-25-05244"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Castaneda</surname><given-names>D.</given-names></name><name name-style="western"><surname>Esparza</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ghamari</surname><given-names>M.</given-names></name><name name-style="western"><surname>Soltanpur</surname><given-names>C.</given-names></name><name name-style="western"><surname>Nazeran</surname><given-names>H.</given-names></name></person-group><article-title>A review on wearable photoplethysmography sensors and their potential future applications in health care</article-title><source>Int. J. Biosens. Bioelectron.</source><year>2018</year><volume>4</volume><elocation-id>195</elocation-id><pub-id pub-id-type="doi">10.15406/ijbsbe.2018.04.00125</pub-id><pub-id pub-id-type="pmid">30906922</pub-id><pub-id pub-id-type="pmcid">PMC6426305</pub-id></element-citation></ref><ref id="B38-sensors-25-05244"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pay</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yumurta&#351;</surname><given-names>A.&#199;.</given-names></name><name name-style="western"><surname>Satti</surname><given-names>D.I.</given-names></name><name name-style="western"><surname>Hui</surname><given-names>J.M.H.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>J.S.K.</given-names></name><name name-style="western"><surname>Mahalwar</surname><given-names>G.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>Y.H.A.</given-names></name><name name-style="western"><surname>Tezen</surname><given-names>O.</given-names></name><name name-style="western"><surname>Birdal</surname><given-names>O.</given-names></name><name name-style="western"><surname>Inan</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Arrhythmias beyond atrial fibrillation detection using smartwatches: A systematic review</article-title><source>Anatol. J. Cardiol.</source><year>2023</year><volume>27</volume><fpage>126</fpage><pub-id pub-id-type="doi">10.14744/AnatolJCardiol.2023.2799</pub-id><pub-id pub-id-type="pmid">36856589</pub-id><pub-id pub-id-type="pmcid">PMC9995551</pub-id></element-citation></ref><ref id="B39-sensors-25-05244"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bog&#225;r</surname><given-names>B.</given-names></name><name name-style="western"><surname>Peto</surname><given-names>D.</given-names></name><name name-style="western"><surname>Sipos</surname><given-names>D.</given-names></name><name name-style="western"><surname>F&#252;redi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Keszthelyi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Betlehem</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pandur</surname><given-names>A.A.</given-names></name></person-group><article-title>Detection of arrhythmias using smartwatches&#8212;A systematic literature review</article-title><source>Healthcare</source><year>2024</year><volume>12</volume><elocation-id>892</elocation-id><pub-id pub-id-type="doi">10.3390/healthcare12090892</pub-id><pub-id pub-id-type="pmid">38727449</pub-id><pub-id pub-id-type="pmcid">PMC11083549</pub-id></element-citation></ref><ref id="B40-sensors-25-05244"><label>40.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Alcaraz Pomares</surname><given-names>A.</given-names></name></person-group><article-title>Revisi&#243;n Sistem&#225;tica de la Detecci&#243;n de Fibrilaci&#243;n Auricular Mediante el Electrocardiograma Realizado por Parte de los Dispositivos Inteligentes de Mu&#241;eca</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dspace.umh.es/bitstream/11000/29274/1/ALCARAZ%20POMARES%2C%20ANA%2C%20TFG.pdf" ext-link-type="uri">https://dspace.umh.es/bitstream/11000/29274/1/ALCARAZ%20POMARES%2C%20ANA%2C%20TFG.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-07-11">(accessed on 11 July 2025)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05244-f001" orientation="portrait"><label>Figure 1</label><caption><p>The pipeline of the proposed model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g001.jpg"/></fig><fig position="float" id="sensors-25-05244-f002" orientation="portrait"><label>Figure 2</label><caption><p>ECG signals from a random patient from the MIT-BIH arrhythmia database in a window of 1000 samples.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g002.jpg"/></fig><fig position="float" id="sensors-25-05244-f003" orientation="portrait"><label>Figure 3</label><caption><p>CNN architecture for MIT-BIH arrhythmia database (4-layer 1D CNN).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g003.jpg"/></fig><fig position="float" id="sensors-25-05244-f004" orientation="portrait"><label>Figure 4</label><caption><p>CNN architecture for UMass Medical School Simband dataset (4-layer 1D CNN).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g004.jpg"/></fig><fig position="float" id="sensors-25-05244-f005" orientation="portrait"><label>Figure 5</label><caption><p>ROC Curve, Precision&#8211;Recall Curve, and Confusion Matrix in UMass Medical School Simband dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g005.jpg"/></fig><fig position="float" id="sensors-25-05244-f006" orientation="portrait"><label>Figure 6</label><caption><p>Loss and accuracy history in the training model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g006.jpg"/></fig><fig position="float" id="sensors-25-05244-f007" orientation="portrait"><label>Figure 7</label><caption><p>Confusion Matrix and Precision&#8211;Recall Curve per class in MIT-BIH arrhythmia database.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g007.jpg"/></fig><fig position="float" id="sensors-25-05244-f008" orientation="portrait"><label>Figure 8</label><caption><p>Grad-CAM in the first input sample on the Simband dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g008.jpg"/></fig><fig position="float" id="sensors-25-05244-f009" orientation="portrait"><label>Figure 9</label><caption><p>Grad-CAM in the first input sample on the MIT-BIH arrhythmia database.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05244-g009.jpg"/></fig><table-wrap position="float" id="sensors-25-05244-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05244-t001_Table 1</object-id><label>Table 1</label><caption><p>A summary of the results of the different works tested on the MIT-BIH Arrhythmia dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Paper</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensitivity<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specificity<break/> (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Xia et al. [<xref rid="B25-sensors-25-05244" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">98.63</td><td align="center" valign="middle" rowspan="1" colspan="1">98.79</td><td align="center" valign="middle" rowspan="1" colspan="1">98.87</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ullah et al. [<xref rid="B26-sensors-25-05244" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">99.11</td><td align="center" valign="middle" rowspan="1" colspan="1">97.91</td><td align="center" valign="middle" rowspan="1" colspan="1">99.61</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Zubair et al. [<xref rid="B27-sensors-25-05244" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">96.36</td><td align="center" valign="middle" rowspan="1" colspan="1">70.60</td><td align="center" valign="middle" rowspan="1" colspan="1">96.16</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Rizqyawan et al. [<xref rid="B28-sensors-25-05244" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">DCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">90.22</td><td align="center" valign="middle" rowspan="1" colspan="1">35.64</td><td align="center" valign="middle" rowspan="1" colspan="1">87.87</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ojha et al. [<xref rid="B29-sensors-25-05244" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">99.53</td><td align="center" valign="middle" rowspan="1" colspan="1">98.24</td><td align="center" valign="middle" rowspan="1" colspan="1">97.58</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Jamil &amp; Rahman [<xref rid="B30-sensors-25-05244" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">DCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">94.84</td><td align="center" valign="middle" rowspan="1" colspan="1">100.00</td><td align="center" valign="middle" rowspan="1" colspan="1">99.60</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Chen et al. [<xref rid="B7-sensors-25-05244" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN + LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">99.32</td><td align="center" valign="middle" rowspan="1" colspan="1">97.50</td><td align="center" valign="middle" rowspan="1" colspan="1">98.70</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Hassan et al. [<xref rid="B8-sensors-25-05244" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN + BiLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">91.00</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Midani et al. [<xref rid="B31-sensors-25-05244" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN + BiLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">99.46</td><td align="center" valign="middle" rowspan="1" colspan="1">97.10</td><td align="center" valign="middle" rowspan="1" colspan="1">99.57</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Alamatsaz et al. [<xref rid="B9-sensors-25-05244" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN + LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">98.24</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Akan et al. [<xref rid="B32-sensors-25-05244" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">Transformer-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Islam et al. [<xref rid="B4-sensors-25-05244" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">Transformer-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">99.14</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">El-Ghaish et al. [<xref rid="B6-sensors-25-05244" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">Transformer-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">99.35</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Kim et al. [<xref rid="B5-sensors-25-05244" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">Transformer-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">99.58</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Busia et al. [<xref rid="B33-sensors-25-05244" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">TinyML</td><td align="center" valign="middle" rowspan="1" colspan="1">98.97</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kim et al. [<xref rid="B34-sensors-25-05244" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TinyML</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05244-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05244-t002_Table 2</object-id><label>Table 2</label><caption><p>A summary of the results of different works tested on smartwatch datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Paper</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Patients</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensitivity<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specificity<break/> (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Avran et al. [<xref rid="B12-sensors-25-05244" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Samsung Galaxy Watch 2</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine<break/>Learning</td><td align="center" valign="middle" rowspan="1" colspan="1">204</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">97</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ploux et al. [<xref rid="B13-sensors-25-05244" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Apple Watch 4</td><td align="left" valign="middle" rowspan="1" colspan="1">DNN</td><td align="center" valign="middle" rowspan="1" colspan="1">260</td><td align="center" valign="middle" rowspan="1" colspan="1">92</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">94</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ford et al. [<xref rid="B14-sensors-25-05244" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Apple Watch 4</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine<break/>Learning</td><td align="center" valign="middle" rowspan="1" colspan="1">125</td><td align="center" valign="middle" rowspan="1" colspan="1">87</td><td align="center" valign="middle" rowspan="1" colspan="1">68</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Abu-Alrub et al. [<xref rid="B15-sensors-25-05244" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Samsung Galaxy Watch 3</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine<break/>Learning</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">81</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Wasserlauf et al. [<xref rid="B16-sensors-25-05244" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Apple Watch 4</td><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">250</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">99</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mannhart et al. [<xref rid="B17-sensors-25-05244" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smartwatch ECG<break/>Samsung Galaxy Watch 4</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">201</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05244-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05244-t003_Table 3</object-id><label>Table 3</label><caption><p>Results in UMass Medical School Simband dataset per class.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Class</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensitivity<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specificity<break/> (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">(N) Normal</td><td align="center" valign="middle" rowspan="1" colspan="1">64.81</td><td align="center" valign="middle" rowspan="1" colspan="1">6.25</td><td align="center" valign="middle" rowspan="1" colspan="1">89.47</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(A) Arrhythmia</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.25</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05244-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05244-t004_Table 4</object-id><label>Table 4</label><caption><p>Results in MIT-BIH arrhythmia database per class.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Class</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensitivity<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specificity<break/> (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">N</td><td align="center" valign="middle" rowspan="1" colspan="1">99.62</td><td align="center" valign="middle" rowspan="1" colspan="1">99.77</td><td align="center" valign="middle" rowspan="1" colspan="1">99.19</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">L</td><td align="center" valign="middle" rowspan="1" colspan="1">99.98</td><td align="center" valign="middle" rowspan="1" colspan="1">99.94</td><td align="center" valign="middle" rowspan="1" colspan="1">99.98</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">R</td><td align="center" valign="middle" rowspan="1" colspan="1">99.98</td><td align="center" valign="middle" rowspan="1" colspan="1">99.94</td><td align="center" valign="middle" rowspan="1" colspan="1">99.98</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">A</td><td align="center" valign="middle" rowspan="1" colspan="1">99.70</td><td align="center" valign="middle" rowspan="1" colspan="1">93.19</td><td align="center" valign="middle" rowspan="1" colspan="1">99.87</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">V</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.93</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05244-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05244-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of our binary model with the state-of-the-art proposals using the UMass Medical School Simband dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Paper</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Patients</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensitivity<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specificity<break/> (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Avran et al. [<xref rid="B12-sensors-25-05244" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Samsung Galaxy Watch 2</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine<break/>Learning</td><td align="center" valign="middle" rowspan="1" colspan="1">204</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">97</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ploux et al. [<xref rid="B13-sensors-25-05244" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Apple Watch 4</td><td align="left" valign="middle" rowspan="1" colspan="1">DNN</td><td align="center" valign="middle" rowspan="1" colspan="1">260</td><td align="center" valign="middle" rowspan="1" colspan="1">92</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">94</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ford et al. [<xref rid="B14-sensors-25-05244" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Apple Watch 4</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine<break/>Learning</td><td align="center" valign="middle" rowspan="1" colspan="1">125</td><td align="center" valign="middle" rowspan="1" colspan="1">87</td><td align="center" valign="middle" rowspan="1" colspan="1">68</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Abu-Alrub et al. [<xref rid="B15-sensors-25-05244" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Samsung Galaxy Watch 3</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine<break/>Learning</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">81</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Wasserlauf et al. [<xref rid="B16-sensors-25-05244" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Apple Watch 4</td><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">250</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">99</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Mannhart et al. [<xref rid="B17-sensors-25-05244" ref-type="bibr">17</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="left" valign="middle" rowspan="1" colspan="1">Smartwatch ECG<break/>Samsung Galaxy Watch 4</td><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">201</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">58</td><td align="center" valign="middle" rowspan="1" colspan="1">75</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Our Proposal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2025</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smartwatch ECG<break/>Samsung Simband 2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.25</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05244-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05244-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of our multiclass model with the state-of-the-art proposals using the MIT-BIH arrhythmia database.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Paper</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensitivity<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specificity<break/> (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Xia et al. [<xref rid="B25-sensors-25-05244" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">98.63</td><td align="center" valign="middle" rowspan="1" colspan="1">98.79</td><td align="center" valign="middle" rowspan="1" colspan="1">98.87</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ullah et al. [<xref rid="B26-sensors-25-05244" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">99.11</td><td align="center" valign="middle" rowspan="1" colspan="1">97.91</td><td align="center" valign="middle" rowspan="1" colspan="1">99.61</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Zubair et al. [<xref rid="B27-sensors-25-05244" ref-type="bibr">27</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">96.36</td><td align="center" valign="middle" rowspan="1" colspan="1">70.60</td><td align="center" valign="middle" rowspan="1" colspan="1">96.16</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Rizqyawan et al. [<xref rid="B28-sensors-25-05244" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">DCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">90.22</td><td align="center" valign="middle" rowspan="1" colspan="1">35.64</td><td align="center" valign="middle" rowspan="1" colspan="1">87.87</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ojha et al. [<xref rid="B29-sensors-25-05244" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">99.53</td><td align="center" valign="middle" rowspan="1" colspan="1">98.24</td><td align="center" valign="middle" rowspan="1" colspan="1">97.58</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Jamil &amp; Rahman [<xref rid="B30-sensors-25-05244" ref-type="bibr">30</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">DCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">94.84</td><td align="center" valign="middle" rowspan="1" colspan="1">100.00</td><td align="center" valign="middle" rowspan="1" colspan="1">99.60</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Chen et al. [<xref rid="B7-sensors-25-05244" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN + LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">99.32</td><td align="center" valign="middle" rowspan="1" colspan="1">97.50</td><td align="center" valign="middle" rowspan="1" colspan="1">98.70</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Hassan et al. [<xref rid="B8-sensors-25-05244" ref-type="bibr">8</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN + BiLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">91.00</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Midani et al. [<xref rid="B31-sensors-25-05244" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN + BiLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">99.46</td><td align="center" valign="middle" rowspan="1" colspan="1">97.10</td><td align="center" valign="middle" rowspan="1" colspan="1">99.57</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Alamatsaz et al. [<xref rid="B9-sensors-25-05244" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">CNN + LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">98.24</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Akan et al. [<xref rid="B32-sensors-25-05244" ref-type="bibr">32</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">Transformer-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Islam et al. [<xref rid="B4-sensors-25-05244" ref-type="bibr">4</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">Transformer-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">99.14</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">El-Ghaish et al. [<xref rid="B6-sensors-25-05244" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">Transformer-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">99.35</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Kim et al. [<xref rid="B5-sensors-25-05244" ref-type="bibr">5</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">Transformer-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">99.58</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Busia et al. [<xref rid="B33-sensors-25-05244" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">TinyML</td><td align="center" valign="middle" rowspan="1" colspan="1">98.97</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Kim et al. [<xref rid="B34-sensors-25-05244" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">TinyML</td><td align="center" valign="middle" rowspan="1" colspan="1">97.00</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Our Proposal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.47</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05244-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05244-t007_Table 7</object-id><label>Table 7</label><caption><p>Variants of the model tested on the UMass Medical School Simband dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Variant</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensitivity<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specificity<break/> (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">With Leave-One-Subject-Out Cross-Validation (LOSO) and without resampling (proposal)</td><td align="center" valign="middle" rowspan="1" colspan="1">64.81</td><td align="center" valign="middle" rowspan="1" colspan="1">89.47</td><td align="center" valign="middle" rowspan="1" colspan="1">6.25</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">With Leave-One-Subject-Out Cross-Validation (LOSO) and with resampling</td><td align="center" valign="middle" rowspan="1" colspan="1">66.05</td><td align="center" valign="middle" rowspan="1" colspan="1">98.72</td><td align="center" valign="middle" rowspan="1" colspan="1">52.10</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Without Leave-One-Subject-Out Cross-Validation (LOSO) and with resampling</td><td align="center" valign="middle" rowspan="1" colspan="1">98.73</td><td align="center" valign="middle" rowspan="1" colspan="1">98.73</td><td align="center" valign="middle" rowspan="1" colspan="1">97.52</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">With three-layer Conv1D</td><td align="center" valign="middle" rowspan="1" colspan="1">43.21</td><td align="center" valign="middle" rowspan="1" colspan="1">59.21</td><td align="center" valign="middle" rowspan="1" colspan="1">52.10</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Size kernel (13, 15, 17, 19)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.04</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05244-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05244-t008_Table 8</object-id><label>Table 8</label><caption><p>Variants of the model tested on the MIT-BIH arrhythmia database.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Variant</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensitivity<break/> (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specificity<break/> (%)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">with Leave-One-Subject-Out cross-validation (LOSO) and without resampling (proposal)</td><td align="center" valign="middle" rowspan="1" colspan="1">99.57</td><td align="center" valign="middle" rowspan="1" colspan="1">99.57</td><td align="center" valign="middle" rowspan="1" colspan="1">99.47</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">with Leave-One-Subject-Out cross-validation (LOSO) and with resampling</td><td align="center" valign="middle" rowspan="1" colspan="1">98.69</td><td align="center" valign="middle" rowspan="1" colspan="1">98.69</td><td align="center" valign="middle" rowspan="1" colspan="1">98.38</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">without Leave-One-Subject-Out cross-validation (LOSO) and with resampling</td><td align="center" valign="middle" rowspan="1" colspan="1">99.64</td><td align="center" valign="middle" rowspan="1" colspan="1">99.64</td><td align="center" valign="middle" rowspan="1" colspan="1">99.90</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">with three-layer Conv1D</td><td align="center" valign="middle" rowspan="1" colspan="1">99.47</td><td align="center" valign="middle" rowspan="1" colspan="1">98.11</td><td align="center" valign="middle" rowspan="1" colspan="1">99.75</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">size kernel (13, 15, 17, 19)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.77</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>