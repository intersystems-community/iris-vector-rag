<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="review-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431044</article-id><article-id pub-id-type="pmcid-ver">PMC12431044.1</article-id><article-id pub-id-type="pmcaid">12431044</article-id><article-id pub-id-type="pmcaiid">12431044</article-id><article-id pub-id-type="doi">10.3390/s25175264</article-id><article-id pub-id-type="publisher-id">sensors-25-05264</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>A Survey of Deep Learning-Based 3D Object Detection Methods for Autonomous Driving Across Different Sensor Modalities</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-0864-5474</contrib-id><name name-style="western"><surname>Valverde</surname><given-names initials="M">Miguel</given-names></name><xref rid="af1-sensors-25-05264" ref-type="aff">1</xref><xref rid="c1-sensors-25-05264" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4271-7996</contrib-id><name name-style="western"><surname>Moutinho</surname><given-names initials="A">Alexandra</given-names></name><xref rid="af2-sensors-25-05264" ref-type="aff">2</xref><xref rid="c1-sensors-25-05264" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-6975-6829</contrib-id><name name-style="western"><surname>Zacchi</surname><given-names initials="JV">Jo&#227;o-Vitor</given-names></name><xref rid="af3-sensors-25-05264" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Meli</surname><given-names initials="E">Enrico</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05264"><label>1</label>Instituto Superior T&#233;cnico, Universidade de Lisboa, 1049-001 Lisbon, Portugal</aff><aff id="af2-sensors-25-05264"><label>2</label>Instituto de Engenharia Mec&#226;nica, Instituto Superior T&#233;cnico, Universidade de Lisboa, 1049-001 Lisbon, Portugal</aff><aff id="af3-sensors-25-05264"><label>3</label>Fraunhofer IKS, 80686 Munich, Germany; <email>joao-vitor.zacchi@iks.fraunhofer.de</email></aff><author-notes><corresp id="c1-sensors-25-05264"><label>*</label>Correspondence: <email>miguel.heitor.valverde@tecnico.ulisboa.pt</email> (M.V.); <email>alexandra.moutinho@tecnico.ulisboa.pt</email> (A.M.)</corresp></author-notes><pub-date pub-type="epub"><day>24</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5264</elocation-id><history><date date-type="received"><day>07</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>29</day><month>7</month><year>2025</year></date><date date-type="accepted"><day>19</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>24</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05264.pdf"/><abstract><p>This paper presents a comprehensive survey of deep learning-based methods for 3D object detection in autonomous driving, focusing on their use of diverse sensor modalities, including monocular cameras, stereo vision, LiDAR, radar, and multi-modal fusion. To systematically organize the literature, a structured taxonomy is proposed that categorizes methods by input modality. The review also outlines the chronological evolution of these approaches, highlighting major architectural developments and paradigm shifts. Furthermore, the surveyed methods are quantitatively compared using standard evaluation metrics across benchmark datasets in autonomous driving scenarios. Overall, this work provides a detailed and modality-agnostic overview of the current landscape of deep learning approaches for 3D object detection in autonomous driving. Results of this work are available in a github open repository.</p></abstract><kwd-group><kwd>3D object detection</kwd><kwd>deep learning</kwd><kwd>monocular camera</kwd><kwd>stereo vision</kwd><kwd>LiDAR</kwd><kwd>radar</kwd><kwd>sensor fusion</kwd><kwd>KITTI</kwd><kwd>nuScenes</kwd><kwd>Waymo</kwd><kwd>autonomous vehicles</kwd></kwd-group><funding-group><award-group><funding-source>Funda&#231;&#227;o para a Ci&#234;ncia e a Tecnologia (FCT) for its financial support via the project LAETA Base Funding</funding-source></award-group><funding-statement>A.M. acknowledges Funda&#231;&#227;o para a Ci&#234;ncia e a Tecnologia (FCT) for its financial support via the project LAETA Base Funding (DOI: 10.54499/UIDB/50022/2020). For the purpose of Open Access, the authors have applied a CC-BY public copyright license to any Author&#8216;s Accepted Manuscript (AAM) version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05264"><title>1. Introduction</title><sec id="sec1dot1-sensors-25-05264"><title>1.1. Overview of Autonomous Vehicles</title><p>Autonomous vehicles (AVs) have emerged as a transformative paradigm in mobility, with the potential to reshape how people and goods are transported and how societies function. Over the past decade, significant advances in sensor technology, computational power, and machine learning have enabled the transition from early concepts to real-world deployment. AVs are now being tested and operated in urban areas, industrial settings, and controlled environments worldwide. Governments and companies continue to invest heavily in their development, driven by the promise of reducing traffic accidents, lowering emissions, and improving effectiveness and efficiency in the transportation of people.</p><p>According to the Society of Automotive Engineers&#8217; (SAE) norm for driving automation [<xref rid="B1-sensors-25-05264" ref-type="bibr">1</xref>], there are six distinct levels (illustrated in <xref rid="sensors-25-05264-f001" ref-type="fig">Figure 1</xref>), ranging from level 0, where the driver is in command of the car, to level 5, where the vehicle assumes complete control over all driving aspects. Levels 1 and 2 of automation, which include advanced driver assistance systems (ADASs) such as braking assistance, cruise control, and lane switching [<xref rid="B2-sensors-25-05264" ref-type="bibr">2</xref>], have now practically become standard in many cars. Safety concerns primarily drove their adoption but also paved the way for the development of more advanced systems.</p><p>For a long time, the automotive industry was stuck at SAE Level 2; however, vehicles with Level 3 automation have recently started entering the market. Notably, the Mercedes S-Class achieved Level 3 certification in Germany and in the United States, while the Honda Legend received the same certification in Japan [<xref rid="B3-sensors-25-05264" ref-type="bibr">3</xref>]. Moreover, robotaxis are already operating at Level 4 [<xref rid="B4-sensors-25-05264" ref-type="bibr">4</xref>], though some argue they can be considered Level 5 due to their full autonomy in constrained environments [<xref rid="B5-sensors-25-05264" ref-type="bibr">5</xref>].</p><fig position="anchor" id="sensors-25-05264-f001" orientation="portrait"><label>Figure 1</label><caption><p>SAE levels of automation [<xref rid="B1-sensors-25-05264" ref-type="bibr">1</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05264-g001.jpg"/></fig><p>Safety is a key driver of AD growth. Studies such as [<xref rid="B6-sensors-25-05264" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05264" ref-type="bibr">7</xref>] highlight that approximately 90% to 94% of accidents are caused by human error. AVs aim to mitigate these risks by eliminating unsafe driver behaviours like distraction and speeding. Additionally, AVs are expected to reduce carbon emissions, smooth traffic flow, increase productivity, and have a positive economic impact. A study from America&#8217;s Workforce and Self-Driving Future [<xref rid="B8-sensors-25-05264" ref-type="bibr">8</xref>] predicts that widespread adoption of AVs could lead to nearly $800 billion in annual social and economic benefits by 2050. Another study [<xref rid="B9-sensors-25-05264" ref-type="bibr">9</xref>] states that autonomous technologies can reduce urban travel time by a third, decrease greenhouse gas emissions by two-thirds, decrease the number of vehicles in crowded cities by 30%, and decrease the need for parking spaces by 40%.</p><p>Autonomous driving (AD), computer vision (CV), robotics, and machine learning (ML) are among the most prominent areas of research at the moment [<xref rid="B10-sensors-25-05264" ref-type="bibr">10</xref>]. AVs are the next step in the evolution of the automobile, as the industry is noticeably moving towards a world where less or even no human interaction is required. <xref rid="sensors-25-05264-f002" ref-type="fig">Figure 2</xref>a illustrates the trajectory of publications in AD since 1964, demonstrating a notable surge at the beginning of the 21st century. <xref rid="sensors-25-05264-f002" ref-type="fig">Figure 2</xref>b zooms on the last two decades (2004 to 2024), revealing a consistent annual publication growth. Over this period, the number of scientific publications on AD published by year increased from 2828 to 78,602, marking an almost 30-fold increase.</p><fig position="anchor" id="sensors-25-05264-f002" orientation="portrait"><label>Figure 2</label><caption><p>Number of publications per year on autonomous vehicles. Source Scopus&#8212;Elsevier (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.scopus.com/home.uri">https://www.scopus.com/home.uri</uri>&#8212; advanced search using (self-driving car) OR (autonomous car) keywords on 25 April 2025).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05264-g002.jpg"/></fig><p>The anticipated benefits of a global shift toward full or partial AVs are widely recognized. However, achieving large-scale deployment remains a complex challenge and is far from imminent. Despite substantial investment and rapid technological progress, fully automated vehicles still face significant obstacles across multiple domains. Concerns over AV safety remain widespread, particularly among vulnerable groups such as individuals with disabilities [<xref rid="B11-sensors-25-05264" ref-type="bibr">11</xref>]. Moreover, documented instances of algorithmic bias, including decisions influenced by race, further complicate public trust and acceptance [<xref rid="B12-sensors-25-05264" ref-type="bibr">12</xref>].</p><p>Not only ethical but technical challenges still persist. Ensuring reliable performance under adverse weather conditions, such as heavy rain, snow, or fog, and in difficult lighting environments, including night-time driving, tunnels, and shaded areas, remains difficult. These scenarios can impair sensor accuracy and disrupt vehicle control systems, underscoring the need for more robust algorithms and advanced sensor fusion techniques to guarantee dependable operation across varied environments [<xref rid="B10-sensors-25-05264" ref-type="bibr">10</xref>]. Given the substantial societal and safety benefits AVs promise, it is critical to address both technical and ethical barriers to enable their successful integration into modern transportation systems.</p><p>An AV must perform all the functions a human driver would, including perceiving the environment, determining its position, anticipating the actions of other road users, planning a trajectory, and executing control commands to follow that trajectory. To achieve this, AV software is typically organized into distinct modules and submodules. While variations exist, a common framework follows the Sense&#8211;Plan&#8211;Act paradigm [<xref rid="B13-sensors-25-05264" ref-type="bibr">13</xref>], illustrated in <xref rid="sensors-25-05264-f003" ref-type="fig">Figure 3</xref>, and includes the following components: Sensors, Perception, Planning, Control, and Car.</p><p>The Sensors module is responsible for acquiring data from a range of sensors, including cameras, Radio Detection and Ranging (radar), Light Detection and Ranging (LiDAR), Global Navigation Satellite Systems (GNSSs), and inertial sensors. The Perception module interprets these data to detect objects and estimate the state of the vehicle. Based on the output of perception, the vehicle plans a trajectory and generates control actions, such as steering and acceleration, to follow the planned path [<xref rid="B14-sensors-25-05264" ref-type="bibr">14</xref>].</p><p>Some of the most complex challenges in autonomous driving stem from the perception stage, which relies on a combination of advanced sensors and state-of-the-art (SoA) algorithms. As an initial stage in the AV software pipeline, the performance of the Perception module directly affects all downstream tasks. Therefore, a thorough understanding of the various techniques used in perception is essential to advance reliable and safe autonomous driving systems.</p><fig position="anchor" id="sensors-25-05264-f003" orientation="portrait"><label>Figure 3</label><caption><p>Typical autonomous vehicle pipeline [<xref rid="B15-sensors-25-05264" ref-type="bibr">15</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05264-g003.jpg"/></fig></sec><sec id="sec1dot2-sensors-25-05264"><title>1.2. Scope, Aims, and Outline</title><p>Three-dimensional object detection plays a central role in autonomous driving by enabling the vehicle to understand and interact with its environment in three dimensions. It involves estimating the position, dimensions, and class of objects in the environment using sensor data. Modern perception systems typically rely on multimodal inputs, such as images from cameras, point clouds from LiDAR, or a combination of both, to extract geometric and semantic information from the scene. With the rapid progress of deep learning (DL), a wide range of 3D object detection methods have emerged, each tailored to specific sensor modalities and data representations. However, these methods are often developed and evaluated in isolation, lacking a unified perspective that spans different input types and architectural paradigms. This fragmentation makes it difficult to assess their relative strengths and limitations, particularly in the context of real-world autonomous driving scenarios. To address this gap, this work presents a structured and comparative review of 3D object detection methods across all major sensor modalities, aiming to clarify performance trends, highlight paradigm shifts, and guide future developments in the field.</p><p>This work aims to provide a comprehensive review of 3D object detection methods tailored for AD, offering an in-depth analysis and structured comparison of different approaches and associated sensor modalities. Unlike existing surveys, our review includes more recent developments and evaluates methods across all types of sensory and application contexts. Although some previous works, such as [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05264" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05264" ref-type="bibr">18</xref>] present similar structures, they were published before 2023 and cover older methods. More recent surveys like [<xref rid="B19-sensors-25-05264" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-05264" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05264" ref-type="bibr">21</xref>] either lack breadth or fail to benchmark and compare methods on standard datasets. Other studies focus narrowly on specific modalities, such as [<xref rid="B22-sensors-25-05264" ref-type="bibr">22</xref>] for image-based methods, [<xref rid="B23-sensors-25-05264" ref-type="bibr">23</xref>] for point cloud-based DL methods, and [<xref rid="B24-sensors-25-05264" ref-type="bibr">24</xref>] for multi-modal techniques. Transformer-based approaches are explored in [<xref rid="B25-sensors-25-05264" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05264" ref-type="bibr">26</xref>].</p><p>This survey presents the first unified and structured review of recent 3D object detection methods using DL across all major sensor modalities. It introduces a refined taxonomy and highlights performance trends and paradigm shifts, to support fair and systematic evaluation in future research. The survey covers developments over the past decade, focusing particularly on methods published in top-tier computer vision venues. Alongside technical insights into 3D detectors, the paper discusses taxonomies, benchmark datasets, evaluation protocols, and open challenges. More specifically, the following contributions are provided:<list list-type="bullet"><list-item><p>The context for the task of 3D object detection, presenting its formulation, the sensor modalities required for it, and finally by presenting benchmark datasets and their respective evaluation metrics.</p></list-item><list-item><p>A comprehensive literature review of camera-based, LiDAR-based, radar-based and multimodal-based 3D perception methods, including an updated taxonomy and discussion of their evolution.</p></list-item><list-item><p>A performance and speed benchmark of selected 3D object detectors using standard datasets and evaluation metrics.</p></list-item></list></p><p>An updated project page listing all methods covered in this work is also maintained at (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://3d-object-detection-hub.github.io/">https://3d-object-detection-hub.github.io/</uri>, accessed on 25 April 2025).</p><p>The structure of this paper is organized as follows. <xref rid="sec2-sensors-25-05264" ref-type="sec">Section 2</xref> introduces the 3D object detection problem, followed by typical data representations and different sensor types. Then, <xref rid="sec3-sensors-25-05264" ref-type="sec">Section 3</xref> summarizes the commonly used datasets and metrics for 3D object detection. <xref rid="sec4-sensors-25-05264" ref-type="sec">Section 4</xref> presents the taxonomy of the methods considered and highlights the major paradigm shifts and developments. Finally, a speed and performance analysis is provided in <xref rid="sec5-sensors-25-05264" ref-type="sec">Section 5</xref>. <xref rid="sec6-sensors-25-05264" ref-type="sec">Section 6</xref> offers concluding remarks and discusses future research trends.</p></sec></sec><sec id="sec2-sensors-25-05264"><title>2. Background</title><sec id="sec2dot1-sensors-25-05264"><title>2.1. Problem Definition</title><p>Object detection (OD) is a fundamental task in CV, aiming to identify and localize objects of interest from sensory data such as images or point clouds (PCs) [<xref rid="B27-sensors-25-05264" ref-type="bibr">27</xref>]. It typically involves a two-stage process: classifying object types and regressing bounding box parameters. While 2D OD has achieved strong performance in benchmarks like KITTI [<xref rid="B28-sensors-25-05264" ref-type="bibr">28</xref>], its lack of depth information limits its usefulness in autonomous driving, particularly for critical tasks such as motion planning and collision avoidance [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>].</p><p>Three-dimensional OD addresses these shortcomings by incorporating depth, allowing objects to be located and characterized in three-dimensional space. This additional dimensionality improves spatial reasoning but introduces greater computational demands and requires high-quality sensory inputs. Unlike its 2D counterpart, 3D OD still faces major challenges, including fewer large-scale annotated datasets and less standardized model architectures [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>]. Additionally, estimating depth from monocular images remains an inherently ill-posed problem, as multiple 3D scenes can project to similar 2D views [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>,<xref rid="B30-sensors-25-05264" ref-type="bibr">30</xref>].</p><p>Formally, 3D OD aims to infer a set of bounding boxes <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from sensory input <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>sensor</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, using a detection model <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>det</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD1-sensors-25-05264"><label>(1)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>det</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>sensor</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Each detected object <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is typically represented as:<disp-formula id="FD2-sensors-25-05264"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>&#952;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the centre coordinates, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the bounding box dimensions length, width, and height, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> is the rotation angle around the z-axis in the world frame, and <italic toggle="yes">c</italic> represents the object category [<xref rid="B18-sensors-25-05264" ref-type="bibr">18</xref>]. The bounding boxes are usually parametrized in sensor-centric or vehicle-centric coordinate frames to support downstream tasks such as object tracking, motion prediction, and trajectory planning [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>].</p></sec><sec id="sec2dot2-sensors-25-05264"><title>2.2. Data Representation</title><p>Three-dimensional data provides richer spatial structure than 2D imagery and can be represented either implicitly, via depth cues embedded in images, or explicitly, through direct geometric encoding. Implicit representations include multi-view images, RGB-D formats that append depth maps to RGB channels, and light fields, which encode light rays as functions of spatial and angular dimensions [<xref rid="B31-sensors-25-05264" ref-type="bibr">31</xref>].</p><p>Explicit representations are more common in 3D OD and include the following:<list list-type="bullet"><list-item><p><bold>Voxels</bold>, which discretize 3D space into volumetric grids;</p></list-item><list-item><p><bold>Point clouds</bold>, composed of unordered 3D points <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, sometimes augmented with intensity or reflectance values;</p></list-item><list-item><p><bold>Meshes</bold>, which represent object surfaces through vertices, edges, and faces [<xref rid="B31-sensors-25-05264" ref-type="bibr">31</xref>].</p></list-item></list></p><p>Among these, point clouds, particularly those obtained from LiDAR, are the most widely used input for 3D OD due to their high geometric fidelity. However, their sparse and irregular structure poses challenges for conventional DL models. Voxelization addresses this by converting point clouds into regular 3D grids, enabling the use of 3D convolutions. Yet, this comes with trade-offs: small voxels increase memory and computation costs cubically, while large voxels reduce spatial resolution and accuracy [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>,<xref rid="B32-sensors-25-05264" ref-type="bibr">32</xref>].</p><p>To balance efficiency and detail, pillar-based representations simplify voxelization by collapsing the vertical dimension, effectively projecting the 3D point cloud into a pseudo-image format. This allows 2D convolutional neural networks to be used while preserving horizontal spatial structure, significantly improving computational efficiency without sacrificing detection accuracy [<xref rid="B31-sensors-25-05264" ref-type="bibr">31</xref>].</p></sec><sec id="sec2dot3-sensors-25-05264"><title>2.3. Sensors</title><p>Several types of sensors can provide raw data for 3D OD. The most widely adopted sensors in AVs are radar, LiDAR, and cameras. Their integration is essential for environmental perception, directly impacting system safety and reliability.</p><p>Based on their interaction with the environment, sensors are divided into active and passive types. Active sensors, such as radar and LiDAR, emit energy signals and measure the reflected responses. Passive sensors, such as cameras, capture ambient energy without emitting any signal. Alternatively, sensors can also be categorized based on the type of information they measure:<list list-type="bullet"><list-item><p><bold>Exteroceptive sensors</bold> measure external variables and observe the surrounding environment. Examples include stereo, flash, infrared, and thermal cameras, as well as radar, LiDAR, and sonar [<xref rid="B33-sensors-25-05264" ref-type="bibr">33</xref>].</p></list-item><list-item><p><bold>Proprioceptive sensors</bold> measure variables related to the vehicle state, providing information about its position, velocity, orientation, and acceleration. Examples include global navigation satellite systems (GNSSs), inertial measurement units (IMUs), ground speed sensors (GSSs), encoders, gyroscopes, and accelerometers [<xref rid="B33-sensors-25-05264" ref-type="bibr">33</xref>].</p></list-item></list></p><p>While some proprioceptive information may support perception tasks, these sensors are primarily used for vehicle state estimation. In contrast, exteroceptive sensors are mainly responsible for detecting static and dynamic objects in the environment. <xref rid="sensors-25-05264-t001" ref-type="table">Table 1</xref> summarizes the main categories of exteroceptive sensors and compares them based on data from [<xref rid="B2-sensors-25-05264" ref-type="bibr">2</xref>,<xref rid="B33-sensors-25-05264" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05264" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-05264" ref-type="bibr">35</xref>].</p><p>It is important to note that <xref rid="sensors-25-05264-t001" ref-type="table">Table 1</xref> provides only a qualitative overview. The classification depends on the specific application and the relative comparison between the considered sensors. Moreover, performance can vary significantly across different models and manufacturers. Therefore, the table should serve only as a general guideline for assessing trade-offs between different sensing modalities. In the following, these trade-offs are further examined and discussed in detail for each individual sensor modality.</p><list list-type="simple"><list-item><p><bold>Monocular cameras</bold> are passive sensors that capture rich appearance information, including texture and colour, at low cost and high resolution. They produce images <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>cam</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, but cannot directly recover depth, limiting their 3D localization capabilities [<xref rid="B2-sensors-25-05264" ref-type="bibr">2</xref>,<xref rid="B18-sensors-25-05264" ref-type="bibr">18</xref>]. Their performance deteriorates under adverse lighting conditions such as night-time, glare, fog, or rain [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>].</p></list-item></list><list list-type="simple"><list-item><p><bold>Stereo vision</bold> systems estimate depth by triangulating points based on the disparity between images captured by two horizontally aligned cameras, enhancing 3D understanding. However, they require precise calibration and are sensitive to low-texture regions and lighting variations [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>,<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>]. Other systems, such as Time-of-Flight cameras, infer depth using infrared pulses but offer lower resolution, while RGB-D sensors like Kinect combine colour and depth for a more complete spatial view.</p></list-item></list><list list-type="simple"><list-item><p><bold>Infrared cameras</bold> detect infrared radiation, including thermal emissions and other wavelengths of the electromagnetic spectrum. They enable perception in dark or low-visibility conditions but typically provide low resolution and are less effective for detailed object classification.</p></list-item></list><list list-type="simple"><list-item><p><bold>Sonar and ultrasonic</bold> sensors emit sound waves to detect nearby obstacles. They are compact, inexpensive, and reliable at short range, but provide low spatial resolution and are unsuitable for complex 3D perception tasks [<xref rid="B2-sensors-25-05264" ref-type="bibr">2</xref>].</p></list-item></list><list list-type="simple"><list-item><p><bold>Radar</bold> systems emit electromagnetic waves and detect their reflections to measure the position and relative velocity of objects using the Doppler effect. They provide long-range robustness and function reliably in adverse weather, although their angular resolution is lower, making fine-grained object detection more challenging [<xref rid="B33-sensors-25-05264" ref-type="bibr">33</xref>].</p></list-item></list><list list-type="simple"><list-item><p><bold>LiDAR</bold> sensors actively scan the environment with laser beams to generate detailed 3D PCs. A typical LiDAR unit emitting <italic toggle="yes">m</italic> beams over <italic toggle="yes">n</italic> rotations produces a range image <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>range</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which can be converted into a PC <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>point</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B18-sensors-25-05264" ref-type="bibr">18</xref>,<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>]. LiDAR provides high spatial accuracy independent of light conditions, although it remains relatively expensive and can be affected by environmental factors like fog, rain, or snow [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>].</p></list-item></list><p>Each sensing modality presents unique trade-offs. Monocular cameras offer high semantic richness at a low cost, but they lack direct depth sensing and are sensitive to environmental conditions [<xref rid="B2-sensors-25-05264" ref-type="bibr">2</xref>,<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>]. Stereo systems incorporate depth information but are still vulnerable to lighting and texture challenges [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>,<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>]. Infrared cameras are advantageous in darkness but typically offer lower resolution, while sonar sensors are reliable for close-range detection yet inadequate for detailed 3D mapping [<xref rid="B2-sensors-25-05264" ref-type="bibr">2</xref>]. Infrared and sonar sensors are often used in Unmanned Aerial Vehicles or as complementary components rather than as primary sensors for object detection in autonomous vehicles.</p><p>For long-range and high-precision sensing, radar and LiDAR are the primary choices. Radar ensures reliable detection under adverse weather while providing velocity information via the Doppler effect [<xref rid="B33-sensors-25-05264" ref-type="bibr">33</xref>]. LiDAR generates highly detailed 3D spatial data, but at a higher cost and with some sensitivity to atmospheric disturbances [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>,<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>].</p><p>Although actual computational cost depends on the specific algorithms used, considering the most commonly adopted methods, sensors can be broadly classified by the computational cost they impose. Monocular and stereo cameras typically incur high computational costs due to high-resolution image processing, making real-time edge-device deployment more challenging, whereas sensors such as sonar, radar, and LiDAR often impose lower-to-medium computational overhead, better suiting resource-constrained applications.</p><p>In the context of AVs, the most common perception stack combines cameras, radar, and LiDAR. Sensor fusion is, therefore, crucial for merging semantic understanding, thereby maximizing robustness against adverse conditions and the advantages of each sensor [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>]. Reliable sensor fusion demands precise calibration [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>] and careful balancing between cost, complexity, and robustness. By integrating complementary strengths, fusion also introduces redundancy, essential for ensuring safe autonomous operation [<xref rid="B33-sensors-25-05264" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05264" ref-type="bibr">34</xref>].</p></sec></sec><sec id="sec3-sensors-25-05264"><title>3. Datasets and Evaluation Metrics</title><sec id="sec3dot1-sensors-25-05264"><title>3.1. Benchmark Datasets</title><p>Numerous datasets have been developed to support the training and evaluation of DL models in CV. These benchmarks have shaped the progression of the field by enabling more diverse training as well as standardized comparisons between methods. In what follows, the most relevant datasets for this problem are reviewed. Most of these datasets were collected in AD scenarios by equipping vehicles with multi-modal sensors and manually annotating the collected data [<xref rid="B18-sensors-25-05264" ref-type="bibr">18</xref>].</p><p>KITTI [<xref rid="B28-sensors-25-05264" ref-type="bibr">28</xref>] was among the first 3D OD datasets and remains widely used due to its accessibility and standardized evaluation protocols. It features real-world driving data captured in Germany using stereo cameras and a Velodyne LiDAR sensor. More recent datasets, such as nuScenes [<xref rid="B36-sensors-25-05264" ref-type="bibr">36</xref>] and Waymo Open [<xref rid="B37-sensors-25-05264" ref-type="bibr">37</xref>], offer greater scene diversity, richer annotations, and broader environmental conditions. nuScenes includes multi-modal data from six cameras, one LiDAR and five radars, covering 1000 driving scenes. The Waymo Open Dataset provides synchronized data from five LiDARs and five cameras, with over 230,000 annotated frames.</p><p>Although KITTI played a foundational role in the development of 3D perception methods, its limited scale and lack of environmental variety have driven a shift toward more comprehensive datasets such as nuScenes and Waymo. These newer benchmarks better support real-world deployment by exposing models to rare events, night-time conditions, and adverse weather scenarios. Due to their scale and greater diversity, nuScenes and Waymo are currently the most commonly used datasets. Nevertheless, class imbalance remains a common issue, reflecting the natural distribution of object types in driving environments [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>].</p><p>Other notable datasets include ApolloScape [<xref rid="B38-sensors-25-05264" ref-type="bibr">38</xref>], ArgoVerse [<xref rid="B39-sensors-25-05264" ref-type="bibr">39</xref>], Lyft Level 5 [<xref rid="B40-sensors-25-05264" ref-type="bibr">40</xref>], and H3D [<xref rid="B41-sensors-25-05264" ref-type="bibr">41</xref>]. <xref rid="sensors-25-05264-t002" ref-type="table">Table 2</xref> summarizes the main characteristics of these datasets, based on information from [<xref rid="B20-sensors-25-05264" ref-type="bibr">20</xref>,<xref rid="B24-sensors-25-05264" ref-type="bibr">24</xref>,<xref rid="B26-sensors-25-05264" ref-type="bibr">26</xref>,<xref rid="B42-sensors-25-05264" ref-type="bibr">42</xref>].</p><list list-type="simple"><list-item><p><bold>KITTI:</bold> The KITTI dataset (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.cvlibs.net/datasets/kitti/">https://www.cvlibs.net/datasets/kitti/</uri>, accessed on 25 April 2025), developed by the Karlsruhe Institute of Technology and Toyota Technological Institute at Chicago, remains one of the most widely used benchmarks for AD. It provides stereo RGB images, LiDAR PCs, and calibration files. The dataset includes 7481 training and 7518 testing frames, with over 80,000 annotated 3D bounding boxes. Objects are labelled as easy, moderate, or hard based on occlusion, truncation, and object size [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>]. Data were collected using a Velodyne LiDAR, stereo cameras, and GNSS/IMU sensors across 22 scenes in urban and highway environments [<xref rid="B32-sensors-25-05264" ref-type="bibr">32</xref>].</p></list-item></list><list list-type="simple"><list-item><p><bold>nuScenes:</bold> The nuScenes dataset (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.nuscenes.org/">https://www.nuscenes.org/</uri>, accessed on 25 April 2025), developed by Motional (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://motional.com/">https://motional.com/</uri>), comprises 1000 20-s driving scenes recorded at 2 Hz. Each scene contains annotations for 23 categories. The sensor suite includes six cameras, a 32-beam LiDAR, and five radars. In total, the dataset features over 390,000 LiDAR sweeps and 1.4 million annotated bounding boxes [<xref rid="B24-sensors-25-05264" ref-type="bibr">24</xref>].</p></list-item></list><list list-type="simple"><list-item><p><bold>Waymo Open:</bold> The Waymo Open Dataset (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://waymo.com/open/">https://waymo.com/open/</uri>, accessed on 25 April 2025) includes approximately 230,000 annotated frames and over 12 million 3D bounding boxes. It provides synchronized data from five LiDAR sensors and five cameras, spanning 798 training, 202 validation, and 150 test segments. Annotated classes cover vehicles, pedestrians, cyclists, and traffic signs [<xref rid="B24-sensors-25-05264" ref-type="bibr">24</xref>].</p></list-item></list></sec><sec id="sec3dot2-sensors-25-05264"><title>3.2. Evaluation Metrics</title><sec id="sec3dot2dot1-sensors-25-05264"><title>3.2.1. General Metrics</title><p>To quantitatively evaluate 3D OD methods, a variety of evaluation metrics have been proposed. These metrics extend foundational concepts from 2D OD by incorporating depth, orientation, and volumetric estimation. While the core ideas, such as precision, recall, and average precision (AP), remain central, they are adapted to operate on 3D bounding boxes.</p><p>The major difference between 2D and 3D detection metrics lies in the matching criteria between ground truths and predictions when calculating precision and recall. The quality of a detection is commonly measured using the Intersection over Union (IoU), which computes the ratio of the overlapping volume between a predicted and ground-truth 3D bounding box. A prediction is typically considered a true positive (TP) if the IoU exceeds a predefined threshold (usually 0.5); otherwise, it is classified as a false positive (FP). Missed detections are treated as false negatives (FNs), and correctly ignored negatives are true negatives (TNs). These categories form the basis of the confusion matrix:<list list-type="bullet"><list-item><p><bold>True Positives</bold> (<inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>): Correctly predicted positives.</p></list-item><list-item><p><bold>False Positives</bold> (<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>): Incorrectly predicted positives.</p></list-item><list-item><p><bold>False Negatives</bold> (<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>): Missed predictions.</p></list-item><list-item><p><bold>True Negatives</bold> (<inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>): Correctly predicted negatives.</p></list-item></list></p><p>Based on these, two key performance metrics are defined:<disp-formula id="FD3-sensors-25-05264"><label>(3)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05264"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Recall</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Precision quantifies the correctness of predicted positives, while recall captures the detector&#8217;s ability to find all relevant objects. These metrics are often visualized through precision&#8211;recall (P-R) curves, from which the average precision (AP) is derived. In theory, both precision and recall should be as close to 1 (one) as possible. However, in practice, they often conflict; improving one may degrade the other.</p><p>The AP for a given class is computed as the area under the P-R curve:<disp-formula id="FD5-sensors-25-05264"><label>(5)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo mathvariant="italic">&#8747;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8739;</mml:mo><mml:mi>r</mml:mi><mml:mo>&#8805;</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>}</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the precision at recall <italic toggle="yes">r</italic>.</p><p>Once the AP is computed for each object class, the mean average precision (mAP) is obtained by averaging across all classes. The setting of the IoU threshold plays a critical role here, as it affects the strictness and comprehensiveness of the evaluation. For instance, the COCO benchmark computes mAP over multiple IoU thresholds, ranging from 0.5 to 0.95 in steps of 0.05, to provide a robust evaluation of localization and classification performance.</p><p>IoU is computed using the volumes of the bounding boxes:<disp-formula id="FD6-sensors-25-05264"><label>(6)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>IoU</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#8745;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>&#8746;</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic> represent the predicted and ground-truth 3D bounding boxes, respectively. This volumetric IoU is more sensitive to spatial misalignment than its 2D counterpart, making evaluation more stringent.</p><p>While AP and mAP remain the principal metrics for assessing detection quality, additional metrics are crucial in real-time or resource-constrained scenarios. These include the number of model parameters and floating-point operations per second (FLOPs), which reflect computational complexity, as well as latency and frames per second (FPS), which assess runtime efficiency. Together, these metrics offer a holistic view of a model&#8217;s accuracy, efficiency, and deployability.</p></sec><sec id="sec3dot2dot2-sensors-25-05264"><title>3.2.2. Dataset-Specific Metrics</title><list list-type="simple"><list-item><p><bold>KITTI:</bold> The KITTI benchmark [<xref rid="B28-sensors-25-05264" ref-type="bibr">28</xref>] remains one of the most commonly used datasets for 3D OD. It evaluates model performance using three primary metrics [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>]:</p></list-item></list><p>
<list list-type="bullet"><list-item><p><bold>AP<sub>2D</sub></bold>: Average precision computed by projecting the predicted 3D bounding boxes into the 2D image plane and calculating 2D IoU.</p></list-item><list-item><p><bold>AP<sub>3D</sub></bold>: Average precision computed using the full 3D bounding box IoU.</p></list-item><list-item><p><bold>AP<sub>BEV</sub></bold>: Average precision computed from a bird&#8217;s-eye view (BEV) projection of the 3D bounding box.</p></list-item></list>
</p><p>In addition to these metrics, KITTI also reports the Average Orientation Similarity (AOS), which assesses the alignment between predicted and ground-truth object orientations. Different object classes have distinct IoU thresholds for evaluation: for instance, 0.7 for cars and 0.5 for pedestrians and cyclists. This accounts for the relative difficulty of localizing small or occluded objects.</p><list list-type="simple"><list-item><p><bold>nuScenes:</bold> The nuScenes benchmark [<xref rid="B36-sensors-25-05264" ref-type="bibr">36</xref>] proposes a more comprehensive evaluation scheme that moves beyond traditional IoU-based matching. The authors argue that using IoU alone does not capture all relevant aspects of detection quality in complex urban environments. Instead, nuScenes introduces centre-based matching, where predicted objects are associated with the ground truth based on their 2D centre distance on the ground plane. The newly introduced scores quantify how closely the predicted objects align with the ground truth not just in terms of location, but also shape, pose, and dynamic behaviour. The final nuScenes Detection Score (NDS) aggregates the mean average precision (mAP) and the mean TP metrics (mTP) into a single holistic score:</p></list-item></list><p>
<disp-formula id="FD7-sensors-25-05264"><label>(7)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:mn>5</mml:mn><mml:mo>&#183;</mml:mo><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
</p><p>This score captures both detection accuracy and multi-attribute consistency, making it a more holistic evaluation metric.</p><list list-type="simple"><list-item><p><bold>Waymo Open Dataset</bold>: The Waymo benchmark [<xref rid="B37-sensors-25-05264" ref-type="bibr">37</xref>] evaluates detection at two levels:</p></list-item></list><p>
<list list-type="bullet"><list-item><p><bold>Level 1 (L1)</bold>: Objects with at least five LiDAR points inside the bounding box.</p></list-item><list-item><p><bold>Level 2 (L2)</bold>: All annotated objects, including sparse detections.</p></list-item></list>
</p><p>To capture different aspects of model performance, the dataset defines multiple variants of average precision. The standard AP measures detection accuracy using IoU-based matching, while APH extends this by incorporating heading angle accuracy, evaluating how well the model predicts object orientation. Additionally, Waymo introduces a variant based on the Hungarian algorithm, which performs optimal one-to-one assignment between predictions and ground truth, particularly beneficial when multiple nearby detections could otherwise result in ambiguity.</p></sec></sec></sec><sec id="sec4-sensors-25-05264"><title>4. Taxonomy and Review</title><p>Artificial Intelligence (AI), and in particular neural networks (NNs), have become foundational to many modern technological applications. DL, a subfield of ML and AI, enables models to learn hierarchical and abstract representations from large volumes of data, significantly improving prediction accuracy across various domains. Its flexibility and high capacity for pattern recognition have made DL especially valuable in AVs, where many SoA systems rely heavily on deep CNNs.</p><p>CNNs have demonstrated exceptional performance in 2D image-based tasks, including classification and object detection. However, their extension to 3D perception, especially OD in PCs, introduces specific challenges. Unlike structured image grids, PCs are inherently unordered, irregular, and sparse, making standard convolution operations unsuitable without significant modification. Specialized architectures and preprocessing techniques are thus required to adapt CNN-based methods to 3D inputs.</p><p>The widespread adoption of DL has been driven by two key enablers: the availability of large-scale annotated datasets and advances in Graphics Processing Unit (GPU) hardware, which support high-throughput parallel computation [<xref rid="B31-sensors-25-05264" ref-type="bibr">31</xref>]. DL models are composed of multiple layers of artificial neurons that transform input data using learnable weights and biases. These weights are optimized during training by minimizing a loss function that quantifies the error between predictions and ground truth [<xref rid="B27-sensors-25-05264" ref-type="bibr">27</xref>]. The methods reviewed in this work are primarily based on supervised learning, where models are trained using labelled datasets to learn relevant spatial and semantic patterns.</p><p>This section presents an overview of the main families of 3D OD approaches, organized according to the type of sensor input and processing strategy. For each modality or hybrid configuration, we highlight the key trends and most influential methods that have shaped the field.</p><sec id="sec4dot1-sensors-25-05264"><title>4.1. Taxonomy of 3D Object Detection</title><p>Compared to 2D OD, 3D OD introduces distinct challenges, particularly in terms of depth perception and spatial representation. When working with image data alone, the primary limitation is the lack of explicit depth information. Two common strategies are used to address this: (i) decoupling the task into 2D OD followed by depth estimation, often using geometric constraints or priors to reconstruct 3D positions; and (ii) employing DL models that directly infer 3D object properties from image inputs. In this work, we focus on the latter approach, where monocular, stereo, and multi-view camera configurations are used to extract depth-aware features through learned representations.</p><p>In contrast, when only PC data are available, typically acquired using LiDAR, the main challenge becomes learning from data that are sparse, unordered, and non-uniform. Various strategies have emerged to process such data effectively, and these are commonly grouped into four categories: point-based methods, which operate directly on raw PC coordinates; voxel-based methods, which discretize the space into regular 3D grids; point&#8211;voxel hybrid approaches, which combine the advantages of both representations; and projection-based methods, which transform 3D data into 2D representations to leverage mature 2D CNN architectures.</p><p>Radar-based 3D detection methods are also gaining attention due to radar&#8217;s robustness in adverse weather and lighting conditions. However, the relatively low spatial resolution and noisy measurements of radar data pose unique challenges. Most radar-based approaches either project radar points into bird&#8217;s-eye view representations or fuse them with other modalities to compensate for their limitations.</p><p>Multi-modal fusion methods attempt to harness the complementary strengths of different sensor modalities, most commonly combining camera and LiDAR data. These methods are generally categorized based on the stage at which the fusion occurs within the perception pipeline: early fusion combines raw sensor data before feature extraction; mid-level fusion integrates intermediate features from each modality; and late fusion merges the outputs of independent modality-specific detectors.</p><p>Although there is some agreement in the literature regarding the categorization of 3D OD methods, variations in terminology and classification criteria remain. To address this, a high-level taxonomy was developed to capture the dominant and most widely adopted strategies. <xref rid="sensors-25-05264-f004" ref-type="fig">Figure 4</xref> illustrates the proposed taxonomy and maps out the representative methods covered in this review.</p><fig position="anchor" id="sensors-25-05264-f004" orientation="portrait"><label>Figure 4</label><caption><p>Taxonomy of 3D object detection methods categorized by input modality.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05264-g004.jpg"/></fig></sec><sec id="sec4dot2-sensors-25-05264"><title>4.2. Camera-Based Methods</title><p>Three-dimensional OD methods often build upon principles inherited from 2D detectors. However, the transition from 2D to 3D introduces increased sensitivity to localization errors. While a few pixels of deviation in 2D detection may still result in acceptable IoU scores, small spatial misalignments in 3D, even at the decimetre scale, can significantly degrade detection accuracy. This section presents the most prominent camera-based strategies for 3D OD, including monocular, stereo, and multi-view or multi-camera approaches.</p><sec id="sec4dot2dot1-sensors-25-05264"><title>4.2.1. Monocular-Based Methods</title><p>Monocular 3D OD aims to predict object depth, orientation, and size using only a single RGB image. Its minimal hardware requirements make it attractive for large-scale deployment, particularly in cost-sensitive or lightweight platforms [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>].</p><p>One of the earliest works, 3DVP [<xref rid="B43-sensors-25-05264" ref-type="bibr">43</xref>], introduced a viewpoint-aware classification model that leveraged sub-category structures. Mono3D [<xref rid="B44-sensors-25-05264" ref-type="bibr">44</xref>] advanced the field by generating 3D proposals on a learned ground plane, providing a robust geometry-aware baseline. Deep3DBox [<xref rid="B45-sensors-25-05264" ref-type="bibr">45</xref>] streamlined the pipeline with a single network predicting object orientation and dimensions end-to-end.</p><p>Subsequent efforts focused on improving both accuracy and efficiency. M3D-RPN [<xref rid="B46-sensors-25-05264" ref-type="bibr">46</xref>] was the first to introduce a two-stage Region Proposal Network tailored for monocular 3D detection. MonoDIS [<xref rid="B47-sensors-25-05264" ref-type="bibr">47</xref>] disentangled depth, shape, and orientation learning into separate branches, improving training stability. Lightweight single-shot architectures like SS3D [<xref rid="B48-sensors-25-05264" ref-type="bibr">48</xref>] and SMOKE [<xref rid="B49-sensors-25-05264" ref-type="bibr">49</xref>] removed the need for dense proposals, achieving real-time inference.</p><p>Recent developments leverage auxiliary knowledge sources. MonoDistill [<xref rid="B50-sensors-25-05264" ref-type="bibr">50</xref>] and MonoSKD [<xref rid="B51-sensors-25-05264" ref-type="bibr">51</xref>] showed how student models can benefit from distilled supervision by larger teacher networks. MonoNeRD [<xref rid="B52-sensors-25-05264" ref-type="bibr">52</xref>] incorporated neural radiance field priors to model fine-grained object appearance and geometry. MonoCD [<xref rid="B53-sensors-25-05264" ref-type="bibr">53</xref>] achieved SoA results without relying on LiDAR or dense proposal mechanisms.</p></sec><sec id="sec4dot2dot2-sensors-25-05264"><title>4.2.2. Stereo-Based Methods</title><p>Stereo-based 3D OD estimates depth using the disparity between stereo image pairs. The resulting depth maps are fused with 2D detection outputs to localize objects in 3D. Compared to monocular approaches, stereo methods benefit from geometric priors and disparity cues that enable more accurate depth perception [<xref rid="B19-sensors-25-05264" ref-type="bibr">19</xref>,<xref rid="B24-sensors-25-05264" ref-type="bibr">24</xref>].</p><p>Early approaches like 3DOP relied on hand-crafted features and sliding-window proposals. Stereo R-CNN [<xref rid="B54-sensors-25-05264" ref-type="bibr">54</xref>] marked a shift toward DL-based stereo detection by integrating a disparity-aware ROI head into a Faster R-CNN backbone. RT3D-Stereo further improved efficiency by introducing a lightweight cost volume and a 3D-aware regression module.</p><p>DSGN [<xref rid="B55-sensors-25-05264" ref-type="bibr">55</xref>] constructed a scene-level representation over stereo cost volumes and refined object predictions using message passing. YOLOStereo3D [<xref rid="B56-sensors-25-05264" ref-type="bibr">56</xref>] adopted a single-shot architecture to streamline the detection process. LIGA-Stereo [<xref rid="B57-sensors-25-05264" ref-type="bibr">57</xref>] combined sparse LiDAR signals with stereo features to guide attention. More recently, StereoDistill [<xref rid="B58-sensors-25-05264" ref-type="bibr">58</xref>] demonstrated that compact student networks can replicate the performance of larger stereo teacher models through structured distillation.</p></sec><sec id="sec4dot2dot3-sensors-25-05264"><title>4.2.3. Multi-View/Multi-Camera-Based Methods</title><p>Multi-camera systems are frequently used in AVs to provide 360&#176; perception of the environment. These methods aggregate multiple views to construct a unified 3D scene representation, often by projecting image features into a bird&#8217;s-eye view (BEV) space [<xref rid="B19-sensors-25-05264" ref-type="bibr">19</xref>]. However, this projection introduces ambiguity due to the absence of explicit depth information, which complicates accurate feature alignment.</p><p>To mitigate this, recent approaches have adopted transformer-based architectures that enhance spatial reasoning and cross-view feature fusion. DETR3D [<xref rid="B59-sensors-25-05264" ref-type="bibr">59</xref>] introduced 3D object queries that attend to relevant regions across all views. PETR [<xref rid="B60-sensors-25-05264" ref-type="bibr">60</xref>] enriched positional encoding to improve object localization. BEVFormer [<xref rid="B61-sensors-25-05264" ref-type="bibr">61</xref>] leveraged temporal consistency across video frames to enhance BEV features. SparseBEV [<xref rid="B62-sensors-25-05264" ref-type="bibr">62</xref>] and RoPETR [<xref rid="B63-sensors-25-05264" ref-type="bibr">63</xref>] proposed attention-efficient alternatives to reduce computational cost while maintaining high spatial fidelity.</p></sec><sec sec-type="discussion" id="sec4dot2dot4-sensors-25-05264"><title>4.2.4. Discussion</title><p>Camera-based methods are attractive due to their affordability and ability to capture rich semantic features such as colour, texture, and appearance. Many 3D detectors for camera inputs borrow architectural principles from well-established 2D detection models [<xref rid="B32-sensors-25-05264" ref-type="bibr">32</xref>]. However, monocular methods continue to face performance limitations. Small depth estimation errors can lead to significant inaccuracies in 3D localization, which is particularly problematic in safety-critical scenarios like AVs.</p><p>To address these limitations, pseudo-LiDAR methods have been proposed, which convert estimated depth into synthetic point clouds. While this approach brings monocular setups closer to LiDAR-like reasoning, its performance remains constrained by depth prediction quality and calibration accuracy. Stereo methods offer improved depth estimation through geometric constraints but require high computational resources for disparity matching and cost volume construction.</p><p>Multi-view approaches enable comprehensive scene understanding by leveraging multiple overlapping views. However, without depth supervision, aligning features across views remains a key challenge. Transformer-based methods have made significant progress in this area, particularly in benchmarks like nuScenes that provide complete camera coverage. These models continue to evolve toward more efficient cross-view fusion and real-time inference.</p><p>In terms of evaluation, traditional 2D metrics such as AP<sub><italic toggle="yes">2D</italic></sub> or orientation similarity scores are insufficient to fully assess 3D detection performance. The most relevant metric for these models is AP<sub><italic toggle="yes">3D</italic></sub>, often accompanied by AP<sub><italic toggle="yes">BEV</italic></sub> to capture localization accuracy in the ground plane. To support fair comparison, <xref rid="sensors-25-05264-t003" ref-type="table">Table 3</xref> summarizes the performance of key camera-based 3D detectors, reporting AP<sub><italic toggle="yes">2D</italic></sub>, AP<sub><italic toggle="yes">3D</italic></sub>, and AP<sub><italic toggle="yes">BEV</italic></sub> on KITTI (for easy (E), moderate (M), and hard (H) difficulty levels), as well as mAP and NDS on nuScenes, and L1/L2 mAP on Waymo. Inference times and hardware details are also included.</p></sec></sec><sec id="sec4dot3-sensors-25-05264"><title>4.3. LiDAR-Based Methods</title><p>Unlike images, where pixels are regularly distributed and can be processed using conventional CNNs, PCs are sparse, unordered, and irregular 3D representations. LiDAR sensors generate such PCs by emitting laser pulses and recording their return time, capturing the high-resolution geometric structure of the environment with strong spatial accuracy [<xref rid="B64-sensors-25-05264" ref-type="bibr">64</xref>]. However, this structure introduces unique challenges for DL models, which are typically designed for dense and grid-like data.</p><p>Traditional LiDAR processing relied on geometric and topological cues to extract object-level information, for example by detecting abrupt changes in surface orientation or reflectivity to infer object boundaries [<xref rid="B65-sensors-25-05264" ref-type="bibr">65</xref>]. While computationally efficient and interpretable, such classical methods often struggle in complex scenes with occlusions, clutter, or sparse measurements.</p><p>Modern DL approaches, by contrast, have been built around more structured or learned representations of the PC. To make LiDAR data compatible with NN architectures, the raw points are typically transformed into one of several possible intermediate formats. Most works in the literature categorize these into four main classes: point-based, voxel-based, hybrid point&#8211;voxel, and projection-based representations. Other alternatives, such as range-view, graph-based, and transformer-based models, have also emerged. Each representation encodes spatial information differently, leading to trade-offs in runtime, memory consumption, and geometric fidelity.</p><sec id="sec4dot3dot1-sensors-25-05264"><title>4.3.1. Projection-Based Methods</title><p>Projection-based methods convert 3D point clouds into 2D representations to leverage efficient and mature 2D CNNs. Common projection types include front view, spherical view, and most prominently, BEV, which projects points onto the <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-plane. BEV is especially popular in AD due to the constrained ground motion of vehicles.</p><p>BEV reduces occlusions, enables consistent metric scaling, and avoids overlapping object bounding boxes. The process usually involves discretizing <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> positions into pixel grids, while collapsing the height dimension [<xref rid="B66-sensors-25-05264" ref-type="bibr">66</xref>]. The resulting pseudo-image encodes statistics such as point density, intensity, and height per cell.</p><p>Early works like PIXOR [<xref rid="B67-sensors-25-05264" ref-type="bibr">67</xref>], Complex-YOLO [<xref rid="B68-sensors-25-05264" ref-type="bibr">68</xref>], and BirdNet [<xref rid="B69-sensors-25-05264" ref-type="bibr">69</xref>] adopted standard 2D CNNs to process BEV inputs, showing the feasibility of this approach. Complex-YOLO refined anchor definitions and introduced orientation modelling in polar coordinates. Despite their speed and simplicity, projection-based methods discard vertical detail and are sensitive to sparsity and resolution settings. Nonetheless, they offer an attractive balance between speed and accuracy in real-time scenarios.</p></sec><sec id="sec4dot3dot2-sensors-25-05264"><title>4.3.2. Point-Based Methods</title><p>Point-based methods operate directly on raw 3D coordinates without intermediate discretization. These approaches preserve fine-grained geometry and avoid quantization artefacts, but must account for the unordered nature of point sets.</p><p>PointNet [<xref rid="B70-sensors-25-05264" ref-type="bibr">70</xref>] was a foundational work in this space, introducing permutation-invariant architectures via shared MLPs and symmetric aggregation functions. However, it mainly captured global context, limiting its performance on local geometric structures. PointNet++ [<xref rid="B71-sensors-25-05264" ref-type="bibr">71</xref>] addressed this limitation through hierarchical feature extraction on local neighbourhoods.</p><p>Subsequent methods such as PointRCNN [<xref rid="B72-sensors-25-05264" ref-type="bibr">72</xref>], 3DSSD, STD, IA-SSD [<xref rid="B73-sensors-25-05264" ref-type="bibr">73</xref>], and PointFormer [<xref rid="B74-sensors-25-05264" ref-type="bibr">74</xref>] improved accuracy by incorporating attention mechanisms, local spatial encoding, and direct 3D box regression. These models maintain high geometric fidelity and excel in preserving object boundaries. However, they often involve expensive neighbourhood search operations and are computationally demanding, making real-time deployment more difficult.</p></sec><sec id="sec4dot3dot3-sensors-25-05264"><title>4.3.3. Voxel-Based Methods</title><p>Voxel-based methods discretize the 3D space into a structured grid of equally sized volumetric elements (voxels), allowing the application of 3D convolutions. Each voxel can encode occupancy, intensity, and local statistics. While this enables efficient parallel processing, voxelization introduces resolution&#8211;memory trade-offs and can result in the loss of fine geometric detail.</p><p>VoxelNet [<xref rid="B75-sensors-25-05264" ref-type="bibr">75</xref>] was the first end-to-end model to apply voxel-based feature learning. SECOND [<xref rid="B76-sensors-25-05264" ref-type="bibr">76</xref>] introduced sparse 3D convolutions to reduce computational cost, while PointPillars [<xref rid="B77-sensors-25-05264" ref-type="bibr">77</xref>] collapsed the vertical dimension, forming pseudo-pillars to enable fast 2D convolution on BEV-like features.</p><p>More recent voxel-based models have incorporated transformer architectures and attention mechanisms to improve performance. HotSpotNet [<xref rid="B78-sensors-25-05264" ref-type="bibr">78</xref>], Voxel R-CNN [<xref rid="B79-sensors-25-05264" ref-type="bibr">79</xref>], VoTr-TSD [<xref rid="B80-sensors-25-05264" ref-type="bibr">80</xref>], and TED [<xref rid="B81-sensors-25-05264" ref-type="bibr">81</xref>] represent this line of progress. TED currently holds state-of-the-art performance among LiDAR-only methods on the KITTI benchmark. These methods strike a balance between regular structure and accuracy but remain constrained by cubic memory growth with increasing resolution. Notably, only around 1% of voxels are occupied in KITTI and 3% in Waymo [<xref rid="B16-sensors-25-05264" ref-type="bibr">16</xref>], highlighting inefficiencies in voxel-based representations.</p></sec><sec id="sec4dot3dot4-sensors-25-05264"><title>4.3.4. Point&#8211;Voxel Hybrid Methods</title><p>Hybrid methods aim to combine the geometric richness of point-based models with the efficiency of voxel-based networks. Typically, voxelized backbones are used for coarse region proposals, which are then refined using point-level features for more precise bounding box prediction.</p><p>Fast Point R-CNN [<xref rid="B82-sensors-25-05264" ref-type="bibr">82</xref>], PV-RCNN [<xref rid="B83-sensors-25-05264" ref-type="bibr">83</xref>], and PDV [<xref rid="B84-sensors-25-05264" ref-type="bibr">84</xref>] exemplify this family of detectors. These models perform repeated point-to-voxel and voxel-to-point transformations, enabling fine-grained multi-scale feature fusion. While they achieve high accuracy and robust object boundary modelling, their bidirectional feature interaction comes at the cost of additional memory usage and runtime complexity.</p></sec><sec id="sec4dot3dot5-sensors-25-05264"><title>4.3.5. Other Representations</title><p>Beyond the dominant four categories, other representations have emerged. Range-based methods treat LiDAR scans as 2D range images, enabling the use of efficient 2D convolutions [<xref rid="B70-sensors-25-05264" ref-type="bibr">70</xref>]. Graph-based methods model PCs as node&#8211;edge structures to capture spatial topology. More recently, transformer-based architectures have gained traction for their ability to model long-range dependencies and support parallel computation, making them increasingly popular in AD perception pipelines [<xref rid="B85-sensors-25-05264" ref-type="bibr">85</xref>].</p></sec><sec sec-type="discussion" id="sec4dot3dot6-sensors-25-05264"><title>4.3.6. Discussion</title><p>LiDAR-based 3D OD has matured significantly, supported by diverse representations and architectural innovations. Each strategy introduces specific advantages and trade-offs.</p><p>Point-based models preserve raw geometry and deliver precise localization but remain computationally demanding and hard to scale to large scenes. Voxel-based models offer regular structure and efficient parallelization, but their memory usage scales cubically with resolution and suffers from data sparsity. Projection-based approaches enable real-time processing via 2D CNNs and are dominant in deployment settings, though their performance is bounded by the quality of projection and resolution settings. Hybrid methods strike a strong accuracy&#8211;runtime balance, but their complexity and synchronization cost limit scalability.</p><p>Ultimately, the choice of representation depends on the application: point-based for highest fidelity, voxel- or projection-based for efficient real-time inference, and hybrid models for balanced performance. A summary of the most representative LiDAR-based detection models, categorized by representation and benchmark performance, is provided in <xref rid="sensors-25-05264-t004" ref-type="table">Table 4</xref>. This enables direct comparison with camera-based approaches in <xref rid="sensors-25-05264-t003" ref-type="table">Table 3</xref>, and illustrates how architectural choices impact detection performance across the chosen datasets.</p></sec></sec><sec id="sec4dot4-sensors-25-05264"><title>4.4. Radar-Based Methods</title><p>Unlike vision-based systems, radar offers consistent performance in challenging environmental conditions, making it well-suited for 3D object detection. The first DL approach, Radar-PointGNN (2021) [<xref rid="B86-sensors-25-05264" ref-type="bibr">86</xref>], modelled the sparse range&#8211;Doppler point cloud as a graph and applied graph neural networks to capture spatial relationships. In 2022, K-Radar [<xref rid="B87-sensors-25-05264" ref-type="bibr">87</xref>] introduced a grid-based fusion of multi-view radar snippets, enabling dense feature extraction across time, and KPConvPillars [<xref rid="B88-sensors-25-05264" ref-type="bibr">88</xref>] adapted the KPConv convolution to radar pillars, significantly improving detection resolution and angular precision.</p><p>More recent methods leverage teacher&#8211;student distillation and large-scale pretraining to overcome data scarcity. RadarDistill (2024) [<xref rid="B89-sensors-25-05264" ref-type="bibr">89</xref>] distilled LiDAR-based feature representations into a radar-only student network, achieving substantial gains in both range and velocity estimation. In 2025, RADLER [<xref rid="B90-sensors-25-05264" ref-type="bibr">90</xref>] presented a multimodal foundation model that pretrains on synchronized radar&#8211;camera&#8211;LiDAR streams, demonstrating that rich cross-modal pretraining can elevate pure radar detection performance to higher levels of performance. <xref rid="sensors-25-05264-t005" ref-type="table">Table 5</xref> aggregates some of the most important radar-based methods compared under the few values reported in benchmarked datasets.</p></sec><sec id="sec4dot5-sensors-25-05264"><title>4.5. Multi-Modal-Based Methods</title><p>Multi-modal methods integrate multiple sensor modalities to exploit their complementary characteristics in 3D OD. The most prevalent fusion strategy combines LiDAR point clouds and RGB images, as these are the primary sensors in autonomous AD platforms. While LiDAR provides accurate geometric structure in 3D, images offer dense semantic and texture rich cues. When fused effectively, these modalities compensate for each other&#8217;s limitations, such as occlusions, sparse depth, or poor lighting, and improve detection robustness [<xref rid="B24-sensors-25-05264" ref-type="bibr">24</xref>,<xref rid="B32-sensors-25-05264" ref-type="bibr">32</xref>].</p><p>Single-modality systems are often insufficient for real-world deployment. Cameras, while rich in appearance information, lack direct depth perception. Conversely, LiDAR sensors offer spatial accuracy but no visual appearance or colour. Fusion thus enhances the overall reliability and accuracy of perception systems, particularly in adverse conditions or sensor failures. Most autonomous vehicles are equipped with both cameras and LiDAR, making sensor fusion a practical and effective design choice.</p><p>Fusion can be performed at different stages of the detection pipeline, broadly categorized into three levels [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>,<xref rid="B32-sensors-25-05264" ref-type="bibr">32</xref>]: <italic toggle="yes">early fusion</italic>, where raw or low-level features are combined; <italic toggle="yes">mid-level fusion</italic>, where intermediate features are merged after being learned independently; and <italic toggle="yes">late fusion</italic>, where modality-specific decisions are integrated at the final stage.</p><sec id="sec4dot5dot1-sensors-25-05264"><title>4.5.1. Early Fusion Methods</title><p>Early fusion strategies integrate raw sensor inputs or low-level features before any deep processing. A common approach is to project LiDAR points onto the image plane and enhance them with pixel-level semantic information. For instance, PointPainting [<xref rid="B91-sensors-25-05264" ref-type="bibr">91</xref>] enriches LiDAR points with semantic segmentation scores from an image-based network, effectively augmenting 3D input with 2D semantic priors. Similarly, Frustum PointNet [<xref rid="B92-sensors-25-05264" ref-type="bibr">92</xref>] first uses 2D detectors to define frustums in 3D space and then applies a point-based network to the subset of points within each frustum. This pipeline was further extended by F-ConvNet [<xref rid="B93-sensors-25-05264" ref-type="bibr">93</xref>] and Frustum PointPillars [<xref rid="B94-sensors-25-05264" ref-type="bibr">94</xref>], which replace the 3D backbone to improve detection. More recently, VirConvNet [<xref rid="B95-sensors-25-05264" ref-type="bibr">95</xref>] has achieved SoA performance on KITTI by employing virtual convolutions across fused representations.</p><p>While early fusion offers tight cross-modal interaction and benefits from direct semantic guidance, it is highly sensitive to calibration errors and temporal misalignment. The requirement for accurate extrinsic and intrinsic calibration means any deviation can degrade performance. Additionally, the preprocessing stages, including 2D segmentation or detection, add computational overhead and increase latency during both training and inference.</p></sec><sec id="sec4dot5dot2-sensors-25-05264"><title>4.5.2. Mid-Level Fusion Methods</title><p>Mid-level, or deep fusion, combines intermediate features extracted independently from each sensor modality. These features are often spatially aligned in a common space, typically BEV, frustum, or image views, before fusion. This allows the network to learn cross-modal interactions while retaining modality-specific inductive biases.</p><p>MV3D [<xref rid="B96-sensors-25-05264" ref-type="bibr">96</xref>] was one of the earliest examples, combining features from LiDAR BEV, LiDAR front view, and image view during the proposal refinement stage. AVOD [<xref rid="B97-sensors-25-05264" ref-type="bibr">97</xref>] extended this by integrating LiDAR and image features for both proposal generation and refinement. ContFuse [<xref rid="B98-sensors-25-05264" ref-type="bibr">98</xref>] introduced continuous convolutions to merge multi-scale features in a pixel-wise manner, although its performance can degrade with sparse or noisy inputs. MMF [<xref rid="B99-sensors-25-05264" ref-type="bibr">99</xref>] incorporated auxiliary tasks such as ground plane estimation and depth completion to improve feature fusion robustness.</p><p>Later models introduced more advanced fusion mechanisms. EPNet [<xref rid="B100-sensors-25-05264" ref-type="bibr">100</xref>] constructs image-aware proposal features by conditioning LiDAR features on image context. Transformer-based methods such as TransFusion [<xref rid="B101-sensors-25-05264" ref-type="bibr">101</xref>] and FUTR3D [<xref rid="B102-sensors-25-05264" ref-type="bibr">102</xref>] introduced cross-modal attention modules, improving feature alignment and information flow across views and modalities.</p><p>Mid-level fusion offers a compelling balance between semantic richness and spatial precision. Unlike early fusion, it avoids the need for raw data alignment, and unlike late fusion, it allows rich interaction between feature spaces. However, aligning intermediate features remains challenging due to differing receptive fields, sampling rates, and spatial resolutions across modalities.</p></sec><sec id="sec4dot5dot3-sensors-25-05264"><title>4.5.3. Late Fusion Methods</title><p>Late fusion approaches process each modality independently through separate detection branches and merge their outputs during post-processing. This may involve strategies such as confidence-weighted fusion, geometric consistency checks, or non-maximum suppression (NMS) across modalities.</p><p>CLOCs [<xref rid="B103-sensors-25-05264" ref-type="bibr">103</xref>] represents a modular late-fusion system that refines and merges 2D and 3D detections based on semantic and geometric constraints. Fast-CLOCs [<xref rid="B104-sensors-25-05264" ref-type="bibr">104</xref>] optimized this pipeline for real-time deployment. While late fusion is computationally efficient and allows for reuse of pretrained single-modality detectors, it lacks the fine-grained interaction and synergy of feature-level fusion. As a result, its performance tends to lag behind early and mid-level methods.</p></sec><sec sec-type="discussion" id="sec4dot5dot4-sensors-25-05264"><title>4.5.4. Discussion</title><p>Multi-modal fusion consistently achieves SoA performance by exploiting the strengths of each sensor: LiDAR ensures precise spatial localization, while images enrich detections with contextual and semantic information. Fusion pipelines are often LiDAR-centric, using LiDAR data to generate proposals and images to refine object categories or adjust box parameters. This configuration has become the dominant architecture in modern systems.</p><p>However, fusion introduces its own set of challenges. Misalignment due to unsynchronized timestamps, calibration drift, and different acquisition frequencies can lead to incorrect correspondences between modalities. Resolution mismatches are also a concern, as many LiDAR points may fall into a single pixel, and vice versa. Additionally, some objects may be visible in only one modality, creating further ambiguity during training and inference. Effective data augmentation must be jointly applied across sensors to avoid inconsistencies [<xref rid="B24-sensors-25-05264" ref-type="bibr">24</xref>,<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>].</p><p>While camera&#8211;LiDAR fusion dominates the literature, radar has gained attention as a complementary sensor. Radar systems are robust under adverse weather, offer long-range detection, and can directly measure velocity. However, radar data are typically sparse and noisy with poor spatial resolution, making stand-alone 3D detection difficult. Moreover, the lack of large-scale radar datasets has limited its development. Instead, radar is usually fused with LiDAR or image data.</p><p>Fusion methods like RadarNet [<xref rid="B105-sensors-25-05264" ref-type="bibr">105</xref>] and RICCARDO [<xref rid="B106-sensors-25-05264" ref-type="bibr">106</xref>] incorporate radar and camera data, while frameworks such as CenterFusion [<xref rid="B107-sensors-25-05264" ref-type="bibr">107</xref>] explore the fusion of LiDAR and radar data. Surveys like [<xref rid="B108-sensors-25-05264" ref-type="bibr">108</xref>] for camera&#8211;radar fusion and [<xref rid="B109-sensors-25-05264" ref-type="bibr">109</xref>] for LiDAR&#8211;radar fusion provide greater detail in these types of methods. Despite its potential, radar fusion remains underexplored compared to camera&#8211;LiDAR fusion and is often used as a supplementary modality.</p><p><xref rid="sensors-25-05264-t006" ref-type="table">Table 6</xref> summarizes the most representative fusion-based 3D OD models, categorized by fusion strategy and sensor combination. It includes detection performance across benchmarks such as KITTI and nuScenes, as well as inference runtimes and year of publication. This taxonomy complements the earlier tables, allowing a systematic comparison of fusion approaches against LiDAR-only and camera-based detectors.</p><table-wrap position="anchor" id="sensors-25-05264-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05264-t003_Table 3</object-id><label>Table 3</label><caption><p>Camera-based 3D object detection results on KITTI car test set, nuScenes test set, and Waymo validation set (for easy (E), moderate (M) and hard (H) difficulty levels).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Year</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>2D</sub></th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>3D</sub></th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>BEV</sub></th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">nuScenes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Waymo</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Time (s)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Hardware</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Code Available</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">E</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">E</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">E</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NDS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L1 mAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L2 mAP</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">
<bold>Monocular Camera:</bold>
</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3DVP [<xref rid="B43-sensors-25-05264" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2015</td><td align="center" valign="middle" rowspan="1" colspan="1">84.95</td><td align="center" valign="middle" rowspan="1" colspan="1">76.98</td><td align="center" valign="middle" rowspan="1" colspan="1">65.78</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">8 cores @ 3.5 GHz (Matlab + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mono3D [<xref rid="B44-sensors-25-05264" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" rowspan="1" colspan="1">80.30</td><td align="center" valign="middle" rowspan="1" colspan="1">67.29</td><td align="center" valign="middle" rowspan="1" colspan="1">62.23</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">4.2</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Matlab + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SubCNN [<xref rid="B110-sensors-25-05264" ref-type="bibr">110</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" rowspan="1" colspan="1">94.26</td><td align="center" valign="middle" rowspan="1" colspan="1">89.98</td><td align="center" valign="middle" rowspan="1" colspan="1">79.78</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 3.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep3DBox [<xref rid="B45-sensors-25-05264" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" rowspan="1" colspan="1">94.71</td><td align="center" valign="middle" rowspan="1" colspan="1">90.19</td><td align="center" valign="middle" rowspan="1" colspan="1">76.82</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">1.5</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep MANTA [<xref rid="B111-sensors-25-05264" ref-type="bibr">111</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">98.89</td><td align="center" valign="middle" rowspan="1" colspan="1">93.50</td><td align="center" valign="middle" rowspan="1" colspan="1">83.21</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3D-RCNN [<xref rid="B112-sensors-25-05264" ref-type="bibr">112</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">90.02</td><td align="center" valign="middle" rowspan="1" colspan="1">89.39</td><td align="center" valign="middle" rowspan="1" colspan="1">80.29</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ROI-10D [<xref rid="B113-sensors-25-05264" ref-type="bibr">113</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">76.56</td><td align="center" valign="middle" rowspan="1" colspan="1">70.16</td><td align="center" valign="middle" rowspan="1" colspan="1">61.15</td><td align="center" valign="middle" rowspan="1" colspan="1">4.32</td><td align="center" valign="middle" rowspan="1" colspan="1">2.02</td><td align="center" valign="middle" rowspan="1" colspan="1">1.46</td><td align="center" valign="middle" rowspan="1" colspan="1">9.78</td><td align="center" valign="middle" rowspan="1" colspan="1">4.91</td><td align="center" valign="middle" rowspan="1" colspan="1">3.74</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 3.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MF3D [<xref rid="B114-sensors-25-05264" ref-type="bibr">114</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">90.43</td><td align="center" valign="middle" rowspan="1" colspan="1">87.33</td><td align="center" valign="middle" rowspan="1" colspan="1">76.78</td><td align="center" valign="middle" rowspan="1" colspan="1">7.08</td><td align="center" valign="middle" rowspan="1" colspan="1">5.18</td><td align="center" valign="middle" rowspan="1" colspan="1">4.68</td><td align="center" valign="middle" rowspan="1" colspan="1">13.73</td><td align="center" valign="middle" rowspan="1" colspan="1">9.62</td><td align="center" valign="middle" rowspan="1" colspan="1">8.22</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoGRNet [<xref rid="B115-sensors-25-05264" ref-type="bibr">115</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">88.65</td><td align="center" valign="middle" rowspan="1" colspan="1">77.94</td><td align="center" valign="middle" rowspan="1" colspan="1">63.31</td><td align="center" valign="middle" rowspan="1" colspan="1">9.61</td><td align="center" valign="middle" rowspan="1" colspan="1">5.74</td><td align="center" valign="middle" rowspan="1" colspan="1">4.25</td><td align="center" valign="middle" rowspan="1" colspan="1">18.19</td><td align="center" valign="middle" rowspan="1" colspan="1">11.17</td><td align="center" valign="middle" rowspan="1" colspan="1">8.73</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA P40</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GS3D [<xref rid="B116-sensors-25-05264" ref-type="bibr">116</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">86.23</td><td align="center" valign="middle" rowspan="1" colspan="1">76.35</td><td align="center" valign="middle" rowspan="1" colspan="1">62.67</td><td align="center" valign="middle" rowspan="1" colspan="1">4.47</td><td align="center" valign="middle" rowspan="1" colspan="1">2.90</td><td align="center" valign="middle" rowspan="1" colspan="1">2.47</td><td align="center" valign="middle" rowspan="1" colspan="1">8.41</td><td align="center" valign="middle" rowspan="1" colspan="1">6.08</td><td align="center" valign="middle" rowspan="1" colspan="1">4.94</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mono3D-PLiDAR [<xref rid="B117-sensors-25-05264" ref-type="bibr">117</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">80.85</td><td align="center" valign="middle" rowspan="1" colspan="1">53.36</td><td align="center" valign="middle" rowspan="1" colspan="1">44.80</td><td align="center" valign="middle" rowspan="1" colspan="1">10.76</td><td align="center" valign="middle" rowspan="1" colspan="1">7.50</td><td align="center" valign="middle" rowspan="1" colspan="1">6.10</td><td align="center" valign="middle" rowspan="1" colspan="1">21.27</td><td align="center" valign="middle" rowspan="1" colspan="1">13.92</td><td align="center" valign="middle" rowspan="1" colspan="1">11.25</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA GeForce 1080 (pytorch)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AM3D [<xref rid="B118-sensors-25-05264" ref-type="bibr">118</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">92.55</td><td align="center" valign="middle" rowspan="1" colspan="1">88.71</td><td align="center" valign="middle" rowspan="1" colspan="1">77.88</td><td align="center" valign="middle" rowspan="1" colspan="1">16.50</td><td align="center" valign="middle" rowspan="1" colspan="1">10.74</td><td align="center" valign="middle" rowspan="1" colspan="1">9.52</td><td align="center" valign="middle" rowspan="1" colspan="1">27.91</td><td align="center" valign="middle" rowspan="1" colspan="1">22.24</td><td align="center" valign="middle" rowspan="1" colspan="1">18.62</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deep Optics [<xref rid="B119-sensors-25-05264" ref-type="bibr">119</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">16.86</td><td align="center" valign="middle" rowspan="1" colspan="1">13.82</td><td align="center" valign="middle" rowspan="1" colspan="1">13.26</td><td align="center" valign="middle" rowspan="1" colspan="1">26.71</td><td align="center" valign="middle" rowspan="1" colspan="1">19.87</td><td align="center" valign="middle" rowspan="1" colspan="1">19.11</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CenterNet [<xref rid="B120-sensors-25-05264" ref-type="bibr">120</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">33.80</td><td align="center" valign="middle" rowspan="1" colspan="1">40.00</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FQNet [<xref rid="B121-sensors-25-05264" ref-type="bibr">121</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">94.72</td><td align="center" valign="middle" rowspan="1" colspan="1">90.17</td><td align="center" valign="middle" rowspan="1" colspan="1">76.78</td><td align="center" valign="middle" rowspan="1" colspan="1">2.77</td><td align="center" valign="middle" rowspan="1" colspan="1">1.51</td><td align="center" valign="middle" rowspan="1" colspan="1">1.01</td><td align="center" valign="middle" rowspan="1" colspan="1">5.40</td><td align="center" valign="middle" rowspan="1" colspan="1">3.23</td><td align="center" valign="middle" rowspan="1" colspan="1">2.46</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.50</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Shift R-CNN [<xref rid="B122-sensors-25-05264" ref-type="bibr">122</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">94.07</td><td align="center" valign="middle" rowspan="1" colspan="1">88.48</td><td align="center" valign="middle" rowspan="1" colspan="1">78.34</td><td align="center" valign="middle" rowspan="1" colspan="1">6.88</td><td align="center" valign="middle" rowspan="1" colspan="1">3.87</td><td align="center" valign="middle" rowspan="1" colspan="1">2.83</td><td align="center" valign="middle" rowspan="1" colspan="1">11.84</td><td align="center" valign="middle" rowspan="1" colspan="1">6.82</td><td align="center" valign="middle" rowspan="1" colspan="1">5.27</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 1.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoFENet [<xref rid="B123-sensors-25-05264" ref-type="bibr">123</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">91.68</td><td align="center" valign="middle" rowspan="1" colspan="1">86.63</td><td align="center" valign="middle" rowspan="1" colspan="1">76.71</td><td align="center" valign="middle" rowspan="1" colspan="1">8.35</td><td align="center" valign="middle" rowspan="1" colspan="1">5.14</td><td align="center" valign="middle" rowspan="1" colspan="1">4.10</td><td align="center" valign="middle" rowspan="1" colspan="1">17.03</td><td align="center" valign="middle" rowspan="1" colspan="1">11.03</td><td align="center" valign="middle" rowspan="1" colspan="1">9.05</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 3.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDIS [<xref rid="B47-sensors-25-05264" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">90.31</td><td align="center" valign="middle" rowspan="1" colspan="1">87.58</td><td align="center" valign="middle" rowspan="1" colspan="1">76.85</td><td align="center" valign="middle" rowspan="1" colspan="1">10.37</td><td align="center" valign="middle" rowspan="1" colspan="1">7.94</td><td align="center" valign="middle" rowspan="1" colspan="1">6.40</td><td align="center" valign="middle" rowspan="1" colspan="1">18.80</td><td align="center" valign="middle" rowspan="1" colspan="1">19.08</td><td align="center" valign="middle" rowspan="1" colspan="1">17.41</td><td align="center" valign="middle" rowspan="1" colspan="1">30.40</td><td align="center" valign="middle" rowspan="1" colspan="1">38.40</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoPSR [<xref rid="B124-sensors-25-05264" ref-type="bibr">124</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">93.63</td><td align="center" valign="middle" rowspan="1" colspan="1">88.50</td><td align="center" valign="middle" rowspan="1" colspan="1">73.36</td><td align="center" valign="middle" rowspan="1" colspan="1">10.76</td><td align="center" valign="middle" rowspan="1" colspan="1">7.25</td><td align="center" valign="middle" rowspan="1" colspan="1">5.85</td><td align="center" valign="middle" rowspan="1" colspan="1">18.33</td><td align="center" valign="middle" rowspan="1" colspan="1">12.58</td><td align="center" valign="middle" rowspan="1" colspan="1">9.91</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 3.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MoVi-3D [<xref rid="B125-sensors-25-05264" ref-type="bibr">125</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">15.19</td><td align="center" valign="middle" rowspan="1" colspan="1">10.90</td><td align="center" valign="middle" rowspan="1" colspan="1">9.26</td><td align="center" valign="middle" rowspan="1" colspan="1">22.76</td><td align="center" valign="middle" rowspan="1" colspan="1">17.03</td><td align="center" valign="middle" rowspan="1" colspan="1">14.85</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RefinedMPL [<xref rid="B126-sensors-25-05264" ref-type="bibr">126</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">88.29</td><td align="center" valign="middle" rowspan="1" colspan="1">65.24</td><td align="center" valign="middle" rowspan="1" colspan="1">53.20</td><td align="center" valign="middle" rowspan="1" colspan="1">18.09</td><td align="center" valign="middle" rowspan="1" colspan="1">11.14</td><td align="center" valign="middle" rowspan="1" colspan="1">8.94</td><td align="center" valign="middle" rowspan="1" colspan="1">28.08</td><td align="center" valign="middle" rowspan="1" colspan="1">17.60</td><td align="center" valign="middle" rowspan="1" colspan="1">13.95</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">M3D-RPN [<xref rid="B46-sensors-25-05264" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">89.04</td><td align="center" valign="middle" rowspan="1" colspan="1">85.08</td><td align="center" valign="middle" rowspan="1" colspan="1">69.26</td><td align="center" valign="middle" rowspan="1" colspan="1">14.76</td><td align="center" valign="middle" rowspan="1" colspan="1">9.71</td><td align="center" valign="middle" rowspan="1" colspan="1">7.42</td><td align="center" valign="middle" rowspan="1" colspan="1">21.02</td><td align="center" valign="middle" rowspan="1" colspan="1">13.67</td><td align="center" valign="middle" rowspan="1" colspan="1">10.23</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.16</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 1.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SS3D [<xref rid="B48-sensors-25-05264" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">92.72</td><td align="center" valign="middle" rowspan="1" colspan="1">84.92</td><td align="center" valign="middle" rowspan="1" colspan="1">70.35</td><td align="center" valign="middle" rowspan="1" colspan="1">10.78</td><td align="center" valign="middle" rowspan="1" colspan="1">7.68</td><td align="center" valign="middle" rowspan="1" colspan="1">6.51</td><td align="center" valign="middle" rowspan="1" colspan="1">16.33</td><td align="center" valign="middle" rowspan="1" colspan="1">11.52</td><td align="center" valign="middle" rowspan="1" colspan="1">9.93</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.048</td><td align="center" valign="middle" rowspan="1" colspan="1">Tesla V100</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoPair [<xref rid="B127-sensors-25-05264" ref-type="bibr">127</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">96.61</td><td align="center" valign="middle" rowspan="1" colspan="1">93.55</td><td align="center" valign="middle" rowspan="1" colspan="1">83.55</td><td align="center" valign="middle" rowspan="1" colspan="1">13.04</td><td align="center" valign="middle" rowspan="1" colspan="1">9.99</td><td align="center" valign="middle" rowspan="1" colspan="1">8.65</td><td align="center" valign="middle" rowspan="1" colspan="1">19.28</td><td align="center" valign="middle" rowspan="1" colspan="1">14.83</td><td align="center" valign="middle" rowspan="1" colspan="1">12.89</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RTM3D [<xref rid="B128-sensors-25-05264" ref-type="bibr">128</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">91.82</td><td align="center" valign="middle" rowspan="1" colspan="1">86.93</td><td align="center" valign="middle" rowspan="1" colspan="1">77.41</td><td align="center" valign="middle" rowspan="1" colspan="1">14.41</td><td align="center" valign="middle" rowspan="1" colspan="1">10.34</td><td align="center" valign="middle" rowspan="1" colspan="1">8.77</td><td align="center" valign="middle" rowspan="1" colspan="1">19.17</td><td align="center" valign="middle" rowspan="1" colspan="1">14.20</td><td align="center" valign="middle" rowspan="1" colspan="1">11.99</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 1.0 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SMOKE [<xref rid="B49-sensors-25-05264" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">93.21</td><td align="center" valign="middle" rowspan="1" colspan="1">87.51</td><td align="center" valign="middle" rowspan="1" colspan="1">77.66</td><td align="center" valign="middle" rowspan="1" colspan="1">14.03</td><td align="center" valign="middle" rowspan="1" colspan="1">9.76</td><td align="center" valign="middle" rowspan="1" colspan="1">7.84</td><td align="center" valign="middle" rowspan="1" colspan="1">20.83</td><td align="center" valign="middle" rowspan="1" colspan="1">14.49</td><td align="center" valign="middle" rowspan="1" colspan="1">12.75</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PatchNet [<xref rid="B129-sensors-25-05264" ref-type="bibr">129</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">15.68</td><td align="center" valign="middle" rowspan="1" colspan="1">11.12</td><td align="center" valign="middle" rowspan="1" colspan="1">10.17</td><td align="center" valign="middle" rowspan="1" colspan="1">22.97</td><td align="center" valign="middle" rowspan="1" colspan="1">16.86</td><td align="center" valign="middle" rowspan="1" colspan="1">14.97</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.38</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">IAFA [<xref rid="B130-sensors-25-05264" ref-type="bibr">130</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">93.08</td><td align="center" valign="middle" rowspan="1" colspan="1">89.46</td><td align="center" valign="middle" rowspan="1" colspan="1">79.83</td><td align="center" valign="middle" rowspan="1" colspan="1">17.81</td><td align="center" valign="middle" rowspan="1" colspan="1">12.01</td><td align="center" valign="middle" rowspan="1" colspan="1">10.61</td><td align="center" valign="middle" rowspan="1" colspan="1">25.88</td><td align="center" valign="middle" rowspan="1" colspan="1">17.88</td><td align="center" valign="middle" rowspan="1" colspan="1">15.35</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Kinematic3D [<xref rid="B131-sensors-25-05264" ref-type="bibr">131</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">89.67</td><td align="center" valign="middle" rowspan="1" colspan="1">71.73</td><td align="center" valign="middle" rowspan="1" colspan="1">54.97</td><td align="center" valign="middle" rowspan="1" colspan="1">19.07</td><td align="center" valign="middle" rowspan="1" colspan="1">12.72</td><td align="center" valign="middle" rowspan="1" colspan="1">9.17</td><td align="center" valign="middle" rowspan="1" colspan="1">26.69</td><td align="center" valign="middle" rowspan="1" colspan="1">17.52</td><td align="center" valign="middle" rowspan="1" colspan="1">13.10</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 1.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KM3D [<xref rid="B132-sensors-25-05264" ref-type="bibr">132</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">96.44</td><td align="center" valign="middle" rowspan="1" colspan="1">91.07</td><td align="center" valign="middle" rowspan="1" colspan="1">81.19</td><td align="center" valign="middle" rowspan="1" colspan="1">16.73</td><td align="center" valign="middle" rowspan="1" colspan="1">11.45</td><td align="center" valign="middle" rowspan="1" colspan="1">9.92</td><td align="center" valign="middle" rowspan="1" colspan="1">23.44</td><td align="center" valign="middle" rowspan="1" colspan="1">16.20</td><td align="center" valign="middle" rowspan="1" colspan="1">14.47</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDMP-3D [<xref rid="B133-sensors-25-05264" ref-type="bibr">133</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">91.15</td><td align="center" valign="middle" rowspan="1" colspan="1">81.70</td><td align="center" valign="middle" rowspan="1" colspan="1">63.12</td><td align="center" valign="middle" rowspan="1" colspan="1">19.71</td><td align="center" valign="middle" rowspan="1" colspan="1">12.78</td><td align="center" valign="middle" rowspan="1" colspan="1">9.80</td><td align="center" valign="middle" rowspan="1" colspan="1">28.08</td><td align="center" valign="middle" rowspan="1" colspan="1">17.89</td><td align="center" valign="middle" rowspan="1" colspan="1">13.44</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoRUn [<xref rid="B134-sensors-25-05264" ref-type="bibr">134</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">95.48</td><td align="center" valign="middle" rowspan="1" colspan="1">87.91</td><td align="center" valign="middle" rowspan="1" colspan="1">78.10</td><td align="center" valign="middle" rowspan="1" colspan="1">19.65</td><td align="center" valign="middle" rowspan="1" colspan="1">12.30</td><td align="center" valign="middle" rowspan="1" colspan="1">10.58</td><td align="center" valign="middle" rowspan="1" colspan="1">27.94</td><td align="center" valign="middle" rowspan="1" colspan="1">17.34</td><td align="center" valign="middle" rowspan="1" colspan="1">15.24</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GrooMeD-NMS [<xref rid="B135-sensors-25-05264" ref-type="bibr">135</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">90.14</td><td align="center" valign="middle" rowspan="1" colspan="1">80.28</td><td align="center" valign="middle" rowspan="1" colspan="1">63.78</td><td align="center" valign="middle" rowspan="1" colspan="1">18.10</td><td align="center" valign="middle" rowspan="1" colspan="1">12.32</td><td align="center" valign="middle" rowspan="1" colspan="1">9.65</td><td align="center" valign="middle" rowspan="1" colspan="1">26.19</td><td align="center" valign="middle" rowspan="1" colspan="1">18.27</td><td align="center" valign="middle" rowspan="1" colspan="1">14.05</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDLE [<xref rid="B136-sensors-25-05264" ref-type="bibr">136</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">93.83</td><td align="center" valign="middle" rowspan="1" colspan="1">90.81</td><td align="center" valign="middle" rowspan="1" colspan="1">80.93</td><td align="center" valign="middle" rowspan="1" colspan="1">17.23</td><td align="center" valign="middle" rowspan="1" colspan="1">12.26</td><td align="center" valign="middle" rowspan="1" colspan="1">10.29</td><td align="center" valign="middle" rowspan="1" colspan="1">24.79</td><td align="center" valign="middle" rowspan="1" colspan="1">18.89</td><td align="center" valign="middle" rowspan="1" colspan="1">16.00</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1"> &#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CaDDN [<xref rid="B137-sensors-25-05264" ref-type="bibr">137</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">93.61</td><td align="center" valign="middle" rowspan="1" colspan="1">80.73</td><td align="center" valign="middle" rowspan="1" colspan="1">71.09</td><td align="center" valign="middle" rowspan="1" colspan="1">19.17</td><td align="center" valign="middle" rowspan="1" colspan="1">13.41</td><td align="center" valign="middle" rowspan="1" colspan="1">11.46</td><td align="center" valign="middle" rowspan="1" colspan="1">27.94</td><td align="center" valign="middle" rowspan="1" colspan="1">18.91</td><td align="center" valign="middle" rowspan="1" colspan="1">17.19</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.63</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoFlex [<xref rid="B138-sensors-25-05264" ref-type="bibr">138</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">96.01</td><td align="center" valign="middle" rowspan="1" colspan="1">91.02</td><td align="center" valign="middle" rowspan="1" colspan="1">83.38</td><td align="center" valign="middle" rowspan="1" colspan="1">19.94</td><td align="center" valign="middle" rowspan="1" colspan="1">13.89</td><td align="center" valign="middle" rowspan="1" colspan="1">12.07</td><td align="center" valign="middle" rowspan="1" colspan="1">28.23</td><td align="center" valign="middle" rowspan="1" colspan="1">19.75</td><td align="center" valign="middle" rowspan="1" colspan="1">16.89</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoRCNN [<xref rid="B139-sensors-25-05264" ref-type="bibr">139</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">91.90</td><td align="center" valign="middle" rowspan="1" colspan="1">86.48</td><td align="center" valign="middle" rowspan="1" colspan="1">66.71</td><td align="center" valign="middle" rowspan="1" colspan="1">18.36</td><td align="center" valign="middle" rowspan="1" colspan="1">12.65</td><td align="center" valign="middle" rowspan="1" colspan="1">10.03</td><td align="center" valign="middle" rowspan="1" colspan="1">25.48</td><td align="center" valign="middle" rowspan="1" colspan="1">18.11</td><td align="center" valign="middle" rowspan="1" colspan="1">14.10</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCOS3D [<xref rid="B140-sensors-25-05264" ref-type="bibr">140</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">35.80</td><td align="center" valign="middle" rowspan="1" colspan="1">42.80</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoEF [<xref rid="B141-sensors-25-05264" ref-type="bibr">141</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">96.32</td><td align="center" valign="middle" rowspan="1" colspan="1">90.88</td><td align="center" valign="middle" rowspan="1" colspan="1">83.27</td><td align="center" valign="middle" rowspan="1" colspan="1">21.29</td><td align="center" valign="middle" rowspan="1" colspan="1">13.87</td><td align="center" valign="middle" rowspan="1" colspan="1">11.71</td><td align="center" valign="middle" rowspan="1" colspan="1">29.03</td><td align="center" valign="middle" rowspan="1" colspan="1">19.70</td><td align="center" valign="middle" rowspan="1" colspan="1">17.26</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GUPNet [<xref rid="B142-sensors-25-05264" ref-type="bibr">142</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">94.15</td><td align="center" valign="middle" rowspan="1" colspan="1">86.45</td><td align="center" valign="middle" rowspan="1" colspan="1">74.18</td><td align="center" valign="middle" rowspan="1" colspan="1">22.26</td><td align="center" valign="middle" rowspan="1" colspan="1">15.02</td><td align="center" valign="middle" rowspan="1" colspan="1">13.12</td><td align="center" valign="middle" rowspan="1" colspan="1">30.29</td><td align="center" valign="middle" rowspan="1" colspan="1">21.19</td><td align="center" valign="middle" rowspan="1" colspan="1">18.20</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PGD [<xref rid="B143-sensors-25-05264" ref-type="bibr">143</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">92.04</td><td align="center" valign="middle" rowspan="1" colspan="1">80.58</td><td align="center" valign="middle" rowspan="1" colspan="1">69.67</td><td align="center" valign="middle" rowspan="1" colspan="1">19.05</td><td align="center" valign="middle" rowspan="1" colspan="1">11.76</td><td align="center" valign="middle" rowspan="1" colspan="1">9.39</td><td align="center" valign="middle" rowspan="1" colspan="1">26.89</td><td align="center" valign="middle" rowspan="1" colspan="1">16.51</td><td align="center" valign="middle" rowspan="1" colspan="1">13.49</td><td align="center" valign="middle" rowspan="1" colspan="1">38.60</td><td align="center" valign="middle" rowspan="1" colspan="1">44.80</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Aug3D-RPN [<xref rid="B144-sensors-25-05264" ref-type="bibr">144</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">85.57</td><td align="center" valign="middle" rowspan="1" colspan="1">77.88</td><td align="center" valign="middle" rowspan="1" colspan="1">61.16</td><td align="center" valign="middle" rowspan="1" colspan="1">17.82</td><td align="center" valign="middle" rowspan="1" colspan="1">12.99</td><td align="center" valign="middle" rowspan="1" colspan="1">9.78</td><td align="center" valign="middle" rowspan="1" colspan="1">26.00</td><td align="center" valign="middle" rowspan="1" colspan="1">17.89</td><td align="center" valign="middle" rowspan="1" colspan="1">14.18</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DD3D [<xref rid="B145-sensors-25-05264" ref-type="bibr">145</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">94.69</td><td align="center" valign="middle" rowspan="1" colspan="1">93.99</td><td align="center" valign="middle" rowspan="1" colspan="1">89.37</td><td align="center" valign="middle" rowspan="1" colspan="1">23.19</td><td align="center" valign="middle" rowspan="1" colspan="1">16.87</td><td align="center" valign="middle" rowspan="1" colspan="1">14.36</td><td align="center" valign="middle" rowspan="1" colspan="1">32.35</td><td align="center" valign="middle" rowspan="1" colspan="1">23.41</td><td align="center" valign="middle" rowspan="1" colspan="1">20.42</td><td align="center" valign="middle" rowspan="1" colspan="1">41.80</td><td align="center" valign="middle" rowspan="1" colspan="1">47.70</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PCT [<xref rid="B146-sensors-25-05264" ref-type="bibr">146</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">96.45</td><td align="center" valign="middle" rowspan="1" colspan="1">88.78</td><td align="center" valign="middle" rowspan="1" colspan="1">78.85</td><td align="center" valign="middle" rowspan="1" colspan="1">21.00</td><td align="center" valign="middle" rowspan="1" colspan="1">13.37</td><td align="center" valign="middle" rowspan="1" colspan="1">11.31</td><td align="center" valign="middle" rowspan="1" colspan="1">29.65</td><td align="center" valign="middle" rowspan="1" colspan="1">19.03</td><td align="center" valign="middle" rowspan="1" colspan="1">15.92</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" rowspan="1" colspan="1">0.045</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Autoshape [<xref rid="B147-sensors-25-05264" ref-type="bibr">147</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">86.51</td><td align="center" valign="middle" rowspan="1" colspan="1">77.60</td><td align="center" valign="middle" rowspan="1" colspan="1">64.40</td><td align="center" valign="middle" rowspan="1" colspan="1">22.47</td><td align="center" valign="middle" rowspan="1" colspan="1">14.17</td><td align="center" valign="middle" rowspan="1" colspan="1">11.36</td><td align="center" valign="middle" rowspan="1" colspan="1">30.66</td><td align="center" valign="middle" rowspan="1" colspan="1">20.08</td><td align="center" valign="middle" rowspan="1" colspan="1">15.95</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DLE [<xref rid="B148-sensors-25-05264" ref-type="bibr">148</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">94.66</td><td align="center" valign="middle" rowspan="1" colspan="1">84.45</td><td align="center" valign="middle" rowspan="1" colspan="1">62.10</td><td align="center" valign="middle" rowspan="1" colspan="1">24.23</td><td align="center" valign="middle" rowspan="1" colspan="1">14.33</td><td align="center" valign="middle" rowspan="1" colspan="1">10.30</td><td align="center" valign="middle" rowspan="1" colspan="1">31.09</td><td align="center" valign="middle" rowspan="1" colspan="1">19.05</td><td align="center" valign="middle" rowspan="1" colspan="1">14.13</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA Tesla V100</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoCon [<xref rid="B149-sensors-25-05264" ref-type="bibr">149</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">22.50</td><td align="center" valign="middle" rowspan="1" colspan="1">16.46</td><td align="center" valign="middle" rowspan="1" colspan="1">13.95</td><td align="center" valign="middle" rowspan="1" colspan="1">31.12</td><td align="center" valign="middle" rowspan="1" colspan="1">22.10</td><td align="center" valign="middle" rowspan="1" colspan="1">19.00</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDistill [<xref rid="B50-sensors-25-05264" ref-type="bibr">50</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">22.97</td><td align="center" valign="middle" rowspan="1" colspan="1">16.03</td><td align="center" valign="middle" rowspan="1" colspan="1">13.60</td><td align="center" valign="middle" rowspan="1" colspan="1">31.87</td><td align="center" valign="middle" rowspan="1" colspan="1">22.59</td><td align="center" valign="middle" rowspan="1" colspan="1">19.72</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDTR [<xref rid="B150-sensors-25-05264" ref-type="bibr">150</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">93.90</td><td align="center" valign="middle" rowspan="1" colspan="1">88.41</td><td align="center" valign="middle" rowspan="1" colspan="1">76.20</td><td align="center" valign="middle" rowspan="1" colspan="1">21.99</td><td align="center" valign="middle" rowspan="1" colspan="1">15.39</td><td align="center" valign="middle" rowspan="1" colspan="1">12.73</td><td align="center" valign="middle" rowspan="1" colspan="1">28.59</td><td align="center" valign="middle" rowspan="1" colspan="1">20.38</td><td align="center" valign="middle" rowspan="1" colspan="1">17.14</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDETR [<xref rid="B151-sensors-25-05264" ref-type="bibr">151</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">93.99</td><td align="center" valign="middle" rowspan="1" colspan="1">86.17</td><td align="center" valign="middle" rowspan="1" colspan="1">76.19</td><td align="center" valign="middle" rowspan="1" colspan="1">24.52</td><td align="center" valign="middle" rowspan="1" colspan="1">16.26</td><td align="center" valign="middle" rowspan="1" colspan="1">13.93</td><td align="center" valign="middle" rowspan="1" colspan="1">32.20</td><td align="center" valign="middle" rowspan="1" colspan="1">21.45</td><td align="center" valign="middle" rowspan="1" colspan="1">18.68</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoJSG [<xref rid="B152-sensors-25-05264" ref-type="bibr">152</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">24.69</td><td align="center" valign="middle" rowspan="1" colspan="1">16.14</td><td align="center" valign="middle" rowspan="1" colspan="1">13.64</td><td align="center" valign="middle" rowspan="1" colspan="1">32.59</td><td align="center" valign="middle" rowspan="1" colspan="1">21.26</td><td align="center" valign="middle" rowspan="1" colspan="1">18.18</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HomoLoss [<xref rid="B153-sensors-25-05264" ref-type="bibr">153</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">95.92</td><td align="center" valign="middle" rowspan="1" colspan="1">90.69</td><td align="center" valign="middle" rowspan="1" colspan="1">80.91</td><td align="center" valign="middle" rowspan="1" colspan="1">21.75</td><td align="center" valign="middle" rowspan="1" colspan="1">14.94</td><td align="center" valign="middle" rowspan="1" colspan="1">13.07</td><td align="center" valign="middle" rowspan="1" colspan="1">29.60</td><td align="center" valign="middle" rowspan="1" colspan="1">20.68</td><td align="center" valign="middle" rowspan="1" colspan="1">17.81</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDDE [<xref rid="B154-sensors-25-05264" ref-type="bibr">154</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.76</td><td align="center" valign="middle" rowspan="1" colspan="1">89.19</td><td align="center" valign="middle" rowspan="1" colspan="1">81.60</td><td align="center" valign="middle" rowspan="1" colspan="1">24.93</td><td align="center" valign="middle" rowspan="1" colspan="1">17.14</td><td align="center" valign="middle" rowspan="1" colspan="1">15.10</td><td align="center" valign="middle" rowspan="1" colspan="1">33.58</td><td align="center" valign="middle" rowspan="1" colspan="1">23.46</td><td align="center" valign="middle" rowspan="1" colspan="1">20.37</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mix-Teaching [<xref rid="B155-sensors-25-05264" ref-type="bibr">155</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.35</td><td align="center" valign="middle" rowspan="1" colspan="1">91.02</td><td align="center" valign="middle" rowspan="1" colspan="1">83.41</td><td align="center" valign="middle" rowspan="1" colspan="1">26.89</td><td align="center" valign="middle" rowspan="1" colspan="1">18.54</td><td align="center" valign="middle" rowspan="1" colspan="1">15.79</td><td align="center" valign="middle" rowspan="1" colspan="1">35.74</td><td align="center" valign="middle" rowspan="1" colspan="1">24.23</td><td align="center" valign="middle" rowspan="1" colspan="1">20.80</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">30</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DCD [<xref rid="B156-sensors-25-05264" ref-type="bibr">156</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.44</td><td align="center" valign="middle" rowspan="1" colspan="1">90.93</td><td align="center" valign="middle" rowspan="1" colspan="1">83.36</td><td align="center" valign="middle" rowspan="1" colspan="1">23.81</td><td align="center" valign="middle" rowspan="1" colspan="1">15.90</td><td align="center" valign="middle" rowspan="1" colspan="1">13.21</td><td align="center" valign="middle" rowspan="1" colspan="1">32.55</td><td align="center" valign="middle" rowspan="1" colspan="1">21.50</td><td align="center" valign="middle" rowspan="1" colspan="1">18.25</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.03</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DEVIANT [<xref rid="B157-sensors-25-05264" ref-type="bibr">157</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">94.42</td><td align="center" valign="middle" rowspan="1" colspan="1">86.64</td><td align="center" valign="middle" rowspan="1" colspan="1">76.69</td><td align="center" valign="middle" rowspan="1" colspan="1">21.88</td><td align="center" valign="middle" rowspan="1" colspan="1">14.46</td><td align="center" valign="middle" rowspan="1" colspan="1">11.89</td><td align="center" valign="middle" rowspan="1" colspan="1">29.65</td><td align="center" valign="middle" rowspan="1" colspan="1">20.44</td><td align="center" valign="middle" rowspan="1" colspan="1">17.43</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 GPU (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Cube R-CNN [<xref rid="B158-sensors-25-05264" ref-type="bibr">158</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">95.78</td><td align="center" valign="middle" rowspan="1" colspan="1">92.72</td><td align="center" valign="middle" rowspan="1" colspan="1">84.81</td><td align="center" valign="middle" rowspan="1" colspan="1">23.59</td><td align="center" valign="middle" rowspan="1" colspan="1">15.01</td><td align="center" valign="middle" rowspan="1" colspan="1">12.06</td><td align="center" valign="middle" rowspan="1" colspan="1">31.70</td><td align="center" valign="middle" rowspan="1" colspan="1">21.20</td><td align="center" valign="middle" rowspan="1" colspan="1">18.43</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MoGDE [<xref rid="B159-sensors-25-05264" ref-type="bibr">159</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">27.07</td><td align="center" valign="middle" rowspan="1" colspan="1">17.88</td><td align="center" valign="middle" rowspan="1" colspan="1">15.66</td><td align="center" valign="middle" rowspan="1" colspan="1">38.38</td><td align="center" valign="middle" rowspan="1" colspan="1">25.60</td><td align="center" valign="middle" rowspan="1" colspan="1">22.91</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ADD [<xref rid="B160-sensors-25-05264" ref-type="bibr">160</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">94.82</td><td align="center" valign="middle" rowspan="1" colspan="1">89.53</td><td align="center" valign="middle" rowspan="1" colspan="1">81.60</td><td align="center" valign="middle" rowspan="1" colspan="1">25.61</td><td align="center" valign="middle" rowspan="1" colspan="1">16.81</td><td align="center" valign="middle" rowspan="1" colspan="1">13.79</td><td align="center" valign="middle" rowspan="1" colspan="1">35.20</td><td align="center" valign="middle" rowspan="1" colspan="1">23.58</td><td align="center" valign="middle" rowspan="1" colspan="1">20.08</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CMKD [<xref rid="B161-sensors-25-05264" ref-type="bibr">161</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">95.14</td><td align="center" valign="middle" rowspan="1" colspan="1">90.28</td><td align="center" valign="middle" rowspan="1" colspan="1">83.91</td><td align="center" valign="middle" rowspan="1" colspan="1">28.55</td><td align="center" valign="middle" rowspan="1" colspan="1">18.69</td><td align="center" valign="middle" rowspan="1" colspan="1">16.77</td><td align="center" valign="middle" rowspan="1" colspan="1">38.98</td><td align="center" valign="middle" rowspan="1" colspan="1">25.82</td><td align="center" valign="middle" rowspan="1" colspan="1">22.80</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoPGC [<xref rid="B162-sensors-25-05264" ref-type="bibr">162</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">24.68</td><td align="center" valign="middle" rowspan="1" colspan="1">17.17</td><td align="center" valign="middle" rowspan="1" colspan="1">14.14</td><td align="center" valign="middle" rowspan="1" colspan="1">32.50</td><td align="center" valign="middle" rowspan="1" colspan="1">23.14</td><td align="center" valign="middle" rowspan="1" colspan="1">20.30</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoATT [<xref rid="B163-sensors-25-05264" ref-type="bibr">163</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">24.72</td><td align="center" valign="middle" rowspan="1" colspan="1">17.37</td><td align="center" valign="middle" rowspan="1" colspan="1">15.00</td><td align="center" valign="middle" rowspan="1" colspan="1">36.87</td><td align="center" valign="middle" rowspan="1" colspan="1">24.42</td><td align="center" valign="middle" rowspan="1" colspan="1">21.88</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">NeurOCS [<xref rid="B164-sensors-25-05264" ref-type="bibr">164</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">96.39</td><td align="center" valign="middle" rowspan="1" colspan="1">91.08</td><td align="center" valign="middle" rowspan="1" colspan="1">81.20</td><td align="center" valign="middle" rowspan="1" colspan="1">29.89</td><td align="center" valign="middle" rowspan="1" colspan="1">18.94</td><td align="center" valign="middle" rowspan="1" colspan="1">15.90</td><td align="center" valign="middle" rowspan="1" colspan="1">37.27</td><td align="center" valign="middle" rowspan="1" colspan="1">24.49</td><td align="center" valign="middle" rowspan="1" colspan="1">20.89</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoNerd [<xref rid="B52-sensors-25-05264" ref-type="bibr">52</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">94.60</td><td align="center" valign="middle" rowspan="1" colspan="1">86.89</td><td align="center" valign="middle" rowspan="1" colspan="1">77.23</td><td align="center" valign="middle" rowspan="1" colspan="1">22.75</td><td align="center" valign="middle" rowspan="1" colspan="1">17.13</td><td align="center" valign="middle" rowspan="1" colspan="1">15.63</td><td align="center" valign="middle" rowspan="1" colspan="1">31.13</td><td align="center" valign="middle" rowspan="1" colspan="1">23.46</td><td align="center" valign="middle" rowspan="1" colspan="1">20.97</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoSKD [<xref rid="B51-sensors-25-05264" ref-type="bibr">51</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">96.68</td><td align="center" valign="middle" rowspan="1" colspan="1">91.34</td><td align="center" valign="middle" rowspan="1" colspan="1">83.69</td><td align="center" valign="middle" rowspan="1" colspan="1">28.43</td><td align="center" valign="middle" rowspan="1" colspan="1">17.35</td><td align="center" valign="middle" rowspan="1" colspan="1">15.01</td><td align="center" valign="middle" rowspan="1" colspan="1">37.12</td><td align="center" valign="middle" rowspan="1" colspan="1">24.08</td><td align="center" valign="middle" rowspan="1" colspan="1">20.37</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ODM3D [<xref rid="B165-sensors-25-05264" ref-type="bibr">165</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">29.75</td><td align="center" valign="middle" rowspan="1" colspan="1">19.09</td><td align="center" valign="middle" rowspan="1" colspan="1">16.93</td><td align="center" valign="middle" rowspan="1" colspan="1">39.41</td><td align="center" valign="middle" rowspan="1" colspan="1">26.02</td><td align="center" valign="middle" rowspan="1" colspan="1">22.76</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoUNI [<xref rid="B166-sensors-25-05264" ref-type="bibr">166</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">94.30</td><td align="center" valign="middle" rowspan="1" colspan="1">88.96</td><td align="center" valign="middle" rowspan="1" colspan="1">78.95</td><td align="center" valign="middle" rowspan="1" colspan="1">24.75</td><td align="center" valign="middle" rowspan="1" colspan="1">16.73</td><td align="center" valign="middle" rowspan="1" colspan="1">13.49</td><td align="center" valign="middle" rowspan="1" colspan="1">33.28</td><td align="center" valign="middle" rowspan="1" colspan="1">23.05</td><td align="center" valign="middle" rowspan="1" colspan="1">19.39</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDSSM [<xref rid="B167-sensors-25-05264" ref-type="bibr">167</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">93.96</td><td align="center" valign="middle" rowspan="1" colspan="1">88.31</td><td align="center" valign="middle" rowspan="1" colspan="1">76.15</td><td align="center" valign="middle" rowspan="1" colspan="1">21.47</td><td align="center" valign="middle" rowspan="1" colspan="1">14.55</td><td align="center" valign="middle" rowspan="1" colspan="1">11.78</td><td align="center" valign="middle" rowspan="1" colspan="1">28.29</td><td align="center" valign="middle" rowspan="1" colspan="1">19.59</td><td align="center" valign="middle" rowspan="1" colspan="1">16.34</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.02</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoCD [<xref rid="B53-sensors-25-05264" ref-type="bibr">53</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">96.43</td><td align="center" valign="middle" rowspan="1" colspan="1">92.91</td><td align="center" valign="middle" rowspan="1" colspan="1">85.55</td><td align="center" valign="middle" rowspan="1" colspan="1">25.53</td><td align="center" valign="middle" rowspan="1" colspan="1">16.59</td><td align="center" valign="middle" rowspan="1" colspan="1">14.53</td><td align="center" valign="middle" rowspan="1" colspan="1">33.41</td><td align="center" valign="middle" rowspan="1" colspan="1">22.81</td><td align="center" valign="middle" rowspan="1" colspan="1">19.57</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoMAE [<xref rid="B168-sensors-25-05264" ref-type="bibr">168</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">25.60</td><td align="center" valign="middle" rowspan="1" colspan="1">18.84</td><td align="center" valign="middle" rowspan="1" colspan="1">16.78</td><td align="center" valign="middle" rowspan="1" colspan="1">34.15</td><td align="center" valign="middle" rowspan="1" colspan="1">24.93</td><td align="center" valign="middle" rowspan="1" colspan="1">21.76</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDiff [<xref rid="B169-sensors-25-05264" ref-type="bibr">169</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">30.18</td><td align="center" valign="middle" rowspan="1" colspan="1">21.02</td><td align="center" valign="middle" rowspan="1" colspan="1">18.16</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDFNet [<xref rid="B170-sensors-25-05264" ref-type="bibr">170</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">25.71</td><td align="center" valign="middle" rowspan="1" colspan="1">19.07</td><td align="center" valign="middle" rowspan="1" colspan="1">15.96</td><td align="center" valign="middle" rowspan="1" colspan="1">33.56</td><td align="center" valign="middle" rowspan="1" colspan="1">24.52</td><td align="center" valign="middle" rowspan="1" colspan="1">21.09</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DPL [<xref rid="B171-sensors-25-05264" ref-type="bibr">171</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">24.19</td><td align="center" valign="middle" rowspan="1" colspan="1">16.67</td><td align="center" valign="middle" rowspan="1" colspan="1">13.83</td><td align="center" valign="middle" rowspan="1" colspan="1">33.16</td><td align="center" valign="middle" rowspan="1" colspan="1">22.12</td><td align="center" valign="middle" rowspan="1" colspan="1">18.74</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dp-M3D [<xref rid="B172-sensors-25-05264" ref-type="bibr">172</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">23.41</td><td align="center" valign="middle" rowspan="1" colspan="1">13.65</td><td align="center" valign="middle" rowspan="1" colspan="1">12.91</td><td align="center" valign="middle" rowspan="1" colspan="1">32.38</td><td align="center" valign="middle" rowspan="1" colspan="1">20.13</td><td align="center" valign="middle" rowspan="1" colspan="1">16.58</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MonoDINO-DETR [<xref rid="B173-sensors-25-05264" ref-type="bibr">173</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pseudo-LiDAR2D [<xref rid="B174-sensors-25-05264" ref-type="bibr">174</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td colspan="18" align="left" valign="middle" rowspan="1">
<italic toggle="yes">
<bold>Stereo Camera:</bold>
</italic>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3DOP [<xref rid="B175-sensors-25-05264" ref-type="bibr">175</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2015</td><td align="center" valign="middle" rowspan="1" colspan="1">92.96</td><td align="center" valign="middle" rowspan="1" colspan="1">89.55</td><td align="center" valign="middle" rowspan="1" colspan="1">79.38</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">3.00</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Matlab + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pseudo-LiDAR [<xref rid="B176-sensors-25-05264" ref-type="bibr">176</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">85.40</td><td align="center" valign="middle" rowspan="1" colspan="1">67.79</td><td align="center" valign="middle" rowspan="1" colspan="1">58.50</td><td align="center" valign="middle" rowspan="1" colspan="1">54.53</td><td align="center" valign="middle" rowspan="1" colspan="1">34.05</td><td align="center" valign="middle" rowspan="1" colspan="1">28.25</td><td align="center" valign="middle" rowspan="1" colspan="1">67.30</td><td align="center" valign="middle" rowspan="1" colspan="1">45.00</td><td align="center" valign="middle" rowspan="1" colspan="1">38.40</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Stereo R-CNN [<xref rid="B54-sensors-25-05264" ref-type="bibr">54</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">93.98</td><td align="center" valign="middle" rowspan="1" colspan="1">85.98</td><td align="center" valign="middle" rowspan="1" colspan="1">71.25</td><td align="center" valign="middle" rowspan="1" colspan="1">47.58</td><td align="center" valign="middle" rowspan="1" colspan="1">30.23</td><td align="center" valign="middle" rowspan="1" colspan="1">23.72</td><td align="center" valign="middle" rowspan="1" colspan="1">61.92</td><td align="center" valign="middle" rowspan="1" colspan="1">41.31</td><td align="center" valign="middle" rowspan="1" colspan="1">33.42</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TLNet [<xref rid="B177-sensors-25-05264" ref-type="bibr">177</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">76.92</td><td align="center" valign="middle" rowspan="1" colspan="1">63.53</td><td align="center" valign="middle" rowspan="1" colspan="1">54.58</td><td align="center" valign="middle" rowspan="1" colspan="1">7.64</td><td align="center" valign="middle" rowspan="1" colspan="1">4.37</td><td align="center" valign="middle" rowspan="1" colspan="1">3.74</td><td align="center" valign="middle" rowspan="1" colspan="1">13.71</td><td align="center" valign="middle" rowspan="1" colspan="1">7.69</td><td align="center" valign="middle" rowspan="1" colspan="1">6.73</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pseudo-LiDAR++ [<xref rid="B178-sensors-25-05264" ref-type="bibr">178</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">94.46</td><td align="center" valign="middle" rowspan="1" colspan="1">82.90</td><td align="center" valign="middle" rowspan="1" colspan="1">75.45</td><td align="center" valign="middle" rowspan="1" colspan="1">61.11</td><td align="center" valign="middle" rowspan="1" colspan="1">42.43</td><td align="center" valign="middle" rowspan="1" colspan="1">36.99</td><td align="center" valign="middle" rowspan="1" colspan="1">78.31</td><td align="center" valign="middle" rowspan="1" colspan="1">58.01</td><td align="center" valign="middle" rowspan="1" colspan="1">51.25</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RT3D-Stereo [<xref rid="B179-sensors-25-05264" ref-type="bibr">179</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">56.53</td><td align="center" valign="middle" rowspan="1" colspan="1">45.81</td><td align="center" valign="middle" rowspan="1" colspan="1">37.63</td><td align="center" valign="middle" rowspan="1" colspan="1">29.90</td><td align="center" valign="middle" rowspan="1" colspan="1">23.28</td><td align="center" valign="middle" rowspan="1" colspan="1">18.96</td><td align="center" valign="middle" rowspan="1" colspan="1">58.81</td><td align="center" valign="middle" rowspan="1" colspan="1">46.82</td><td align="center" valign="middle" rowspan="1" colspan="1">38.38</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DSGN [<xref rid="B55-sensors-25-05264" ref-type="bibr">55</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">95.53</td><td align="center" valign="middle" rowspan="1" colspan="1">86.43</td><td align="center" valign="middle" rowspan="1" colspan="1">78.75</td><td align="center" valign="middle" rowspan="1" colspan="1">73.50</td><td align="center" valign="middle" rowspan="1" colspan="1">52.18</td><td align="center" valign="middle" rowspan="1" colspan="1">45.14</td><td align="center" valign="middle" rowspan="1" colspan="1">82.90</td><td align="center" valign="middle" rowspan="1" colspan="1">65.05</td><td align="center" valign="middle" rowspan="1" colspan="1">56.60</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.67</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA Tesla V100</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">OC-Stereo [<xref rid="B180-sensors-25-05264" ref-type="bibr">180</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">87.39</td><td align="center" valign="middle" rowspan="1" colspan="1">74.60</td><td align="center" valign="middle" rowspan="1" colspan="1">62.56</td><td align="center" valign="middle" rowspan="1" colspan="1">55.15</td><td align="center" valign="middle" rowspan="1" colspan="1">37.60</td><td align="center" valign="middle" rowspan="1" colspan="1">30.25</td><td align="center" valign="middle" rowspan="1" colspan="1">68.89</td><td align="center" valign="middle" rowspan="1" colspan="1">51.47</td><td align="center" valign="middle" rowspan="1" colspan="1">42.97</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ZoomNet [<xref rid="B181-sensors-25-05264" ref-type="bibr">181</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">94.22</td><td align="center" valign="middle" rowspan="1" colspan="1">83.92</td><td align="center" valign="middle" rowspan="1" colspan="1">69.00</td><td align="center" valign="middle" rowspan="1" colspan="1">55.98</td><td align="center" valign="middle" rowspan="1" colspan="1">38.64</td><td align="center" valign="middle" rowspan="1" colspan="1">30.97</td><td align="center" valign="middle" rowspan="1" colspan="1">72.94</td><td align="center" valign="middle" rowspan="1" colspan="1">54.91</td><td align="center" valign="middle" rowspan="1" colspan="1">44.14</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Disp R-CNN [<xref rid="B182-sensors-25-05264" ref-type="bibr">182</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">93.45</td><td align="center" valign="middle" rowspan="1" colspan="1">82.64</td><td align="center" valign="middle" rowspan="1" colspan="1">70.45</td><td align="center" valign="middle" rowspan="1" colspan="1">68.21</td><td align="center" valign="middle" rowspan="1" colspan="1">45.78</td><td align="center" valign="middle" rowspan="1" colspan="1">37.73</td><td align="center" valign="middle" rowspan="1" colspan="1">79.76</td><td align="center" valign="middle" rowspan="1" colspan="1">58.62</td><td align="center" valign="middle" rowspan="1" colspan="1">47.73</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.387</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pseudo-LiDAR E2E [<xref rid="B183-sensors-25-05264" ref-type="bibr">183</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">64.75</td><td align="center" valign="middle" rowspan="1" colspan="1">43.92</td><td align="center" valign="middle" rowspan="1" colspan="1">38.14</td><td align="center" valign="middle" rowspan="1" colspan="1">79.60</td><td align="center" valign="middle" rowspan="1" colspan="1">58.80</td><td align="center" valign="middle" rowspan="1" colspan="1">52.10</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CDN [<xref rid="B184-sensors-25-05264" ref-type="bibr">184</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">95.85</td><td align="center" valign="middle" rowspan="1" colspan="1">87.19</td><td align="center" valign="middle" rowspan="1" colspan="1">79.43</td><td align="center" valign="middle" rowspan="1" colspan="1">74.52</td><td align="center" valign="middle" rowspan="1" colspan="1">54.22</td><td align="center" valign="middle" rowspan="1" colspan="1">46.36</td><td align="center" valign="middle" rowspan="1" colspan="1">83.32</td><td align="center" valign="middle" rowspan="1" colspan="1">66.24</td><td align="center" valign="middle" rowspan="1" colspan="1">57.65</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.60</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CG-Stereo [<xref rid="B185-sensors-25-05264" ref-type="bibr">185</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">96.31</td><td align="center" valign="middle" rowspan="1" colspan="1">90.38</td><td align="center" valign="middle" rowspan="1" colspan="1">82.80</td><td align="center" valign="middle" rowspan="1" colspan="1">74.39</td><td align="center" valign="middle" rowspan="1" colspan="1">53.58</td><td align="center" valign="middle" rowspan="1" colspan="1">46.50</td><td align="center" valign="middle" rowspan="1" colspan="1">85.29</td><td align="center" valign="middle" rowspan="1" colspan="1">66.44</td><td align="center" valign="middle" rowspan="1" colspan="1">58.95</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.57</td><td align="center" valign="middle" rowspan="1" colspan="1">GeForce RTX 2080 Ti</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RTS3D [<xref rid="B186-sensors-25-05264" ref-type="bibr">186</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">58.51</td><td align="center" valign="middle" rowspan="1" colspan="1">37.38</td><td align="center" valign="middle" rowspan="1" colspan="1">31.12</td><td align="center" valign="middle" rowspan="1" colspan="1">72.17</td><td align="center" valign="middle" rowspan="1" colspan="1">45.22</td><td align="center" valign="middle" rowspan="1" colspan="1">38.48</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RT3D-GMP [<xref rid="B187-sensors-25-05264" ref-type="bibr">187</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">62.41</td><td align="center" valign="middle" rowspan="1" colspan="1">51.95</td><td align="center" valign="middle" rowspan="1" colspan="1">39.14</td><td align="center" valign="middle" rowspan="1" colspan="1">16.23</td><td align="center" valign="middle" rowspan="1" colspan="1">11.41</td><td align="center" valign="middle" rowspan="1" colspan="1">10.12</td><td align="center" valign="middle" rowspan="1" colspan="1">69.14</td><td align="center" valign="middle" rowspan="1" colspan="1">59.00</td><td align="center" valign="middle" rowspan="1" colspan="1">45.49</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOStereo3D [<xref rid="B56-sensors-25-05264" ref-type="bibr">56</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">94.81</td><td align="center" valign="middle" rowspan="1" colspan="1">82.15</td><td align="center" valign="middle" rowspan="1" colspan="1">62.17</td><td align="center" valign="middle" rowspan="1" colspan="1">65.68</td><td align="center" valign="middle" rowspan="1" colspan="1">41.25</td><td align="center" valign="middle" rowspan="1" colspan="1">30.42</td><td align="center" valign="middle" rowspan="1" colspan="1">76.10</td><td align="center" valign="middle" rowspan="1" colspan="1">50.28</td><td align="center" valign="middle" rowspan="1" colspan="1">36.86</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU 1080Ti</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SIDE [<xref rid="B188-sensors-25-05264" ref-type="bibr">188</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">47.69</td><td align="center" valign="middle" rowspan="1" colspan="1">30.82</td><td align="center" valign="middle" rowspan="1" colspan="1">25.68</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LIGA-Stereo [<xref rid="B57-sensors-25-05264" ref-type="bibr">57</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">96.43</td><td align="center" valign="middle" rowspan="1" colspan="1">93.82</td><td align="center" valign="middle" rowspan="1" colspan="1">86.19</td><td align="center" valign="middle" rowspan="1" colspan="1">81.39</td><td align="center" valign="middle" rowspan="1" colspan="1">64.66</td><td align="center" valign="middle" rowspan="1" colspan="1">57.22</td><td align="center" valign="middle" rowspan="1" colspan="1">88.15</td><td align="center" valign="middle" rowspan="1" colspan="1">76.78</td><td align="center" valign="middle" rowspan="1" colspan="1">67.40</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">StereoCenterNet [<xref rid="B189-sensors-25-05264" ref-type="bibr">189</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">96.61</td><td align="center" valign="middle" rowspan="1" colspan="1">91.27</td><td align="center" valign="middle" rowspan="1" colspan="1">93.50</td><td align="center" valign="middle" rowspan="1" colspan="1">49.44</td><td align="center" valign="middle" rowspan="1" colspan="1">31.30</td><td align="center" valign="middle" rowspan="1" colspan="1">25.62</td><td align="center" valign="middle" rowspan="1" colspan="1">62.97</td><td align="center" valign="middle" rowspan="1" colspan="1">42.12</td><td align="center" valign="middle" rowspan="1" colspan="1">35.37</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ESGN [<xref rid="B190-sensors-25-05264" ref-type="bibr">190</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">44.09</td><td align="center" valign="middle" rowspan="1" colspan="1">32.60</td><td align="center" valign="middle" rowspan="1" colspan="1">29.10</td><td align="center" valign="middle" rowspan="1" colspan="1">65.80</td><td align="center" valign="middle" rowspan="1" colspan="1">46.39</td><td align="center" valign="middle" rowspan="1" colspan="1">38.42</td><td align="center" valign="middle" rowspan="1" colspan="1">78.10</td><td align="center" valign="middle" rowspan="1" colspan="1">58.12</td><td align="center" valign="middle" rowspan="1" colspan="1">49.28</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pseudo-Stereo [<xref rid="B191-sensors-25-05264" ref-type="bibr">191</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">95.75</td><td align="center" valign="middle" rowspan="1" colspan="1">90.27</td><td align="center" valign="middle" rowspan="1" colspan="1">82.32</td><td align="center" valign="middle" rowspan="1" colspan="1">23.74</td><td align="center" valign="middle" rowspan="1" colspan="1">17.74</td><td align="center" valign="middle" rowspan="1" colspan="1">15.14</td><td align="center" valign="middle" rowspan="1" colspan="1">32.64</td><td align="center" valign="middle" rowspan="1" colspan="1">23.76</td><td align="center" valign="middle" rowspan="1" colspan="1">20.64</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DSGN++ [<xref rid="B192-sensors-25-05264" ref-type="bibr">192</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">98.08</td><td align="center" valign="middle" rowspan="1" colspan="1">95.70</td><td align="center" valign="middle" rowspan="1" colspan="1">88.27</td><td align="center" valign="middle" rowspan="1" colspan="1">83.21</td><td align="center" valign="middle" rowspan="1" colspan="1">67.37</td><td align="center" valign="middle" rowspan="1" colspan="1">59.91</td><td align="center" valign="middle" rowspan="1" colspan="1">88.55</td><td align="center" valign="middle" rowspan="1" colspan="1">78.94</td><td align="center" valign="middle" rowspan="1" colspan="1">69.74</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">GeForce RTX 2080 Ti</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DID-M3D [<xref rid="B193-sensors-25-05264" ref-type="bibr">193</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">94.29</td><td align="center" valign="middle" rowspan="1" colspan="1">91.04</td><td align="center" valign="middle" rowspan="1" colspan="1">81.31</td><td align="center" valign="middle" rowspan="1" colspan="1">24.40</td><td align="center" valign="middle" rowspan="1" colspan="1">16.29</td><td align="center" valign="middle" rowspan="1" colspan="1">13.75</td><td align="center" valign="middle" rowspan="1" colspan="1">32.95</td><td align="center" valign="middle" rowspan="1" colspan="1">22.76</td><td align="center" valign="middle" rowspan="1" colspan="1">19.83</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DMF [<xref rid="B194-sensors-25-05264" ref-type="bibr">194</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">89.50</td><td align="center" valign="middle" rowspan="1" colspan="1">85.49</td><td align="center" valign="middle" rowspan="1" colspan="1">82.52</td><td align="center" valign="middle" rowspan="1" colspan="1">77.55</td><td align="center" valign="middle" rowspan="1" colspan="1">67.33</td><td align="center" valign="middle" rowspan="1" colspan="1">62.44</td><td align="center" valign="middle" rowspan="1" colspan="1">84.64</td><td align="center" valign="middle" rowspan="1" colspan="1">80.29</td><td align="center" valign="middle" rowspan="1" colspan="1">76.05</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">StereoDistill [<xref rid="B58-sensors-25-05264" ref-type="bibr">58</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">97.61</td><td align="center" valign="middle" rowspan="1" colspan="1">93.43</td><td align="center" valign="middle" rowspan="1" colspan="1">87.71</td><td align="center" valign="middle" rowspan="1" colspan="1">81.66</td><td align="center" valign="middle" rowspan="1" colspan="1">66.39</td><td align="center" valign="middle" rowspan="1" colspan="1">57.39</td><td align="center" valign="middle" rowspan="1" colspan="1">89.03</td><td align="center" valign="middle" rowspan="1" colspan="1">78.59</td><td align="center" valign="middle" rowspan="1" colspan="1">69.34</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PS-SVDM [<xref rid="B195-sensors-25-05264" ref-type="bibr">195</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">94.49</td><td align="center" valign="middle" rowspan="1" colspan="1">87.55</td><td align="center" valign="middle" rowspan="1" colspan="1">78.21</td><td align="center" valign="middle" rowspan="1" colspan="1">29.22</td><td align="center" valign="middle" rowspan="1" colspan="1">18.13</td><td align="center" valign="middle" rowspan="1" colspan="1">15.35</td><td align="center" valign="middle" rowspan="1" colspan="1">38.18</td><td align="center" valign="middle" rowspan="1" colspan="1">24.82</td><td align="center" valign="middle" rowspan="1" colspan="1">20.89</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td colspan="18" align="left" valign="middle" rowspan="1">
<italic toggle="yes">
<bold>Multi-View Camera:</bold>
</italic>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3DOMV [<xref rid="B96-sensors-25-05264" ref-type="bibr">96</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MVRA [<xref rid="B196-sensors-25-05264" ref-type="bibr">196</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">95.87</td><td align="center" valign="middle" rowspan="1" colspan="1">94.98</td><td align="center" valign="middle" rowspan="1" colspan="1">82.52</td><td align="center" valign="middle" rowspan="1" colspan="1">5.19</td><td align="center" valign="middle" rowspan="1" colspan="1">3.27</td><td align="center" valign="middle" rowspan="1" colspan="1">2.49</td><td align="center" valign="middle" rowspan="1" colspan="1">9.05</td><td align="center" valign="middle" rowspan="1" colspan="1">5.84</td><td align="center" valign="middle" rowspan="1" colspan="1">4.50</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DETR3D [<xref rid="B59-sensors-25-05264" ref-type="bibr">59</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">41.2</td><td align="center" valign="middle" rowspan="1" colspan="1">47.9</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BEVDet [<xref rid="B197-sensors-25-05264" ref-type="bibr">197</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">39.8</td><td align="center" valign="middle" rowspan="1" colspan="1">46.3</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEVDepth [<xref rid="B198-sensors-25-05264" ref-type="bibr">198</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#215;</td></tr></tbody></table></table-wrap><table-wrap position="anchor" id="sensors-25-05264-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05264-t004_Table 4</object-id><label>Table 4</label><caption><p>LiDAR-based 3D object detection results on KITTI car test set, nuScenes test set, and Waymo validation set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Year</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>2D</sub></th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>3D</sub></th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>BEV</sub></th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">nuScenes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Waymo</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Time (s)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Hardware</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Code Available</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">E</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">E</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">E</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NDS</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L1 mAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L2 mAP</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ImVoxelNet [<xref rid="B199-sensors-25-05264" ref-type="bibr">199</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">89.90</td><td align="center" valign="middle" rowspan="1" colspan="1">79.09</td><td align="center" valign="middle" rowspan="1" colspan="1">69.45</td><td align="center" valign="middle" rowspan="1" colspan="1">17.15</td><td align="center" valign="middle" rowspan="1" colspan="1">10.97</td><td align="center" valign="middle" rowspan="1" colspan="1">9.15</td><td align="center" valign="middle" rowspan="1" colspan="1">25.19</td><td align="center" valign="middle" rowspan="1" colspan="1">16.37</td><td align="center" valign="middle" rowspan="1" colspan="1">13.58</td><td align="center" valign="middle" rowspan="1" colspan="1">41.2</td><td align="center" valign="middle" rowspan="1" colspan="1">47.9</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PETR [<xref rid="B60-sensors-25-05264" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">44.5</td><td align="center" valign="middle" rowspan="1" colspan="1">50.4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">STS [<xref rid="B200-sensors-25-05264" ref-type="bibr">200</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">42.2</td><td align="center" valign="middle" rowspan="1" colspan="1">52.9</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BEVerse [<xref rid="B201-sensors-25-05264" ref-type="bibr">201</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">39.3</td><td align="center" valign="middle" rowspan="1" colspan="1">53.1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BEVFormer [<xref rid="B61-sensors-25-05264" ref-type="bibr">61</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">48.1</td><td align="center" valign="middle" rowspan="1" colspan="1">56.9</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SOLOFusion</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">54.0</td><td align="center" valign="middle" rowspan="1" colspan="1">61.9</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PolarFormer [<xref rid="B202-sensors-25-05264" ref-type="bibr">202</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">45.6</td><td align="center" valign="middle" rowspan="1" colspan="1">54.3</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FocalPETR [<xref rid="B203-sensors-25-05264" ref-type="bibr">203</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">46.5</td><td align="center" valign="middle" rowspan="1" colspan="1">51.6</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BEV Distill [<xref rid="B204-sensors-25-05264" ref-type="bibr">204</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">49.6</td><td align="center" valign="middle" rowspan="1" colspan="1">59.4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HoP [<xref rid="B205-sensors-25-05264" ref-type="bibr">205</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">62.4</td><td align="center" valign="middle" rowspan="1" colspan="1">68.5</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SparseBEV [<xref rid="B62-sensors-25-05264" ref-type="bibr">62</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">60.3</td><td align="center" valign="middle" rowspan="1" colspan="1">67.5</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">StreamPETR [<xref rid="B206-sensors-25-05264" ref-type="bibr">206</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">55.0</td><td align="center" valign="middle" rowspan="1" colspan="1">63.1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PolarBEVDet [<xref rid="B207-sensors-25-05264" ref-type="bibr">207</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">55.8</td><td align="center" valign="middle" rowspan="1" colspan="1">63.5</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RoPETR [<xref rid="B63-sensors-25-05264" ref-type="bibr">63</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">64.8</td><td align="center" valign="middle" rowspan="1" colspan="1">70.9</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td colspan="18" align="left" valign="middle" rowspan="1">
<italic toggle="yes">
<bold>Projection-Based:</bold>
</italic>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C-YOLO [<xref rid="B68-sensors-25-05264" ref-type="bibr">68</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">67.72</td><td align="center" valign="middle" rowspan="1" colspan="1">64.00</td><td align="center" valign="middle" rowspan="1" colspan="1">63.01</td><td align="center" valign="middle" rowspan="1" colspan="1">85.89</td><td align="center" valign="middle" rowspan="1" colspan="1">77.40</td><td align="center" valign="middle" rowspan="1" colspan="1">77.33</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TopNet [<xref rid="B208-sensors-25-05264" ref-type="bibr">208</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">58.04</td><td align="center" valign="middle" rowspan="1" colspan="1">45.85</td><td align="center" valign="middle" rowspan="1" colspan="1">41.11</td><td align="center" valign="middle" rowspan="1" colspan="1">12.67</td><td align="center" valign="middle" rowspan="1" colspan="1">9.28</td><td align="center" valign="middle" rowspan="1" colspan="1">7.95</td><td align="center" valign="middle" rowspan="1" colspan="1">80.16</td><td align="center" valign="middle" rowspan="1" colspan="1">68.16</td><td align="center" valign="middle" rowspan="1" colspan="1">63.43</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" rowspan="1" colspan="1">NVIDIA GeForce 1080 Ti (TF-GPU)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BirdNet [<xref rid="B69-sensors-25-05264" ref-type="bibr">69</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">79.30</td><td align="center" valign="middle" rowspan="1" colspan="1">57.12</td><td align="center" valign="middle" rowspan="1" colspan="1">55.16</td><td align="center" valign="middle" rowspan="1" colspan="1">40.99</td><td align="center" valign="middle" rowspan="1" colspan="1">27.26</td><td align="center" valign="middle" rowspan="1" colspan="1">25.32</td><td align="center" valign="middle" rowspan="1" colspan="1">84.17</td><td align="center" valign="middle" rowspan="1" colspan="1">59.83</td><td align="center" valign="middle" rowspan="1" colspan="1">57.35</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">Titan Xp (Caffe)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PIXOR [<xref rid="B67-sensors-25-05264" ref-type="bibr">67</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">81.70</td><td align="center" valign="middle" rowspan="1" colspan="1">77.05</td><td align="center" valign="middle" rowspan="1" colspan="1">72.95</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FVNet [<xref rid="B209-sensors-25-05264" ref-type="bibr">209</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">86.14</td><td align="center" valign="middle" rowspan="1" colspan="1">77.19</td><td align="center" valign="middle" rowspan="1" colspan="1">69.27</td><td align="center" valign="middle" rowspan="1" colspan="1">65.43</td><td align="center" valign="middle" rowspan="1" colspan="1">57.34</td><td align="center" valign="middle" rowspan="1" colspan="1">51.85</td><td align="center" valign="middle" rowspan="1" colspan="1">78.04</td><td align="center" valign="middle" rowspan="1" colspan="1">65.03</td><td align="center" valign="middle" rowspan="1" colspan="1">57.89</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MODet [<xref rid="B210-sensors-25-05264" ref-type="bibr">210</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">66.06</td><td align="center" valign="middle" rowspan="1" colspan="1">62.54</td><td align="center" valign="middle" rowspan="1" colspan="1">60.04</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">90.80</td><td align="center" valign="middle" rowspan="1" colspan="1">87.56</td><td align="center" valign="middle" rowspan="1" colspan="1">82.69</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">GTX1080Ti</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HDNet [<xref rid="B211-sensors-25-05264" ref-type="bibr">211</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">89.14</td><td align="center" valign="middle" rowspan="1" colspan="1">86.57</td><td align="center" valign="middle" rowspan="1" colspan="1">78.32</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PIXOR++ [<xref rid="B211-sensors-25-05264" ref-type="bibr">211</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">93.28</td><td align="center" valign="middle" rowspan="1" colspan="1">86.01</td><td align="center" valign="middle" rowspan="1" colspan="1">80.11</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BirdNet+ [<xref rid="B212-sensors-25-05264" ref-type="bibr">212</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">92.61</td><td align="center" valign="middle" rowspan="1" colspan="1">86.73</td><td align="center" valign="middle" rowspan="1" colspan="1">81.80</td><td align="center" valign="middle" rowspan="1" colspan="1">76.15</td><td align="center" valign="middle" rowspan="1" colspan="1">64.04</td><td align="center" valign="middle" rowspan="1" colspan="1">59.79</td><td align="center" valign="middle" rowspan="1" colspan="1">87.43</td><td align="center" valign="middle" rowspan="1" colspan="1">81.85</td><td align="center" valign="middle" rowspan="1" colspan="1">75.36</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">Titan Xp (Caffe)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MGTANet [<xref rid="B213-sensors-25-05264" ref-type="bibr">213</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">67.50</td><td align="center" valign="middle" rowspan="1" colspan="1">72.70</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GPA3D [<xref rid="B214-sensors-25-05264" ref-type="bibr">214</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td colspan="18" align="left" valign="middle" rowspan="1">
<italic toggle="yes">
<bold>Voxel-Based:</bold>
</italic>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Vote3D [<xref rid="B215-sensors-25-05264" ref-type="bibr">215</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2015</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VoxelNet [<xref rid="B75-sensors-25-05264" ref-type="bibr">75</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">77.82</td><td align="center" valign="middle" rowspan="1" colspan="1">64.17</td><td align="center" valign="middle" rowspan="1" colspan="1">57.51</td><td align="center" valign="middle" rowspan="1" colspan="1">87.95</td><td align="center" valign="middle" rowspan="1" colspan="1">78.39</td><td align="center" valign="middle" rowspan="1" colspan="1">71.29</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SECOND [<xref rid="B76-sensors-25-05264" ref-type="bibr">76</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">83.13</td><td align="center" valign="middle" rowspan="1" colspan="1">73.66</td><td align="center" valign="middle" rowspan="1" colspan="1">66.20</td><td align="center" valign="middle" rowspan="1" colspan="1">89.39</td><td align="center" valign="middle" rowspan="1" colspan="1">83.77</td><td align="center" valign="middle" rowspan="1" colspan="1">78.59</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointPillars [<xref rid="B77-sensors-25-05264" ref-type="bibr">77</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">94.00</td><td align="center" valign="middle" rowspan="1" colspan="1">91.19</td><td align="center" valign="middle" rowspan="1" colspan="1">88.17</td><td align="center" valign="middle" rowspan="1" colspan="1">82.58</td><td align="center" valign="middle" rowspan="1" colspan="1">74.31</td><td align="center" valign="middle" rowspan="1" colspan="1">68.99</td><td align="center" valign="middle" rowspan="1" colspan="1">90.07</td><td align="center" valign="middle" rowspan="1" colspan="1">86.56</td><td align="center" valign="middle" rowspan="1" colspan="1">82.81</td><td align="center" valign="middle" rowspan="1" colspan="1">40.10</td><td align="center" valign="middle" rowspan="1" colspan="1">55.00</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.016</td><td align="center" valign="middle" rowspan="1" colspan="1">1080 Ti + Intel i7</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HotSpotNet [<xref rid="B78-sensors-25-05264" ref-type="bibr">78</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">96.21</td><td align="center" valign="middle" rowspan="1" colspan="1">92.81</td><td align="center" valign="middle" rowspan="1" colspan="1">89.80</td><td align="center" valign="middle" rowspan="1" colspan="1">87.60</td><td align="center" valign="middle" rowspan="1" colspan="1">78.31</td><td align="center" valign="middle" rowspan="1" colspan="1">73.34</td><td align="center" valign="middle" rowspan="1" colspan="1">94.06</td><td align="center" valign="middle" rowspan="1" colspan="1">88.09</td><td align="center" valign="middle" rowspan="1" colspan="1">83.24</td><td align="center" valign="middle" rowspan="1" colspan="1">59.30</td><td align="center" valign="middle" rowspan="1" colspan="1">66.00</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Voxel R-CNN [<xref rid="B79-sensors-25-05264" ref-type="bibr">79</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">96.49</td><td align="center" valign="middle" rowspan="1" colspan="1">95.11</td><td align="center" valign="middle" rowspan="1" colspan="1">92.45</td><td align="center" valign="middle" rowspan="1" colspan="1">90.90</td><td align="center" valign="middle" rowspan="1" colspan="1">81.62</td><td align="center" valign="middle" rowspan="1" colspan="1">77.06</td><td align="center" valign="middle" rowspan="1" colspan="1">94.85</td><td align="center" valign="middle" rowspan="1" colspan="1">88.83</td><td align="center" valign="middle" rowspan="1" colspan="1">86.13</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">75.59</td><td align="center" valign="middle" rowspan="1" colspan="1">66.59</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 3.0 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VoTr-TSD [<xref rid="B80-sensors-25-05264" ref-type="bibr">80</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">95.95</td><td align="center" valign="middle" rowspan="1" colspan="1">94.81</td><td align="center" valign="middle" rowspan="1" colspan="1">92.24</td><td align="center" valign="middle" rowspan="1" colspan="1">89.90</td><td align="center" valign="middle" rowspan="1" colspan="1">82.09</td><td align="center" valign="middle" rowspan="1" colspan="1">79.14</td><td align="center" valign="middle" rowspan="1" colspan="1">94.03</td><td align="center" valign="middle" rowspan="1" colspan="1">90.34</td><td align="center" valign="middle" rowspan="1" colspan="1">86.14</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">74.95</td><td align="center" valign="middle" rowspan="1" colspan="1">65.91</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TED [<xref rid="B81-sensors-25-05264" ref-type="bibr">81</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.64</td><td align="center" valign="middle" rowspan="1" colspan="1">96.03</td><td align="center" valign="middle" rowspan="1" colspan="1">93.35</td><td align="center" valign="middle" rowspan="1" colspan="1">91.61</td><td align="center" valign="middle" rowspan="1" colspan="1">85.28</td><td align="center" valign="middle" rowspan="1" colspan="1">80.68</td><td align="center" valign="middle" rowspan="1" colspan="1">95.44</td><td align="center" valign="middle" rowspan="1" colspan="1">92.05</td><td align="center" valign="middle" rowspan="1" colspan="1">87.30</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VoxSeT [<xref rid="B216-sensors-25-05264" ref-type="bibr">216</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.16</td><td align="center" valign="middle" rowspan="1" colspan="1">95.23</td><td align="center" valign="middle" rowspan="1" colspan="1">90.49</td><td align="center" valign="middle" rowspan="1" colspan="1">88.53</td><td align="center" valign="middle" rowspan="1" colspan="1">82.06</td><td align="center" valign="middle" rowspan="1" colspan="1">77.46</td><td align="center" valign="middle" rowspan="1" colspan="1">92.70</td><td align="center" valign="middle" rowspan="1" colspan="1">89.07</td><td align="center" valign="middle" rowspan="1" colspan="1">86.29</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.033</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FocalsConv [<xref rid="B217-sensors-25-05264" ref-type="bibr">217</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.30</td><td align="center" valign="middle" rowspan="1" colspan="1">95.28</td><td align="center" valign="middle" rowspan="1" colspan="1">92.69</td><td align="center" valign="middle" rowspan="1" colspan="1">90.55</td><td align="center" valign="middle" rowspan="1" colspan="1">82.28</td><td align="center" valign="middle" rowspan="1" colspan="1">77.59</td><td align="center" valign="middle" rowspan="1" colspan="1">92.67</td><td align="center" valign="middle" rowspan="1" colspan="1">89.00</td><td align="center" valign="middle" rowspan="1" colspan="1">86.33</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PillarNet [<xref rid="B218-sensors-25-05264" ref-type="bibr">218</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">66.00</td><td align="center" valign="middle" rowspan="1" colspan="1">71.40</td><td align="center" valign="middle" rowspan="1" colspan="1">83.23</td><td align="center" valign="middle" rowspan="1" colspan="1">76.09</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SWFormer [<xref rid="B219-sensors-25-05264" ref-type="bibr">219</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">77.8</td><td align="center" valign="middle" rowspan="1" colspan="1">69.2</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PV-GNN [<xref rid="B220-sensors-25-05264" ref-type="bibr">220</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">91.64</td><td align="center" valign="middle" rowspan="1" colspan="1">82.49</td><td align="center" valign="middle" rowspan="1" colspan="1">77.28</td><td align="center" valign="middle" rowspan="1" colspan="1">95.09</td><td align="center" valign="middle" rowspan="1" colspan="1">92.38</td><td align="center" valign="middle" rowspan="1" colspan="1">87.44</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">
<bold>Point-Based:</bold>
</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">iPOD [<xref rid="B221-sensors-25-05264" ref-type="bibr">221</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">90.20</td><td align="center" valign="middle" rowspan="1" colspan="1">89.30</td><td align="center" valign="middle" rowspan="1" colspan="1">87.37</td><td align="center" valign="middle" rowspan="1" colspan="1">71.40</td><td align="center" valign="middle" rowspan="1" colspan="1">53.46</td><td align="center" valign="middle" rowspan="1" colspan="1">48.34</td><td align="center" valign="middle" rowspan="1" colspan="1">86.93</td><td align="center" valign="middle" rowspan="1" colspan="1">83.98</td><td align="center" valign="middle" rowspan="1" colspan="1">77.85</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointRCNN [<xref rid="B72-sensors-25-05264" ref-type="bibr">72</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">95.92</td><td align="center" valign="middle" rowspan="1" colspan="1">91.90</td><td align="center" valign="middle" rowspan="1" colspan="1">87.11</td><td align="center" valign="middle" rowspan="1" colspan="1">86.96</td><td align="center" valign="middle" rowspan="1" colspan="1">75.64</td><td align="center" valign="middle" rowspan="1" colspan="1">70.70</td><td align="center" valign="middle" rowspan="1" colspan="1">92.13</td><td align="center" valign="middle" rowspan="1" colspan="1">87.39</td><td align="center" valign="middle" rowspan="1" colspan="1">82.72</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">STD [<xref rid="B222-sensors-25-05264" ref-type="bibr">222</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">96.14</td><td align="center" valign="middle" rowspan="1" colspan="1">93.22</td><td align="center" valign="middle" rowspan="1" colspan="1">90.53</td><td align="center" valign="middle" rowspan="1" colspan="1">87.95</td><td align="center" valign="middle" rowspan="1" colspan="1">79.71</td><td align="center" valign="middle" rowspan="1" colspan="1">75.09</td><td align="center" valign="middle" rowspan="1" colspan="1">94.74</td><td align="center" valign="middle" rowspan="1" colspan="1">89.19</td><td align="center" valign="middle" rowspan="1" colspan="1">86.42</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointRGCN [<xref rid="B223-sensors-25-05264" ref-type="bibr">223</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">96.19</td><td align="center" valign="middle" rowspan="1" colspan="1">92.67</td><td align="center" valign="middle" rowspan="1" colspan="1">87.66</td><td align="center" valign="middle" rowspan="1" colspan="1">85.97</td><td align="center" valign="middle" rowspan="1" colspan="1">75.73</td><td align="center" valign="middle" rowspan="1" colspan="1">70.60</td><td align="center" valign="middle" rowspan="1" colspan="1">91.63</td><td align="center" valign="middle" rowspan="1" colspan="1">87.49</td><td align="center" valign="middle" rowspan="1" colspan="1">90.73</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.26</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ V100 (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3DSSD [<xref rid="B224-sensors-25-05264" ref-type="bibr">224</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">97.69</td><td align="center" valign="middle" rowspan="1" colspan="1">95.10</td><td align="center" valign="middle" rowspan="1" colspan="1">92.18</td><td align="center" valign="middle" rowspan="1" colspan="1">88.36</td><td align="center" valign="middle" rowspan="1" colspan="1">79.57</td><td align="center" valign="middle" rowspan="1" colspan="1">74.55</td><td align="center" valign="middle" rowspan="1" colspan="1">92.66</td><td align="center" valign="middle" rowspan="1" colspan="1">89.02</td><td align="center" valign="middle" rowspan="1" colspan="1">85.86</td><td align="center" valign="middle" rowspan="1" colspan="1">42.60</td><td align="center" valign="middle" rowspan="1" colspan="1">56.40</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Point-GNN [<xref rid="B225-sensors-25-05264" ref-type="bibr">225</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">96.58</td><td align="center" valign="middle" rowspan="1" colspan="1">93.50</td><td align="center" valign="middle" rowspan="1" colspan="1">88.35</td><td align="center" valign="middle" rowspan="1" colspan="1">88.33</td><td align="center" valign="middle" rowspan="1" colspan="1">79.47</td><td align="center" valign="middle" rowspan="1" colspan="1">72.29</td><td align="center" valign="middle" rowspan="1" colspan="1">93.11</td><td align="center" valign="middle" rowspan="1" colspan="1">89.17</td><td align="center" valign="middle" rowspan="1" colspan="1">83.90</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.60</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointFormer [<xref rid="B74-sensors-25-05264" ref-type="bibr">74</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">87.13</td><td align="center" valign="middle" rowspan="1" colspan="1">77.06</td><td align="center" valign="middle" rowspan="1" colspan="1">69.25</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">53.60</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EPNet++ [<xref rid="B226-sensors-25-05264" ref-type="bibr">226</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">96.73</td><td align="center" valign="middle" rowspan="1" colspan="1">95.17</td><td align="center" valign="middle" rowspan="1" colspan="1">92.10</td><td align="center" valign="middle" rowspan="1" colspan="1">91.37</td><td align="center" valign="middle" rowspan="1" colspan="1">81.96</td><td align="center" valign="middle" rowspan="1" colspan="1">76.71</td><td align="center" valign="middle" rowspan="1" colspan="1">95.41</td><td align="center" valign="middle" rowspan="1" colspan="1">89.00</td><td align="center" valign="middle" rowspan="1" colspan="1">85.73</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SASA [<xref rid="B227-sensors-25-05264" ref-type="bibr">227</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.01</td><td align="center" valign="middle" rowspan="1" colspan="1">95.35</td><td align="center" valign="middle" rowspan="1" colspan="1">92.42</td><td align="center" valign="middle" rowspan="1" colspan="1">88.76</td><td align="center" valign="middle" rowspan="1" colspan="1">82.16</td><td align="center" valign="middle" rowspan="1" colspan="1">77.16</td><td align="center" valign="middle" rowspan="1" colspan="1">92.87</td><td align="center" valign="middle" rowspan="1" colspan="1">89.51</td><td align="center" valign="middle" rowspan="1" colspan="1">86.35</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">IA-SSD [<xref rid="B73-sensors-25-05264" ref-type="bibr">73</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.10</td><td align="center" valign="middle" rowspan="1" colspan="1">93.56</td><td align="center" valign="middle" rowspan="1" colspan="1">90.68</td><td align="center" valign="middle" rowspan="1" colspan="1">88.27</td><td align="center" valign="middle" rowspan="1" colspan="1">80.32</td><td align="center" valign="middle" rowspan="1" colspan="1">75.10</td><td align="center" valign="middle" rowspan="1" colspan="1">92.79</td><td align="center" valign="middle" rowspan="1" colspan="1">89.33</td><td align="center" valign="middle" rowspan="1" colspan="1">84.35</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DFAF3D [<xref rid="B228-sensors-25-05264" ref-type="bibr">228</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">96.58</td><td align="center" valign="middle" rowspan="1" colspan="1">93.32</td><td align="center" valign="middle" rowspan="1" colspan="1">90.24</td><td align="center" valign="middle" rowspan="1" colspan="1">88.59</td><td align="center" valign="middle" rowspan="1" colspan="1">79.37</td><td align="center" valign="middle" rowspan="1" colspan="1">72.21</td><td align="center" valign="middle" rowspan="1" colspan="1">93.14</td><td align="center" valign="middle" rowspan="1" colspan="1">89.45</td><td align="center" valign="middle" rowspan="1" colspan="1">84.22</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HINTED [<xref rid="B229-sensors-25-05264" ref-type="bibr">229</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">95.16</td><td align="center" valign="middle" rowspan="1" colspan="1">90.97</td><td align="center" valign="middle" rowspan="1" colspan="1">85.55</td><td align="center" valign="middle" rowspan="1" colspan="1">84.00</td><td align="center" valign="middle" rowspan="1" colspan="1">74.13</td><td align="center" valign="middle" rowspan="1" colspan="1">67.03</td><td align="center" valign="middle" rowspan="1" colspan="1">90.61</td><td align="center" valign="middle" rowspan="1" colspan="1">86.01</td><td align="center" valign="middle" rowspan="1" colspan="1">79.29</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td colspan="18" align="left" valign="middle" rowspan="1">
<italic toggle="yes">
<bold>Point&#8211;Voxel Hybrid:</bold>
</italic>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PVCNN [<xref rid="B230-sensors-25-05264" ref-type="bibr">230</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fast Point R-CNN [<xref rid="B82-sensors-25-05264" ref-type="bibr">82</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">96.13</td><td align="center" valign="middle" rowspan="1" colspan="1">93.18</td><td align="center" valign="middle" rowspan="1" colspan="1">87.68</td><td align="center" valign="middle" rowspan="1" colspan="1">85.29</td><td align="center" valign="middle" rowspan="1" colspan="1">77.40</td><td align="center" valign="middle" rowspan="1" colspan="1">70.24</td><td align="center" valign="middle" rowspan="1" colspan="1">90.87</td><td align="center" valign="middle" rowspan="1" colspan="1">87.84</td><td align="center" valign="middle" rowspan="1" colspan="1">80.52</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PV-RCNN [<xref rid="B83-sensors-25-05264" ref-type="bibr">83</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">98.17</td><td align="center" valign="middle" rowspan="1" colspan="1">94.70</td><td align="center" valign="middle" rowspan="1" colspan="1">92.04</td><td align="center" valign="middle" rowspan="1" colspan="1">90.25</td><td align="center" valign="middle" rowspan="1" colspan="1">81.43</td><td align="center" valign="middle" rowspan="1" colspan="1">76.82</td><td align="center" valign="middle" rowspan="1" colspan="1">94.98</td><td align="center" valign="middle" rowspan="1" colspan="1">90.65</td><td align="center" valign="middle" rowspan="1" colspan="1">86.14</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">77.51</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SA-SSD [<xref rid="B231-sensors-25-05264" ref-type="bibr">231</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">97.92</td><td align="center" valign="middle" rowspan="1" colspan="1">95.16</td><td align="center" valign="middle" rowspan="1" colspan="1">90.15</td><td align="center" valign="middle" rowspan="1" colspan="1">88.75</td><td align="center" valign="middle" rowspan="1" colspan="1">79.79</td><td align="center" valign="middle" rowspan="1" colspan="1">74.16</td><td align="center" valign="middle" rowspan="1" colspan="1">95.03</td><td align="center" valign="middle" rowspan="1" colspan="1">91.03</td><td align="center" valign="middle" rowspan="1" colspan="1">85.96</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BADet [<xref rid="B232-sensors-25-05264" ref-type="bibr">232</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">98.65</td><td align="center" valign="middle" rowspan="1" colspan="1">95.34</td><td align="center" valign="middle" rowspan="1" colspan="1">90.28</td><td align="center" valign="middle" rowspan="1" colspan="1">89.28</td><td align="center" valign="middle" rowspan="1" colspan="1">81.61</td><td align="center" valign="middle" rowspan="1" colspan="1">76.59</td><td align="center" valign="middle" rowspan="1" colspan="1">95.23</td><td align="center" valign="middle" rowspan="1" colspan="1">91.32</td><td align="center" valign="middle" rowspan="1" colspan="1">86.48</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pyramid-PV [<xref rid="B233-sensors-25-05264" ref-type="bibr">233</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">95.88</td><td align="center" valign="middle" rowspan="1" colspan="1">95.13</td><td align="center" valign="middle" rowspan="1" colspan="1">92.62</td><td align="center" valign="middle" rowspan="1" colspan="1">88.39</td><td align="center" valign="middle" rowspan="1" colspan="1">82.08</td><td align="center" valign="middle" rowspan="1" colspan="1">77.49</td><td align="center" valign="middle" rowspan="1" colspan="1">92.19</td><td align="center" valign="middle" rowspan="1" colspan="1">88.84</td><td align="center" valign="middle" rowspan="1" colspan="1">86.21</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.07</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DVFENet [<xref rid="B234-sensors-25-05264" ref-type="bibr">234</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">95.35</td><td align="center" valign="middle" rowspan="1" colspan="1">94.57</td><td align="center" valign="middle" rowspan="1" colspan="1">91.77</td><td align="center" valign="middle" rowspan="1" colspan="1">86.20</td><td align="center" valign="middle" rowspan="1" colspan="1">79.18</td><td align="center" valign="middle" rowspan="1" colspan="1">74.58</td><td align="center" valign="middle" rowspan="1" colspan="1">90.93</td><td align="center" valign="middle" rowspan="1" colspan="1">87.68</td><td align="center" valign="middle" rowspan="1" colspan="1">84.60</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PDV [<xref rid="B84-sensors-25-05264" ref-type="bibr">84</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.07</td><td align="center" valign="middle" rowspan="1" colspan="1">95.00</td><td align="center" valign="middle" rowspan="1" colspan="1">92.44</td><td align="center" valign="middle" rowspan="1" colspan="1">90.43</td><td align="center" valign="middle" rowspan="1" colspan="1">81.86</td><td align="center" valign="middle" rowspan="1" colspan="1">77.36</td><td align="center" valign="middle" rowspan="1" colspan="1">94.56</td><td align="center" valign="middle" rowspan="1" colspan="1">90.48</td><td align="center" valign="middle" rowspan="1" colspan="1">86.23</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EQ-PVRCNN [<xref rid="B235-sensors-25-05264" ref-type="bibr">235</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">98.23</td><td align="center" valign="middle" rowspan="1" colspan="1">95.32</td><td align="center" valign="middle" rowspan="1" colspan="1">92.65</td><td align="center" valign="middle" rowspan="1" colspan="1">90.13</td><td align="center" valign="middle" rowspan="1" colspan="1">82.01</td><td align="center" valign="middle" rowspan="1" colspan="1">77.53</td><td align="center" valign="middle" rowspan="1" colspan="1">94.55</td><td align="center" valign="middle" rowspan="1" colspan="1">89.09</td><td align="center" valign="middle" rowspan="1" colspan="1">86.40</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PVT-SSD [<xref rid="B236-sensors-25-05264" ref-type="bibr">236</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">96.75</td><td align="center" valign="middle" rowspan="1" colspan="1">95.90</td><td align="center" valign="middle" rowspan="1" colspan="1">90.69</td><td align="center" valign="middle" rowspan="1" colspan="1">90.65</td><td align="center" valign="middle" rowspan="1" colspan="1">82.29</td><td align="center" valign="middle" rowspan="1" colspan="1">76.85</td><td align="center" valign="middle" rowspan="1" colspan="1">95.23</td><td align="center" valign="middle" rowspan="1" colspan="1">91.63</td><td align="center" valign="middle" rowspan="1" colspan="1">86.43</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Py + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PG-RCNN [<xref rid="B237-sensors-25-05264" ref-type="bibr">237</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">96.66</td><td align="center" valign="middle" rowspan="1" colspan="1">95.40</td><td align="center" valign="middle" rowspan="1" colspan="1">90.55</td><td align="center" valign="middle" rowspan="1" colspan="1">89.38</td><td align="center" valign="middle" rowspan="1" colspan="1">82.13</td><td align="center" valign="middle" rowspan="1" colspan="1">77.33</td><td align="center" valign="middle" rowspan="1" colspan="1">93.39</td><td align="center" valign="middle" rowspan="1" colspan="1">89.46</td><td align="center" valign="middle" rowspan="1" colspan="1">86.54</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 1.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Uni3DETR [<xref rid="B238-sensors-25-05264" ref-type="bibr">238</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> &#10003;</td></tr></tbody></table></table-wrap><table-wrap position="anchor" id="sensors-25-05264-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05264-t005_Table 5</object-id><label>Table 5</label><caption><p>Radar-based 3D object detection results on KITTI car test set, nuScenes test set, and Waymo validation set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Year</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>2D</sub></th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>3D</sub></th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>BEV</sub></th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">nuScenes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Waymo</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Time (s)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Hardware</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Code Available</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
E
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
M
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
H
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
E
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
M
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
H
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
E
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
M
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
H
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mAP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
NDS
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
L1
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
L2
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Radar-PointGNN [<xref rid="B86-sensors-25-05264" ref-type="bibr">86</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">3.4</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">K-Radar [<xref rid="B87-sensors-25-05264" ref-type="bibr">87</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KPConvPillars [<xref rid="B88-sensors-25-05264" ref-type="bibr">88</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">4.9</td><td align="center" valign="middle" rowspan="1" colspan="1">13.9</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dual Radar [<xref rid="B239-sensors-25-05264" ref-type="bibr">239</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CenterRadarNet [<xref rid="B240-sensors-25-05264" ref-type="bibr">240</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RadarDistill [<xref rid="B89-sensors-25-05264" ref-type="bibr">89</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">20.5</td><td align="center" valign="middle" rowspan="1" colspan="1">43.7</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RADLER [<xref rid="B90-sensors-25-05264" ref-type="bibr">90</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> &#10003;</td></tr></tbody></table></table-wrap><table-wrap position="anchor" id="sensors-25-05264-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05264-t006_Table 6</object-id><label>Table 6</label><caption><p>Multi-modal-based 3D object detection results on KITTI car test set, nuScenes test set, and Waymo validation set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Year</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>2D</sub></th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>3D</sub></th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP<sub>BEV</sub></th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">nuScenes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Waymo</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Time (s)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Hardware</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Code Available</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
E
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
M
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
H
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
E
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
M
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
H
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
E
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
M
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
H
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
mAP
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
NDS
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
L1
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
L2
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">
<bold>Early Fusion:</bold>
</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">F-PointNet [<xref rid="B92-sensors-25-05264" ref-type="bibr">92</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">95.85</td><td align="center" valign="middle" rowspan="1" colspan="1">95.17</td><td align="center" valign="middle" rowspan="1" colspan="1">85.42</td><td align="center" valign="middle" rowspan="1" colspan="1">82.19</td><td align="center" valign="middle" rowspan="1" colspan="1">69.79</td><td align="center" valign="middle" rowspan="1" colspan="1">60.59</td><td align="center" valign="middle" rowspan="1" colspan="1">91.17</td><td align="center" valign="middle" rowspan="1" colspan="1">84.67</td><td align="center" valign="middle" rowspan="1" colspan="1">74.77</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.17</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 3.0 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">F-ConvNet [<xref rid="B93-sensors-25-05264" ref-type="bibr">93</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">95.85</td><td align="center" valign="middle" rowspan="1" colspan="1">92.19</td><td align="center" valign="middle" rowspan="1" colspan="1">80.09</td><td align="center" valign="middle" rowspan="1" colspan="1">87.36</td><td align="center" valign="middle" rowspan="1" colspan="1">76.39</td><td align="center" valign="middle" rowspan="1" colspan="1">66.69</td><td align="center" valign="middle" rowspan="1" colspan="1">91.51</td><td align="center" valign="middle" rowspan="1" colspan="1">85.84</td><td align="center" valign="middle" rowspan="1" colspan="1">76.11</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.47</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RoarNet [<xref rid="B241-sensors-25-05264" ref-type="bibr">241</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Complexer-YOLO [<xref rid="B242-sensors-25-05264" ref-type="bibr">242</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">91.92</td><td align="center" valign="middle" rowspan="1" colspan="1">84.16</td><td align="center" valign="middle" rowspan="1" colspan="1">79.62</td><td align="center" valign="middle" rowspan="1" colspan="1">55.93</td><td align="center" valign="middle" rowspan="1" colspan="1">47.34</td><td align="center" valign="middle" rowspan="1" colspan="1">42.60</td><td align="center" valign="middle" rowspan="1" colspan="1">77.24</td><td align="center" valign="middle" rowspan="1" colspan="1">68.96</td><td align="center" valign="middle" rowspan="1" colspan="1">64.95</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 3.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointPainting [<xref rid="B91-sensors-25-05264" ref-type="bibr">91</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">98.39</td><td align="center" valign="middle" rowspan="1" colspan="1">92.58</td><td align="center" valign="middle" rowspan="1" colspan="1">89.71</td><td align="center" valign="middle" rowspan="1" colspan="1">82.11</td><td align="center" valign="middle" rowspan="1" colspan="1">71.70</td><td align="center" valign="middle" rowspan="1" colspan="1">67.08</td><td align="center" valign="middle" rowspan="1" colspan="1">92.45</td><td align="center" valign="middle" rowspan="1" colspan="1">88.11</td><td align="center" valign="middle" rowspan="1" colspan="1">83.36</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.40</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FusionPainting [<xref rid="B243-sensors-25-05264" ref-type="bibr">243</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MVP [<xref rid="B244-sensors-25-05264" ref-type="bibr">244</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">F-PointPillars [<xref rid="B94-sensors-25-05264" ref-type="bibr">94</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">4 cores @ 3.0 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointAugmenting [<xref rid="B245-sensors-25-05264" ref-type="bibr">245</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">89.14</td><td align="center" valign="middle" rowspan="1" colspan="1">86.57</td><td align="center" valign="middle" rowspan="1" colspan="1">78.32</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VirConvNet [<xref rid="B95-sensors-25-05264" ref-type="bibr">95</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">98.00</td><td align="center" valign="middle" rowspan="1" colspan="1">97.27</td><td align="center" valign="middle" rowspan="1" colspan="1">94.53</td><td align="center" valign="middle" rowspan="1" colspan="1">92.48</td><td align="center" valign="middle" rowspan="1" colspan="1">87.20</td><td align="center" valign="middle" rowspan="1" colspan="1">82.45</td><td align="center" valign="middle" rowspan="1" colspan="1">95.99</td><td align="center" valign="middle" rowspan="1" colspan="1">93.52</td><td align="center" valign="middle" rowspan="1" colspan="1">90.38</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HDF [<xref rid="B246-sensors-25-05264" ref-type="bibr">246</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2025</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">
<bold>Mid-Level Fusion:</bold>
</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MV3D [<xref rid="B96-sensors-25-05264" ref-type="bibr">96</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" rowspan="1" colspan="1">96.47</td><td align="center" valign="middle" rowspan="1" colspan="1">90.83</td><td align="center" valign="middle" rowspan="1" colspan="1">78.63</td><td align="center" valign="middle" rowspan="1" colspan="1">74.97</td><td align="center" valign="middle" rowspan="1" colspan="1">63.63</td><td align="center" valign="middle" rowspan="1" colspan="1">54.00</td><td align="center" valign="middle" rowspan="1" colspan="1">86.62</td><td align="center" valign="middle" rowspan="1" colspan="1">78.93</td><td align="center" valign="middle" rowspan="1" colspan="1">69.80</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.36</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AVOD [<xref rid="B97-sensors-25-05264" ref-type="bibr">97</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">95.17</td><td align="center" valign="middle" rowspan="1" colspan="1">89.88</td><td align="center" valign="middle" rowspan="1" colspan="1">82.83</td><td align="center" valign="middle" rowspan="1" colspan="1">76.39</td><td align="center" valign="middle" rowspan="1" colspan="1">66.47</td><td align="center" valign="middle" rowspan="1" colspan="1">60.23</td><td align="center" valign="middle" rowspan="1" colspan="1">89.75</td><td align="center" valign="middle" rowspan="1" colspan="1">84.95</td><td align="center" valign="middle" rowspan="1" colspan="1">78.32</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">Titan X (Pascal)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PointFusion [<xref rid="B247-sensors-25-05264" ref-type="bibr">247</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2017</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ContFuse [<xref rid="B98-sensors-25-05264" ref-type="bibr">98</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">83.68</td><td align="center" valign="middle" rowspan="1" colspan="1">68.78</td><td align="center" valign="middle" rowspan="1" colspan="1">61.67</td><td align="center" valign="middle" rowspan="1" colspan="1">94.07</td><td align="center" valign="middle" rowspan="1" colspan="1">85.35</td><td align="center" valign="middle" rowspan="1" colspan="1">75.88</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MVXNet [<xref rid="B248-sensors-25-05264" ref-type="bibr">248</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">83.20</td><td align="center" valign="middle" rowspan="1" colspan="1">72.70</td><td align="center" valign="middle" rowspan="1" colspan="1">65.20</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PI-RCNN [<xref rid="B249-sensors-25-05264" ref-type="bibr">249</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">96.17</td><td align="center" valign="middle" rowspan="1" colspan="1">92.66</td><td align="center" valign="middle" rowspan="1" colspan="1">87.68</td><td align="center" valign="middle" rowspan="1" colspan="1">84.37</td><td align="center" valign="middle" rowspan="1" colspan="1">74.82</td><td align="center" valign="middle" rowspan="1" colspan="1">70.03</td><td align="center" valign="middle" rowspan="1" colspan="1">91.44</td><td align="center" valign="middle" rowspan="1" colspan="1">85.81</td><td align="center" valign="middle" rowspan="1" colspan="1">81.00</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MCF3D [<xref rid="B250-sensors-25-05264" ref-type="bibr">250</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MMF [<xref rid="B99-sensors-25-05264" ref-type="bibr">99</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">97.41</td><td align="center" valign="middle" rowspan="1" colspan="1">94.25</td><td align="center" valign="middle" rowspan="1" colspan="1">91.80</td><td align="center" valign="middle" rowspan="1" colspan="1">88.40</td><td align="center" valign="middle" rowspan="1" colspan="1">77.43</td><td align="center" valign="middle" rowspan="1" colspan="1">70.22</td><td align="center" valign="middle" rowspan="1" colspan="1">93.67</td><td align="center" valign="middle" rowspan="1" colspan="1">88.21</td><td align="center" valign="middle" rowspan="1" colspan="1">81.99</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3D-CVF [<xref rid="B251-sensors-25-05264" ref-type="bibr">251</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">96.78</td><td align="center" valign="middle" rowspan="1" colspan="1">93.36</td><td align="center" valign="middle" rowspan="1" colspan="1">86.11</td><td align="center" valign="middle" rowspan="1" colspan="1">89.20</td><td align="center" valign="middle" rowspan="1" colspan="1">80.05</td><td align="center" valign="middle" rowspan="1" colspan="1">73.11</td><td align="center" valign="middle" rowspan="1" colspan="1">93.52</td><td align="center" valign="middle" rowspan="1" colspan="1">89.56</td><td align="center" valign="middle" rowspan="1" colspan="1">82.45</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.06</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EPNet [<xref rid="B100-sensors-25-05264" ref-type="bibr">100</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">96.15</td><td align="center" valign="middle" rowspan="1" colspan="1">94.44</td><td align="center" valign="middle" rowspan="1" colspan="1">89.99</td><td align="center" valign="middle" rowspan="1" colspan="1">89.81</td><td align="center" valign="middle" rowspan="1" colspan="1">79.28</td><td align="center" valign="middle" rowspan="1" colspan="1">74.59</td><td align="center" valign="middle" rowspan="1" colspan="1">94.22</td><td align="center" valign="middle" rowspan="1" colspan="1">88.47</td><td align="center" valign="middle" rowspan="1" colspan="1">83.69</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EPNet++ [<xref rid="B226-sensors-25-05264" ref-type="bibr">226</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" rowspan="1" colspan="1">96.73</td><td align="center" valign="middle" rowspan="1" colspan="1">95.17</td><td align="center" valign="middle" rowspan="1" colspan="1">92.10</td><td align="center" valign="middle" rowspan="1" colspan="1">91.37</td><td align="center" valign="middle" rowspan="1" colspan="1">81.96</td><td align="center" valign="middle" rowspan="1" colspan="1">76.71</td><td align="center" valign="middle" rowspan="1" colspan="1">95.41</td><td align="center" valign="middle" rowspan="1" colspan="1">89.00</td><td align="center" valign="middle" rowspan="1" colspan="1">85.73</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TransFusion [<xref rid="B101-sensors-25-05264" ref-type="bibr">101</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">68.90</td><td align="center" valign="middle" rowspan="1" colspan="1">71.70</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BEVFusion [<xref rid="B252-sensors-25-05264" ref-type="bibr">252</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FUTR3D [<xref rid="B102-sensors-25-05264" ref-type="bibr">102</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">69.40</td><td align="center" valign="middle" rowspan="1" colspan="1">72.10</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DeepFusion [<xref rid="B253-sensors-25-05264" ref-type="bibr">253</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MSMDFusion [<xref rid="B254-sensors-25-05264" ref-type="bibr">254</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CAT-Det [<xref rid="B255-sensors-25-05264" ref-type="bibr">255</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">95.97</td><td align="center" valign="middle" rowspan="1" colspan="1">94.71</td><td align="center" valign="middle" rowspan="1" colspan="1">92.07</td><td align="center" valign="middle" rowspan="1" colspan="1">89.87</td><td align="center" valign="middle" rowspan="1" colspan="1">81.32</td><td align="center" valign="middle" rowspan="1" colspan="1">76.68</td><td align="center" valign="middle" rowspan="1" colspan="1">92.59</td><td align="center" valign="middle" rowspan="1" colspan="1">90.07</td><td align="center" valign="middle" rowspan="1" colspan="1">85.82</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.30</td><td align="center" valign="middle" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python + C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">HMFI [<xref rid="B256-sensors-25-05264" ref-type="bibr">256</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" rowspan="1" colspan="1">96.29</td><td align="center" valign="middle" rowspan="1" colspan="1">95.16</td><td align="center" valign="middle" rowspan="1" colspan="1">92.45</td><td align="center" valign="middle" rowspan="1" colspan="1">88.90</td><td align="center" valign="middle" rowspan="1" colspan="1">81.93</td><td align="center" valign="middle" rowspan="1" colspan="1">77.30</td><td align="center" valign="middle" rowspan="1" colspan="1">93.04</td><td align="center" valign="middle" rowspan="1" colspan="1">89.17</td><td align="center" valign="middle" rowspan="1" colspan="1">86.37</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LoGoNet [<xref rid="B257-sensors-25-05264" ref-type="bibr">257</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">96.60</td><td align="center" valign="middle" rowspan="1" colspan="1">95.55</td><td align="center" valign="middle" rowspan="1" colspan="1">93.07</td><td align="center" valign="middle" rowspan="1" colspan="1">91.80</td><td align="center" valign="middle" rowspan="1" colspan="1">85.06</td><td align="center" valign="middle" rowspan="1" colspan="1">80.74</td><td align="center" valign="middle" rowspan="1" colspan="1">95.48</td><td align="center" valign="middle" rowspan="1" colspan="1">91.52</td><td align="center" valign="middle" rowspan="1" colspan="1">87.09</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (C/C++)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SDVRF [<xref rid="B258-sensors-25-05264" ref-type="bibr">258</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SupFusion [<xref rid="B259-sensors-25-05264" ref-type="bibr">259</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FGFusion [<xref rid="B260-sensors-25-05264" ref-type="bibr">260</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VCD [<xref rid="B261-sensors-25-05264" ref-type="bibr">261</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UniTR [<xref rid="B262-sensors-25-05264" ref-type="bibr">262</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.90</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">
<bold>Late Fusion:</bold>
</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CLOCS [<xref rid="B103-sensors-25-05264" ref-type="bibr">103</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2020</td><td align="center" valign="middle" rowspan="1" colspan="1">96.77</td><td align="center" valign="middle" rowspan="1" colspan="1">96.07</td><td align="center" valign="middle" rowspan="1" colspan="1">91.11</td><td align="center" valign="middle" rowspan="1" colspan="1">89.16</td><td align="center" valign="middle" rowspan="1" colspan="1">82.28</td><td align="center" valign="middle" rowspan="1" colspan="1">77.23</td><td align="center" valign="middle" rowspan="1" colspan="1">92.91</td><td align="center" valign="middle" rowspan="1" colspan="1">89.48</td><td align="center" valign="middle" rowspan="1" colspan="1">86.42</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">1 core @ 2.5 GHz (Python)</td><td align="center" valign="middle" rowspan="1" colspan="1">&#215;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fast-CLOCS [<xref rid="B104-sensors-25-05264" ref-type="bibr">104</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#8211;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GPU @ 2.5 GHz (Python)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td></tr></tbody></table></table-wrap></sec></sec></sec><sec id="sec5-sensors-25-05264"><title>5. Evaluation and Discussion</title><p>Camera-based methods are lightweight, cost-effective, and easy to deploy, making them particularly attractive for applications with limited hardware constraints. They preserve rich semantic cues such as texture, colour, and object appearance, which are advantageous for visual understanding and object classification. However, due to the inherent challenge of regressing depth from 2D projections, camera-based detectors often underperform in spatial metrics such as 3D IoU, localization precision, and orientation accuracy [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>].</p><p>Stereo and multi-view systems partially mitigate these issues by leveraging geometric constraints, enabling disparity estimation and improved depth perception. Despite this, their accuracy still lags behind that of LiDAR-based systems, especially in large-scale outdoor environments. Multi-camera configurations, while offering broader spatial coverage, require complex calibration and are mostly reported on datasets such as nuScenes, which support full 360&#176; camera rings.</p><p>LiDAR sensors, by contrast, directly capture 3D spatial coordinates, providing accurate and dense measurements of the environment. This makes them particularly effective for estimating object position, size, and orientation. LiDAR-based methods consistently outperform camera-only approaches in 3D detection tasks and show greater robustness under varying lighting conditions [<xref rid="B24-sensors-25-05264" ref-type="bibr">24</xref>]. However, they come with their own drawbacks: LiDAR hardware is significantly more expensive, bulkier, and prone to performance degradation in adverse weather (e.g., fog, rain, or snow) [<xref rid="B32-sensors-25-05264" ref-type="bibr">32</xref>]. Additionally, point density decreases with distance, reducing resolution for far-away objects. Radars solve some of these problems but still lack the accuracy and resolution of LiDAR.</p><p>To overcome the individual limitations of single-sensor systems, multi-modal fusion has become an increasingly prevalent approach. By integrating LiDAR&#8217;s spatial precision with the semantic richness of images, fusion-based models yield improved scene understanding, higher detection accuracy, and better robustness to occlusions or sensor dropout [<xref rid="B29-sensors-25-05264" ref-type="bibr">29</xref>]. Nonetheless, these methods introduce new complexities: precise extrinsic calibration is required, and data from different sensors must be carefully synchronized and spatially aligned. Fusion also increases computational burden, making real-time inference more challenging.</p><p><xref rid="sensors-25-05264-t003" ref-type="table">Table 3</xref>, <xref rid="sensors-25-05264-t004" ref-type="table">Table 4</xref>, <xref rid="sensors-25-05264-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-05264-t006" ref-type="table">Table 6</xref> provide a comprehensive evaluation of 3D OD performance across a wide range of methods, organized by sensor modality, representation type, and publication year. Each table reports accuracy metrics on the KITTI, nuScenes, and Waymo datasets, with additional inference time metrics (reported for KITTI) and public code availability. This taxonomy enables direct comparisons between monocular, stereo, LiDAR-only, radar-only, and fusion-based pipelines.</p><p>A consistent hierarchy emerges: LiDAR-based methods outperform monocular and stereo-based ones in 3D localization, while multi-modal methods tend to achieve the best overall results. These trends underscore the inherent difficulty of monocular depth estimation and highlight the advantages of incorporating complementary sensory cues.</p><p>From early 2D-proposal-based monocular models to state-of-the-art transformer-based fusion frameworks, the field of 3D OD has progressively shifted toward hybrid pipelines that balance spatial accuracy, semantic reasoning, and computational efficiency. This evolution is reflected not only in architectural choices but also in quantitative improvements on benchmark datasets.</p><p>An important trend highlighted in the tables is the trade-off between inference speed and detection accuracy. While transformer-based and two-stage architectures dominate in terms of AP and 3D IoU scores, single-stage detectors and projection-based models have gained traction for real-time deployment. For instance, models such as PointPillars, PIXOR, and SMOKE offer reduced latency while maintaining competitive accuracy. Fusion-based methods like TransFusion and EPNet achieve high accuracy but are computationally heavier, whereas late fusion methods like Fast-CLOCs enable faster inference with minimal architectural change.</p><p>Recent models such as TED, PV-RCNN, and CenterFusion exemplify the growing ability to combine high accuracy with low latency, even in multi-modal setups. TED currently holds state-of-the-art performance among LiDAR-only methods on the KITTI benchmark, while PV-RCNN combines voxel-grid structure with point-level refinement for balanced performance. BEV and pillar-based projections dominate low-latency deployment scenarios, especially in AD settings where speed is critical.</p><p>Focusing specifically on camera-based methods, steady gains in AP have been observed over time. Monocular detectors, though inherently limited by depth ambiguity, have benefited from advances in deep regression, geometric priors, and knowledge distillation. Stereo and multi-view approaches improve upon monocular baselines by leveraging depth cues, with many of the highest-performing models being reported on the nuScenes benchmark due to its extensive camera coverage. Inference times have also improved over time, making real-time deployment increasingly feasible.</p><p>On the other hand, radar-only based methods still showcase limited accuracy performance in the few reported metrics on benchmarked datasets. They lack the resolution and semantic information capturing required for proper 3D understanding. However, they can provide information that other sensors cannot and can operate in more extreme environments where other sensors are less suitable, thereby greatly reinforcing a system&#8217;s robustness and reliability when fused. Still, works like [<xref rid="B263-sensors-25-05264" ref-type="bibr">263</xref>] highlight how radar-only 3D object detection methods can achieve success in specific applications and how this area holds significant potential for further advancement.</p><p>In summary, there is no universally optimal modality or representation for 3D OD. Each approach entails trade-offs between semantic richness, geometric fidelity, latency, cost, and robustness. The ideal method depends on application-specific constraints such as sensor availability, environmental variability, and real-time requirements. Future research will likely focus on three main directions: improving robustness to sensor degradation, optimizing fusion architectures for scalable deployment, and developing more generalizable representations that support cross-domain transfer.</p><p>As foundation models and self-supervised learning continue to mature, they will likely play a central role in the next generation of 3D OD systems. Scalable, modality-agnostic perception frameworks that unify sparse and dense data sources will be critical in enabling safe and reliable autonomous operation across diverse scenarios.</p><p><xref rid="sensors-25-05264-t003" ref-type="table">Table 3</xref>, <xref rid="sensors-25-05264-t004" ref-type="table">Table 4</xref>, <xref rid="sensors-25-05264-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-05264-t006" ref-type="table">Table 6</xref> constitute the core of this survey. Together, they provide a structured and comprehensive summary of the most influential 3D OD methods developed over the past decade. A total of 205 methods were reviewed and are reported in these tables, spanning a wide range of input modalities, data representations, and architectural paradigms. These methods are categorized by sensor type (monocular, stereo, multi-camera, LiDAR, and fusion-based), ordered chronologically by publication year, and benchmarked using standardized metrics from the KITTI, nuScenes, and Waymo datasets. Inference time (when available) is also included for the KITTI benchmark, along with an indicator of public code availability.</p><p>This structured compilation enables readers to trace the chronological and methodological evolution of the field, compare performance trends across modalities, and identify prevailing design trade-offs. It also serves as a reference point for researchers seeking to position new contributions within the broader landscape.</p><p>To facilitate wider access and enable dynamic exploration of 3D OD methods, a dedicated online repository was created: (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://3d-object-detection-hub.github.io/">https://3d-object-detection-hub.github.io/</uri>, accessed on 25 April 2025). This interactive website contains an extended collection of detection models, grouped by input modality, data representation, and publication year. The platform is designed to be continuously updated and searchable, supporting users in exploring methods across monocular-, stereo-, LiDAR-, radar-, and fusion-based pipelines.</p><p>The construction of this survey involved a multi-stage methodology. Initially, existing review papers and survey studies were consulted to establish a foundational taxonomy of input modalities, processing strategies, and detection paradigms. These sources provided both historical context and the initial references for influential models. From there, models were categorized according to the taxonomy developed in <xref rid="sec4-sensors-25-05264" ref-type="sec">Section 4</xref>, distinguishing between representation types such as point-based, voxel-based, projection-based and hybrid models.</p><p>To obtain reliable performance metrics, official benchmark leaderboards, such as those of KITTI, nuScenes, and Waymo, were systematically reviewed. For each method, metrics were gathered from the benchmark websites whenever publicly available. When benchmark entries were missing or incomplete, values were extracted directly from the original publications. Inference time was also recorded, with particular emphasis on KITTI runtime to compare real-time capability. All data presented in <xref rid="sensors-25-05264-t003" ref-type="table">Table 3</xref>, <xref rid="sensors-25-05264-t004" ref-type="table">Table 4</xref>, <xref rid="sensors-25-05264-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-05264-t006" ref-type="table">Table 6</xref> were manually verified to ensure consistency, dataset compatibility, and fair metric alignment across methods. Where necessary, metadata were aggregated from supplementary material or code repositories to fill gaps in the original publications.</p><p>Each table entry is annotated by sensor modality (e.g., monocular, stereo, LiDAR, multi-modal) and representation type (e.g., voxel, point, projection, hybrid). Methods are listed chronologically by year of publication to illustrate architectural progression. Accuracy metrics are reported separately for the KITTI, nuScenes, and Waymo datasets, providing cross-benchmark comparability. An additional column denotes whether the method&#8217;s codebase is publicly available, enabling practical reproducibility and further experimentation.</p><p>This work thus aims to offer one of the most comprehensive and up-to-date surveys of the 3D OD methods available. In contrast to prior surveys that focus narrowly on single-modality pipelines, this review takes a modality-agnostic perspective and includes both classical and SoA methods up to May 2025, capturing recent advances that have not yet appeared in other reviews or benchmark summary papers.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-05264"><title>6. Conclusions</title><p>This work presented a comprehensive survey of 3D object detection methods, spanning monocular, stereo, multi-camera, radar, LiDAR, and multi-modal fusion approaches. A consistent taxonomy was proposed to organize the field, covering different input modalities and data representations. The survey traced the historical development of these methods and benchmarked them using metrics from publicly available datasets such as KITTI, nuScenes, and Waymo. In total, 205 methods were analysed, and their performance was synthesized across accuracy, inference time, and implementation availability.</p><p>Monocular methods are the most lightweight and scalable but are limited by their lack of direct depth sensing. Stereo systems partially address this with geometric priors, though their effectiveness declines at longer distances and under occlusion. LiDAR-based methods excel in spatial accuracy and robustness but are hindered by high cost, sparse point clouds at long ranges, and degraded performance in adverse weather. Radar-based methods exist and can be a good choice for specific use cases but are usually coupled with other sensors. Multi-modal fusion strategies consistently achieve superior accuracy and resilience by combining complementary sensor inputs, but they also introduce additional complexity in calibration, synchronization, and training.</p><p>Beyond benchmarking, the survey consolidates fragmented knowledge from recent papers, official leaderboards, and existing reviews into a unified and accessible framework. The dedicated website accompanying this thesis serves as a continuously updated repository of methods and results, supporting reproducibility and further research. Overall, this work contributes a modality-agnostic, data-driven, and taxonomically structured reference for 3D perception in autonomous systems.</p><p>The field of 3D object detection continues to evolve rapidly, with new architectural paradigms and sensing modalities expanding its capabilities. Recent trends show a gradual transition from purely CNN-based architectures to Transformer-based models that better capture long-range dependencies and global context. Other emerging paradigms such as knowledge distillation, NeRF-based scene representations, and foundation models trained on large-scale multimodal data are also gaining traction. The field is moving toward scalable, robust real-time solutions suitable for deployment in real-world AV systems. Heavy focus is being placed on enhancing robustness under challenging conditions and ensuring scalability for large-scale AV deployments. Techniques that reduce latency, energy consumption, and hardware requirements, without sacrificing detection accuracy, will be key enablers of future perception systems. The groundwork laid by this work provides a strong foundation for further exploration into robust, efficient, and generalizable 3D detection pipelines for autonomous applications.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.V., A.M. and J.-V.Z.; methodology, M.V., A.M. and J.-V.Z.; investigation, M.V.; implementation and evaluation, M.V.; writing&#8212;original draft preparation, M.V.; writing&#8212;review and editing, M.V., A.M. and J.-V.Z.; supervision, A.M. and J.-V.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original data presented in the study will be made openly available at GitHub upon publication.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">AD</td><td align="left" valign="middle" rowspan="1" colspan="1">Autonomous Driving</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ADAS</td><td align="left" valign="middle" rowspan="1" colspan="1">Advanced Driver Assistance Systems</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AI</td><td align="left" valign="middle" rowspan="1" colspan="1">Artificial Intelligence</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AP</td><td align="left" valign="middle" rowspan="1" colspan="1">Average Precision</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AV</td><td align="left" valign="middle" rowspan="1" colspan="1">Autonomous Vehicle</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BEV</td><td align="left" valign="middle" rowspan="1" colspan="1">Bird&#8217;s-Eye View</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CV</td><td align="left" valign="middle" rowspan="1" colspan="1">Computer Vision</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DL</td><td align="left" valign="middle" rowspan="1" colspan="1">Deep Learning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GNSS</td><td align="left" valign="middle" rowspan="1" colspan="1">Global Navigation Satellite System</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IoU</td><td align="left" valign="middle" rowspan="1" colspan="1">Intersection over Union</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IMU</td><td align="left" valign="middle" rowspan="1" colspan="1">Inertial Measurement Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LiDAR</td><td align="left" valign="middle" rowspan="1" colspan="1">Light Detection and Ranging</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mAP</td><td align="left" valign="middle" rowspan="1" colspan="1">Median Average Precision</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ML</td><td align="left" valign="middle" rowspan="1" colspan="1">Machine Learning</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NDS</td><td align="left" valign="middle" rowspan="1" colspan="1">nuScenes Detection Score</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NDS</td><td align="left" valign="middle" rowspan="1" colspan="1">Non-Maximum Suppression</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NN</td><td align="left" valign="middle" rowspan="1" colspan="1">Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OD</td><td align="left" valign="middle" rowspan="1" colspan="1">Object Detection</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PC</td><td align="left" valign="middle" rowspan="1" colspan="1">Point Cloud</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Radar</td><td align="left" valign="middle" rowspan="1" colspan="1">Radio Detection and Ranging</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAE</td><td align="left" valign="middle" rowspan="1" colspan="1">Society of Automotive Engineers</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SoA</td><td align="left" valign="middle" rowspan="1" colspan="1">State-of-the-Art</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Sonar</td><td align="left" valign="middle" rowspan="1" colspan="1">Sound Navigation and Ranging</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SLAM</td><td align="left" valign="middle" rowspan="1" colspan="1">Simultaneous Localization and Mapping</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ToF</td><td align="left" valign="middle" rowspan="1" colspan="1">Time-of-Flight</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05264"><label>1.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>SAE</collab></person-group><source>Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles</source><comment>SAE International</comment><publisher-name>On-Road Automated Driving (ORAD) Committee</publisher-name><publisher-loc>Warrendale, PA, USA</publisher-loc><year>2021</year></element-citation></ref><ref id="B2-sensors-25-05264"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Brummelen</surname><given-names>J.</given-names></name><name name-style="western"><surname>O&#8217;brien</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gruyer</surname><given-names>D.</given-names></name><name name-style="western"><surname>Najjaran</surname><given-names>H.</given-names></name></person-group><article-title>Autonomous vehicle perception: The technology of today and tomorrow</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2018</year><volume>89</volume><fpage>384</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1016/j.trc.2018.02.012</pub-id></element-citation></ref><ref id="B3-sensors-25-05264"><label>3.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jeffs</surname><given-names>J.</given-names></name><name name-style="western"><surname>He</surname><given-names>M.X.</given-names></name></person-group><source>Autonomous Cars, Robotaxis and Sensors 2024&#8211;2044</source><publisher-name>IDTechEx</publisher-name><publisher-loc>Cambridge, UK</publisher-loc><year>2023</year></element-citation></ref><ref id="B4-sensors-25-05264"><label>4.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>Waymo LLC</collab></person-group><source>On the Road to Fully Self-Driving</source><comment>Waymo Safety Report</comment><publisher-name>Waymo LLC.</publisher-name><publisher-loc>Mountain View, CA, USA</publisher-loc><year>2021</year></element-citation></ref><ref id="B5-sensors-25-05264"><label>5.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Ackerman</surname><given-names>E.</given-names></name></person-group><article-title>What Full Autonomy Means for the Waymo Driver</article-title><source>IEEE Spectrum</source><year>2021</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://spectrum.ieee.org/full-autonomy-waymo-driver" ext-link-type="uri">https://spectrum.ieee.org/full-autonomy-waymo-driver</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2021-03-04">(accessed on 4 March 2021)</date-in-citation></element-citation></ref><ref id="B6-sensors-25-05264"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dingus</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Antin</surname><given-names>J.F.</given-names></name><name name-style="western"><surname>Perez</surname><given-names>M.</given-names></name><name name-style="western"><surname>Buchanan-King</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hankey</surname><given-names>J.</given-names></name></person-group><article-title>Driver crash risk factors and prevalence evaluation using naturalistic driving data</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>2016</year><volume>113</volume><fpage>2636</fpage><lpage>2641</lpage><pub-id pub-id-type="doi">10.1073/pnas.1513271113</pub-id><pub-id pub-id-type="pmid">26903657</pub-id><pub-id pub-id-type="pmcid">PMC4790996</pub-id></element-citation></ref><ref id="B7-sensors-25-05264"><label>7.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>S.</given-names></name></person-group><source>Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey</source><publisher-name>National Highway Traffic Safety Administration</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2015</year></element-citation></ref><ref id="B8-sensors-25-05264"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Montgomery</surname><given-names>W.</given-names></name><name name-style="western"><surname>Mudge</surname><given-names>R.</given-names></name><name name-style="western"><surname>Groshen</surname><given-names>E.L.</given-names></name><name name-style="western"><surname>Helper</surname><given-names>S.</given-names></name><name name-style="western"><surname>MacDuffie</surname><given-names>J.P.</given-names></name><name name-style="western"><surname>Carson</surname><given-names>C.</given-names></name></person-group><source>America&#8217;s Workforce Self-Driving Future: Realizing Productivity Gains and Spurring Economic Growth</source><publisher-name>Securing America&#8217;s Future Energy</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2018</year></element-citation></ref><ref id="B9-sensors-25-05264"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chehri</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mouftah</surname><given-names>H.T.</given-names></name></person-group><article-title>Autonomous vehicles in the sustainable cities, the beginning of a green adventure</article-title><source>Sustain. Cities Soc.</source><year>2019</year><volume>51</volume><fpage>101751</fpage><pub-id pub-id-type="doi">10.1016/j.scs.2019.101751</pub-id></element-citation></ref><ref id="B10-sensors-25-05264"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Dhall</surname><given-names>A.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>D.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group><article-title>Real-time 3D traffic cone detection for autonomous driving</article-title><source>Proceedings of the 2019 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Paris, France</conf-loc><conf-date>9&#8211;12 June 2019</conf-date><fpage>494</fpage><lpage>501</lpage></element-citation></ref><ref id="B11-sensors-25-05264"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hudson</surname><given-names>J.</given-names></name><name name-style="western"><surname>Orviska</surname><given-names>M.</given-names></name><name name-style="western"><surname>Hunady</surname><given-names>J.</given-names></name></person-group><article-title>People&#8217;s attitudes to autonomous vehicles</article-title><source>Transp. Res. Part A Policy Pract.</source><year>2019</year><volume>121</volume><fpage>164</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.tra.2018.08.018</pub-id></element-citation></ref><ref id="B12-sensors-25-05264"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hulse</surname><given-names>L.M.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Galea</surname><given-names>E.R.</given-names></name></person-group><article-title>Relationships with road users, risk, gender and age</article-title><source>Saf. Sci.</source><year>2018</year><volume>102</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.ssci.2017.10.001</pub-id></element-citation></ref><ref id="B13-sensors-25-05264"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Srivastava</surname><given-names>A.</given-names></name></person-group><article-title>Sense-Plan-Act in Robotic Applications</article-title><source>Proceedings of the Intelligent Robotics Seminar</source><conf-loc>Macao, China</conf-loc><conf-date>4&#8211;8 November 2019</conf-date><pub-id pub-id-type="doi">10.13140/RG.2.2.21308.36481</pub-id></element-citation></ref><ref id="B14-sensors-25-05264"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Betz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wischnewski</surname><given-names>A.</given-names></name><name name-style="western"><surname>Heilmeier</surname><given-names>A.</given-names></name><name name-style="western"><surname>Nobis</surname><given-names>F.</given-names></name><name name-style="western"><surname>Stahl</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hermansdorfer</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lohmann</surname><given-names>B.</given-names></name><name name-style="western"><surname>Lienkamp</surname><given-names>M.</given-names></name></person-group><article-title>What can we learn from autonomous level-5 motorsport?</article-title><source>Proceedings of the 9th International Munich Chassis Symposium 2018</source><conf-loc>Munich, Germany</conf-loc><conf-date>12&#8211;13 June 2018</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Wiesbaden, Germany</publisher-loc><year>2019</year></element-citation></ref><ref id="B15-sensors-25-05264"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Betz</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liniger</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rosolia</surname><given-names>U.</given-names></name><name name-style="western"><surname>Karle</surname><given-names>P.</given-names></name><name name-style="western"><surname>Behl</surname><given-names>M.</given-names></name><name name-style="western"><surname>Krovi</surname><given-names>V.</given-names></name><name name-style="western"><surname>Mangharam</surname><given-names>R.</given-names></name></person-group><article-title>Autonomous Vehicles on the Edge: A Survey on Autonomous Vehicle Racing</article-title><source>IEEE Open J. Intell. Transp. Syst.</source><year>2022</year><volume>3</volume><fpage>458</fpage><lpage>488</lpage><pub-id pub-id-type="doi">10.1109/OJITS.2022.3181510</pub-id></element-citation></ref><ref id="B16-sensors-25-05264"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qian</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>3D object detection for autonomous driving: A survey</article-title><source>Pattern Recognit.</source><year>2022</year><volume>130</volume><fpage>108796</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2022.108796</pub-id></element-citation></ref><ref id="B17-sensors-25-05264"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wen</surname><given-names>L.H.</given-names></name><name name-style="western"><surname>Jo</surname><given-names>K.H.</given-names></name></person-group><article-title>Deep learning-based perception systems for autonomous driving: A comprehensive survey</article-title><source>Neurocomputing</source><year>2022</year><volume>489</volume><fpage>255</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2021.08.155</pub-id></element-citation></ref><ref id="B18-sensors-25-05264"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>3D object detection for autonomous driving: A comprehensive survey</article-title><source>Int. J. Comput. Vis.</source><year>2023</year><volume>131</volume><fpage>1909</fpage><lpage>1963</lpage><pub-id pub-id-type="doi">10.1007/s11263-023-01790-1</pub-id></element-citation></ref><ref id="B19-sensors-25-05264"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name></person-group><article-title>2D and 3D object detection algorithms from images: A Survey</article-title><source>Array</source><year>2023</year><volume>19</volume><fpage>100305</fpage><pub-id pub-id-type="doi">10.1016/j.array.2023.100305</pub-id></element-citation></ref><ref id="B20-sensors-25-05264"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pravallika</surname><given-names>A.</given-names></name><name name-style="western"><surname>Hashmi</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>A.</given-names></name></person-group><article-title>Deep Learning Frontiers in 3D Object Detection: A Comprehensive Review for Autonomous Driving</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>173936</fpage><lpage>173980</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3456893</pub-id></element-citation></ref><ref id="B21-sensors-25-05264"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>H.</given-names></name></person-group><article-title>A Survey of Deep Learning-Driven 3D Object Detection: Sensor Modalities, Technical Architectures, and Applications</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>3668</elocation-id><pub-id pub-id-type="doi">10.3390/s25123668</pub-id><pub-id pub-id-type="pmid">40573555</pub-id><pub-id pub-id-type="pmcid">PMC12196975</pub-id></element-citation></ref><ref id="B22-sensors-25-05264"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Simonelli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ricci</surname><given-names>E.</given-names></name></person-group><article-title>3d object detection from images for autonomous driving: A survey</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>46</volume><fpage>3537</fpage><lpage>3556</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3346386</pub-id><pub-id pub-id-type="pmid">38145536</pub-id></element-citation></ref><ref id="B23-sensors-25-05264"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bennamoun</surname><given-names>M.</given-names></name></person-group><article-title>Deep learning for 3d point clouds: A survey</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>43</volume><fpage>4338</fpage><lpage>4364</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.3005434</pub-id><pub-id pub-id-type="pmid">32750799</pub-id></element-citation></ref><ref id="B24-sensors-25-05264"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ji</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Multi-modal 3d object detection in autonomous driving: A survey</article-title><source>Int. J. Comput. Vis.</source><year>2023</year><volume>131</volume><fpage>2122</fpage><lpage>2152</lpage><pub-id pub-id-type="doi">10.1007/s11263-023-01784-z</pub-id></element-citation></ref><ref id="B25-sensors-25-05264"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lahoud</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>F.S.</given-names></name><name name-style="western"><surname>Cholakkal</surname><given-names>H.</given-names></name><name name-style="western"><surname>Anwer</surname><given-names>R.M.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.H.</given-names></name></person-group><article-title>3D vision with transformers: A survey</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arXiv.2208.04309</pub-id><pub-id pub-id-type="arxiv">2208.04309</pub-id></element-citation></ref><ref id="B26-sensors-25-05264"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name></person-group><article-title>A Systematic Survey of Transformer-Based 3D Object Detection for Autonomous Driving: Methods, Challenges and Trends</article-title><source>Drones</source><year>2024</year><volume>8</volume><elocation-id>412</elocation-id><pub-id pub-id-type="doi">10.3390/drones8080412</pub-id></element-citation></ref><ref id="B27-sensors-25-05264"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Calvo</surname><given-names>E.L.</given-names></name><name name-style="western"><surname>Taveira</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kahl</surname><given-names>F.</given-names></name><name name-style="western"><surname>Gustafsson</surname><given-names>N.</given-names></name><name name-style="western"><surname>Larsson</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tonderski</surname><given-names>A.</given-names></name></person-group><article-title>Timepillars: Temporally-recurrent 3d lidar object detection</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2312.17260</pub-id></element-citation></ref><ref id="B28-sensors-25-05264"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Geiger</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lenz</surname><given-names>P.</given-names></name><name name-style="western"><surname>Stiller</surname><given-names>C.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Vision meets robotics: The kitti dataset</article-title><source>Int. J. Robot. Res.</source><year>2013</year><volume>32</volume><fpage>1231</fpage><lpage>1237</lpage><pub-id pub-id-type="doi">10.1177/0278364913491297</pub-id></element-citation></ref><ref id="B29-sensors-25-05264"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arnold</surname><given-names>E.</given-names></name><name name-style="western"><surname>Al-Jarrah</surname><given-names>O.Y.</given-names></name><name name-style="western"><surname>Dianati</surname><given-names>M.</given-names></name><name name-style="western"><surname>Fallah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Oxtoby</surname><given-names>D.</given-names></name><name name-style="western"><surname>Mouzakitis</surname><given-names>A.</given-names></name></person-group><article-title>A survey on 3d object detection methods for autonomous driving applications</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2019</year><volume>20</volume><fpage>3782</fpage><lpage>3795</lpage><pub-id pub-id-type="doi">10.1109/TITS.2019.2892405</pub-id></element-citation></ref><ref id="B30-sensors-25-05264"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nagiub</surname><given-names>A.S.</given-names></name><name name-style="western"><surname>Fayez</surname><given-names>M.</given-names></name><name name-style="western"><surname>Khaled</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ghoniemy</surname><given-names>S.</given-names></name></person-group><article-title>3D object detection for autonomous driving: A comprehensive review</article-title><source>Proceedings of the 2024 6th International Conference on Computing and Informatics (ICCI)</source><conf-loc>Cairo, Egypt</conf-loc><conf-date>6&#8211;7 March 2024</conf-date><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="B31-sensors-25-05264"><label>31.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name></person-group><source>Deep Learning for 3D Point Clouds</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2025</year></element-citation></ref><ref id="B32-sensors-25-05264"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>F.</given-names></name></person-group><article-title>A survey of 3D object detection</article-title><source>Multimed. Tools Appl.</source><year>2021</year><volume>80</volume><fpage>29617</fpage><lpage>29641</lpage><pub-id pub-id-type="doi">10.1007/s11042-021-11137-y</pub-id></element-citation></ref><ref id="B33-sensors-25-05264"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fayyad</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jaradat</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Gruyer</surname><given-names>D.</given-names></name><name name-style="western"><surname>Najjaranngharam</surname><given-names>H.</given-names></name></person-group><article-title>Deep Learning Sensor Fusion: Vehicle Perception and Localization: A Review</article-title><source>Sensors</source><year>2022</year><volume>20</volume><elocation-id>4220</elocation-id><pub-id pub-id-type="doi">10.3390/s20154220</pub-id><pub-id pub-id-type="pmcid">PMC7436174</pub-id><pub-id pub-id-type="pmid">32751275</pub-id></element-citation></ref><ref id="B34-sensors-25-05264"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yurtsever</surname><given-names>E.</given-names></name><name name-style="western"><surname>Lambert</surname><given-names>J.</given-names></name><name name-style="western"><surname>Carballo</surname><given-names>A.</given-names></name><name name-style="western"><surname>Takeda</surname><given-names>K.</given-names></name></person-group><article-title>A survey of autonomous driving: Common practices and emerging technologies</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>58443</fpage><lpage>58469</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2983149</pub-id></element-citation></ref><ref id="B35-sensors-25-05264"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>F.</given-names></name></person-group><article-title>Delving into the Secrets of BEV 3D Object Detection in Autonomous Driving: A Comprehensive Survey</article-title><source>Authorea Prepr.</source><year>2025</year><pub-id pub-id-type="doi">10.36227/techrxiv.173221675.59410416/v1</pub-id></element-citation></ref><ref id="B36-sensors-25-05264"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Caesar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Bankiti</surname><given-names>V.</given-names></name><name name-style="western"><surname>Lang</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Vora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liong</surname><given-names>V.E.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Krishnan</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Baldan</surname><given-names>G.</given-names></name><name name-style="western"><surname>Beijbom</surname><given-names>O.</given-names></name></person-group><article-title>nuscenes: A multimodal dataset for autonomous driving</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>11621</fpage><lpage>11631</lpage></element-citation></ref><ref id="B37-sensors-25-05264"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kretzschmar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Dotiwalla</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chouard</surname><given-names>A.</given-names></name><name name-style="western"><surname>Patnaik</surname><given-names>V.</given-names></name><name name-style="western"><surname>Tsui</surname><given-names>P.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Caine</surname><given-names>B.</given-names></name><etal/></person-group><article-title>Scalability in perception for autonomous driving: Waymo open dataset</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>2446</fpage><lpage>2454</lpage></element-citation></ref><ref id="B38-sensors-25-05264"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Geng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name></person-group><article-title>The apolloscape open dataset for autonomous driving and its application</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2019</year><volume>42</volume><fpage>2702</fpage><lpage>2719</lpage><pub-id pub-id-type="doi">10.1109/tpami.2019.2926463</pub-id><pub-id pub-id-type="pmid">31283496</pub-id></element-citation></ref><ref id="B39-sensors-25-05264"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>M.F.</given-names></name><name name-style="western"><surname>Lambert</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sangkloy</surname><given-names>P.</given-names></name><name name-style="western"><surname>Singh</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bak</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hartnett</surname><given-names>A.</given-names></name></person-group><article-title>Argoverse: 3d tracking and forecasting with rich maps</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>8748</fpage><lpage>8757</lpage></element-citation></ref><ref id="B40-sensors-25-05264"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Houston</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zuidhof</surname><given-names>G.</given-names></name><name name-style="western"><surname>Bergamini</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jain</surname><given-names>A.</given-names></name><name name-style="western"><surname>Omari</surname><given-names>S.</given-names></name><name name-style="western"><surname>Iglovikov</surname><given-names>V.</given-names></name><name name-style="western"><surname>Ondruska</surname><given-names>P.</given-names></name></person-group><article-title>One thousand and one hours: Self-driving motion prediction dataset</article-title><source>Proceedings of the Conference on Robot Learning</source><conf-loc>London, UK</conf-loc><conf-date>8&#8211;11 November 2021</conf-date><fpage>409</fpage><lpage>418</lpage></element-citation></ref><ref id="B41-sensors-25-05264"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Patil</surname><given-names>A.</given-names></name><name name-style="western"><surname>Malla</surname><given-names>S.</given-names></name><name name-style="western"><surname>Gang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.T.</given-names></name></person-group><article-title>The h3d dataset for full-surround 3d multi-object detection and tracking in crowded urban scenes</article-title><source>Proceedings of the 2019 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>20&#8211;24 May 2019</conf-date><fpage>9552</fpage><lpage>9557</lpage></element-citation></ref><ref id="B42-sensors-25-05264"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zamanakos</surname><given-names>G.</given-names></name><name name-style="western"><surname>Tsochatzidis</surname><given-names>L.</given-names></name><name name-style="western"><surname>Amanatiadis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pratikakis</surname><given-names>I.</given-names></name></person-group><article-title>A comprehensive survey of LIDAR-based 3D object detection methods with deep learning for autonomous driving</article-title><source>Comput. Graph.</source><year>2021</year><volume>99</volume><fpage>153</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.cag.2021.07.003</pub-id></element-citation></ref><ref id="B43-sensors-25-05264"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Savarese</surname><given-names>S.</given-names></name></person-group><article-title>Data-driven 3d voxel patterns for object category recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>1903</fpage><lpage>1911</lpage></element-citation></ref><ref id="B44-sensors-25-05264"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kundu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fidler</surname><given-names>S.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Monocular 3d object detection for autonomous driving</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#8211;1 July 2016</conf-date><fpage>2147</fpage><lpage>2156</lpage></element-citation></ref><ref id="B45-sensors-25-05264"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mousavian</surname><given-names>A.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Flynn</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kosecka</surname><given-names>J.</given-names></name></person-group><article-title>3d bounding box estimation using deep learning and geometry</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>7074</fpage><lpage>7082</lpage></element-citation></ref><ref id="B46-sensors-25-05264"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Brazil</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>M3d-rpn: Monocular 3d region proposal network for object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>9287</fpage><lpage>9296</lpage></element-citation></ref><ref id="B47-sensors-25-05264"><label>47.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Simonelli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bulo</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Porzi</surname><given-names>L.</given-names></name><name name-style="western"><surname>L&#243;pez-Antequera</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kontschieder</surname><given-names>P.</given-names></name></person-group><article-title>Disentangling monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>1991</fpage><lpage>1999</lpage></element-citation></ref><ref id="B48-sensors-25-05264"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Limaye</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mathew</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nagori</surname><given-names>S.</given-names></name><name name-style="western"><surname>Swami</surname><given-names>P.K.</given-names></name><name name-style="western"><surname>Maji</surname><given-names>D.</given-names></name><name name-style="western"><surname>Desappan</surname><given-names>K.</given-names></name></person-group><article-title>SS3D: Single shot 3D object detector</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2004.14674</pub-id><pub-id pub-id-type="arxiv">2004.14674</pub-id></element-citation></ref><ref id="B49-sensors-25-05264"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>T&#243;th</surname><given-names>R.</given-names></name></person-group><article-title>Smoke: Single-stage monocular 3d object detection via keypoint estimation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>996</fpage><lpage>997</lpage></element-citation></ref><ref id="B50-sensors-25-05264"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>X.A.A.</given-names></name></person-group><article-title>Monodistill: Learning spatial features for monocular 3d object detection</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arXiv.2201.10830</pub-id><pub-id pub-id-type="arxiv">2201.10830</pub-id></element-citation></ref><ref id="B51-sensors-25-05264"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>J.</given-names></name></person-group><article-title>MonoSKD: General distillation framework for monocular 3D object detection via Spearman correlation coefficient</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2310.11316</pub-id><pub-id pub-id-type="arxiv">2310.11316</pub-id></element-citation></ref><ref id="B52-sensors-25-05264"><label>52.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Qian</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>D.</given-names></name></person-group><article-title>Mononerd: Nerf-like representations for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>6814</fpage><lpage>6824</lpage></element-citation></ref><ref id="B53-sensors-25-05264"><label>53.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>P.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>Y.</given-names></name></person-group><article-title>Monocd: Monocular 3d object detection with complementary depths</article-title><source>Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>10248</fpage><lpage>10257</lpage></element-citation></ref><ref id="B54-sensors-25-05264"><label>54.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>S.</given-names></name></person-group><article-title>Stereo r-cnn based 3d object detection for autonomous driving</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15 &#8211;20 June 2019</conf-date></element-citation></ref><ref id="B55-sensors-25-05264"><label>55.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Dsgn: Deep stereo geometry network for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date></element-citation></ref><ref id="B56-sensors-25-05264"><label>56.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name></person-group><article-title>Yolostereo3d: A step back to 2d for efficient stereo 3d detection</article-title><source>Proceedings of the 2021 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Xi&#8217;an, China</conf-loc><conf-date>30 May&#8211;5 June 2021</conf-date><fpage>13018</fpage><lpage>13024</lpage></element-citation></ref><ref id="B57-sensors-25-05264"><label>57.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Liga-stereo: Learning lidar geometry aware representations for stereo-based 3d detector</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>3153</fpage><lpage>3163</lpage></element-citation></ref><ref id="B58-sensors-25-05264"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>E.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name></person-group><article-title>Stereodistill: Pick the cream from lidar for distilling stereo-based 3d object detection</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>1790</fpage><lpage>1798</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i2.25268</pub-id></element-citation></ref><ref id="B59-sensors-25-05264"><label>59.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guizilini</surname><given-names>V.C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Solomon</surname><given-names>J.</given-names></name></person-group><article-title>Detr3d: 3d object detection from multi-view images via 3d-to-2d queries</article-title><source>Proceedings of the Conference on Robot Learning</source><conf-loc>Auckland, New Zealand</conf-loc><conf-date>14&#8211;18 December 2022</conf-date><fpage>180</fpage><lpage>191</lpage></element-citation></ref><ref id="B60-sensors-25-05264"><label>60.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Petr: Position embedding transformation for multi-view 3d object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>531</fpage><lpage>548</lpage></element-citation></ref><ref id="B61-sensors-25-05264"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>E.</given-names></name><name name-style="western"><surname>Sima</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>T.</given-names></name></person-group><article-title>Bevformer: Learning bird&#8217;s-eye-view representation from lidar-camera via spatiotemporal transformers</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>47</volume><fpage>2020</fpage><lpage>2036</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2024.3515454</pub-id><pub-id pub-id-type="pmid">40030479</pub-id></element-citation></ref><ref id="B62-sensors-25-05264"><label>62.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Teng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Sparsebev: High-performance sparse 3d object detection from multi-camera videos</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>18580</fpage><lpage>18590</lpage></element-citation></ref><ref id="B63-sensors-25-05264"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ji</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ni</surname><given-names>T.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>RoPETR: Improving Temporal Camera-Only 3D Detection by Integrating Enhanced Rotary Position Embedding</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2504.12643</pub-id></element-citation></ref><ref id="B64-sensors-25-05264"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name></person-group><article-title>Deep Learning on Point Clouds and Its Application: A Survey</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>4188</elocation-id><pub-id pub-id-type="doi">10.3390/s19194188</pub-id><pub-id pub-id-type="pmid">31561639</pub-id><pub-id pub-id-type="pmcid">PMC6806315</pub-id></element-citation></ref><ref id="B65-sensors-25-05264"><label>65.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jo</surname><given-names>K.</given-names></name></person-group><article-title>3D Point Cloud Segmentation: A survey</article-title><source>Proceedings of the IEEE Conference on Robotics, Automation and Mechatronics</source><conf-loc>Kagawa, Japan</conf-loc><conf-date>4&#8211;7 August 2013</conf-date></element-citation></ref><ref id="B66-sensors-25-05264"><label>66.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Xuan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>Y.</given-names></name></person-group><article-title>Multimodal Data Fusion for BEV Perception</article-title><source>Master&#8217;s Thesis</source><publisher-name>University of Gothenburg</publisher-name><publisher-loc>Gothenburg, Sweden</publisher-loc><year>2024</year></element-citation></ref><ref id="B67-sensors-25-05264"><label>67.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>PIXOR: Real-time 3D Object Detection from Point Clouds</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>7652</fpage><lpage>7660</lpage></element-citation></ref><ref id="B68-sensors-25-05264"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Simon</surname><given-names>M.</given-names></name><name name-style="western"><surname>Milz</surname><given-names>S.</given-names></name><name name-style="western"><surname>Amende</surname><given-names>K.</given-names></name><name name-style="western"><surname>Gross</surname><given-names>H.M.</given-names></name></person-group><article-title>Complex-yolo: Real-time 3d object detection on point clouds</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1803.06199</pub-id></element-citation></ref><ref id="B69-sensors-25-05264"><label>69.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Beltr&#225;n</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guindel</surname><given-names>C.</given-names></name><name name-style="western"><surname>Moreno</surname><given-names>F.M.</given-names></name><name name-style="western"><surname>Cruzado</surname><given-names>D.</given-names></name><name name-style="western"><surname>Garc&#237;a</surname><given-names>F.</given-names></name><name name-style="western"><surname>De La Escalera</surname><given-names>A.</given-names></name></person-group><article-title>Birdnet: A 3d object detection framework from lidar information</article-title><source>Proceedings of the 2018 21st International Conference on Intelligent Transportation Systems</source><conf-loc>Maui, HI, USA</conf-loc><conf-date>4&#8211;7 November 2018</conf-date><fpage>3517</fpage><lpage>3523</lpage></element-citation></ref><ref id="B70-sensors-25-05264"><label>70.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Mo</surname><given-names>K.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>Pointnet: Deep learning on point sets for 3d classification and segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>652</fpage><lpage>660</lpage></element-citation></ref><ref id="B71-sensors-25-05264"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>Pointnet++: Deep hierarchical feature learning on point sets in a metric space</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>30</volume><fpage>5105</fpage><lpage>5114</lpage></element-citation></ref><ref id="B72-sensors-25-05264"><label>72.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Pointrcnn: 3d object proposal generation and detection from point cloud</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>770</fpage><lpage>779</lpage></element-citation></ref><ref id="B73-sensors-25-05264"><label>73.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name></person-group><article-title>Not all points are equal: Learning highly efficient point-based detectors for 3d lidar point clouds</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>18953</fpage><lpage>18962</lpage></element-citation></ref><ref id="B74-sensors-25-05264"><label>74.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Song</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.E.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name></person-group><article-title>3d object detection with pointformer</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>7463</fpage><lpage>7472</lpage></element-citation></ref><ref id="B75-sensors-25-05264"><label>75.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tuzel</surname><given-names>O.</given-names></name></person-group><article-title>Voxelnet: End-to-end learning for point cloud based 3d object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>4490</fpage><lpage>4499</lpage></element-citation></ref><ref id="B76-sensors-25-05264"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title>SECOND: Sparsely Embedded Convolutional Detection</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>3337</elocation-id><pub-id pub-id-type="doi">10.3390/s18103337</pub-id><pub-id pub-id-type="pmid">30301196</pub-id><pub-id pub-id-type="pmcid">PMC6210968</pub-id></element-citation></ref><ref id="B77-sensors-25-05264"><label>77.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lang</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Vora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Caesar</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Beijbom</surname><given-names>O.</given-names></name></person-group><article-title>PointPillars: Fast Encoders for Object Detection from Point Clouds</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>12697</fpage><lpage>12705</lpage></element-citation></ref><ref id="B78-sensors-25-05264"><label>78.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>K.</given-names></name><name name-style="western"><surname>Yuille</surname><given-names>A.</given-names></name></person-group><article-title>Object as hotspots: An anchor-free 3d object detection approach via firing of hotspots</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>68</fpage><lpage>84</lpage></element-citation></ref><ref id="B79-sensors-25-05264"><label>79.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Voxel r-cnn: Towards high performance voxel-based 3d object detection</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2021</year><volume>35</volume><fpage>1201</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1609/aaai.v35i2.16207</pub-id></element-citation></ref><ref id="B80-sensors-25-05264"><label>80.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name></person-group><article-title>Voxel transformer for 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>3164</fpage><lpage>3173</lpage></element-citation></ref><ref id="B81-sensors-25-05264"><label>81.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>Transformation-equivariant 3d object detection for autonomous driving</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>2795</fpage><lpage>2802</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i3.25380</pub-id></element-citation></ref><ref id="B82-sensors-25-05264"><label>82.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Fast point r-cnn</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>9775</fpage><lpage>9784</lpage></element-citation></ref><ref id="B83-sensors-25-05264"><label>83.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>10529</fpage><lpage>10538</lpage></element-citation></ref><ref id="B84-sensors-25-05264"><label>84.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.S.</given-names></name><name name-style="western"><surname>Kuai</surname><given-names>T.</given-names></name><name name-style="western"><surname>Waslander</surname><given-names>S.L.</given-names></name></person-group><article-title>Point density-aware voxels for lidar 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>8469</fpage><lpage>8478</lpage></element-citation></ref><ref id="B85-sensors-25-05264"><label>85.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lai-Dang</surname><given-names>Q.V.</given-names></name></person-group><article-title>A survey of vision transformers in autonomous driving: Current trends and future directions</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2403.07542</pub-id><pub-id pub-id-type="arxiv">2403.07542</pub-id></element-citation></ref><ref id="B86-sensors-25-05264"><label>86.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Svenningsson</surname><given-names>P.</given-names></name><name name-style="western"><surname>Fioranelli</surname><given-names>F.</given-names></name><name name-style="western"><surname>Yarovoy</surname><given-names>A.</given-names></name></person-group><article-title>Radar-pointgnn: Graph based object recognition for unstructured radar point-cloud data</article-title><source>Proceedings of the 2021 IEEE Radar Conference (RadarConf21)</source><conf-loc>Atlanta, GA, USA</conf-loc><conf-date>8&#8211;14 May 2021</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B87-sensors-25-05264"><label>87.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paek</surname><given-names>D.H.</given-names></name><name name-style="western"><surname>Kong</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Wijaya</surname><given-names>K.T.</given-names></name></person-group><article-title>K-radar: 4d radar object detection for autonomous driving in various weather conditions</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>3819</fpage><lpage>3829</lpage></element-citation></ref><ref id="B88-sensors-25-05264"><label>88.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ulrich</surname><given-names>M.</given-names></name><name name-style="western"><surname>Braun</surname><given-names>S.</given-names></name><name name-style="western"><surname>K&#246;hler</surname><given-names>D.</given-names></name><name name-style="western"><surname>Niederl&#246;hner</surname><given-names>D.</given-names></name><name name-style="western"><surname>Faion</surname><given-names>F.</given-names></name><name name-style="western"><surname>Gl&#228;ser</surname><given-names>C.</given-names></name><name name-style="western"><surname>Blume</surname><given-names>H.</given-names></name></person-group><article-title>Improved orientation estimation and detection with hybrid object detection networks for automotive radar</article-title><source>Proceedings of the 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Macau, China</conf-loc><conf-date>8&#8211;12 October 2022</conf-date><fpage>111</fpage><lpage>117</lpage></element-citation></ref><ref id="B89-sensors-25-05264"><label>89.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kum</surname><given-names>D.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>J.W.</given-names></name></person-group><article-title>Radardistill: Boosting radar-based object detection performance via knowledge distillation from lidar features</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>15491</fpage><lpage>15500</lpage></element-citation></ref><ref id="B90-sensors-25-05264"><label>90.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hoffmann</surname><given-names>R.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wysocki</surname><given-names>O.</given-names></name><name name-style="western"><surname>Schwab</surname><given-names>B.</given-names></name><name name-style="western"><surname>Kolbe</surname><given-names>T.H.</given-names></name><name name-style="western"><surname>Cremers</surname><given-names>D.</given-names></name></person-group><article-title>RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning</article-title><source>Proceedings of the Computer Vision and Pattern Recognition Conference</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>10&#8211;17 June 2025</conf-date><fpage>4452</fpage><lpage>4461</lpage></element-citation></ref><ref id="B91-sensors-25-05264"><label>91.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vora</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lang</surname><given-names>A.H.</given-names></name><name name-style="western"><surname>Helou</surname><given-names>B.</given-names></name><name name-style="western"><surname>Beijbom</surname><given-names>O.</given-names></name></person-group><article-title>Pointpainting: Sequential fusion for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>4604</fpage><lpage>4612</lpage></element-citation></ref><ref id="B92-sensors-25-05264"><label>92.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>C.R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.</given-names></name><name name-style="western"><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>Frustum pointnets for 3d object detection from rgb-d data</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>918</fpage><lpage>927</lpage></element-citation></ref><ref id="B93-sensors-25-05264"><label>93.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>K.</given-names></name></person-group><article-title>Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</article-title><source>Proceedings of the 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Macau, China</conf-loc><conf-date>3&#8211;8 November 2019</conf-date><fpage>1742</fpage><lpage>1749</lpage></element-citation></ref><ref id="B94-sensors-25-05264"><label>94.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Paigwar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sierra-Gonzalez</surname><given-names>D.</given-names></name><name name-style="western"><surname>Erkent</surname><given-names>&#214;.</given-names></name><name name-style="western"><surname>Laugier</surname><given-names>C.</given-names></name></person-group><article-title>Frustum-pointpillars: A multi-stage approach for 3d object detection using rgb camera and lidar</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>2926</fpage><lpage>2933</lpage></element-citation></ref><ref id="B95-sensors-25-05264"><label>95.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>Virtual sparse convolution for multimodal 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>21653</fpage><lpage>21662</lpage></element-citation></ref><ref id="B96-sensors-25-05264"><label>96.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>T.</given-names></name></person-group><article-title>Multi-view 3d object detection network for autonomous driving</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#8211;29 October 2017</conf-date><fpage>1907</fpage><lpage>1915</lpage></element-citation></ref><ref id="B97-sensors-25-05264"><label>97.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ku</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mozifian</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><name name-style="western"><surname>Harakeh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Waslander</surname><given-names>S.L.</given-names></name></person-group><article-title>Joint 3d proposal generation and object detection from view aggregation</article-title><source>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Madrid, Spain</conf-loc><conf-date>1&#8211;5 October 2018</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B98-sensors-25-05264"><label>98.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Deep continuous fusion for multi-sensor 3d object detection</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>10&#8211;13 September 2018</conf-date><fpage>641</fpage><lpage>656</lpage></element-citation></ref><ref id="B99-sensors-25-05264"><label>99.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Multi-task multi-sensor fusion for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>7345</fpage><lpage>7353</lpage></element-citation></ref><ref id="B100-sensors-25-05264"><label>100.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name></person-group><article-title>Epnet: Enhancing point features with image semantics for 3d object detection</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><comment>Proceedings, Part XV 16</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>35</fpage><lpage>52</lpage></element-citation></ref><ref id="B101-sensors-25-05264"><label>101.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tai</surname><given-names>C.L.</given-names></name></person-group><article-title>Transfusion: Robust lidar-camera fusion for 3d object detection with transformers</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>1090</fpage><lpage>1099</lpage></element-citation></ref><ref id="B102-sensors-25-05264"><label>102.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Futr3d: A unified sensor fusion framework for 3d detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>172</fpage><lpage>181</lpage></element-citation></ref><ref id="B103-sensors-25-05264"><label>103.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Morris</surname><given-names>D.</given-names></name><name name-style="western"><surname>Radha</surname><given-names>H.</given-names></name></person-group><article-title>CLOCs: Camera-LiDAR object candidates fusion for 3D object detection</article-title><source>Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Las Vegas, Nevada, USA</conf-loc><conf-date>25&#8211;29 October 2020</conf-date><fpage>10386</fpage><lpage>10393</lpage></element-citation></ref><ref id="B104-sensors-25-05264"><label>104.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Morris</surname><given-names>D.</given-names></name><name name-style="western"><surname>Radha</surname><given-names>H.</given-names></name></person-group><article-title>Fast-CLOCs: Fast camera-LiDAR object candidates fusion for 3D object detection</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2022</conf-date><fpage>187</fpage><lpage>196</lpage></element-citation></ref><ref id="B105-sensors-25-05264"><label>105.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Casas</surname><given-names>S.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Radarnet: Exploiting radar for robust perception of dynamic objects</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Online</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>496</fpage><lpage>512</lpage></element-citation></ref><ref id="B106-sensors-25-05264"><label>106.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Morris</surname><given-names>D.</given-names></name></person-group><article-title>RICCARDO: Radar Hit Prediction and Convolution for Camera-Radar 3D Object Detection</article-title><source>Proceedings of the Computer Vision and Pattern Recognition Conference</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>10&#8211;17 June 2025</conf-date><fpage>22276</fpage><lpage>22285</lpage></element-citation></ref><ref id="B107-sensors-25-05264"><label>107.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Nabati</surname><given-names>R.</given-names></name><name name-style="western"><surname>Qi</surname><given-names>H.</given-names></name></person-group><article-title>Centerfusion: Center-based radar and camera fusion for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Virtual</conf-loc><conf-date>5&#8211;9 January 2021</conf-date><fpage>1527</fpage><lpage>1536</lpage></element-citation></ref><ref id="B108-sensors-25-05264"><label>108.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>K.</given-names></name><name name-style="western"><surname>He</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>J.</given-names></name></person-group><article-title>Radar and camera fusion for object detection and tracking: A comprehensive survey</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2410.19872</pub-id><pub-id pub-id-type="doi">10.1109/COMST.2025.3599596</pub-id></element-citation></ref><ref id="B109-sensors-25-05264"><label>109.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Giuffrida</surname><given-names>L.</given-names></name><name name-style="western"><surname>Masera</surname><given-names>G.</given-names></name><name name-style="western"><surname>Martina</surname><given-names>M.</given-names></name></person-group><article-title>A survey of automotive radar and lidar signal processing and architectures</article-title><source>Chips</source><year>2023</year><volume>2</volume><fpage>243</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.3390/chips2040015</pub-id></element-citation></ref><ref id="B110-sensors-25-05264"><label>110.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Savarese</surname><given-names>S.</given-names></name></person-group><article-title>Subcategory-aware convolutional neural networks for object proposals and detection</article-title><source>Proceedings of the 2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Santa Rosa, CA, USA</conf-loc><conf-date>24&#8211;31 March 2017</conf-date><fpage>924</fpage><lpage>933</lpage></element-citation></ref><ref id="B111-sensors-25-05264"><label>111.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chabot</surname><given-names>F.</given-names></name><name name-style="western"><surname>Chaouch</surname><given-names>M.</given-names></name><name name-style="western"><surname>Rabarisoa</surname><given-names>J.</given-names></name><name name-style="western"><surname>Teuliere</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chateau</surname><given-names>T.</given-names></name></person-group><article-title>Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>2040</fpage><lpage>2049</lpage></element-citation></ref><ref id="B112-sensors-25-05264"><label>112.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kundu</surname><given-names>A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Rehg</surname><given-names>J.M.</given-names></name></person-group><article-title>3d-rcnn: Instance-level 3d object reconstruction via render-and-compare</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>3559</fpage><lpage>3568</lpage></element-citation></ref><ref id="B113-sensors-25-05264"><label>113.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Manhardt</surname><given-names>F.</given-names></name><name name-style="western"><surname>Kehl</surname><given-names>W.</given-names></name><name name-style="western"><surname>Gaidon</surname><given-names>A.</given-names></name></person-group><article-title>Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>2069</fpage><lpage>2078</lpage></element-citation></ref><ref id="B114-sensors-25-05264"><label>114.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name></person-group><article-title>Multi-level fusion based 3d object detection from monocular images</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>2345</fpage><lpage>2353</lpage></element-citation></ref><ref id="B115-sensors-25-05264"><label>115.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name></person-group><article-title>Monogrnet: A geometric reasoning network for monocular 3d object localization</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2019</year><volume>33</volume><fpage>8851</fpage><lpage>8858</lpage><pub-id pub-id-type="doi">10.1609/aaai.v33i01.33018851</pub-id></element-citation></ref><ref id="B116-sensors-25-05264"><label>116.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Gs3d: An efficient 3d object detection framework for autonomous driving</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>1019</fpage><lpage>1028</lpage></element-citation></ref><ref id="B117-sensors-25-05264"><label>117.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Weng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kitani</surname><given-names>K.</given-names></name></person-group><article-title>Monocular 3d object detection with pseudo-lidar point cloud</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date></element-citation></ref><ref id="B118-sensors-25-05264"><label>118.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.A.A.</given-names></name></person-group><article-title>Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>6851</fpage><lpage>6860</lpage></element-citation></ref><ref id="B119-sensors-25-05264"><label>119.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wetzstein</surname><given-names>G.</given-names></name></person-group><article-title>Deep optics for monocular depth estimation and 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>10193</fpage><lpage>10202</lpage></element-citation></ref><ref id="B120-sensors-25-05264"><label>120.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kr&#228;henb&#252;hl</surname><given-names>P.</given-names></name></person-group><article-title>Objects as points</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1904.07850</pub-id></element-citation></ref><ref id="B121-sensors-25-05264"><label>121.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name></person-group><article-title>Deep fitting degree scoring network for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>1057</fpage><lpage>1066</lpage></element-citation></ref><ref id="B122-sensors-25-05264"><label>122.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Naiden</surname><given-names>A.</given-names></name><name name-style="western"><surname>Paunescu</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>G.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>B.</given-names></name><name name-style="western"><surname>Leordeanu</surname><given-names>M.</given-names></name></person-group><article-title>Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints</article-title><source>Proceedings of the 2019 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>22&#8211;25 September 2019</conf-date><fpage>61</fpage><lpage>65</lpage></element-citation></ref><ref id="B123-sensors-25-05264"><label>123.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name></person-group><article-title>Monofenet: Monocular 3d object detection with feature enhancement networks</article-title><source>IEEE Trans. Image Process.</source><year>2019</year><volume>29</volume><fpage>2753</fpage><lpage>2765</lpage><pub-id pub-id-type="doi">10.1109/TIP.2019.2952201</pub-id><pub-id pub-id-type="pmid">31725382</pub-id></element-citation></ref><ref id="B124-sensors-25-05264"><label>124.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ku</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pon</surname><given-names>A.D.</given-names></name><name name-style="western"><surname>Waslander</surname><given-names>S.L.</given-names></name></person-group><article-title>Monocular 3d object detection leveraging accurate proposals and shape reconstruction</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>11867</fpage><lpage>11876</lpage></element-citation></ref><ref id="B125-sensors-25-05264"><label>125.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Simonelli</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bulo</surname><given-names>S.R.</given-names></name><name name-style="western"><surname>Porzi</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ricci</surname><given-names>E.</given-names></name><name name-style="western"><surname>Kontschieder</surname><given-names>P.</given-names></name></person-group><article-title>Towards generalization across depth for monocular 3d object detection</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><comment>Proceedings, Part XXII 16</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>767</fpage><lpage>782</lpage></element-citation></ref><ref id="B126-sensors-25-05264"><label>126.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vianney</surname><given-names>J.M.U.</given-names></name><name name-style="western"><surname>Aich</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>B.</given-names></name></person-group><article-title>Refinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1911.09712</pub-id></element-citation></ref><ref id="B127-sensors-25-05264"><label>127.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tai</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>K.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name></person-group><article-title>Monopair: Monocular 3d object detection using pairwise spatial relationships</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>12093</fpage><lpage>12102</lpage></element-citation></ref><ref id="B128-sensors-25-05264"><label>128.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>F.</given-names></name></person-group><article-title>Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Online</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>644</fpage><lpage>660</lpage></element-citation></ref><ref id="B129-sensors-25-05264"><label>129.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name></person-group><article-title>Rethinking pseudo-lidar representation</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><comment>Proceedings, Part XIII 16</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>311</fpage><lpage>327</lpage></element-citation></ref><ref id="B130-sensors-25-05264"><label>130.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Song</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Iafa: Instance-aware feature aggregation for 3d object detection from a single image</article-title><source>Proceedings of the Asian Conference on Computer Vision</source><conf-loc>Kyoto, Japan</conf-loc><conf-date>30 November&#8211;4 December 2020</conf-date></element-citation></ref><ref id="B131-sensors-25-05264"><label>131.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Brazil</surname><given-names>G.</given-names></name><name name-style="western"><surname>Pons-Moll</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name></person-group><article-title>Kinematic 3d object detection in monocular video</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><comment>Proceedings, Part XXIII 16</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>135</fpage><lpage>152</lpage></element-citation></ref><ref id="B132-sensors-25-05264"><label>132.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Monocular 3d detection with geometric constraint embedding and semi-supervised training</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>6</volume><fpage>5565</fpage><lpage>5572</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3061343</pub-id></element-citation></ref><ref id="B133-sensors-25-05264"><label>133.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Du</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>G.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>X.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Depth-conditioned dynamic message propagation for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2021</conf-date><fpage>454</fpage><lpage>463</lpage></element-citation></ref><ref id="B134-sensors-25-05264"><label>134.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>W.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xiong</surname><given-names>L.</given-names></name></person-group><article-title>Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>10379</fpage><lpage>10388</lpage></element-citation></ref><ref id="B135-sensors-25-05264"><label>135.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Brazil</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>Groomed-nms: Grouped mathematically differentiable nms for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>8973</fpage><lpage>8983</lpage></element-citation></ref><ref id="B136-sensors-25-05264"><label>136.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name></person-group><article-title>Delving into localization errors for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2021</conf-date><fpage>4721</fpage><lpage>4730</lpage></element-citation></ref><ref id="B137-sensors-25-05264"><label>137.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Reading</surname><given-names>C.</given-names></name><name name-style="western"><surname>Harakeh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chae</surname><given-names>J.</given-names></name><name name-style="western"><surname>Waslander</surname><given-names>S.L.</given-names></name></person-group><article-title>Categorical depth distribution network for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>8555</fpage><lpage>8564</lpage></element-citation></ref><ref id="B138-sensors-25-05264"><label>138.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name></person-group><article-title>Objects are different: Flexible monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>3289</fpage><lpage>3298</lpage></element-citation></ref><ref id="B139-sensors-25-05264"><label>139.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>T.K.</given-names></name></person-group><article-title>Geometry-based distance decomposition for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>15172</fpage><lpage>15181</lpage></element-citation></ref><ref id="B140-sensors-25-05264"><label>140.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>D.</given-names></name></person-group><article-title>Fcos3d: Fully convolutional one-stage monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>913</fpage><lpage>922</lpage></element-citation></ref><ref id="B141-sensors-25-05264"><label>141.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Q.</given-names></name></person-group><article-title>Monocular 3d object detection: An extrinsic parameter free approach</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>7556</fpage><lpage>7566</lpage></element-citation></ref><ref id="B142-sensors-25-05264"><label>142.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name></person-group><article-title>Geometry uncertainty projection network for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>3111</fpage><lpage>3121</lpage></element-citation></ref><ref id="B143-sensors-25-05264"><label>143.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xinge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>D.</given-names></name></person-group><article-title>Probabilistic and geometric depth: Detecting objects in perspective</article-title><source>Proceedings of the Conference on Robot Learning</source><conf-loc>Auckland, New Zealand</conf-loc><conf-date>14&#8211;18 December 2022</conf-date><fpage>1475</fpage><lpage>1485</lpage></element-citation></ref><ref id="B144-sensors-25-05264"><label>144.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>X.S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Aug3d-rpn: Improving monocular 3d object detection by synthetic images with virtual depth</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2107.13269</pub-id></element-citation></ref><ref id="B145-sensors-25-05264"><label>145.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ambrus</surname><given-names>R.</given-names></name><name name-style="western"><surname>Guizilini</surname><given-names>V.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gaidon</surname><given-names>A.</given-names></name></person-group><article-title>Is pseudo-lidar needed for monocular 3d object detection?</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>3142</fpage><lpage>3152</lpage></element-citation></ref><ref id="B146-sensors-25-05264"><label>146.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>He</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>X.</given-names></name></person-group><article-title>Progressive coordinate transforms for monocular 3d object detection</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>13364</fpage><lpage>13377</lpage></element-citation></ref><ref id="B147-sensors-25-05264"><label>147.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>F.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Autoshape: Real-time shape-aware monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>15641</fpage><lpage>15650</lpage></element-citation></ref><ref id="B148-sensors-25-05264"><label>148.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Van Gool</surname><given-names>L.</given-names></name><name name-style="western"><surname>Timofte</surname><given-names>R.</given-names></name></person-group><article-title>Deep line encoding for monocular 3d object detection and depth prediction</article-title><source>Proceedings of the 32nd British Machine Vision Conference (BMVC 2021)</source><conf-loc>Virtual</conf-loc><conf-date>22&#8211;25 November 2021</conf-date><publisher-name>BMVA Press</publisher-name><publisher-loc>Durham, UK</publisher-loc><year>2021</year><fpage>354</fpage></element-citation></ref><ref id="B149-sensors-25-05264"><label>149.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xue</surname><given-names>N.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>T.</given-names></name></person-group><article-title>Learning auxiliary monocular contexts helps monocular 3d object detection</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2022</year><volume>36</volume><fpage>1810</fpage><lpage>1818</lpage><pub-id pub-id-type="doi">10.1609/aaai.v36i2.20074</pub-id></element-citation></ref><ref id="B150-sensors-25-05264"><label>150.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>K.C.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>T.H.</given-names></name><name name-style="western"><surname>Su</surname><given-names>H.T.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>W.H.</given-names></name></person-group><article-title>Monodtr: Monocular 3d object detection with depth-aware transformer</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>4012</fpage><lpage>4021</lpage></element-citation></ref><ref id="B151-sensors-25-05264"><label>151.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>P.</given-names></name></person-group><article-title>Monodetr: Depth-guided transformer for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>9155</fpage><lpage>9166</lpage></element-citation></ref><ref id="B152-sensors-25-05264"><label>152.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lian</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Monojsg: Joint semantic and geometric cost volume for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>1070</fpage><lpage>1079</lpage></element-citation></ref><ref id="B153-sensors-25-05264"><label>153.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Fan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>X.S.</given-names></name></person-group><article-title>Homography loss for monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>1080</fpage><lpage>1089</lpage></element-citation></ref><ref id="B154-sensors-25-05264"><label>154.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Qu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name></person-group><article-title>Diversity matters: Fully exploiting depth clues for reliable monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>2791</fpage><lpage>2800</lpage></element-citation></ref><ref id="B155-sensors-25-05264"><label>155.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name></person-group><article-title>Mix-teaching: A simple, unified and effective semi-supervised learning framework for monocular 3d object detection</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2023</year><volume>33</volume><fpage>6832</fpage><lpage>6844</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2023.3270728</pub-id></element-citation></ref><ref id="B156-sensors-25-05264"><label>156.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>Densely constrained depth estimator for monocular 3d object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>718</fpage><lpage>734</lpage></element-citation></ref><ref id="B157-sensors-25-05264"><label>157.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Brazil</surname><given-names>G.</given-names></name><name name-style="western"><surname>Corona</surname><given-names>E.</given-names></name><name name-style="western"><surname>Parchami</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name></person-group><article-title>Deviant: Depth equivariant network for monocular 3d object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>664</fpage><lpage>683</lpage></element-citation></ref><ref id="B158-sensors-25-05264"><label>158.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Brazil</surname><given-names>G.</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>A.</given-names></name><name name-style="western"><surname>Straub</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ravi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gkioxari</surname><given-names>G.</given-names></name></person-group><article-title>Omni3d: A large benchmark and model for 3d object detection in the wild</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>13154</fpage><lpage>13164</lpage></element-citation></ref><ref id="B159-sensors-25-05264"><label>159.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>M.</given-names></name></person-group><article-title>Mogde: Boosting mobile monocular 3d object detection with ground depth estimation</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>2033</fpage><lpage>2045</lpage></element-citation></ref><ref id="B160-sensors-25-05264"><label>160.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Attention-based depth distillation with 3d-aware positional encoding for monocular 3d object detection</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>2892</fpage><lpage>2900</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i3.25391</pub-id></element-citation></ref><ref id="B161-sensors-25-05264"><label>161.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hong</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>Y.</given-names></name></person-group><article-title>Cross-modality knowledge distillation network for monocular 3d object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>87</fpage><lpage>104</lpage></element-citation></ref><ref id="B162-sensors-25-05264"><label>162.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gan</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>G.</given-names></name><name name-style="western"><surname>Pu</surname><given-names>J.</given-names></name></person-group><article-title>Monopgc: Monocular 3d object detection with pixel geometry contexts</article-title><source>Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>London, UK</conf-loc><conf-date>29 May&#8211;2 June 2023</conf-date><fpage>4842</fpage><lpage>4849</lpage></element-citation></ref><ref id="B163-sensors-25-05264"><label>163.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>M.</given-names></name></person-group><article-title>Monoatt: Online monocular 3d object detection with adaptive token transformer</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>17493</fpage><lpage>17503</lpage></element-citation></ref><ref id="B164-sensors-25-05264"><label>164.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Min</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Schulter</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Dunn</surname><given-names>E.</given-names></name><name name-style="western"><surname>Chandraker</surname><given-names>M.</given-names></name></person-group><article-title>Neurocs: Neural nocs supervision for monocular 3d object localization</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>21404</fpage><lpage>21414</lpage></element-citation></ref><ref id="B165-sensors-25-05264"><label>165.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>C.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>W.</given-names></name></person-group><article-title>Alleviating foreground sparsity for semi-supervised monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2024</conf-date><fpage>7542</fpage><lpage>7552</lpage></element-citation></ref><ref id="B166-sensors-25-05264"><label>166.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jinrang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name></person-group><article-title>Monouni: A unified vehicle and infrastructure-side monocular 3d object detection network with sufficient depth clues</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2023</year><volume>36</volume><fpage>11703</fpage><lpage>11715</lpage></element-citation></ref><ref id="B167-sensors-25-05264"><label>167.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vu</surname><given-names>K.D.</given-names></name><name name-style="western"><surname>Tran</surname><given-names>T.T.</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>D.D.</given-names></name></person-group><article-title>MonoDSSMs: Efficient Monocular 3D Object Detection with Depth-Aware State Space Models</article-title><source>Proceedings of the Asian Conference on Computer Vision</source><conf-loc>Hanoi, Vietnam</conf-loc><conf-date>8&#8211;12 December 2024</conf-date><fpage>3883</fpage><lpage>3900</lpage></element-citation></ref><ref id="B168-sensors-25-05264"><label>168.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>S.</given-names></name></person-group><article-title>MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2405.07696</pub-id></element-citation></ref><ref id="B169-sensors-25-05264"><label>169.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ranasinghe</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hegde</surname><given-names>D.</given-names></name><name name-style="western"><surname>Patel</surname><given-names>V.M.</given-names></name></person-group><article-title>Monodiff: Monocular 3d object detection and pose estimation with diffusion models</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>10659</fpage><lpage>10670</lpage></element-citation></ref><ref id="B170-sensors-25-05264"><label>170.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Di</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>W.</given-names></name></person-group><article-title>MonoDFNet: Monocular 3D Object Detection with Depth Fusion and Adaptive Optimization</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>760</elocation-id><pub-id pub-id-type="doi">10.3390/s25030760</pub-id><pub-id pub-id-type="pmid">39943401</pub-id><pub-id pub-id-type="pmcid">PMC11819749</pub-id></element-citation></ref><ref id="B171-sensors-25-05264"><label>171.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name></person-group><article-title>Decoupled pseudo-labeling for semi-supervised monocular 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>16923</fpage><lpage>16932</lpage></element-citation></ref><ref id="B172-sensors-25-05264"><label>172.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>A.</given-names></name></person-group><article-title>Dp-M3D: Monocular 3D object detection algorithm with depth perception capability</article-title><source>Knowl.-Based Syst.</source><year>2025</year><volume>318</volume><fpage>113539</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2025.113539</pub-id></element-citation></ref><ref id="B173-sensors-25-05264"><label>173.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Moon</surname><given-names>S.</given-names></name><name name-style="western"><surname>Nah</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shim</surname><given-names>D.H.</given-names></name></person-group><article-title>MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2502.00315</pub-id></element-citation></ref><ref id="B174-sensors-25-05264"><label>174.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>R.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Phuong</surname><given-names>M.C.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>K.</given-names></name></person-group><article-title>Pseudo-LiDAR with Two-Dimensional Instance for Monocular Three-Dimensional Object Tracking</article-title><source>IEEE Access</source><year>2025</year><volume>13</volume><fpage>45771</fpage><lpage>45783</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2025.3549790</pub-id></element-citation></ref><ref id="B175-sensors-25-05264"><label>175.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kundu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Berneshawi</surname><given-names>A.G.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fidler</surname><given-names>S.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>3d object proposals for accurate object class detection</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2015</year><volume>28</volume><fpage>424</fpage><lpage>432</lpage></element-citation></ref><ref id="B176-sensors-25-05264"><label>176.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chao</surname><given-names>W.L.</given-names></name><name name-style="western"><surname>Garg</surname><given-names>D.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>M.</given-names></name><name name-style="western"><surname>Weinberger</surname><given-names>K.Q.</given-names></name></person-group><article-title>Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>8445</fpage><lpage>8453</lpage></element-citation></ref><ref id="B177-sensors-25-05264"><label>177.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name></person-group><article-title>Triangulation learning network: From monocular to stereo 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>7615</fpage><lpage>7623</lpage></element-citation></ref><ref id="B178-sensors-25-05264"><label>178.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chao</surname><given-names>W.L.</given-names></name><name name-style="western"><surname>Garg</surname><given-names>D.</given-names></name><name name-style="western"><surname>Pleiss</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>M.</given-names></name><name name-style="western"><surname>Weinberger</surname><given-names>K.Q.</given-names></name></person-group><article-title>Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1906.06310</pub-id></element-citation></ref><ref id="B179-sensors-25-05264"><label>179.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>K&#246;nigshof</surname><given-names>H.</given-names></name><name name-style="western"><surname>Salscheider</surname><given-names>N.O.</given-names></name><name name-style="western"><surname>Stiller</surname><given-names>C.</given-names></name></person-group><article-title>Realtime 3d object detection for automated driving using stereo vision and semantic information</article-title><source>Proceedings of the 2019 IEEE Intelligent Transportation Systems Conference (ITSC)</source><conf-loc>Auckland, New Zealand</conf-loc><conf-date>27&#8211;30 October 2019</conf-date><fpage>1405</fpage><lpage>1410</lpage></element-citation></ref><ref id="B180-sensors-25-05264"><label>180.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ku</surname><given-names>J.</given-names></name><name name-style="western"><surname>Waslander</surname><given-names>S.L.</given-names></name></person-group><article-title>Confidence guided stereo 3D object detection with split depth estimation</article-title><source>Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>25&#8211;29 October 2020</conf-date><fpage>5776</fpage><lpage>5783</lpage></element-citation></ref><ref id="B181-sensors-25-05264"><label>181.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>X.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>E.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>A.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name></person-group><article-title>Zoomnet: Part-aware adaptive zooming neural network for 3d object detection</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2020</year><volume>34</volume><fpage>12557</fpage><lpage>12564</lpage><pub-id pub-id-type="doi">10.1609/aaai.v34i07.6945</pub-id></element-citation></ref><ref id="B182-sensors-25-05264"><label>182.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>H.</given-names></name></person-group><article-title>Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>10548</fpage><lpage>10557</lpage></element-citation></ref><ref id="B183-sensors-25-05264"><label>183.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qian</surname><given-names>R.</given-names></name><name name-style="western"><surname>Garg</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>You</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>M.</given-names></name><name name-style="western"><surname>Weinberger</surname><given-names>K.Q.</given-names></name><name name-style="western"><surname>Chao</surname><given-names>W.L.</given-names></name></person-group><article-title>End-to-end pseudo-lidar for image-based 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>5881</fpage><lpage>5890</lpage></element-citation></ref><ref id="B184-sensors-25-05264"><label>184.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garg</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hariharan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>M.</given-names></name><name name-style="western"><surname>Weinberger</surname><given-names>K.Q.</given-names></name><name name-style="western"><surname>Chao</surname><given-names>W.L.</given-names></name></person-group><article-title>Wasserstein distances for stereo disparity estimation</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>22517</fpage><lpage>22529</lpage></element-citation></ref><ref id="B185-sensors-25-05264"><label>185.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Pon</surname><given-names>A.D.</given-names></name><name name-style="western"><surname>Ku</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Waslander</surname><given-names>S.L.</given-names></name></person-group><article-title>Object-centric stereo matching for 3d object detection</article-title><source>Proceedings of the 2020 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Paris, France</conf-loc><conf-date>31 May&#8211;31 August 2020</conf-date><fpage>8383</fpage><lpage>8389</lpage></element-citation></ref><ref id="B186-sensors-25-05264"><label>186.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Su</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name></person-group><article-title>Rts3d: Real-time stereo 3d detection from 4d feature-consistency embedding space for autonomous driving</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2021</year><volume>35</volume><fpage>1930</fpage><lpage>1939</lpage><pub-id pub-id-type="doi">10.1609/aaai.v35i3.16288</pub-id></element-citation></ref><ref id="B187-sensors-25-05264"><label>187.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>K&#246;nigshof</surname><given-names>H.</given-names></name><name name-style="western"><surname>Stiller</surname><given-names>C.</given-names></name></person-group><article-title>Learning-based shape estimation with grid map patches for realtime 3D object detection for automated driving</article-title><source>Proceedings of the 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Rhodes, Greece</conf-loc><conf-date>20&#8211;23 September 2020</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B188-sensors-25-05264"><label>188.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y.</given-names></name></person-group><article-title>Side: Center-based stereo 3d detector with structure-aware instance depth estimation</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2022</conf-date><fpage>119</fpage><lpage>128</lpage></element-citation></ref><ref id="B189-sensors-25-05264"><label>189.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Stereo CenterNet-based 3D object detection for autonomous driving</article-title><source>Neurocomputing</source><year>2022</year><volume>471</volume><fpage>219</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2021.11.048</pub-id></element-citation></ref><ref id="B190-sensors-25-05264"><label>190.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>ESGN: Efficient stereo geometry network for fast 3D object detection</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2022</year><volume>34</volume><fpage>2000</fpage><lpage>2009</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2022.3202810</pub-id></element-citation></ref><ref id="B191-sensors-25-05264"><label>191.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.N.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>Y.</given-names></name></person-group><article-title>Pseudo-stereo for monocular 3d object detection in autonomous driving</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>887</fpage><lpage>897</lpage></element-citation></ref><ref id="B192-sensors-25-05264"><label>192.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Dsgn++: Exploiting visual-spatial relation for stereo-based 3d detectors</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>45</volume><fpage>4416</fpage><lpage>4429</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3197236</pub-id><pub-id pub-id-type="pmid">35939470</pub-id></element-citation></ref><ref id="B193-sensors-25-05264"><label>193.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>D.</given-names></name></person-group><article-title>Did-m3d: Decoupling instance depth for monocular 3d object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>71</fpage><lpage>88</lpage></element-citation></ref><ref id="B194-sensors-25-05264"><label>194.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>W.</given-names></name></person-group><article-title>Disparity-based multiscale fusion network for transportation detection</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>18855</fpage><lpage>18863</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3161977</pub-id></element-citation></ref><ref id="B195-sensors-25-05264"><label>195.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name></person-group><article-title>Svdm: Single-view diffusion model for pseudo-stereo 3d object detection</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2307.02270</pub-id></element-citation></ref><ref id="B196-sensors-25-05264"><label>196.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Choi</surname><given-names>H.M.</given-names></name><name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Hyun</surname><given-names>Y.</given-names></name></person-group><article-title>Multi-view reprojection architecture for orientation estimation</article-title><source>Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27&#8211;28 October 2019</conf-date><fpage>2357</fpage><lpage>2366</lpage></element-citation></ref><ref id="B197-sensors-25-05264"><label>197.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Du</surname><given-names>D.</given-names></name></person-group><article-title>Bevdet: High-performance multi-camera 3d object detection in bird-eye-view</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2112.11790</pub-id></element-citation></ref><ref id="B198-sensors-25-05264"><label>198.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Bevdepth: Acquisition of reliable depth for multi-view 3d object detection</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>1477</fpage><lpage>1485</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i2.25233</pub-id></element-citation></ref><ref id="B199-sensors-25-05264"><label>199.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rukhovich</surname><given-names>D.</given-names></name><name name-style="western"><surname>Vorontsova</surname><given-names>A.</given-names></name><name name-style="western"><surname>Konushin</surname><given-names>A.</given-names></name></person-group><article-title>Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2022</conf-date><fpage>2397</fpage><lpage>2406</lpage></element-citation></ref><ref id="B200-sensors-25-05264"><label>200.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Min</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ge</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>D.</given-names></name></person-group><article-title>Sts: Surround-view temporal stereo for multi-view 3d detection</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2208.10145</pub-id></element-citation></ref><ref id="B201-sensors-25-05264"><label>201.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>W.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>J.</given-names></name></person-group><article-title>Beverse: Unified perception and prediction in birds-eye-view for vision-centric autonomous driving</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2205.09743</pub-id></element-citation></ref><ref id="B202-sensors-25-05264"><label>202.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Miao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.G.</given-names></name></person-group><article-title>Polarformer: Multi-camera 3d object detection with polar transformer</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>1042</fpage><lpage>1050</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i1.25185</pub-id></element-citation></ref><ref id="B203-sensors-25-05264"><label>203.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Focal-petr: Embracing foreground for efficient multi-camera 3d object detection</article-title><source>IEEE Trans. Intell. Veh.</source><year>2023</year><volume>9</volume><fpage>1481</fpage><lpage>1489</lpage><pub-id pub-id-type="doi">10.1109/TIV.2023.3332608</pub-id></element-citation></ref><ref id="B204-sensors-25-05264"><label>204.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>F.</given-names></name></person-group><article-title>Bevdistill: Cross-modal bev distillation for multi-view 3d object detection</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2211.09386</pub-id></element-citation></ref><ref id="B205-sensors-25-05264"><label>205.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Keutzer</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kitani</surname><given-names>K.</given-names></name><name name-style="western"><surname>Tomizuka</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhan</surname><given-names>W.</given-names></name></person-group><article-title>Time will tell: New outlooks and a baseline for temporal multi-view 3d object detection</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arXiv.2210.02443</pub-id><pub-id pub-id-type="arxiv">2210.02443</pub-id></element-citation></ref><ref id="B206-sensors-25-05264"><label>206.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Exploring object-centric temporal modeling for efficient multi-view 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>3621</fpage><lpage>3631</lpage></element-citation></ref><ref id="B207-sensors-25-05264"><label>207.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>X.</given-names></name></person-group><article-title>PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object Detection in Bird&#8217;s-Eye-View</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2408.16200</pub-id></element-citation></ref><ref id="B208-sensors-25-05264"><label>208.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wirges</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fischer</surname><given-names>T.</given-names></name><name name-style="western"><surname>Stiller</surname><given-names>C.</given-names></name><name name-style="western"><surname>Frias</surname><given-names>J.B.</given-names></name></person-group><article-title>Object detection and classification in occupancy grid maps using deep convolutional networks</article-title><source>Proceedings of the 2018 21st International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Maui, HI, USA</conf-loc><conf-date>4&#8211;7 November 2018</conf-date><fpage>3530</fpage><lpage>3535</lpage></element-citation></ref><ref id="B209-sensors-25-05264"><label>209.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>L.</given-names></name></person-group><article-title>FVNet: 3D front-view proposal generation for real-time object detection from point clouds</article-title><source>Proceedings of the 2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)</source><conf-loc>Huaqiao, China</conf-loc><conf-date>19&#8211;21 October 2019</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B210-sensors-25-05264"><label>210.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>Accurate and Real-Time Object Detection Based on Bird&#8217;s Eye View on 3D Point Clouds</article-title><source>Proceedings of the 2019 International Conference on 3D Vision (3DV)</source><conf-loc>Quebec City, QC, Canada</conf-loc><conf-date>16&#8211;19 September 2019</conf-date><fpage>214</fpage><lpage>221</lpage></element-citation></ref><ref id="B211-sensors-25-05264"><label>211.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>M.</given-names></name><name name-style="western"><surname>Urtasun</surname><given-names>R.</given-names></name></person-group><article-title>Hdnet: Exploiting hd maps for 3d object detection</article-title><source>Proceedings of the Conference on Robot Learning</source><conf-loc>Z&#252;rich, Switzerland</conf-loc><conf-date>29&#8211;31 October 2018</conf-date><fpage>146</fpage><lpage>155</lpage></element-citation></ref><ref id="B212-sensors-25-05264"><label>212.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barrera</surname><given-names>A.</given-names></name><name name-style="western"><surname>Beltran</surname><given-names>J.</given-names></name><name name-style="western"><surname>Guindel</surname><given-names>C.</given-names></name><name name-style="western"><surname>Iglesias</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Garcia</surname><given-names>F.</given-names></name></person-group><article-title>Birdnet+: Two-stage 3d object detection in lidar through a sparsity-invariant bird&#8217;s eye view</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>160299</fpage><lpage>160316</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3131389</pub-id></element-citation></ref><ref id="B213-sensors-25-05264"><label>213.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koh</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>J.W.</given-names></name></person-group><article-title>Mgtanet: Encoding sequential lidar points using long short-term motion-guided temporal attention for 3d object detection</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>1179</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.1609/aaai.v37i1.25200</pub-id></element-citation></ref><ref id="B214-sensors-25-05264"><label>214.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Bingbing</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>W.</given-names></name></person-group><article-title>Gpa-3d: Geometry-aware prototype alignment for unsupervised domain adaptive 3d object detection from point clouds</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;6 October 2023</conf-date><fpage>6394</fpage><lpage>6403</lpage></element-citation></ref><ref id="B215-sensors-25-05264"><label>215.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>D.Z.</given-names></name><name name-style="western"><surname>Posner</surname><given-names>I.</given-names></name></person-group><article-title>Voting for voting in online point cloud object detection</article-title><source>Proceedings of the Robotics: Science and Systems</source><conf-loc>Rome, Italy</conf-loc><conf-date>13&#8211;17 July 2015</conf-date><volume>Volume 1</volume><fpage>10</fpage><lpage>15</lpage></element-citation></ref><ref id="B216-sensors-25-05264"><label>216.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Voxel set transformer: A set-to-set approach to 3d object detection from point clouds</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>8417</fpage><lpage>8427</lpage></element-citation></ref><ref id="B217-sensors-25-05264"><label>217.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Focal sparse convolutional networks for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>5428</fpage><lpage>5437</lpage></element-citation></ref><ref id="B218-sensors-25-05264"><label>218.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>C.</given-names></name></person-group><article-title>Pillarnet: Real-time and high-performance pillar-based 3d object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>35</fpage><lpage>52</lpage></element-citation></ref><ref id="B219-sensors-25-05264"><label>219.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>P.</given-names></name><name name-style="western"><surname>Tan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>F.</given-names></name><name name-style="western"><surname>Leng</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name></person-group><article-title>Swformer: Sparse window transformer for 3d object detection in point clouds</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>426</fpage><lpage>442</lpage></element-citation></ref><ref id="B220-sensors-25-05264"><label>220.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name></person-group><article-title>PV-GNN: Point-Voxel 3D Object Detection based on Graph Neural Network</article-title><source>Res. Sq.</source><year>2024</year><pub-id pub-id-type="doi">10.21203/rs.3.rs-4598182/v1</pub-id></element-citation></ref><ref id="B221-sensors-25-05264"><label>221.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Ipod: Intensive point-based object detector for point cloud</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1812.05276</pub-id><pub-id pub-id-type="arxiv">1812.05276</pub-id></element-citation></ref><ref id="B222-sensors-25-05264"><label>222.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Std: Sparse-to-dense 3d object detector for point cloud</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>1951</fpage><lpage>1960</lpage></element-citation></ref><ref id="B223-sensors-25-05264"><label>223.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zarzar</surname><given-names>J.</given-names></name><name name-style="western"><surname>Giancola</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ghanem</surname><given-names>B.</given-names></name></person-group><article-title>PointRGCN: Graph convolution networks for 3D vehicles detection refinement</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1911.12236</pub-id><pub-id pub-id-type="arxiv">1911.12236</pub-id></element-citation></ref><ref id="B224-sensors-25-05264"><label>224.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>3dssd: Point-based 3d single stage object detector</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>11040</fpage><lpage>11048</lpage></element-citation></ref><ref id="B225-sensors-25-05264"><label>225.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>W.</given-names></name><name name-style="western"><surname>Rajkumar</surname><given-names>R.</given-names></name></person-group><article-title>Point-gnn: Graph neural network for 3d object detection in a point cloud</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>1711</fpage><lpage>1719</lpage></element-citation></ref><ref id="B226-sensors-25-05264"><label>226.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name></person-group><article-title>EPNet++: Cascade bi-directional fusion for multi-modal 3D object detection</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>45</volume><fpage>8324</fpage><lpage>8341</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3228806</pub-id><pub-id pub-id-type="pmid">37015370</pub-id></element-citation></ref><ref id="B227-sensors-25-05264"><label>227.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tao</surname><given-names>D.</given-names></name></person-group><article-title>Sasa: Semantics-augmented set abstraction for point-based 3d object detection</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2022</year><volume>36</volume><fpage>221</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1609/aaai.v36i1.19897</pub-id></element-citation></ref><ref id="B228-sensors-25-05264"><label>228.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>B.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>W.</given-names></name></person-group><article-title>DFAF3D: A dual-feature-aware anchor-free single-stage 3D detector for point clouds</article-title><source>Image Vis. Comput.</source><year>2023</year><volume>129</volume><fpage>104594</fpage><pub-id pub-id-type="doi">10.1016/j.imavis.2022.104594</pub-id></element-citation></ref><ref id="B229-sensors-25-05264"><label>229.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xia</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style="western"><surname>Xing</surname><given-names>L.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wen</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>Hinted: Hard instance enhanced detector with mixed-density feature fusion for sparsely-supervised 3D object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>15321</fpage><lpage>15330</lpage></element-citation></ref><ref id="B230-sensors-25-05264"><label>230.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Han</surname><given-names>S.</given-names></name></person-group><article-title>Point-voxel cnn for efficient 3d deep learning</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2019</year><volume>32</volume><fpage>965</fpage><lpage>975</lpage></element-citation></ref><ref id="B231-sensors-25-05264"><label>231.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hua</surname><given-names>X.S.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Structure aware single-stage 3d object detection from point cloud</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>11873</fpage><lpage>11882</lpage></element-citation></ref><ref id="B232-sensors-25-05264"><label>232.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qian</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>BADet: Boundary-aware 3D object detection from point clouds</article-title><source>Pattern Recognit.</source><year>2022</year><volume>125</volume><fpage>108524</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2022.108524</pub-id></element-citation></ref><ref id="B233-sensors-25-05264"><label>233.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Niu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name></person-group><article-title>Pyramid r-cnn: Towards better performance and adaptability for 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>2723</fpage><lpage>2732</lpage></element-citation></ref><ref id="B234-sensors-25-05264"><label>234.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xia</surname><given-names>G.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Su</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name></person-group><article-title>DVFENet: Dual-branch voxel feature extraction network for 3D object detection</article-title><source>Neurocomputing</source><year>2021</year><volume>459</volume><fpage>201</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2021.06.046</pub-id></element-citation></ref><ref id="B235-sensors-25-05264"><label>235.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>A unified query-based paradigm for point cloud understanding</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>8541</fpage><lpage>8551</lpage></element-citation></ref><ref id="B236-sensors-25-05264"><label>236.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>B.</given-names></name><name name-style="western"><surname>He</surname><given-names>T.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name></person-group><article-title>Pvt-ssd: Single-stage 3d object detector with point-voxel transformer</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>13476</fpage><lpage>13487</lpage></element-citation></ref><ref id="B237-sensors-25-05264"><label>237.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Koo</surname><given-names>I.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>I.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>H.S.</given-names></name><name name-style="western"><surname>Jeon</surname><given-names>W.J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>C.</given-names></name></person-group><article-title>Pg-rcnn: Semantic surface point generation for 3d object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>18142</fpage><lpage>18151</lpage></element-citation></ref><ref id="B238-sensors-25-05264"><label>238.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><article-title>Uni3detr: Unified 3d detection transformer</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2023</year><volume>36</volume><fpage>39876</fpage><lpage>39896</lpage></element-citation></ref><ref id="B239-sensors-25-05264"><label>239.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Song</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><etal/></person-group><article-title>Dual radar: A multi-modal dataset with dual 4d radar for autononous driving</article-title><source>Sci. Data</source><year>2025</year><volume>12</volume><fpage>439</fpage><pub-id pub-id-type="doi">10.1038/s41597-025-04698-2</pub-id><pub-id pub-id-type="pmid">40082463</pub-id><pub-id pub-id-type="pmcid">PMC11907064</pub-id></element-citation></ref><ref id="B240-sensors-25-05264"><label>240.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>J.H.</given-names></name><name name-style="western"><surname>Kuan</surname><given-names>S.Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>H.I.</given-names></name><name name-style="western"><surname>Latapie</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Hwang</surname><given-names>J.N.</given-names></name></person-group><article-title>Centerradarnet: Joint 3d object detection and tracking framework using 4d fmcw radar</article-title><source>Proceedings of the 2024 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Abu Dhabi, United Arab Emirates</conf-loc><conf-date>27&#8211;30 October 2024</conf-date><fpage>998</fpage><lpage>1004</lpage></element-citation></ref><ref id="B241-sensors-25-05264"><label>241.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Shin</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kwon</surname><given-names>Y.P.</given-names></name><name name-style="western"><surname>Tomizuka</surname><given-names>M.</given-names></name></person-group><article-title>Roarnet: A robust 3d object detection based on region approximation refinement</article-title><source>Proceedings of the 2019 IEEE Intelligent Vehicles Symposium (IV)</source><conf-loc>Paris, France</conf-loc><conf-date>9&#8211;12 June 2019</conf-date><fpage>2510</fpage><lpage>2515</lpage></element-citation></ref><ref id="B242-sensors-25-05264"><label>242.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Simon</surname><given-names>M.</given-names></name><name name-style="western"><surname>Amende</surname><given-names>K.</given-names></name><name name-style="western"><surname>Kraus</surname><given-names>A.</given-names></name><name name-style="western"><surname>Honer</surname><given-names>J.</given-names></name><name name-style="western"><surname>Samann</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kaulbersch</surname><given-names>H.</given-names></name><name name-style="western"><surname>Milz</surname><given-names>S.</given-names></name><name name-style="western"><surname>Michael Gross</surname><given-names>H.</given-names></name></person-group><article-title>Complexer-yolo: Real-time 3d object detection and tracking on semantic point clouds</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>16&#8211;20 June 2019</conf-date></element-citation></ref><ref id="B243-sensors-25-05264"><label>243.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Fang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Fusionpainting: Multimodal fusion with adaptive attention for 3d object detection</article-title><source>Proceedings of the 2021 IEEE International Intelligent Transportation Systems Conference (ITSC)</source><conf-loc>Indianapolis, IN, USA</conf-loc><conf-date>19&#8211;22 September 2021</conf-date><fpage>3047</fpage><lpage>3054</lpage></element-citation></ref><ref id="B244-sensors-25-05264"><label>244.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kr&#228;henb&#252;hl</surname><given-names>P.</given-names></name></person-group><article-title>Multimodal virtual point 3d detection</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>16494</fpage><lpage>16507</lpage></element-citation></ref><ref id="B245-sensors-25-05264"><label>245.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>Pointaugmenting: Cross-modal augmentation for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>11794</fpage><lpage>11803</lpage></element-citation></ref><ref id="B246-sensors-25-05264"><label>246.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>Height-Adaptive Deformable Multi-Modal Fusion for 3D Object Detection</article-title><source>IEEE Access</source><year>2025</year><volume>13</volume><fpage>52385</fpage><lpage>52396</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2025.3553372</pub-id></element-citation></ref><ref id="B247-sensors-25-05264"><label>247.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>D.</given-names></name><name name-style="western"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style="western"><surname>Jain</surname><given-names>A.</given-names></name></person-group><article-title>Pointfusion: Deep sensor fusion for 3d bounding box estimation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>244</fpage><lpage>253</lpage></element-citation></ref><ref id="B248-sensors-25-05264"><label>248.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sindagi</surname><given-names>V.A.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Tuzel</surname><given-names>O.</given-names></name></person-group><article-title>Mvx-net: Multimodal voxelnet for 3d object detection</article-title><source>Proceedings of the 2019 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>20&#8211;24 May 2019</conf-date><fpage>7276</fpage><lpage>7282</lpage></element-citation></ref><ref id="B249-sensors-25-05264"><label>249.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xiang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>D.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name></person-group><article-title>PI-RCNN: An efficient multi-sensor 3D object detector with point-based attentive cont-conv fusion module</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2020</year><volume>34</volume><fpage>12460</fpage><lpage>12467</lpage><pub-id pub-id-type="doi">10.1609/aaai.v34i07.6933</pub-id></element-citation></ref><ref id="B250-sensors-25-05264"><label>250.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>H.</given-names></name></person-group><article-title>MCF3D: Multi-stage complementary fusion for multi-sensor 3D object detection</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>90801</fpage><lpage>90814</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2927012</pub-id></element-citation></ref><ref id="B251-sensors-25-05264"><label>251.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yoo</surname><given-names>J.H.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name><name name-style="western"><surname>Choi</surname><given-names>J.W.</given-names></name></person-group><article-title>3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection</article-title><source>Proceedings of the Computer Vision&#8212;ECCV 2020: 16th European Conference</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#8211;28 August 2020</conf-date><comment>Proceedings, Part XXVII 16</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>720</fpage><lpage>736</lpage></element-citation></ref><ref id="B252-sensors-25-05264"><label>252.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Amini</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Rus</surname><given-names>D.L.</given-names></name><name name-style="western"><surname>Han</surname><given-names>S.</given-names></name></person-group><article-title>Bevfusion: Multi-task multi-sensor fusion with unified bird&#8217;s-eye view representation</article-title><source>Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>London, UK</conf-loc><conf-date>29 May&#8211;2 June 2023</conf-date><fpage>2774</fpage><lpage>2781</lpage></element-citation></ref><ref id="B253-sensors-25-05264"><label>253.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>A.W.</given-names></name><name name-style="western"><surname>Meng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Caine</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ngiam</surname><given-names>J.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.V.</given-names></name><etal/></person-group><article-title>Deepfusion: Lidar-camera deep fusion for multi-modal 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>17182</fpage><lpage>17191</lpage></element-citation></ref><ref id="B254-sensors-25-05264"><label>254.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Jiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jie</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y.G.</given-names></name></person-group><article-title>Msmdfusion: Fusing lidar and camera at multiple scales with multi-depth seeds for 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>21643</fpage><lpage>21652</lpage></element-citation></ref><ref id="B255-sensors-25-05264"><label>255.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>D.</given-names></name></person-group><article-title>Cat-det: Contrastively augmented transformer for multi-modal 3d object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>908</fpage><lpage>917</lpage></element-citation></ref><ref id="B256-sensors-25-05264"><label>256.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>B.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>T.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>He</surname><given-names>L.</given-names></name></person-group><article-title>Homogeneous multi-modal feature fusion and interaction for 3D object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>691</fpage><lpage>707</lpage></element-citation></ref><ref id="B257-sensors-25-05264"><label>257.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>T.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>Logonet: Towards accurate 3d object detection with local-to-global cross-modal fusion</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>17524</fpage><lpage>17534</lpage></element-citation></ref><ref id="B258-sensors-25-05264"><label>258.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>B.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name></person-group><article-title>SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.08304</pub-id></element-citation></ref><ref id="B259-sensors-25-05264"><label>259.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Kang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>N.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>R.</given-names></name></person-group><article-title>SupFusion: Supervised LiDAR-camera fusion for 3D object detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>22014</fpage><lpage>22024</lpage></element-citation></ref><ref id="B260-sensors-25-05264"><label>260.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>J.</given-names></name></person-group><article-title>Fgfusion: Fine-grained lidar-camera fusion for 3d object detection</article-title><source>Proceedings of the Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</source><conf-loc>Xiamen, China</conf-loc><conf-date>13&#8211;15 October 2023</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2023</year><fpage>505</fpage><lpage>517</lpage></element-citation></ref><ref id="B261-sensors-25-05264"><label>261.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sima</surname><given-names>C.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Qiao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Leveraging vision-centric multi-modal expertise for 3d object detection</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2023</year><volume>36</volume><fpage>38504</fpage><lpage>38519</lpage></element-citation></ref><ref id="B262-sensors-25-05264"><label>262.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Schiele</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Unitr: A unified and efficient multi-modal transformer for bird&#8217;s-eye-view representation</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#8211;3 October 2023</conf-date><fpage>6792</fpage><lpage>6802</lpage></element-citation></ref><ref id="B263-sensors-25-05264"><label>263.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Viadero-Monasterio</surname><given-names>F.</given-names></name><name name-style="western"><surname>Alonso-Renter&#237;a</surname><given-names>L.</given-names></name><name name-style="western"><surname>P&#233;rez-Oria</surname><given-names>J.</given-names></name><name name-style="western"><surname>Viadero-Rueda</surname><given-names>F.</given-names></name></person-group><article-title>Radar-based pedestrian and vehicle detection and identification for driving assistance</article-title><source>Vehicles</source><year>2024</year><volume>6</volume><fpage>1185</fpage><lpage>1199</lpage><pub-id pub-id-type="doi">10.3390/vehicles6030056</pub-id></element-citation></ref></ref-list></back><floats-group><table-wrap position="float" id="sensors-25-05264-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05264-t001_Table 1</object-id><label>Table 1</label><caption><p>Exteroceptive sensors performance comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensor</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Range</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cost</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Comput. <break/>Cost</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Depth</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Colour</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Affected by <break/>Illumination</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Affected by <break/>Weather</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Monocular Camera</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">Small</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Stereo Camera</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Infrared Camera</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Small</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sonar/Ultrasonic</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Small</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Radar</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LiDAR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Medium</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Large</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Yes</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05264-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05264-t002_Table 2</object-id><label>Table 2</label><caption><p>Common datasets for 3D object detection in autonomous vehicles.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># <break/>Cameras</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># <break/>LiDARs</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># <break/>Scenes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"># <break/>Classes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Locations</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Night</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Rain</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Annotated <break/>3D BBoxes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Annotated <break/>Frames</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">KITTI [<xref rid="B28-sensors-25-05264" ref-type="bibr">28</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2012</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">22</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">Germany</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">80k</td><td align="center" valign="middle" rowspan="1" colspan="1">15k</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ApolloScape [<xref rid="B38-sensors-25-05264" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">73</td><td align="center" valign="middle" rowspan="1" colspan="1">27</td><td align="center" valign="middle" rowspan="1" colspan="1">China</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">70k</td><td align="center" valign="middle" rowspan="1" colspan="1">80k</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">nuScenes [<xref rid="B36-sensors-25-05264" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" rowspan="1" colspan="1">23</td><td align="center" valign="middle" rowspan="1" colspan="1">USA/Singapore</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">1.4M</td><td align="center" valign="middle" rowspan="1" colspan="1">40k</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ArgoVerse [<xref rid="B39-sensors-25-05264" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">113</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">USA</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">993K</td><td align="center" valign="middle" rowspan="1" colspan="1">22k</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Waymo Open [<xref rid="B37-sensors-25-05264" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">1150</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">USA</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">Yes</td><td align="center" valign="middle" rowspan="1" colspan="1">12M</td><td align="center" valign="middle" rowspan="1" colspan="1">230k</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lyft Level 5 [<xref rid="B40-sensors-25-05264" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">366</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">USA</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">No</td><td align="center" valign="middle" rowspan="1" colspan="1">1.3M</td><td align="center" valign="middle" rowspan="1" colspan="1">46k</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H3D [<xref rid="B41-sensors-25-05264" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">160</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">USA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.1M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27k</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>