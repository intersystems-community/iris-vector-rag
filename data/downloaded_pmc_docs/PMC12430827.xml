<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Int J Surg</journal-id><journal-id journal-id-type="iso-abbrev">Int J Surg</journal-id><journal-id journal-id-type="pmc-domain-id">2035</journal-id><journal-id journal-id-type="pmc-domain">lwwopen</journal-id><journal-id journal-id-type="publisher-id">JS9</journal-id><journal-title-group><journal-title>International Journal of Surgery (London, England)</journal-title></journal-title-group><issn pub-type="ppub">1743-9191</issn><issn pub-type="epub">1743-9159</issn><custom-meta-group><custom-meta><meta-name>pmc-is-collection-domain</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-collection-title</meta-name><meta-value>Lippincott Open Access</meta-value></custom-meta></custom-meta-group></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430827</article-id><article-id pub-id-type="pmcid-ver">PMC12430827.1</article-id><article-id pub-id-type="pmcaid">12430827</article-id><article-id pub-id-type="pmcaiid">12430827</article-id><article-id pub-id-type="pmid">40503769</article-id><article-id pub-id-type="doi">10.1097/JS9.0000000000002699</article-id><article-id pub-id-type="publisher-id">IJS-D-25-00832</article-id><article-version-alternatives><article-version article-version-type="pmc-version">1</article-version><article-version vocab="JAV" vocab-identifier="http://www.niso.org/publications/rp/RP-8-2008.pdf" article-version-type="Version of Record">3</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Cross-Sectional Study</subject></subj-group></article-categories><title-group><article-title>Real-time automatic detection of gynecological laparoscopic surgical instruments and exploration in surgical skills assessment application: a cross-sectional study</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Wei</surname><given-names initials="H">Huanyu</given-names></name><degrees>MD,</degrees><email>602706121@qq.com</email><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref><xref rid="aff2" ref-type="aff">
<sup>b</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Deng</surname><given-names initials="L">Li</given-names></name><degrees>MD, PhD</degrees><email>denglitmmu@163.com</email><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="X">Xueju</given-names></name><degrees>MD</degrees><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Tan</surname><given-names initials="W">Wenwei</given-names></name><degrees>MD</degrees><email>421474016@qq.com</email><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="Y">Yi</given-names></name><degrees>PhD</degrees><xref rid="aff3" ref-type="aff">
<sup>c</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yi</surname><given-names initials="B">Bin</given-names></name><degrees>MD, PhD</degrees><xref rid="aff4" ref-type="aff">
<sup>d</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="Y">Yudi</given-names></name><degrees>MD</degrees><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="R">Ruiwei</given-names></name><degrees>MD</degrees><email>526361176@qq.com</email><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liang</surname><given-names initials="X">Xiaolong</given-names></name><degrees>MD</degrees><email>283785027@qq.com</email><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Chen</surname><given-names initials="Y">Yin</given-names></name><degrees>MD</degrees><email>cheny-2007@163.com</email><xref rid="aff5" ref-type="aff">
<sup>e</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="H">Hui</given-names></name><degrees>PhD</degrees><email>1159954346@qq.com</email><xref rid="aff6" ref-type="aff">
<sup>f</sup>
</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Tang</surname><given-names initials="S">Shuai</given-names></name><degrees>MD, PhD</degrees><email>tstmmu@126.com</email><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref><xref rid="COR0001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5985-5244</contrib-id><name name-style="western"><surname>Wang</surname><given-names initials="Y">Yanzhou</given-names></name><degrees>MD, PhD</degrees><email>w.y.z@foxmail.com</email><xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref><xref rid="COR0001" ref-type="corresp">*</xref></contrib><aff id="aff1"><label>a</label>Department of Obstetrics and Gynecology, The First Affiliated Hospital of Army Medical University (Third Military Medical University), Chongqing, China</aff><aff id="aff2"><label>b</label>Department of Obstetrics and Gynecology, No. 960 Hospital of the Joint Service Support Force of the Chinese People&#8217;s Liberation Army, Jinan, China</aff><aff id="aff3"><label>c</label>Department of Digital Medicine, College of Biomedical Engineering and Medical Imaging, Army Medical University (Third Military Medical University), Chongqing, China</aff><aff id="aff4"><label>d</label>Department of Anesthesiology, The First Affiliated Hospital of Army Medical University (Third Military Medical University), Chongqing, China</aff><aff id="aff5"><label>e</label>Department of Obstetrics and Gynecology, The 958th Army Hospital of the Chinese People&#8217;s Liberation Army (958th Hospital), Chongqing, China</aff><aff id="aff6"><label>f</label>Hunan Provincial Engineering Research Center for Gynecological Intelligent Diagnostic and Treatment Equipment Products, Hunan, China</aff></contrib-group><author-notes><corresp id="COR0001"><label>*</label>Corresponding authors. Address: Department of Obstetrics and Gynecology, The First Affiliated Hospital (Southwest Hospital), Army Medical University, Chongqing 400038, China. Tel.: +86 18680893816. E-mail: <email>w.y.z@foxmail.com</email> (Y. Wang); Tel.: +86 15826104432. E-mail: <email>tstmmu@126.com</email> (S. Tang).</corresp></author-notes><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><pub-date pub-type="epub"><day>12</day><month>6</month><year>2025</year></pub-date><volume>111</volume><issue>9</issue><issue-id pub-id-type="pmc-issue-id">496813</issue-id><fpage>5882</fpage><lpage>5892</lpage><history><date date-type="received"><day>11</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>31</day><month>5</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>12</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>Copyright &#169; 2025 The Author(s). Published by Wolters Kluwer Health, Inc.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License 4.0</ext-link> (CCBY), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="js9-111-5882.pdf"/><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="js9-111-5882.pdf"/><abstract><sec><title>Background:</title><p>Automatic detection of surgical instruments is essential for artificial intelligence surgery. This study aimed to construct a large-scale dataset of gynecological laparoscopic surgical instruments based on real surgical scenarios, achieve high-precision real-time detection of surgical instruments, and explore their potential application in surgical skill evaluation.</p></sec><sec><title>Materials and methods:</title><p>This cross-sectional study collected 265 gynecological laparoscopic surgical videos from two medical centers for instrument detection. Videos were divided into training and testing sets in a 4:1 ratio, with 161&#160;348 instrument instances extracted. The instruments were detected using Real-Time Models for Object Detection (RTMDet). The mean average precision, sensitivity, and F1 score served as evaluation metrics. External validation was conducted on an independent dataset from a third medical center. Additionally, we further compared the RTMDet with the state-of-the-art PP-YOLOE model on the same dataset. Furthermore, this study performed real-time tracking of instruments during the vaginal cuff suturing step of laparoscopic hysterectomy and compared the differences in kinematic data between proficient and non-proficient videos.</p></sec><sec><title>Results:</title><p>The mean average precision, sensitivity, and F1 score for nine types of surgical instruments were 91.75%, 94.29%, and 93.00%, respectively. External validation on the independent dataset demonstrated robust performance. In the comparison with PP-YOLOE, RTMDet demonstrated superior performance in all metrics. In the comparative analysis of kinematic data, the proficient group demonstrated significantly lesser path lengths and inter-quartile range, shorter moving times, and higher movement velocities for instruments used by both hands compared to the non-proficient group.</p></sec><sec><title>Conclusions:</title><p>This study established a large-scale, real scenario-based database of gynecological laparoscopic instruments. Using the RTMDet model, high-precision real-time detection and tracking of multiple instruments were achieved. Furthermore, this study identified several instrument kinematic metrics that can be used for surgical skill assessment, providing a reference for the objective quantification of the subjective Global Operative Assessment of Laparoscopic Skills.</p></sec></abstract><kwd-group><kwd>artificial intelligence surgery</kwd><kwd>cross-sectional study</kwd><kwd>deep learning</kwd><kwd>gynecological laparoscopic surgery</kwd><kwd>surgical instruments detection</kwd><kwd>surgical skill assessment</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>OPEN-ACCESS</meta-name><meta-value>TRUE</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro"><title>Introduction</title><p>Artificial intelligence (AI) has recently witnessed significant advancements, notably penetrating the field of medicine with remarkable achievements in imaging and pathological diagnosis<sup>[<xref rid="R1" ref-type="bibr">1</xref>,<xref rid="R2" ref-type="bibr">2</xref>]</sup>. However, its progress in the surgical field has been slower. Drawing parallels with autonomous driving technology, the ultimate goal for AI in surgery is fully automated surgical procedures<sup>[<xref rid="R3" ref-type="bibr">3</xref>]</sup>, wherein robotic systems can independently perform operations. However, this ambitious goal will need a step-by-step approach<sup>[<xref rid="R4" ref-type="bibr">4</xref>]</sup>.</p><p>In a 2022 white paper discussing the definitions of AI and autonomous actions in clinical surgery, six levels of advancement for automated surgical robots were defined: no autonomy, robot assistance, task autonomy, conditional autonomy, high autonomy, and full autonomy<sup>[<xref rid="R3" ref-type="bibr">3</xref>]</sup>. Currently, the evolution of surgical autonomy primarily resides at the level of robotic assistance<sup>[<xref rid="R5" ref-type="bibr">5</xref>]</sup>, where robots provide mechanical assistance with negligible autonomy. A key step toward greater autonomy is environmental perception<sup>[<xref rid="R6" ref-type="bibr">6</xref>,<xref rid="R7" ref-type="bibr">7</xref>]</sup>, including automated detection of surgical instruments and anatomical structures. Research into surgical tool detection could lay a solid foundation for recognizing more complex organs and tissues, thereby providing an ideal entry point for exploring the application of AI in surgery.</p><p>In the field of surgical instrument detection, computer vision-based methods have gained increasing attention due to their noninvasive nature and ability to achieve high recognition accuracy without instrument modification. The emergence of deep learning, along with representative models like Convolutional Neural Networks (CNNs), the YOLO series, and transformer-based architectures, has significantly advanced the accuracy of object recognition. Despite the advancements in computer vision-based methods, their practical application in surgical environments is still constrained by several critical limitations, particularly in dataset construction and algorithm design.</p><p>High-precision automated detection of surgical instruments primarily relies on high-quality data annotation and well-designed model architecture.<sup>[<xref rid="R8" ref-type="bibr">8</xref>-<xref rid="R10" ref-type="bibr">10</xref>]</sup> Despite studies reporting satisfactory detection accuracy,<sup>[<xref rid="R11" ref-type="bibr">11</xref>-<xref rid="R13" ref-type="bibr">13</xref>]</sup> several challenges remain in these two areas. First, regarding dataset construction, most studies depend on public datasets where images are composed of pre-selected images, often excluding blurred images, and typically sourced from a single medical institution. This can lead to overfitting, where a model performs exceptionally well on a specific dataset but poorly in real surgical environments<sup>[<xref rid="R14" ref-type="bibr">14</xref>]</sup>, limiting its practical application. Moreover, some instruments frequently used in gynecological laparoscopic surgery, such as myoma-grasping forceps, are not included in these public datasets, rendering them unusable for research in the automatic detection of gynecological laparoscopic instruments. Second, regarding algorithm design, detection accuracy and efficiency are the most critical indicators in its practical application. However, most current object detection algorithms fail to balance accuracy and efficiency, limiting their use in real surgical scenarios. The real-time multitask detection (RTMDet) model, proposed by Lyu <italic toggle="yes">et al</italic>. in 2022 and based on deep learning, has made significant progress in addressing this challenge<sup>[<xref rid="R15" ref-type="bibr">15</xref>]</sup>. However, its application has predominantly focused on the industrial field, with limited research in medical image detection.</p><p>Therefore, we aimed to retrospectively collect surgical video recordings and develop a dataset for gynecological laparoscopic instruments used in real-world scenarios, to automate the recognition, localization, and tracking of these instruments using a deep-learning model called RTMDet. Furthermore, we explored the potential of instrument detection to assess surgical skills, particularly during the critical phase of vaginal stump suturing in laparoscopic hysterectomy. This cross-sectional study has been reported in line with the Strengthening The Reporting Of Cohort Studies in Surgery (STROCSS) guidelines<sup>[<xref rid="R16" ref-type="bibr">16</xref>]</sup>.</p></sec><sec sec-type="methods"><title>Material and methods</title><sec><title>Ethics statement</title><p>This study protocol was approved by the ethics committee of Army Medical University, Chongqing, China on April 28, 2024 (approval number: KY2024147), and was conducted in accordance with the principles of the Declaration of Helsinki. As this was a retrospective study, the requirement for informed consent was waived. We followed the STROBE reporting guidelines. The work was reported in line with the STROCSS criteria<sup>[<xref rid="R16" ref-type="bibr">16</xref>]</sup>.<boxed-text id="UBT0001" position="float" orientation="portrait"><sec><title>HIGHLIGHTS</title><p>
<list list-type="bullet"><list-item><p>A large-scale gynecological laparoscopic surgical instrument database was developed.</p></list-item><list-item><p>High-precision real-time multiple instrument detection and tracking were achieved.</p></list-item><list-item><p>Several instrument kinematic metrics were identified for surgical skill assessment, providing an objective quantification reference for the subjective Global Operative Assessment of Laparoscopic Skills criteria.</p></list-item></list></p></sec></boxed-text></p></sec><sec><title>Preparation of the surgical instrument dataset</title><p>This cross-sectional study included 265 surgical videos from the obstetrics and gynecology departments of The First Affiliated Hospital of Army Medical University and The 958th Army Hospital of the Chinese People&#8217;s Liberation Army. These videos were gathered from 1 March 2022 to 31 July 2022. The inclusion criterion was surgical videos of traditional laparoscopic procedures performed for any gynecological disease. The video resolutions were 1920&#160;&#215;&#160;1080, 720&#160;&#215;&#160;576, and 720&#160;&#215;&#160;480 pixels (frame rate: 24&#8211;30 fps). Robotic surgery, as well as incomplete or improperly formatted video recordings, were excluded.</p><p>The videos were subsequently divided into training and testing datasets in a 4:1 ratio. Considering the high similarity between surgical instruments and background information in consecutive frames, we selectively extracted frames at 8-s intervals to avoid data redundancy and reduce ineffective training samples. Irrelevant scenes and frames lacking the target surgical instruments were removed manually.</p><p>Twelve surgical instruments were incorporated (Fig. <xref rid="F1" ref-type="fig">1</xref>A), including two types of bipolar forceps (SI-1, 2), a monopolar hook electrode (SI-3), a needle holder (SI-4), two types of ultrasound knives (SI-5, 6), two types of grasping forceps (SI-7, 8), scissors (SI-9), suction irrigation tubes (SI-10), myoma-grasping forceps (SI-11), and a myoma drill (SI-12). Notably, to reduce model complexity and align with clinical requirements, the two types of ultrasound knives and grasping forceps were consolidated into singular categories based on their similar appearances and functionalities. Five computer researchers annotated the images with bounding boxes under the guidance of two gynecologists (Fig. <xref rid="F1" ref-type="fig">1</xref>B).<fig position="float" id="F1" orientation="portrait"><label>Figure 1.</label><caption><p>Illustrations and annotation methods of surgical instruments. Panel A shows the illustrations of 12 surgical instruments, with SI-5 and SI-6, as well as SI-7 and SI-8, consolidated into single categories for the study. Panel B demonstrates the manual annotation method for surgical instruments.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="js9-111-5882-g001.jpg"/></fig>
</p></sec><sec><title>Preparation of the external validation dataset</title><p>Most existing open-source public datasets mainly cover robotic surgical instruments or those for laparoscopic cholecystectomy, which differ significantly in shape from the instruments included in this study. Therefore, we collected videos of gynecological laparoscopic surgeries from No. 960 Hospital of the Joint Service Support Force of the Chinese People&#8217;s Liberation Army for external validation. The data processing methods were consistent with those used in the preparation of the surgical instrument dataset. During the manual annotation phase, all surgical instruments in the external validation set were labeled by a single gynecologist.</p></sec><sec><title>Detection of surgical instruments</title><p>A deep learning-based object detection algorithm was used to detect the surgical instruments. The model adopted was developed based on RTMDet by incorporating specific modifications into the original framework. The primary modification involved pruning optimization<sup>[<xref rid="R17" ref-type="bibr">17</xref>]</sup>, which was dynamically adjusted during the experimental process to ensure the model met the frame rate requirements for practical applications. The constructed network comprises three major components: a modified backbone network based on CSPNeXt<sup>[<xref rid="R18" ref-type="bibr">18</xref>]</sup>, a feature-augmentation module using PANet<sup>[<xref rid="R19" ref-type="bibr">19</xref>]</sup>, and a detection head (Fig. <xref rid="F2" ref-type="fig">2</xref>). The backbone network primarily generates downsampled feature maps with factors of 8, 16, and 32. The feature-augmentation module enhances these images by integrating feature maps from various hierarchical levels, and the detection head interprets these maps to generate predictive outcomes, including confidence scores, classification labels, and coordinates of the detected instruments. Predictions by the model were considered valid only if the generated confidence scores exceeded a threshold of 0.2. After detection, results were refined through post-processing to produce the final bounding boxes. Specifically, we applied non-maximum suppression (NMS) with a threshold of 0.3 to filter overlapping bounding boxes and retain the most accurate predictions. Model pre-training was accomplished using the Microsoft Common Objects in Context (MS COCO) dataset<sup>[<xref rid="R20" ref-type="bibr">20</xref>]</sup>.<fig position="float" id="F2" orientation="portrait"><label>Figure 2.</label><caption><p>Diagram of the network structure. Conv2d, Convolution Layer; BN, Batch Normalization; SiLU, Sigmoid Linear Unit; MaxPool, Max Pooling; CBA, Convolutional Block Attention; ConCat, Concat Layer; CSP, Cross Stage Partial; SPP, Spatial Pyramid Pooling; &#10753;, Element-wise Sum; Deconv, Deconvolution.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="js9-111-5882-g002.jpg"/></fig>
</p><p>For data management, the dataset comprised images at three different resolutions. However, only the images resized to 960&#160;&#215;&#160;544 pixels were used in the inference stage. This resolution was finalized by downsampling or resizing the original input, followed by strategic zero padding along the inferior and right margins. Before network submission, each RGB image was individually standardized. Specifically, for each pixel in the three channels (red, green, and blue), the mean values (123.675 for red, 116.28 for green, and 103.53 for blue) were subtracted, and then the results were divided by variance (58.395 for red, 57.12 for green, and 57.375 for blue). Data augmentation was implemented as an online augmentation scheme and applied to each input batch. To ensure the reproducibility and reference value of the test results, data augmentation was not performed on the test set. The computational framework for training employed eight NVIDIA Tesla V100 GPUs with a batch size of 256, using the AdamW optimizer with an initial learning rate of 0.004. A detailed comparison of the hyperparameters used in this study with the original RTMDet implementation is provided in Supplemental Digital Content, Table 1, available at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://links.lww.com/JS9/E332" ext-link-type="uri">http://links.lww.com/JS9/E332</ext-link>. Specific details of the data augmentation methodologies are provided in Supplemental Digital Content, Table 2, available at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://links.lww.com/JS9/E333" ext-link-type="uri">http://links.lww.com/JS9/E333</ext-link>. In total, 300 epochs were conducted to derive the final model, which was subsequently implemented in an NVIDIA Tesla V100 within the inference environment. Additionally, we further compared the RTMDet with the state-of-the-art PP-YOLOE model on the same surgical instrument dataset.</p></sec><sec><title>Preparation of the vaginal stump suturing dataset</title><p>Eleven surgical video recordings of laparoscopic hysterectomies distinct from the surgical instrument dataset were included in the suturing dataset. Segments of the vaginal stump sutures were extracted manually. Subsequently, three clinical experts independently assessed the videos based on the Global Operative Assessment of Laparoscopic Skills (GOALS)<sup>[<xref rid="R21" ref-type="bibr">21</xref>]</sup>, which evaluates laparoscopic surgery performance according to the following five fundamental facets: depth perception, bimanual dexterity, efficiency, tissue handling, and autonomy. Each aspect was scored on a rating of 1&#8211;5. Given the impracticality of the experts in ascertaining the absence or presence of auxiliary guidance from another physician, the autonomy criterion was omitted. Consequently, the possible score for each video ranged from 4 to 20. The average scores were calculated, and the videos were classified as proficient or non-proficient using a cutoff score of 12. This assessment was performed in a single-blind manner to avoid bias related to the surgeon&#8217;s identity.</p></sec><sec><title>Tracking of surgical instruments</title><p>Employing the object detection algorithm, the categories and bounding boxes of the surgical instruments present in the videos of vaginal stump suturing were predicted frame-by-frame. It is important to note that vaginal stump suturing is primarily performed using grasping forceps and needle holders. In special cases, such as when a suction irrigation tube is needed to clear a blood-contaminated surgical field, other instruments may appear. However, in the analyzed surgical videos, only these two instruments were present. During prediction, they might be incorrectly classified as other instruments. To avoid trajectory interruptions from such mispredictions, we retained only the detection boxes corresponding to these two types of surgical instruments. Subsequently, the centroid coordinates of the predicted bounding boxes were computed and recorded. If an instrument was detected in successive frames, a line was drawn linking the pertinent points, thereby generating a trajectory that delineates the instrument&#8217;s motion. The length of these trajectories embodies the path length traversed by the instruments, quantified in pixels, whereas the comprehensive tally of points disseminated along the trajectory signifies the moving time of the instruments, quantified in frames. Consequently, the quotient derived by dividing the instrument&#8217;s path length by its moving time was defined as the movement velocity. Given the complexity of motion trajectories during the entire vaginal stump suturing phase, the process was methodologically decomposed based on individual stitches. The start point of a single stitch was defined as when the needle initially contacted the vaginal stump tissue, and the endpoint was defined as when the needle contacted the tissue again for the next stitch.</p></sec><sec><title>Evaluation metrics</title><p>Precision, sensitivity (recall), and F1 score served as metrics to evaluate the predictive capabilities of the models, which are characterized by the quantities of true positives (TP), false positives (FP), and false negatives (FN). High precision indicates a lower risk of mistakenly identifying instruments. High sensitivity signifies a reduced risk of missed identifications. The F1 score, which is the harmonic mean of precision and sensitivity, effectively balances the performance of both measures. The calculation formulas are as follows:
<disp-formula id="UM0001"><mml:math id="M1" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="UM0002"><mml:math id="M2" overflow="scroll"><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="UM0003"><mml:math id="M3" overflow="scroll"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#8901;</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Prediction was defined as a TP when the intersection over union (IoU) is &gt;0.3; otherwise, it was defined as an FN. An FP was defined as a predicted bounding box without a corresponding ground-truth bounding box. IoU was utilized to assess the accuracy of the instrument localization by calculating the ratio of the intersection to the union of the predicted and actual localization boxes. The calculation formula is as follows:
<disp-formula id="UM0004"><mml:math id="M4" overflow="scroll"><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mfenced open="(" close=")"><mml:mi>P</mml:mi></mml:mfenced><mml:mrow><mml:mo>&#8745;</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mfenced open="(" close=")"><mml:mi>P</mml:mi></mml:mfenced><mml:mrow><mml:mo>&#8746;</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <italic toggle="yes">P</italic> indicates the predicted bounding boxes, and <italic toggle="yes">GT</italic> indicates the manually annotated ground-truth bounding boxes.</p></sec><sec><title>Statistical analyses</title><p>Statistical analyses were performed using the Statistical Package for the SPSS (version 26.0). Continuous variables are described as median with interquartile range and were subjected to comparative analysis using the Mann&#8211;Whitney <italic toggle="yes">U</italic> test. Statistical significance was set at <italic toggle="yes">P</italic> &lt;&#160;0.05.</p></sec></sec><sec sec-type="results"><title>Results</title><sec><title>Detection of surgical instruments</title><p>A total of 265 surgical videos from the two medical centers were included. The extraction process resulted in 95&#160;088 images capturing 161&#160;348 instances of 12 surgical instruments, including 130&#160;496 and 30&#160;852 instances in the training and testing sets, respectively (Fig. <xref rid="F3" ref-type="fig">3</xref>A). The performance of each surgical instrument in the testing set is shown in Figure <xref rid="F3" ref-type="fig">3</xref>B. The ultrasonic knife achieved the highest F1 score of 97.09%, whereas the F1 score of the suction irrigation tube dropped notably to 85.36%. The model generally achieved a mean average precision (mAP) of 91.75%, a sensitivity of 94.29%, and an F1 score of 93.00% in the testing set. Instances displaying the automatic detection of surgical instruments are depicted in Figure <xref rid="F4" ref-type="fig">4</xref>A, demonstrating their capability to identify and locate multiple surgical instruments within an image. The adopted model achieved real-time detection of surgical instruments at 30 fps, as illustrated in Supplemental Digital Content, Video 1, available at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://links.lww.com/JS9/E337" ext-link-type="uri">http://links.lww.com/JS9/E337</ext-link>.<fig position="float" id="F3" orientation="portrait"><label>Figure 3.</label><caption><p>Quantities and prediction performance of surgical instruments. Panel A depicts the distribution of surgical instruments in the training and testing sets. Panel B shows the prediction performance for each surgical instrument.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="js9-111-5882-g003.jpg"/></fig>
<fig position="float" id="F4" orientation="portrait"><label>Figure 4.</label><caption><p>Prediction images and erroneous predictions of the model. Yellow box indicates the ground-truth box, while green indicates the predicted one. Panel A shows the prediction images. Panel B shows the instances of erroneous predictions. Yellow dots indicate instances of missed detection of instruments. Incorrect category prediction at correct location is classified as a missed detection.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="js9-111-5882-g004.jpg"/></fig>
</p><p>During the construction of the external validation dataset, the data were sourced from a single medical center, resulting in limited coverage of surgical instrument types. Specifically, data for bipolar forceps 2 and myoma drill were not collected. In the external validation set, a total of 1401 images were included, capturing 2564 targets. On the external validation dataset, the RTMDet achieved an mAP of 87.57%, a sensitivity of 93.37%, and an F1 score of 90.37%. The detailed predictive performance for each surgical instrument is provided in Supplemental Digital Content, Table 3, available at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://links.lww.com/JS9/E334" ext-link-type="uri">http://links.lww.com/JS9/E334</ext-link>.</p><p>Specific prediction errors manifested under the following circumstances: (1) the model missed the detection of bipolar forceps 2 and incorrectly predicted it as grasping forceps due to the similarity in their external shapes (Fig. <xref rid="F4" ref-type="fig">4</xref>Ba); (2) suboptimal image quality leading to missed detection and prediction errors, including conditions such as fog, motion blurring, and excessively dark scenes (Fig. <xref rid="F4" ref-type="fig">4</xref>Bb, c); (3) the emergence of numerous identical predictions for a single instrument is a phenomenon particularly prevalent in suction irrigation tubes (Fig. <xref rid="F4" ref-type="fig">4</xref>Bd); (4) erroneous predictions of suction irrigation tubes under conditions in which the tips of the surgical instruments are obscured, rendering only the shaft visible (Fig. <xref rid="F4" ref-type="fig">4</xref>Be); (5) if the tip of an instrument was severely occluded, it was customarily omitted from the annotations. Nonetheless, the use of random cropping during the training process may induce the model to generate prediction boxes for instruments that are only partially discernible (Fig. <xref rid="F4" ref-type="fig">4</xref>Bf).</p><p>We have further compared the RTMDet model with the state-of-the-art PP-YOLOE model on the same surgical instrument dataset. The PP-YOLOE achieved a mAP of 90.66%, a sensitivity of 93.05%, and an F1 score of 91.78% in the testing set. The experimental results showed that RTMDet outperformed PP-YOLOE in all three metrics, fully validating the superiority of RTMDet in the task of automatic surgical instrument detection. The specific comparison data are shown in Table <xref rid="T1" ref-type="table">1</xref>.<table-wrap position="float" id="T1" orientation="portrait"><label>Table 1</label><caption><p>Comparison of RTMDet with PP-YOLOE</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" colspan="1">Instrument</th><th colspan="3" align="left" rowspan="1">RTMDet</th><th colspan="3" align="left" rowspan="1">PP-YOLOE</th></tr><tr><th align="left" rowspan="1" colspan="1">Precision (%)</th><th align="left" rowspan="1" colspan="1">Sensitivity (%)</th><th align="left" rowspan="1" colspan="1">F1 score (%)</th><th align="left" rowspan="1" colspan="1">Precision (%)</th><th align="left" rowspan="1" colspan="1">Sensitivity (%)</th><th align="left" rowspan="1" colspan="1">F1 score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Bipolar forceps 1</td><td align="center" rowspan="1" colspan="1">95.49</td><td align="center" rowspan="1" colspan="1">97.15</td><td align="center" rowspan="1" colspan="1">96.31</td><td align="center" rowspan="1" colspan="1">93.21</td><td align="center" rowspan="1" colspan="1">96.33</td><td align="center" rowspan="1" colspan="1">94.74</td></tr><tr><td align="left" rowspan="1" colspan="1">Bipolar forceps 2</td><td align="center" rowspan="1" colspan="1">94.41</td><td align="center" rowspan="1" colspan="1">92.39</td><td align="center" rowspan="1" colspan="1">93.39</td><td align="center" rowspan="1" colspan="1">92.55</td><td align="center" rowspan="1" colspan="1">93.12</td><td align="center" rowspan="1" colspan="1">92.83</td></tr><tr><td align="left" rowspan="1" colspan="1">Hook electrode monopolar</td><td align="center" rowspan="1" colspan="1">88.35</td><td align="center" rowspan="1" colspan="1">94.11</td><td align="center" rowspan="1" colspan="1">91.14</td><td align="center" rowspan="1" colspan="1">89.11</td><td align="center" rowspan="1" colspan="1">90.35</td><td align="center" rowspan="1" colspan="1">89.73</td></tr><tr><td align="left" rowspan="1" colspan="1">Needle holder</td><td align="center" rowspan="1" colspan="1">94.94</td><td align="center" rowspan="1" colspan="1">97.04</td><td align="center" rowspan="1" colspan="1">95.98</td><td align="center" rowspan="1" colspan="1">92.77</td><td align="center" rowspan="1" colspan="1">95.13</td><td align="center" rowspan="1" colspan="1">93.94</td></tr><tr><td align="left" rowspan="1" colspan="1">Ultrasound knife</td><td align="center" rowspan="1" colspan="1">95.89</td><td align="center" rowspan="1" colspan="1">98.32</td><td align="center" rowspan="1" colspan="1">97.09</td><td align="center" rowspan="1" colspan="1">96.32</td><td align="center" rowspan="1" colspan="1">97.55</td><td align="center" rowspan="1" colspan="1">96.93</td></tr><tr><td align="left" rowspan="1" colspan="1">Grasping forceps</td><td align="center" rowspan="1" colspan="1">90.19</td><td align="center" rowspan="1" colspan="1">97.16</td><td align="center" rowspan="1" colspan="1">93.55</td><td align="center" rowspan="1" colspan="1">89.03</td><td align="center" rowspan="1" colspan="1">95.49</td><td align="center" rowspan="1" colspan="1">92.15</td></tr><tr><td align="left" rowspan="1" colspan="1">Scissors</td><td align="center" rowspan="1" colspan="1">89.23</td><td align="center" rowspan="1" colspan="1">93.41</td><td align="center" rowspan="1" colspan="1">91.27</td><td align="center" rowspan="1" colspan="1">89.91</td><td align="center" rowspan="1" colspan="1">94.15</td><td align="center" rowspan="1" colspan="1">91.98</td></tr><tr><td align="left" rowspan="1" colspan="1">Suction irrigation tube</td><td align="center" rowspan="1" colspan="1">77.44</td><td align="center" rowspan="1" colspan="1">95.08</td><td align="center" rowspan="1" colspan="1">85.36</td><td align="center" rowspan="1" colspan="1">79.01</td><td align="center" rowspan="1" colspan="1">90.76</td><td align="center" rowspan="1" colspan="1">84.48</td></tr><tr><td align="left" rowspan="1" colspan="1">Myoma grasping forceps</td><td align="center" rowspan="1" colspan="1">99.26</td><td align="center" rowspan="1" colspan="1">81.71</td><td align="center" rowspan="1" colspan="1">89.63</td><td align="center" rowspan="1" colspan="1">91.11</td><td align="center" rowspan="1" colspan="1">83.88</td><td align="center" rowspan="1" colspan="1">87.35</td></tr><tr><td align="left" rowspan="1" colspan="1">Myoma drill</td><td align="center" rowspan="1" colspan="1">92.32</td><td align="center" rowspan="1" colspan="1">96.56</td><td align="center" rowspan="1" colspan="1">94.39</td><td align="center" rowspan="1" colspan="1">93.55</td><td align="center" rowspan="1" colspan="1">93.74</td><td align="center" rowspan="1" colspan="1">93.64</td></tr><tr><td align="left" rowspan="1" colspan="1">AVG</td><td align="center" rowspan="1" colspan="1">91.75</td><td align="center" rowspan="1" colspan="1">94.29</td><td align="center" rowspan="1" colspan="1">92.81</td><td align="center" rowspan="1" colspan="1">90.66</td><td align="center" rowspan="1" colspan="1">93.05</td><td align="center" rowspan="1" colspan="1">91.78</td></tr></tbody></table></table-wrap>
</p></sec><sec><title>Tracking of surgical instruments</title><p>Eleven vaginal stump suturing videos were included for surgical skill analysis. Three clinical experts evaluated the surgical videos according to the GOALS criteria. Finally, seven videos were deemed proficient, and the remainder were labeled non-proficient. Detailed scoring results are shown in Supplemental Digital Content, Table 4, available at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://links.lww.com/JS9/E335" ext-link-type="uri">http://links.lww.com/JS9/E335</ext-link>. The stitch counts in the proficient and non-proficient groups were 62 and 28 stitches, respectively. Subsequently, the motion trajectories of the instruments were generated. Figure <xref rid="F5" ref-type="fig">5</xref>Aa&#8211;h indicates the motion trajectories of each stitch in the proficient group, whereas i&#8211;p indicates those in the non-proficient group. The proficient group exhibited more streamlined motion trajectories and a marked similarity in the trajectories of each stitch compared with the non-proficient group. Furthermore, a detailed computation was conducted to determine the variances in the kinematic data. The proficient group demonstrated significantly lesser path lengths and inter-quartile range, shorter moving times, and higher movement velocities for grasping forceps and needle holders than the non-proficient group (Fig. <xref rid="F5" ref-type="fig">5</xref>B). The detailed kinematic data are presented in Supplemental Digital Content, Table 5, available at: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://links.lww.com/JS9/E336" ext-link-type="uri">http://links.lww.com/JS9/E336</ext-link>.<fig position="float" id="F5" orientation="portrait"><label>Figure 5.</label><caption><p>Kinematic data of surgical instruments. Panel A shows the instrument trajectories. (a&#8211;h) Motion trajectories of each stitch in the proficient group. (i&#8211;p) Motion trajectories in the non-proficient group. Within these images, green lines indicate the trajectories of the grasping forceps, while red lines indicate the trajectories of the needle holder. Panel B shows the comparison of the kinematic data of each surgical instrument per stitch between the proficient and non-proficient groups.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="js9-111-5882-g005.jpg"/></fig>
</p></sec></sec><sec sec-type="discussion"><title>Discussion</title><p>To the best of our knowledge, this is the largest-scale study on the automatic detection of laparoscopic surgical instruments and is the first application of RTMDet in the medical field. We developed a large-scale database of gynecological laparoscopic instruments and achieved high-precision real-time detection of multiple surgical instruments. Furthermore, we analyzed the differences in kinematic characteristics between videos labeled as proficient or non-proficient. The initial exploration affirmed the potential value of instrument detection for assessing surgical skills.</p><p>In object detection tasks, there is often a delicate balance to be maintained between accuracy and efficiency, which can be described as enhancing the precision often comes at the cost of reduced inference speed or increased consumption of computational resources. However, RTMDet achieves a remarkable balance between high accuracy and low latency. This is accomplished by improving efficiency and precision using large-kernel depth-wise convolution and applying soft labels in dynamic label assignments. Additionally, the integration of advanced training techniques further amplifies its performance. These improvements make RTMDet particularly suitable for medical applications that require real-time analysis and decision-making. In this study, by employing the RTMDet algorithm, all three metrics (precision, sensitivity, and F1 score) surpassed the 90% threshold. Xu <italic toggle="yes">et al</italic>. compared PP-YOLOE with several mainstream object detection models, such as YOLO series and EfficientDet, and demonstrated that PP-YOLOE achieved the best detection performance on the COCO dataset<sup>[<xref rid="R22" ref-type="bibr">22</xref>]</sup>. Based on this finding, we further compared the RTMDet model with the state-of-the-art PP-YOLOE model on the same surgical instrument dataset. The experimental results showed that RTMDet outperformed PP-YOLOE in accuracy, recall, and F1 score, fully validating the superiority of RTMDet in the task of automatic surgical instrument detection. For comparative analysis, we conducted a review of similar studies. Yamazaki <italic toggle="yes">et al</italic>. utilized YOLOv3 to detect 14 surgical instruments, achieving an average precision of 87% and sensitivity of 83%<sup>[<xref rid="R13" ref-type="bibr">13</xref>]</sup>. Vardazaryan <italic toggle="yes">et al</italic>. evaluated several architectures and achieved an mAP of 87.4% and a sensitivity of 88.8%<sup>[<xref rid="R23" ref-type="bibr">23</xref>]</sup>. Although different datasets were used, and direct comparison might have inherent limitations, surpassing 90% in accuracy, sensitivity, and F1 score sufficiently demonstrated the unique advantages of RTMDet in surgical instrument detection tasks.</p><p>While the RTMDet demonstrated exceptional performance across most surgical instruments, the suction irrigation tube exhibited a relatively lower F1 score of 85.36%, primarily attributed to a lower precision rate of 77.44%. This indicates that 22.56% of the predicted samples were FP, suggesting challenges in distinguishing the suction irrigation tube from other instruments. Specifically, the model exhibited a tendency to misclassify the shafts of other instruments as the suction irrigation tube, a phenomenon likely attributable to visual similarities in the shaft portion of these instruments. This issue was further compounded by the complex visual environment inherent to surgical settings, where instruments often overlap or were partially occluded, thereby increasing the likelihood of misclassification. Future work needs to explore strategies to address these challenges, including targeted data augmentation and the adoption of lightweight architectures, with the aim of enhancing the model&#8217;s ability to accurately distinguish the suction irrigation tube from other surgical instruments.</p><p>Some drawbacks of mainstream public datasets include single-source data and limited coverage of surgical scenarios. To enhance the model&#8217;s generalization and predictive performance, we adopted several critical measures during dataset construction. First, surgical videos from two medical centers were included in the dataset, covering almost all types of gynecologic laparoscopic instruments, which ensured the broad applicability of the model. Second, we established a large-scale dataset by collecting numerous surgical videos. The increase in data volume allowed the model to acquire a broader range of surgical scenarios and instrument characteristics during training, thereby improving its performance across varied surgical environments. Finally, during manual annotation, we provided annotations even when the images were blurred or fogged, provided that the instrument type could be identified by the annotators. The meticulous annotation strategy effectively enhanced the model&#8217;s robustness, enabling it to perform excellently in complex surgical scenarios.</p><p>In this study, we achieved high-precision real-time detection of surgical instruments and conducted corresponding application tests, further demonstrating its value in surgical skill assessment. Previous studies have demonstrated that surgical skills profoundly influence patient outcomes, particularly in complicated surgical procedures.<sup>[<xref rid="R24" ref-type="bibr">24</xref>-<xref rid="R26" ref-type="bibr">26</xref>]</sup> Video-based assessment can provide surgeons with comprehensive feedback, improving their surgical capabilities<sup>[<xref rid="R27" ref-type="bibr">27</xref>]</sup>. Traditional video-based assessments typically rely on retrospective review of surgical videos by experts, which is time-consuming and may suffer from unavoidable biases<sup>[<xref rid="R28" ref-type="bibr">28</xref>]</sup>. Consequently, there is a pressing need for an objective and automated assessment tool to improve the current situation. For example, Jin <italic toggle="yes">et al</italic>. analyzed timelines, heat maps, and trajectories of surgical instruments during laparoscopic cholecystectomy, reporting significant differences in tool utilization, operational focus areas, and movement patterns among surgeons with different competencies<sup>[<xref rid="R29" ref-type="bibr">29</xref>]</sup>. However, their findings lacked quantification and thus failed to standardize surgical skill evaluation metrics. As is well-known, suturing is a fundamental skill for surgeons. Specifically, the procedural complexity of vaginal cuff suturing during laparoscopic hysterectomy exhibits minimal variation among patients. Consequently, the current study utilized vaginal cuff suturing as a focal point, generated trajectories of surgical instruments, and explored the differences in surgical videos labeled as proficient or non-proficient. Through a comparative analysis of videos of vaginal stump suturing performed by surgeons with different skill levels, we discovered that the proficient group exhibited reduced path length and moving time, increased kinematic velocity, and reduced interquartile range for each suture, which correspond to bimanual dexterity, efficiency, tissue handling, and depth perception outlined in the GOALS. Therefore, these subjective indicators can be described using quantifiable metrics. This imparts greater significance to our research and offers the potential to provide valuable feedback to surgical teams in the future, thereby playing a crucial role in enhancing surgical education.</p><p>Beyond its role in surgical skill assessment, the analysis of surgical instrument motion trajectories holds broader implications for optimizing surgical workflows and enhancing training methodologies. By analyzing these motion trajectories, it is possible to optimize surgical workflow design and provide data support for standardizing surgical techniques. Furthermore, real-time alerts can be triggered when surgical instruments significantly deviate from the optimal trajectories, enabling young surgeons to adjust their operations promptly and reduce surgical risks. Additionally, post-operative analysis of instrument motion trajectories can identify patterns of efficiency and inefficiency in surgical operations. This helps young surgeons pinpoint areas for improvement and achieve targeted skill enhancements. These technological advancements can transform surgical training from traditional experiential teaching to an intelligent and standardized approach, ultimately leading to significant improvements in surgical training efficiency.</p><p>Our study has some limitations. First, although the model proposed has been validated using an external dataset from another medical center, it is still in the proof-of-concept stage. Ideally, validation should be performed on a publicly available dataset to ensure broader generalizability. However, existing open-source datasets primarily focus on surgical instruments used in robotic surgeries or laparoscopic cholecystectomies, which differ significantly in shape from the instruments included in our study. Therefore, we had to rely on our own constructed external validation set. Second, all the surgical videos in this study were in a two-dimensional format. Consequently, the Z-axis was not included in the trajectory, which may have led to systematic errors. Disparate distances between the camera and the vaginal stump across videos can lead to systematic errors as well. Third, the detection performance of the suction irrigation tube was less satisfactory than that of other instruments. Future studies could address this issue by introducing lightweight architectures to improve the model&#8217;s ability to distinguish between instruments. Furthermore, although this study identified several potential indicators for assessing surgical skills, we believe that there are still numerous undiscovered differential indicators within kinematic metrics. Finally, the current study&#8217;s grouping design may overlook intermediate skill-level surgical cases, leading to extreme differences in results that may not fully reflect the continuity and diversity of skill levels in actual clinical practice. In future studies, it is necessary to further refine the grouping into expert, intermediate, and novice categories. This will help validate the applicability and sensitivity of these kinematic metrics across different skill levels, thereby providing a more comprehensive understanding of the complexity of surgical skill assessment and developing more universal evaluation methods.</p></sec><sec sec-type="conclusions"><title>Conclusion</title><p>We developed a large-scale gynecological laparoscopic surgical instrument database based on real surgical scenarios, with detailed annotations of the positions and types of surgical instruments. For the first time, we applied the deep learning-based RTMDet to achieve high-precision real-time detection and tracking of multiple instruments within this database. In comparison with the state-of-the-art PP-YOLOE, RTMDet has shown superior performance, further confirming its effectiveness in the task of automatic surgical instrument detection. Furthermore, we identified key differences in the kinematic data of instruments between surgical videos of varying proficiency levels. These differences will aid in evaluating surgical skills and providing timely feedback to surgical teams, which may play a significant role in surgical education.</p></sec></body><back><ack><title>Acknowledgements</title><p>We would like to thank Editage (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="www.editage.com" ext-link-type="uri">www.editage.com</ext-link>) for English language editing.</p></ack><fn-group><fn fn-type="other"><p>H.W., L.D., and X.W. contributed equally to the work and are, therefore, co-first authors.</p></fn><fn fn-type="other"><p>Sponsorships or competing interests that may be relevant to content are disclosed at the end of this article.</p></fn><fn fn-type="equal"><p>Published online 12 June 2025</p></fn></fn-group><sec><title>Ethical approval</title><p>This study protocol was approved by the ethics committee of The First Affiliated Hospital of Army Medical University, Chongqing, China, on 28 April 2024 (approval number: KY2024147), and was conducted in accordance with the principles of the Declaration of Helsinki. We followed the STROBE reporting guidelines.</p></sec><sec><title>Consent</title><p>As this was a retrospective study, the requirement for informed consent was waived.</p></sec><sec><title>Sources of funding</title><p>This study was funded by Chongqing Technology Innovation and Application Development Special Key Projects (CSTB2022TIAD-KPX0154) and the National Key Research and Development Program of China (2023YFC2706000). The funding source had no role in the design and conduct of the study, collection, management, analysis, and interpretation of the data, preparation, review, or approval of the manuscript, or decision to submit the manuscript for publication.</p></sec><sec><title>Author contributions</title><p>Conceptualization: Y.W., S.T., H.W., L.D., B.Y. Data curation: Y.L., L.D., R.W., X.L., Y.C., W.T., X.W. Formal analysis: H.W., W.T., X.W. Funding acquisition: Y.W. Investigation: Y.L., R.W., X.L., W.T., L.D., B.Y. Methodology: Y.W., H.W., Y.C., B.Y. Project administration: Y.W., S.T. Resources: Y.W., H.W. Software: Y.W., H.W. Supervision: Y.W., S.T. Writing &#8211; original draft: Y.W., S.T., H.W., X.W., L.D. Writing &#8211; review and editing: Y.W., S.T.</p></sec><sec sec-type="COI-statement"><title>Conflicts of interest disclosure</title><p>The authors have declared no conflicts of interest.</p></sec><sec><title>Research registration unique identifying number (UIN)</title><p>Chictr.org Identifier: ChiCTR2400085622.</p></sec><sec><title>Guarantor</title><p>Dr. Yanzhou Wang and Dr. Shuai Tang take full responsibility for the work and the conduct of the study, had access to the data, and controlled the decision to publish.</p></sec><sec><title>Provenance and peer review</title><p>Not commissioned, externally peer-reviewed.</p></sec><sec sec-type="data-availability"><title>Data availability statement</title><p>For the identification of surgical instruments, we have invested significant effort in the manual annotation of surgical instruments. Considering the potential for further research using this dataset, we are currently not sharing the original data and source code. However, we are open to collaboration and can discuss data sharing on a case-by-case basis. Please contact the corresponding author for more information.</p></sec><sec><title>Data access, responsibility, and analysis</title><p>Dr. Yanzhou Wang and Dr. Shuai Tang had full access to all the data in the study and take full responsibility for the integrity of the data and the accuracy of the data analysis.</p></sec><ref-list><title>References</title><ref id="R1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chamberlin</surname><given-names>J</given-names></name><name name-style="western"><surname>Kocher</surname><given-names>MR</given-names></name><name name-style="western"><surname>Waltz</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>Automated detection of lung nodules and coronary artery calcium using artificial intelligence on low-dose CT scans for lung cancer screening: accuracy and prognostic value</article-title>. <source>BMC Med</source><year>2021</year>;<volume>19</volume>:<fpage>55</fpage>.<pub-id pub-id-type="pmid">33658025</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1186/s12916-021-01928-3</pub-id><pub-id pub-id-type="pmcid">PMC7931546</pub-id></mixed-citation></ref><ref id="R2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bera</surname><given-names>K</given-names></name><name name-style="western"><surname>Schalper</surname><given-names>KA</given-names></name><name name-style="western"><surname>Rimm</surname><given-names>DL</given-names></name><name name-style="western"><surname>Velcheti</surname><given-names>V</given-names></name><name name-style="western"><surname>Madabhushi</surname><given-names>A</given-names></name></person-group>. <article-title>Artificial intelligence in digital pathology &#8211; new tools for diagnosis and precision oncology</article-title>. <source>Nat Rev Clin Oncol</source><year>2019</year>;<volume>16</volume>:<fpage>703</fpage>&#8211;<lpage>15</lpage>.<pub-id pub-id-type="pmid">31399699</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41571-019-0252-y</pub-id><pub-id pub-id-type="pmcid">PMC6880861</pub-id></mixed-citation></ref><ref id="R3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Andrew</surname><given-names>AG</given-names></name><name name-style="western"><surname>Frank</surname><given-names>A</given-names></name><name name-style="western"><surname>Konrad</surname><given-names>K</given-names></name><etal/></person-group>. <article-title>White paper: definitions of artificial intelligence and autonomous actions in clinical surgery</article-title>. <source>Art Intel Surg</source><year>2022</year>;<volume>2</volume>:<fpage>93</fpage>&#8211;<lpage>100</lpage>.</mixed-citation></ref><ref id="R4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>GZ</given-names></name><name name-style="western"><surname>Cambias</surname><given-names>J</given-names></name><name name-style="western"><surname>Cleary</surname><given-names>K</given-names></name><etal/></person-group>. <article-title>Medical robotics &#8211; regulatory, ethical, and legal considerations for increasing levels of autonomy</article-title>. <source>Sci Rob</source><year>2017</year>;<volume>2</volume>:<fpage>eaam8638</fpage>.<pub-id pub-id-type="doi" assigning-authority="pmc">10.1126/scirobotics.aam8638</pub-id><pub-id pub-id-type="pmid">33157870</pub-id></mixed-citation></ref><ref id="R5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vasey</surname><given-names>B</given-names></name><name name-style="western"><surname>Lippert</surname><given-names>KAN</given-names></name><name name-style="western"><surname>Khan</surname><given-names>DZ</given-names></name><etal/></person-group>. <article-title>Intraoperative applications of artificial intelligence in robotic surgery: a scoping review of current development stages and levels of autonomy</article-title>. <source>Ann Surg</source><year>2023</year>;<volume>278</volume>:<fpage>896</fpage>&#8211;<lpage>903</lpage>.<pub-id pub-id-type="pmid">36177855</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1097/SLA.0000000000005700</pub-id><pub-id pub-id-type="pmcid">PMC10631501</pub-id></mixed-citation></ref><ref id="R6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>G</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>B</given-names></name><etal/></person-group>. <article-title>Advances of surgical robotics: image-guided classification and application</article-title>. <source>Natl Sci Rev</source><year>2024</year>;<volume>11</volume>:<fpage>nwae186</fpage>.<pub-id pub-id-type="pmid">39144738</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/nsr/nwae186</pub-id><pub-id pub-id-type="pmcid">PMC11321255</pub-id></mixed-citation></ref><ref id="R7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fiorini</surname><given-names>P</given-names></name><name name-style="western"><surname>Goldberg</surname><given-names>KY</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Taylor</surname><given-names>RH</given-names></name></person-group>. <article-title>Concepts and trends in autonomy for robot-assisted surgery</article-title>. <source>Proc IEEE Inst Electr Electron Eng</source><year>2022</year>;<volume>110</volume>:<fpage>993</fpage>&#8211;<lpage>1011</lpage>.<pub-id pub-id-type="pmid">35911127</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/JPROC.2022.3176828</pub-id><pub-id pub-id-type="pmcid">PMC7613181</pub-id></mixed-citation></ref><ref id="R8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Q</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z</given-names></name><name name-style="western"><surname>Gu</surname><given-names>L</given-names></name></person-group>. <article-title>Visual detection and tracking algorithms for minimally invasive surgical instruments: a comprehensive review of the state-of-the-art</article-title>. <source>Rob Autom Syst</source><year>2022</year>;<volume>149</volume>:<fpage>103945</fpage>.</mixed-citation></ref><ref id="R9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rueckert</surname><given-names>T</given-names></name><name name-style="western"><surname>Rueckert</surname><given-names>D</given-names></name><name name-style="western"><surname>Palm</surname><given-names>C</given-names></name></person-group>. <article-title>Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: a review of the state of the art</article-title>. <source>Comput Biol Med</source><year>2024</year>;<volume>169</volume>:<fpage>107929</fpage>.<pub-id pub-id-type="pmid">38184862</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.compbiomed.2024.107929</pub-id></mixed-citation></ref><ref id="R10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rodrigues</surname><given-names>M</given-names></name><name name-style="western"><surname>Mayo</surname><given-names>M</given-names></name><name name-style="western"><surname>Patros</surname><given-names>P</given-names></name></person-group>. <article-title>Surgical tool datasets for machine learning research: a survey</article-title>. <source>Int J Comput Vis</source><year>2022</year>;<volume>130</volume>:<fpage>2222</fpage>&#8211;<lpage>48</lpage>.</mixed-citation></ref><ref id="R11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alshirbaji</surname><given-names>TA</given-names></name><name name-style="western"><surname>Jalal</surname><given-names>NA</given-names></name><name name-style="western"><surname>Mller</surname><given-names>K</given-names></name></person-group>. <article-title>Surgical tool classification in laparoscopic videos using convolutional neural network</article-title>. <source>Curr Dir Biomed Eng</source><year>2018</year>;<volume>4</volume>:<fpage>407</fpage>&#8211;<lpage>10</lpage>.</mixed-citation></ref><ref id="R12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Twinanda</surname><given-names>AP</given-names></name><name name-style="western"><surname>Shehata</surname><given-names>S</given-names></name><name name-style="western"><surname>Mutter</surname><given-names>D</given-names></name><name name-style="western"><surname>Marescaux</surname><given-names>J</given-names></name><name name-style="western"><surname>de Mathelin</surname><given-names>M</given-names></name><name name-style="western"><surname>Padoy</surname><given-names>N</given-names></name></person-group>. <article-title>EndoNet: a deep architecture for recognition tasks on laparoscopic videos</article-title>. <source>IEEE Trans Med Imag</source><year>2017</year>;<volume>36</volume>:<fpage>86</fpage>&#8211;<lpage>97</lpage>.<pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2016.2593957</pub-id><pub-id pub-id-type="pmid">27455522</pub-id></mixed-citation></ref><ref id="R13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yamazaki</surname><given-names>Y</given-names></name><name name-style="western"><surname>Kanaji</surname><given-names>S</given-names></name><name name-style="western"><surname>Matsuda</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>Automated surgical instrument detection from laparoscopic gastrectomy video images using an open source convolutional neural network platform</article-title>. <source>J Am Coll Surg</source><year>2020</year>;<volume>230</volume>:<fpage>725</fpage>&#8211;<lpage>732.e1</lpage>.<pub-id pub-id-type="pmid">32156655</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jamcollsurg.2020.01.037</pub-id></mixed-citation></ref><ref id="R14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kitaguchi</surname><given-names>D</given-names></name><name name-style="western"><surname>Takeshita</surname><given-names>N</given-names></name><name name-style="western"><surname>Hasegawa</surname><given-names>H</given-names></name><name name-style="western"><surname>Ito</surname><given-names>M</given-names></name></person-group>. <article-title>Artificial intelligence-based computer vision in surgery: recent advances and future perspectives</article-title>. <source>Ann Gastroenterol Surg</source><year>2022</year>;<volume>6</volume>:<fpage>29</fpage>&#8211;<lpage>36</lpage>.<pub-id pub-id-type="pmid">35106412</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/ags3.12513</pub-id><pub-id pub-id-type="pmcid">PMC8786689</pub-id></mixed-citation></ref><ref id="R15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lyu</surname><given-names>C</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>W</given-names></name><name name-style="western"><surname>Huang</surname><given-names>H</given-names></name><etal/></person-group>. <article-title>RTMDet: an empirical study of designing real-time object detectors</article-title>. <source>arXiv</source><year>2022</year>:<fpage>2212.07784</fpage>.</mixed-citation></ref><ref id="R16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Agha</surname><given-names>RA</given-names></name><name name-style="western"><surname>Mathew</surname><given-names>G</given-names></name><name name-style="western"><surname>Rashid</surname><given-names>R</given-names></name><etal/></person-group>. <article-title>Revised strengthening the reporting of cohort, cross-sectional and case-control studies in surgery (STROCSS) guideline: an update for the age of artificial intelligence</article-title>, <source>Premier J Sci</source><volume>2025</volume>:<fpage>100081</fpage>.</mixed-citation></ref><ref id="R17"><label>[17]</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Sunil</surname><given-names>V</given-names></name><name name-style="western"><surname>Salem</surname><given-names>A</given-names></name></person-group>. <article-title>Methods for pruning deep neural networks</article-title>. <comment>arXiv. 2020</comment>:<fpage>2011.00241</fpage>.</mixed-citation></ref><ref id="R18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>K</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H</given-names></name><name name-style="western"><surname>Yan</surname><given-names>X</given-names></name></person-group>. <article-title>An improved YOLOv5-based underwater object-detection framework</article-title>. <source>Sensors (Basel)</source><year>2023</year>;<day>23</day>.<pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s23073693</pub-id><pub-id pub-id-type="pmcid">PMC10099368</pub-id><pub-id pub-id-type="pmid">37050753</pub-id></mixed-citation></ref><ref id="R19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>S</given-names></name><name name-style="western"><surname>Qi</surname><given-names>L</given-names></name><name name-style="western"><surname>Qin</surname><given-names>H</given-names></name><name name-style="western"><surname>Shi</surname><given-names>J</given-names></name><name name-style="western"><surname>Jia</surname><given-names>J</given-names></name></person-group>. <article-title>Path aggregation network for instance segmentation</article-title>. <source>arXiv</source><year>2018</year>:<fpage>1803.01534</fpage>.</mixed-citation></ref><ref id="R20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T-Y</given-names></name><name name-style="western"><surname>Maire</surname><given-names>M</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>Microsoft COCO: common objects in context</article-title>. <source>arXiv</source><year>2015</year>:<fpage>1405.0312</fpage>.</mixed-citation></ref><ref id="R21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vassiliou</surname><given-names>MC</given-names></name><name name-style="western"><surname>Feldman</surname><given-names>LS</given-names></name><name name-style="western"><surname>Andrew</surname><given-names>CG</given-names></name><etal/></person-group>. <article-title>A global assessment tool for evaluation of intraoperative laparoscopic skills</article-title>. <source>Am J Surg</source><year>2005</year>;<volume>190</volume>:<fpage>107</fpage>&#8211;<lpage>13</lpage>.<pub-id pub-id-type="pmid">15972181</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.amjsurg.2005.04.004</pub-id></mixed-citation></ref><ref id="R22"><label>[22]</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>S</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name><name name-style="western"><surname>Lv</surname><given-names>W</given-names></name><etal/></person-group>. <article-title>PP-YOLOE: an evolved version of YOLO</article-title>. <comment>arXiv. 2022</comment>:<fpage>2203.16250</fpage>.</mixed-citation></ref><ref id="R23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vardazaryan</surname><given-names>A</given-names></name><name name-style="western"><surname>Mutter</surname><given-names>D</given-names></name><name name-style="western"><surname>Marescaux</surname><given-names>J</given-names></name><name name-style="western"><surname>Padoy</surname><given-names>N</given-names></name></person-group>. <article-title>Weakly-supervised learning for tool localization in laparoscopic videos</article-title>. <source>arXiv</source><year>2018</year>:<fpage>1806.05573</fpage>.</mixed-citation></ref><ref id="R24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Birkmeyer</surname><given-names>JD</given-names></name><name name-style="western"><surname>Finks</surname><given-names>JF</given-names></name><name name-style="western"><surname>O&#8217;Reilly</surname><given-names>A</given-names></name><etal/></person-group>. <article-title>Surgical skill and complication rates after bariatric surgery</article-title>. <source>N Engl J Med</source><year>2013</year>;<volume>369</volume>:<fpage>1434</fpage>&#8211;<lpage>42</lpage>.<pub-id pub-id-type="pmid">24106936</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1056/NEJMsa1300625</pub-id></mixed-citation></ref><ref id="R25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Curtis</surname><given-names>NJ</given-names></name><name name-style="western"><surname>Foster</surname><given-names>JD</given-names></name><name name-style="western"><surname>Miskovic</surname><given-names>D</given-names></name><etal/></person-group>. <article-title>Association of surgical skill assessment with clinical outcomes in cancer surgery</article-title>. <source>JAMA Surg</source><year>2020</year>;<volume>155</volume>:<fpage>590</fpage>&#8211;<lpage>98</lpage>.<pub-id pub-id-type="pmid">32374371</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamasurg.2020.1004</pub-id><pub-id pub-id-type="pmcid">PMC7203671</pub-id></mixed-citation></ref><ref id="R26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ichikawa</surname><given-names>N</given-names></name><name name-style="western"><surname>Homma</surname><given-names>S</given-names></name><name name-style="western"><surname>Funakoshi</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>Impact of technically qualified surgeons on laparoscopic colorectal resection outcomes: results of a propensity score-matching analysis</article-title>. <source>BJS Open</source><year>2020</year>;<volume>4</volume>: <fpage>486</fpage>&#8211;<lpage>98</lpage>.<pub-id pub-id-type="pmid">32207580</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/bjs5.50263</pub-id><pub-id pub-id-type="pmcid">PMC7260420</pub-id></mixed-citation></ref><ref id="R27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yanik</surname><given-names>E</given-names></name><name name-style="western"><surname>Schwaitzberg</surname><given-names>S</given-names></name><name name-style="western"><surname>De</surname><given-names>S</given-names></name></person-group>. <article-title>Deep learning for video-based assessment in surgery</article-title>. <source>JAMA Surg</source><year>2024</year>;<volume>159</volume>:<fpage>957</fpage>&#8211;<lpage>58</lpage>.<pub-id pub-id-type="pmid">38837128</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamasurg.2024.1510</pub-id></mixed-citation></ref><ref id="R28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hung</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Anandkumar</surname><given-names>A</given-names></name></person-group>. <article-title>Deep learning to automate technical skills assessment in robotic surgery</article-title>. <source>JAMA Surg</source><year>2021</year>;<volume>156</volume>: <fpage>1059</fpage>&#8211;<lpage>60</lpage>.<pub-id pub-id-type="pmid">34524401</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamasurg.2021.3651</pub-id></mixed-citation></ref><ref id="R29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>A</given-names></name><name name-style="western"><surname>Yeung</surname><given-names>S</given-names></name><name name-style="western"><surname>Jopling</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>Tool detection and operative skill assessment in surgical videos using region-based convolutional neural networks</article-title>. <source>arXiv</source><year>2018</year>:<fpage>1802.08774</fpage>.</mixed-citation></ref></ref-list></back></article></pmc-articleset>