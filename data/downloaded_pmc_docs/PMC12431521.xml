<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431521</article-id><article-id pub-id-type="pmcid-ver">PMC12431521.1</article-id><article-id pub-id-type="pmcaid">12431521</article-id><article-id pub-id-type="pmcaiid">12431521</article-id><article-id pub-id-type="doi">10.3390/s25175587</article-id><article-id pub-id-type="publisher-id">sensors-25-05587</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>CrackNet-Weather: An Effective Pavement Crack Detection Method Under Adverse Weather Conditions</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="W">Wei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-05587" ref-type="aff">1</xref><xref rid="c1-sensors-25-05587" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-8191-6391</contrib-id><name name-style="western"><surname>Yu</surname><given-names initials="X">Xiaoru</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><xref rid="af1-sensors-25-05587" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-1658-7887</contrib-id><name name-style="western"><surname>Jing</surname><given-names initials="B">Bin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-05587" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Tang</surname><given-names initials="Z">Ziqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-05587" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-3289-4212</contrib-id><name name-style="western"><surname>Zhang</surname><given-names initials="W">Wei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05587" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-7123-4401</contrib-id><name name-style="western"><surname>Wang</surname><given-names initials="S">Shengyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05587" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Xiao</surname><given-names initials="Y">Yao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af1-sensors-25-05587" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Li</surname><given-names initials="S">Shu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-25-05587" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Yang</surname><given-names initials="L">Liping</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05587" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Matikas</surname><given-names initials="TE">Theodore E.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05587"><label>1</label>College of Computer Science and Technology, Changchun University, No. 6543, Satellite Road, Changchun 130022, China</aff><aff id="af2-sensors-25-05587"><label>2</label>School of Construction Engineering, Jilin University, No. 2699, Qianjin Street, Changchun 130012, China</aff><author-notes><corresp id="c1-sensors-25-05587"><label>*</label>Correspondence: <email>wangwei@ccu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>07</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5587</elocation-id><history><date date-type="received"><day>29</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>27</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>06</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>07</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05587.pdf"/><abstract><p>Accurate pavement crack detection under adverse weather conditions is essential for road safety and effective pavement maintenance. However, factors such as reduced visibility, background noise, and irregular crack morphology make this task particularly challenging in real-world environments. To address these challenges, we propose CrackNet-Weather, which is a robust and efficient detection method that systematically incorporates three key modules: a Haar Wavelet Downsampling Block (HWDB) for enhanced frequency information preservation, a Strip Pooling Bottleneck Block (SPBB) for multi-scale and context-aware feature fusion, and a Dynamic Sampling Upsampling Block (DSUB) for content-adaptive spatial feature reconstruction. Extensive experiments conducted on a challenging dataset containing both rainy and snowy weather demonstrate that CrackNet-Weather significantly outperforms mainstream baseline models, achieving notable improvements in mean Average Precision, especially for low-contrast, fine, and irregular cracks. Furthermore, our method maintains a favorable balance between detection accuracy and computational complexity, making it well suited for practical road inspection and large-scale deployment. These results confirm the effectiveness and practicality of CrackNet-Weather in addressing the challenges of real-world pavement crack detection under adverse weather conditions.</p></abstract><kwd-group><kwd>pavement crack detection</kwd><kwd>CrackNet-Weather</kwd><kwd>Haar Wavelet Downsampling Block</kwd><kwd>Strip Pooling Bottleneck Block</kwd><kwd>Dynamic Sampling Upsampling Block</kwd></kwd-group><funding-group><award-group><funding-source>Intelligent Pavement Defect Detection System Development</funding-source><award-id>2024JBH05LU7</award-id></award-group><funding-statement>This work was supported by the horizontal research project &#8220;Intelligent Pavement Defect Detection System Development&#8221; (Grant No. 2024JBH05LU7).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05587"><title>1. Introduction</title><p>Pavement crack detection is essential for traffic safety and road infrastructure maintenance. With&#160;more than 64 million kilometers of roads worldwide [<xref rid="B1-sensors-25-05587" ref-type="bibr">1</xref>], pavement defects such as cracks and potholes are major contributors to vehicle damage and traffic accidents [<xref rid="B2-sensors-25-05587" ref-type="bibr">2</xref>]. According to the World Health Organization, road traffic crashes claim over 1.3 million lives each year, and&#160;adverse weather conditions&#8212;including rain and snow&#8212;can increase accident rates by up to 50% [<xref rid="B3-sensors-25-05587" ref-type="bibr">3</xref>]. These extreme weather events not only accelerate pavement deterioration but also significantly complicate crack detection, as&#160;visibility is reduced and road surfaces become obscured by precipitation [<xref rid="B4-sensors-25-05587" ref-type="bibr">4</xref>]. In&#160;such environments, rain streaks and snowflakes act as visual noise, masking surface textures and interfering with conventional image-based detection algorithms [<xref rid="B5-sensors-25-05587" ref-type="bibr">5</xref>]. Consequently, existing detection systems often struggle to reliably recognize cracks under adverse weather conditions [<xref rid="B6-sensors-25-05587" ref-type="bibr">6</xref>]. This underscores the urgent need for robust and adaptive crack detection methods that can maintain high accuracy and stability across a wide range of challenging weather&#160;scenarios.</p><p>Conventional approaches for identifying pavement cracks&#8212;such as manual surveys or basic image analysis&#8212;have long been the norm, but&#160;these methods are inefficient and easily influenced by human subjectivity and environmental changes [<xref rid="B7-sensors-25-05587" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05587" ref-type="bibr">8</xref>]. As&#160;technology has advanced, deep learning and computer vision have transformed the field with&#160;models like YOLO [<xref rid="B9-sensors-25-05587" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05587" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05587" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05587" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05587" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05587" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05587" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05587" ref-type="bibr">16</xref>] becoming popular for their speed and accuracy in detecting pavement damage. These models excel at handling large datasets and learning diverse visual patterns without handcrafted features [<xref rid="B17-sensors-25-05587" ref-type="bibr">17</xref>]. Despite their strengths, most current models are developed and validated using datasets captured in good weather [<xref rid="B18-sensors-25-05587" ref-type="bibr">18</xref>], leaving them vulnerable when applied to images from rainy or snowy conditions. Rain, snow, and&#160;lighting variations introduce complex visual disturbances, further reducing the reliability of crack detection in real-world scenarios [<xref rid="B19-sensors-25-05587" ref-type="bibr">19</xref>]. This gap underlines the need for robust detection solutions that remain effective regardless of the weather [<xref rid="B20-sensors-25-05587" ref-type="bibr">20</xref>].</p><p>Object detection constitutes a cornerstone of computer vision research and continues to evolve rapidly, which is propelled by the growing demand for high-precision and real-time automated inspection systems [<xref rid="B21-sensors-25-05587" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-05587" ref-type="bibr">22</xref>]. The&#160;YOLO (You Only Look Once) family of detectors [<xref rid="B23-sensors-25-05587" ref-type="bibr">23</xref>] has played a pivotal role in this evolution. Meanwhile, methods like EfficientDet [<xref rid="B24-sensors-25-05587" ref-type="bibr">24</xref>] have introduced compound scaling and attention-based modules to optimize detection efficiency, and&#160;transformer-based [<xref rid="B25-sensors-25-05587" ref-type="bibr">25</xref>] frameworks such as DETR [<xref rid="B26-sensors-25-05587" ref-type="bibr">26</xref>] have redefined detection paradigms through end-to-end architectures. In&#160;the specific context of pavement crack detection, these object detection models enable the rapid, accurate localization of surface defects across varied and challenging environments. Compared to segmentation-based approaches [<xref rid="B27-sensors-25-05587" ref-type="bibr">27</xref>], detection models provide superior inference speed and adaptability, greatly improving the practicality and reliability of large-scale road monitoring systems. Collectively, these methodological advances establish a strong technological foundation for robust crack detection, especially in scenarios requiring resilience to complex weather-induced visual disturbances [<xref rid="B28-sensors-25-05587" ref-type="bibr">28</xref>]. However, even with these advances, there remains a need for further architectural improvements to address the unique challenges posed by extreme weather in pavement crack&#160;detection.</p><p>The pavement crack detection task involves identifying several common types of pavement distress. In&#160;this study, the&#160;dataset [<xref rid="B29-sensors-25-05587" ref-type="bibr">29</xref>] is divided into four defect categories: class_0 (longitudinal cracks), class_1 (transverse cracks), class_2 (alligator cracks), and&#160;class_3 (potholes), which represent the most prevalent and safety-critical forms of road surface damage encountered in real-world scenarios. In&#160;practical road monitoring applications, extreme weather presents several key challenges to accurate crack detection, as&#160;illustrated in <xref rid="sensors-25-05587-f001" ref-type="fig">Figure 1</xref>. These challenges are summarized as follows: Low visibility refers to situations where rain, fog, or&#160;other adverse weather conditions blur or darken the scene, reducing the clarity and visibility of cracks. Low visibility often leads to missing or incomplete crack region detection, particularly for fine or shallow cracks [<xref rid="B30-sensors-25-05587" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05587" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05587" ref-type="bibr">32</xref>]. Background noise is characterized by the presence of extraneous visual elements such as raindrops, snowflakes, shadows, vehicles, or&#160;strong road markings. These background elements act as visual occluders, masking or confusing the true crack features, and&#160;they can result in false detections or missed cracks. Complex cracks refers to defects that are irregular, fragmented, or&#160;highly interconnected&#8212;such as alligator cracks. Their irregular shapes and spatial distribution increase the difficulty of both detection and accurate localization, especially under adverse environmental conditions. <xref rid="sensors-25-05587-f001" ref-type="fig">Figure 1</xref> provides representative examples of these challenging scenarios. The&#160;first group of images demonstrates occlusion and background noise caused by precipitation and other environmental factors; the second group illustrates the reduced contrast and blurred features in low-visibility conditions; and the third group shows the challenges associated with complex and fragmented crack patterns. The&#160;green bounding boxes in each image indicate the annotated defect regions. These challenges highlight the urgent need for robust pavement crack detection frameworks capable of addressing environmental interference and preserving detection accuracy in real-world, weather-perturbed&#160;environments.</p><p>To address the aforementioned challenges, this paper proposes an improved pavement crack detection method based on YOLOv12. The&#160;method systematically incorporates three key modules aimed at enhancing feature robustness, spatial detail preservation, and&#160;adaptability in adverse weather conditions. Specifically, we replace the standard downsampling convolutions in the backbone with the Haar Wavelet Downsampling Block (HWDB) [<xref rid="B33-sensors-25-05587" ref-type="bibr">33</xref>], which effectively preserves both low-frequency and high-frequency information during spatial reduction. In&#160;both the backbone and neck, all Cross-Stage Partial modules with 2 &#215; 2 convolutional kernels (C3K2) are replaced by the Strip Pooling Bottleneck Block (SPBB) [<xref rid="B34-sensors-25-05587" ref-type="bibr">34</xref>], which achieves multi-scale and directional context aggregation and improves the detection of elongated, fragmented, and&#160;low-contrast cracks. In&#160;the upsampling stage, the&#160;Dynamic Sampling Upsampling Block (DSUB) [<xref rid="B35-sensors-25-05587" ref-type="bibr">35</xref>] is introduced in place of conventional upsampling operators, enabling content-adaptive spatial feature reconstruction and enhanced boundary detail&#160;recovery.</p><p>The main contributions of this paper are summarized as follows:</p><p>(1) To address the scarcity of extreme weather samples, we selected 2600 images from the RDD2022 [<xref rid="B29-sensors-25-05587" ref-type="bibr">29</xref>] Japan dataset and applied simulation algorithms to generate realistic rain and snow conditions for each image. This process resulted in a diverse experimental dataset encompassing clear, rainy, and&#160;snowy scenarios, which provides a solid data foundation for robust crack detection research in complex and adverse&#160;environments.</p><p>(2) We propose a novel pavement crack detection method based on YOLOv12, integrating the HWDB, SPBB, and&#160;DSUB modules, which significantly enhances detection performance under complex and adverse weather&#160;conditions.</p><p>(3) To address the challenges of information loss and poor recognition of low-contrast cracks caused by precipitation and occlusion, we design the HWDB module, which effectively preserves detailed and edge information through Haar wavelet&#160;decomposition.</p><p>(4) To tackle the problems of elongated, fragmented, and&#160;directionally ambiguous cracks, the&#160;SPBB module is introduced in both the backbone and neck, leveraging strip pooling and multi-branch context aggregation to enhance the representation of directional and global&#160;features.</p><p>(5) To improve the recovery of fine details and crack boundaries in the upsampling stage, we construct the DSUB module, which enables a content-aware reconstruction of high-resolution feature maps via the adaptive prediction of spatial offsets and&#160;scaling.</p><p>(6) Extensive experiments on the aforementioned multi-scenario datasets as well as the Norwegian dataset of RDD2022 show that our approach achieves superior accuracy and robustness compared to mainstream baseline methods, especially under severe weather&#160;conditions.</p></sec><sec id="sec2-sensors-25-05587"><title>2. Related&#160;Work</title><sec id="sec2dot1-sensors-25-05587"><title>2.1. Recent Progress in Object&#160;Detection
</title><p>Modern object detection has been revolutionized by the rapid evolution of one-stage detectors, among&#160;which the YOLO (You Only Look Once) family stands out as the most prominent and widely adopted framework. Classic versions such as YOLOv5 and YOLOv8 have set benchmarks for balancing accuracy and inference speed, while the release of YOLOv10 [<xref rid="B13-sensors-25-05587" ref-type="bibr">13</xref>], YOLOv11 [<xref rid="B14-sensors-25-05587" ref-type="bibr">14</xref>],YOLOv12 [<xref rid="B15-sensors-25-05587" ref-type="bibr">15</xref>] and the latest YOLOv13 [<xref rid="B16-sensors-25-05587" ref-type="bibr">16</xref>] have further advanced the state of the art through innovations in network architecture, training strategies, and&#160;model scalability. In&#160;addition, transformer-based detectors such as DETR [<xref rid="B26-sensors-25-05587" ref-type="bibr">26</xref>] and RT-DETR [<xref rid="B36-sensors-25-05587" ref-type="bibr">36</xref>] have introduced new paradigms for end-to-end object detection, leveraging global attention mechanisms to improve robustness in complex visual environments. These models have been increasingly applied to road surface inspection tasks, enabling the efficient and accurate identification of various pavement defects, including longitudinal cracks, transverse cracks, alligator cracks, and&#160;potholes. In parallel, several effective architectural modules have been introduced to further enhance the performance and robustness of object detection models. For&#160;instance, Xu&#160;et&#160;al. [<xref rid="B33-sensors-25-05587" ref-type="bibr">33</xref>] designed a Haar wavelet-based downsampling module that better preserves spatial and boundary information, improving segmentation accuracy. Hou&#160;et&#160;al. [<xref rid="B34-sensors-25-05587" ref-type="bibr">34</xref>] proposed strip pooling, which is a&#160;lightweight spatial pooling method that efficiently captures long-range contextual information for scene parsing. Liu&#160;et&#160;al. [<xref rid="B35-sensors-25-05587" ref-type="bibr">35</xref>] presented DySample, which is an&#160;ultra-lightweight and efficient dynamic upsampling module based on content-aware point sampling, which achieves superior accuracy and computational efficiency across various visual tasks. Nevertheless, even with these advanced detectors, substantial challenges remain when operating under extreme weather conditions such as heavy rain or snow [<xref rid="B37-sensors-25-05587" ref-type="bibr">37</xref>]. Adverse environments introduce occlusions, visual artifacts, and&#160;reduced contrast, which hinder reliable feature extraction and increase the rates of false positives and missed detections [<xref rid="B38-sensors-25-05587" ref-type="bibr">38</xref>]. These limitations highlight the pressing need for enhanced detection frameworks that can maintain robustness and accuracy in real-world, weather-perturbed&#160;scenarios.</p></sec><sec id="sec2dot2-sensors-25-05587"><title>2.2. Similar&#160;Works</title><p>Many researchers have already undertaken similar work on solving the problem of pavement crack detection [<xref rid="B39-sensors-25-05587" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05587" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05587" ref-type="bibr">41</xref>]. Gavilan&#160;et&#160;al. [<xref rid="B42-sensors-25-05587" ref-type="bibr">42</xref>] propose an adaptive pavement crack detection framework that leverages pavement classification and multi-stage preprocessing to improve the reliability of automatic road distress assessment. The&#160;system utilizes an SVM-based classifier ensemble to tailor detection parameters to different pavement types, effectively reducing false positives from non-crack features and enhancing overall crack detection accuracy. Hu&#160;et&#160;al. [<xref rid="B43-sensors-25-05587" ref-type="bibr">43</xref>] employ YOLOv5-based deep learning models for automatic pavement crack detection, demonstrating superior accuracy and speed over traditional approaches. Their work highlights the effectiveness of deep learning for identifying cracks of varying severity, though&#160;challenges persist in more complex or visually degraded environments. Fan&#160;et&#160;al. [<xref rid="B44-sensors-25-05587" ref-type="bibr">44</xref>] propose a shadow-removal-oriented crack detection framework and introduce a dedicated shadow-crack dataset to address the interference of shadows in pavement crack detection. Their method effectively improves detection accuracy in the presence of shadows and varying brightness caused by seasonal and weather changes. Despite significant progress in deep learning-based pavement crack detection, robust and efficient detection under adverse weather conditions remains an open challenge. Existing models often experience degraded performance when dealing with low visibility, background noise, and&#160;complex crack morphologies. Motivated by these gaps, this paper presents CrackNet-Weather, a&#160;model that systematically enhances frequency information preservation, context-aware feature fusion, and&#160;content-adaptive upsampling, aiming to achieve robust pavement crack detection in rain, snow, and&#160;other challenging weather&#160;scenarios.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-05587"><title>3. Methodology</title><sec id="sec3dot1-sensors-25-05587"><title>3.1. Architecture of the Proposed&#160;Method</title><p>As illustrated in <xref rid="sensors-25-05587-f002" ref-type="fig">Figure 2</xref>a, we propose a substantially enhanced pavement crack detection framework, built upon YOLOv12 [<xref rid="B15-sensors-25-05587" ref-type="bibr">15</xref>], that is specifically tailored for the challenging conditions encountered in adverse weather scenarios. Recognizing that standard detection architectures often fail to maintain accuracy in the presence of reduced visibility, precipitation-induced noise, and&#160;irregular crack morphologies, we systematically redesign the backbone and upsampling stages of YOLOv12. By&#160;introducing specialized modules at these key locations, our approach significantly improves the network&#8217;s robustness and detection performance under real-world rainy and snowy&#160;environments.</p><p>In the backbone, we replace all standard convolutional downsampling layers with the Haar Wavelet Downsampling Block (HWDB) [<xref rid="B33-sensors-25-05587" ref-type="bibr">33</xref>]. Conventional downsampling operations, such as strided convolution or pooling, frequently result in the loss of crucial edge and texture details&#8212;a limitation that becomes even more severe in the presence of low-contrast or partially occluded regions caused by adverse weather. To&#160;address this challenge, the HWDB leverages Haar wavelet transformation to decompose feature maps into both low- and high-frequency components, enabling the network to capture subtle crack boundaries and fine structural cues that would otherwise be eliminated by traditional methods. This targeted integration markedly enhances the network&#8217;s capacity to preserve and propagate essential features, leading to the more reliable detection of faint, blurred, or&#160;partially obscured cracks in complex real-world&#160;conditions.</p><p>For multi-scale and contextual feature fusion, we replace all C3K2 modules with the Strip Pooling Bottleneck Block (SPBB) [<xref rid="B34-sensors-25-05587" ref-type="bibr">34</xref>]. While the original C3K2 modules are computationally efficient, they exhibit clear limitations in capturing long-range dependencies and directional characteristics&#8212;an ability that is critical for accurately detecting fragmented or elongated cracks, particularly when these cracks are partially obscured by precipitation such as raindrops or snowflakes. By&#160;adopting a dual-branch design that incorporates both strip pooling and global average pooling, the SPBB module enables the aggregation of local and global contextual features with&#160;specific emphasis on enhancing representations along horizontal and vertical orientations. This targeted enhancement substantially boosts the network&#8217;s ability to model directionality and maintain spatial continuity, thereby improving the detection and structural understanding of cracks under complex and visually challenging&#160;conditions.</p><p>In the upsampling stage, we replace all traditional upsampling operations, such as nearest neighbor or bilinear interpolation, with&#160;the Dynamic Sampling Upsampling Block (DSUB) [<xref rid="B35-sensors-25-05587" ref-type="bibr">35</xref>]. Conventional upsampling methods typically employ fixed, content-independent sampling strategies, which often result in blurred crack boundaries and the loss of fine structural details&#8212;issues that are particularly pronounced for small or low-contrast cracks under adverse weather. The DSUB module addresses these challenges by predicting adaptive offsets and scales for each spatial position, thus enabling content-aware dynamic sampling that accurately reconstructs feature details and preserves essential edge information. This targeted approach significantly enhances the recovery of spatial details during upsampling, yielding sharper crack boundaries and more precise localization, even in visually degraded or complex real-world&#160;scenarios.</p><p>Our improved network first extracts features using the HWDB-augmented backbone, then fuses multi-scale and directional features with SPBB modules, and&#160;finally restores spatial details with the DSUB module before outputting high-quality crack detection and classification results. Each structural innovation we design closely corresponds to a key challenge in crack detection under adverse weather conditions, ensuring both theoretical soundness and practical&#160;robustness.</p></sec><sec id="sec3dot2-sensors-25-05587"><title>3.2. Strip Pooling Bottleneck&#160;Block</title><p>In adverse weather conditions such as rain and snow, the&#160;visibility of pavement cracks is significantly reduced. Cracks often appear as elongated, fragmented, or&#160;low-contrast structures, sometimes partially occluded, which greatly increases the difficulty of automatic detection. The&#160;main challenges lie in the fact that conventional convolutional architectures have limited capacity for global context modeling and long-range dependency, making it difficult to accurately detect or recover cracks that are blurred or occluded. Moreover, cracks usually exhibit strong directional patterns (e.g., horizontal or vertical extension), while ordinary networks struggle to effectively capture such directional information. Therefore, designing an efficient feature extraction module that can aggregate multi-scale context, directional information, and&#160;global semantics is critical to improve crack detection performance under adverse&#160;weather.</p><p>To address these issues, we introduce the Strip Pooling Bottleneck Block (SPBB), as&#160;illustrated in <xref rid="sensors-25-05587-f002" ref-type="fig">Figure 2</xref>b. This block utilizes multi-branch aggregation and double residual connections to efficiently integrate global, local, and&#160;direction-aware features, significantly enhancing the robustness and representation power of the network for complex pavement crack scenarios. The&#160;detailed process is as follows:</p><p>Given the input feature <italic toggle="yes">X</italic>, a&#160;<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution is first applied to extract spatial structure information, resulting in feature <italic toggle="yes">F</italic>. Then, a&#160;<inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution is performed for channel adjustment and feature mixing, and&#160;the result is used as input to the upper and lower branches, which are denoted as <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD1-sensors-25-05587"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05587"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05587"><label>(3)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">X</italic> is the input feature map, <italic toggle="yes">F</italic> is the output after the <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denote <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution operations, and&#160;<inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are the inputs to the upper and lower branches, respectively.</p><p>In the upper branch, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is passed through three sub-paths. The&#160;first and third sub-branches perform global average pooling (GAP) followed by <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution, while the second sub-branch applies a <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution directly. The&#160;outputs are then concatenated along the channel dimension to produce <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD4-sensors-25-05587"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>GAP</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-05587"><label>(5)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05587"><label>(6)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>GAP</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05587"><label>(7)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>GAP</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes global average pooling, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are global context features, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the local detail feature, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Concat</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates channel concatenation, and&#160;<inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the output of the upper&#160;branch.</p><p>In the lower branch, <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is processed by horizontal and vertical strip pooling, i.e.,&#160;average pooling along the width and height, respectively, which are each followed by <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution. The&#160;resulting features are then concatenated to obtain <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD8-sensors-25-05587"><label>(8)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>AvgPool</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05587"><label>(9)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>AvgPool</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-05587"><label>(10)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>AvgPool</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>AvgPool</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denote average pooling along the height and width, respectively; <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the features aggregated in the horizontal and vertical directions, and&#160;<inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the output of the lower&#160;branch.</p><p>The outputs of the upper and lower branches are concatenated along the channel dimension to form <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which is then added to the output <italic toggle="yes">F</italic> of the first <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution to obtain the first residual output <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD11-sensors-25-05587"><label>(11)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD12-sensors-25-05587"><label>(12)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the concatenated multi-branch feature, <italic toggle="yes">F</italic> is the <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution output, and&#160;<inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the first residual&#160;output.</p><p>Finally, the&#160;first residual output <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is added to the original input <italic toggle="yes">X</italic> to obtain the final output <italic toggle="yes">Y</italic> of the SPB module:<disp-formula id="FD13-sensors-25-05587"><label>(13)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the output after the first residual connection, <italic toggle="yes">X</italic> is the input feature, and <italic toggle="yes">Y</italic> is the final output of the SPB&#160;module.</p><p>In summary, the&#160;SPBB module effectively integrates global context, local spatial information, and&#160;direction-aware features, significantly improving the network&#8217;s ability to detect elongated, fragmented, low-contrast, and&#160;strongly directional cracks. The&#160;dual residual connections ensure sufficient information flow and feature diversity throughout the module, providing a robust and expressive high-level semantic feature for subsequent detection heads. Overall, the&#160;proposed SPB module lays a solid foundation for crack detection in extreme environments and greatly enhances the model&#8217;s generalization capability in complex&#160;scenarios.</p></sec><sec id="sec3dot3-sensors-25-05587"><title>3.3. Haar Wavelet Downsampling Block (HWDB)</title><p>Conventional downsampling techniques, such as max or average pooling, inevitably result in the loss of critical spatial details and high-frequency information&#8212;an issue particularly detrimental for detecting small, faint, or&#160;low-contrast pavement cracks in adverse weather conditions such as rain and snow. Under&#160;these circumstances, cracks often exhibit blurred contours, weak intensity, and&#160;are prone to occlusion or distortion by environmental noise, as&#160;shown in <xref rid="sensors-25-05587-f002" ref-type="fig">Figure 2</xref>a. Preserving both global structure and fine-grained edge cues during downsampling is thus essential for robust and reliable crack detection in real-world outdoor&#160;scenes.</p><p>To address these challenges, we introduce an information-preserving the Haar Wavelet Downsampling Block (HWDB), as&#160;illustrated in <xref rid="sensors-25-05587-f002" ref-type="fig">Figure 2</xref>c. Rather than reducing spatial resolution through direct aggregation, the HWDB module leverages an orthogonal frequency decomposition to retain vital spatial information. Specifically, for&#160;each <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> spatial region in the input feature map <italic toggle="yes">X</italic>, the HWDB module computes one low-frequency and three high-frequency components according to<disp-formula id="FD14-sensors-25-05587"><label>(14)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the values of each <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> patch in <italic toggle="yes">X</italic>, <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the low-frequency (approximation) coefficient, and&#160;<inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent horizontal, vertical, and&#160;diagonal high-frequency components, respectively.</p><p>These four sub-bands, each retaining the spatial dimensions <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">C</italic> channels, are concatenated along the channel axis to form a frequency-aware feature representation:<disp-formula id="FD15-sensors-25-05587"><label>(15)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
with <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. To&#160;further promote adaptive feature fusion and dimensionality reduction, the HWDB module employs a <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution followed by batch normalization and a ReLU activation:<disp-formula id="FD16-sensors-25-05587"><label>(16)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>BN</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> projects <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channels back to <italic toggle="yes">C</italic>, <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>BN</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes batch normalization, and&#160;<inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>ReLU</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the activation function. The&#160;resulting <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> serves as the downsampled feature, preserving both global context and high-frequency edge cues vital for subsequent crack detection&#160;stages.</p><p>This orthogonal transform enables the HWDB module to capture and transmit spatial structure, boundary information, and&#160;textural details that would otherwise be lost in traditional downsampling. By&#160;systematically integrating the HWDB module into all downsampling layers of our detection backbone, the&#160;network gains the capacity to robustly localize and delineate cracks&#8212;even under heavy rain, snow, and&#160;low-visibility conditions&#8212;thereby providing a&#160;reliable and information-rich feature foundation tailored for adverse-weather pavement crack&#160;detection.</p></sec><sec id="sec3dot4-sensors-25-05587"><title>3.4. Dynamic Sampling Upsampling Block (DSUB)</title><p>Under complex conditions such as rain and snow, pavement cracks often exhibit elongated, fragmented, and&#160;low-contrast characteristics, making the spatial structural recovery of feature upsampling crucial for accurate detection. Traditional upsampling methods, such as bilinear interpolation, cannot adaptively adjust sampling locations based on content, leading to blurred edges and the loss of details. As&#160;shown in <xref rid="sensors-25-05587-f002" ref-type="fig">Figure 2</xref>d, to&#160;address this, we introduce the Dynamic Sampling Upsampling Block into our method, which implements content-adaptive dynamic upsampling and effectively enhances the recovery of structural and fine&#160;details.</p><p>Specifically, given an input feature map <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, we first use convolutional branches to predict the offset <italic toggle="yes">O</italic> and scale <italic toggle="yes">S</italic> for each spatial location as<disp-formula id="FD17-sensors-25-05587"><label>(17)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">O</italic> is the output of the offset branch with shape <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">S</italic> is the output of the scale branch with shape <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Then, the&#160;offset and scale are multiplied element-wise to obtain a content-adaptive scaled offset:<disp-formula id="FD18-sensors-25-05587"><label>(18)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>O</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msup><mml:mi>O</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> represents the adaptively scaled offset with the same shape as <italic toggle="yes">O</italic>.</p><p>Next, <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msup><mml:mi>O</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is added to the initial reference grid <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mtext>_</mml:mtext><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of the target space to produce the content-adaptive sampling points:<disp-formula id="FD19-sensors-25-05587"><label>(19)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>O</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mtext>_</mml:mtext><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mtext>_</mml:mtext><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the standard sampling positions for each pixel in the target space (typically generated by meshgrid and normalized to <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>The sampling points <italic toggle="yes">P</italic> are then reshaped to match the target high-resolution space. After&#160;that, the&#160;reshaped <italic toggle="yes">P</italic> is added to the standard coordinate grid <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> of the target space, yielding the final dynamic sampling coordinates:<disp-formula id="FD20-sensors-25-05587"><label>(20)</label><mml:math id="mm80" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the standard coordinate grid for the upsampled output space with&#160;shape <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Afterwards, the&#160;features are passed through a pixel shuffle operation to reorganize spatial and channel information:<disp-formula id="FD21-sensors-25-05587"><label>(21)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mtext>_</mml:mtext><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, the&#160;<inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mtext>_</mml:mtext><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> function is employed, using <italic toggle="yes">G</italic> as the sampling coordinates to perform dynamic content-aware sampling on the input features, thereby producing the final high-resolution output:<disp-formula id="FD22-sensors-25-05587"><label>(22)</label><mml:math id="mm85" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mtext>_</mml:mtext><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mtext>_</mml:mtext><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the pixel shuffle operation, <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the feature map after shuffling, and&#160;<inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the final output with shape <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>By following this content-adaptive dynamic sampling process, the&#160;introduced DySample module allows each upsampled pixel to extract features from the optimal spatial location, substantially enhancing the recovery of spatial details and edge information. This provides a robust feature foundation for high-resolution crack detection in complex&#160;environments.</p></sec></sec><sec id="sec4-sensors-25-05587"><title>4. Experiment</title><sec id="sec4dot1-sensors-25-05587"><title>4.1. Datasets and Evaluation&#160;Metrics</title><p>To comprehensively evaluate the robustness and generalization ability of our method in complex real-world environments, we construct three datasets based on the RDD2022 [<xref rid="B29-sensors-25-05587" ref-type="bibr">29</xref>] Japan subset. First, the&#160;original RDD2022 Japan dataset, which contains a variety of pavement types and multiple crack categories (such as longitudinal cracks, transverse cracks, alligator cracks, and&#160;potholes), collected under normal weather conditions, is used as the clear-weather baseline. In addition to the rain/snow-augmented Japan dataset, we introduce the Norway subset of RDD2022 as an independent test set to assess the generalization performance of our model. The&#160;Norway dataset was collected by the Norwegian Public Road Administration using high-resolution vehicle-mounted cameras and covers both expressways and county roads. Notably, the&#160;Norway images capture a&#160;wide range of post-rain, snow-covered, and&#160;overcast conditions, closely resembling the challenging real-world scenarios our method is designed to address. As&#160;highlighted in the original RDD2022 paper, the&#160;inclusion of the Norway subset enhances the dataset&#8217;s diversity and provides an excellent testbed for evaluating model robustness and transferability across geographical regions and adverse environmental conditions. <xref rid="sensors-25-05587-t001" ref-type="table">Table 1</xref> summarizes the key statistics of the two datasets used in this study, including the number of training and validation images and the number of annotated bounding boxes for each defect category. <xref rid="sensors-25-05587-f003" ref-type="fig">Figure 3</xref> shows representative image samples from each of the four major defect classes, providing a clear visualization of the differences in appearance and annotation&#160;style.</p><p>To simulate realistic rainy conditions, we design a procedural rain synthesis pipeline that overlays multiple layers of randomly distributed lines with varying lengths and angles onto the original images, simulating rain streaks of different intensity and direction. Each layer of rain streaks is further enhanced by applying motion blur along the main rain direction, resulting in a more natural dynamic effect. The&#160;rain layers are then fused with the original image using adjustable transparency to control the severity of the rain effect. By&#160;adjusting the relevant parameters, we are able to synthesize a wide range of rainfall scenarios, from&#160;light drizzle to heavy downpour, providing challenging samples for crack detection under various precipitation intensities. Finally, for&#160;snowy conditions, we employ a two-stage synthesis approach. First, we generate a transparent snowflake mask for each image, consisting of thousands of randomly distributed snow particles with variable shapes, sizes, transparency, and&#160;trailing effects, capturing the spatial layering and depth of real snowfall. The&#160;snow mask is then blended with the clear-weather image at a specified transparency level, resulting in realistic snow occlusion where cracks and pavement may be partially covered. All datasets are split into training and validation sets at a ratio of 10:3, ensuring a balanced distribution of crack categories and weather conditions and providing a solid foundation for the evaluation of our detection&#160;framework.</p><p>For evaluation, we adopt several standard metrics to assess the effectiveness of our method in pavement crack detection, including Precision, mAP@0.5, mAP@0.5:0.95, and&#160;category-wise average precision for each type of crack. Precision measures the proportion of correctly predicted crack instances among all positive detections, reflecting the ability of model to reduce false positives. In&#160;addition, we further used precision&#8211;recall (PR) curves to compare the performance of the baseline model and the proposed CrackNet-Weather at different IoU thresholds to better characterize the stability and recall performance in low-contrast scenarios and the fine crack detection of the model. mAP@0.5 is a widely used benchmark in object detection, while mAP@0.5:0.95 provides a comprehensive evaluation by averaging performance across multiple IoU thresholds. In&#160;addition to the overall results, we also report the average precision for each individual crack category, including longitudinal cracks, transverse cracks, alligator cracks, and&#160;potholes, to&#160;evaluate the detection accuracy for different defect types. We further report the number of model parameters to indicate model size and resource requirements, and&#160;we use GFLOPs to reflect computational complexity and efficiency. All experimental results are evaluated on the validation set to ensure a fair and objective&#160;comparison.</p></sec><sec id="sec4dot2-sensors-25-05587"><title>4.2. Implementation&#160;Details</title><p>All experiments were conducted using the Ultralytics YOLOv12 framework on a&#160;single NVIDIA RTX 3090 GPU.(NVIDIA Corporation, Santa Clara, CA, USA) During&#160;training, the&#160;model was trained for 220 epochs with a&#160;batch size of 32, and&#160;all input images were resized to 640 &#215; 640 pixels. The&#160;optimizer used was stochastic gradient descent (SGD) with the default learning rate as provided by the official implementation. To&#160;improve the generalization capability of the model, a&#160;range of data augmentation strategies were employed during training, including scale augmentation (with the scale factor set to 0.5), mosaic augmentation (set to 1.0), and&#160;copy-paste augmentation (set to 0.1); mixup augmentation was not used. Mixed precision training (AMP) was disabled to ensure stable convergence. The&#160;dataset was organized in COCO format, and&#160;the ratio of training set to validation set was 10:3. Unless&#160;otherwise specified, all other hyperparameters and training procedures followed the official Ultralytics YOLOv12&#160;implementation.</p></sec><sec id="sec4dot3-sensors-25-05587"><title>4.3. Ablation&#160;Studies</title><p>To systematically evaluate the effectiveness of each proposed module, we conducted ablation experiments based on the YOLOv12-s. We analyzed the impact of the HWDB [<xref rid="B33-sensors-25-05587" ref-type="bibr">33</xref>], SPBB [<xref rid="B34-sensors-25-05587" ref-type="bibr">34</xref>], and&#160;DSUB [<xref rid="B35-sensors-25-05587" ref-type="bibr">35</xref>] modules, both individually and in combination, on&#160;detection accuracy and model complexity. The&#160;experiments were performed under two adverse weather conditions (rain and snow) and covered four types of pavement cracks. All of the detection results reported in <xref rid="sensors-25-05587-t002" ref-type="table">Table 2</xref> are mAP@0.5:0.95 in percentage form. The&#160;corresponding computational complexity (FLOPs) and number of parameters for each configuration are listed in <xref rid="sensors-25-05587-t003" ref-type="table">Table 3</xref>.</p><p><bold>Effect of the Strip Pooling Bottleneck Block (SPBB)</bold>. After&#160;incorporating the Strip Pooling Bottleneck Block (SPBB), the&#160;overall detection accuracy (mAP) increases significantly: from 16.0 to 17.0 under rain and from 15.1 to 16.3 under snow. Notably, the&#160;performance for transverse and alligator cracks under rain is markedly enhanced with&#160;the mAP for transverse cracks increasing from 7.5 to 8.8 and for alligator cracks from 27.0 to 28.8. While accuracy improves, the&#160;FLOPs rise moderately from 21.5 G to 29.5 G, and&#160;the parameter count slightly decreases from 9.25 M to 9.10 M. This indicates that the SPBB module enhances the ability of model to capture directional and long-range contextual features with only a slight reduction in computational efficiency. Considering that real-world road monitoring often involves many elongated, directional, and&#160;occluded cracks, the SPBB module greatly improves the robustness of the model in complex and challenging&#160;environments.</p><p><bold>Effect of the Haar Wavelet Downsampling Block (HWDB)</bold>. The Haar Wavelet Downsampling Block (HWDB) also yields outstanding results with the&#160;mAP rising to 17.3 under rain and 16.2 under snow. The&#160;most significant gain appears in the detection of potholes, where the mAP increases from 16.0 to 18.1 under rain. Importantly, the HWDB module drastically reduces the number of model parameters (from 9.25 M to 8.19 M) and lowers FLOPs to 18.9 G, making the model more lightweight. This demonstrates that the HWDB module can effectively retain important structural information during downsampling while considerably reducing model complexity, enhancing the detection of low-contrast and weak-texture cracks. The HWDB module is thus well suited for resource-constrained or large-scale road monitoring&#160;applications.</p><p><bold>Effect of the Dynamic Sampling Upsampling Block (DSUB)</bold>. The Dynamic Sampling Upsampling Block (DSUB) primarily improves the recovery of fine-grained structural information, especially for transverse cracks, where the mAP under rain increases substantially from 7.5 to 10.4. The&#160;overall mAP reaches 16.9/15.9 (rain/snow). The&#160;introduction of the DSUB has a negligible effect on FLOPs (21.3 G) and does not increase the parameter count (9.25 M). This shows that the DSUB, by&#160;means of content-adaptive spatial feature reconstruction, greatly enhances the perception of crack boundaries and small objects in complex backgrounds for the model while maintaining practical inference speed. This capability is essential for the timely detection of early-stage pavement&#160;crack.</p><p>In conclusion, the&#160;systematic ablation study demonstrates that the proposed three modules not only lead to significant improvements in detection accuracy across different crack categories and adverse weather conditions but&#160;also maintain a well-balanced computational complexity. Specifically, the&#160;SPBB, HWDB, and&#160;DSUB modules each provide targeted structural enhancements for challenging crack features such as occlusions, low contrast, and&#160;fine or fragmented patterns that frequently occur in real-world road environments. Regardless of the type&#8212;longitudinal, transverse, alligator, or&#160;potholes&#8212;these innovative modules show strong adaptability and robustness, meeting the dual demands for complex scene recognition and efficient inference in practical road monitoring. Importantly, the&#160;overall model maintains a reasonable parameter size and FLOPs while achieving improved accuracy, which is beneficial for deployment on edge devices or in resource-constrained engineering scenarios. Taken together, the&#160;experimental results confirm that the improved architecture proposed in this study offers a more reliable and efficient solution for pavement crack detection under adverse weather and complex scene&#160;conditions.</p></sec><sec id="sec4dot4-sensors-25-05587"><title>4.4. Comparisons</title><p>To further validate the effectiveness and generalization capability of our proposed method in the field of pavement crack detection, we conducted comparisons with several representative mainstream object detection models, including YOLOv8-s, YOLOv10-s, YOLOv11-s, YOLOv12-s, and&#160;YOLOv13-s. All models were trained and evaluated on the same rainy and snowy pavement crack detection dataset. The&#160;experimental results are presented in <xref rid="sensors-25-05587-t004" ref-type="table">Table 4</xref>, where mAP@0.5:0.95, FLOPs, and&#160;the number of parameters comprehensively reflect the detection ability and model&#160;complexity.</p><p>As shown in <xref rid="sensors-25-05587-t004" ref-type="table">Table 4</xref>, our method achieves the best overall detection performance under both rainy and snowy conditions with&#160;an average mAP of 17.8 (rain) and 16.8 (snow). Compared to common baselines such as YOLOv8-s and YOLOv12-s, our method improves the mAP by 0.9/0.8 and 1.8/1.7 points (rain/snow), respectively. For&#160;the challenging alligator cracks and potholes categories, which are difficult due to their complex shapes and low contrast, our method achieves mAP values of 29.7/29.3 and 19.1/17.0, respectively, demonstrating a more pronounced advantage over other&#160;models.</p><p>In terms of computational complexity, although&#160;the FLOPs (26.9 G) of our method are slightly higher than those of YOLOv8-s and YOLOv12-s, the&#160;parameter count is only 8.05 M, which is the&#160;lowest among all of the compared models. Considering the significant improvement in detection accuracy achieved by our method, this moderate increase in FLOPs is acceptable. Overall, our method can effectively improve the detection accuracy of pavement cracks under adverse weather and complex pavement conditions while maintaining reasonable model complexity. The&#160;model is suitable for road inspection scenarios with high accuracy requirements and also takes into account inference speed and resource consumption, which offers clear advantages for addressing challenges such as environmental interference and hard-to-detect targets in real-world road monitoring. These results further demonstrate the practicality and application potential of our method in the field of intelligent pavement crack&#160;detection.</p><p>To further verify the generalization capability of the proposed method, we conducted additional experiments on the Norway pavement defect dataset. The&#160;comparison results with mainstream detection models are summarized in <xref rid="sensors-25-05587-t005" ref-type="table">Table 5</xref>. Our method (CrackNet) achieves an AP <sub>50:95</sub> of 9.22% and an AP<sub>50</sub> of 24.24%, which outperforms YOLOv8-s, YOLOv11-s, YOLOv12-s, and&#160;YOLOv13-s, and&#160;these results are comparable to or better than the two-stage models Faster R-CNN and Cascade. In&#160;particular, CrackNet surpasses YOLOv12-s by 1.83 percentage points in AP<sub>50:95</sub> and 5.42 percentage points in AP<sub>50</sub>, demonstrating superior generalization performance on an unseen&#160;dataset.</p><p>Meanwhile, CrackNet maintains a lower computational complexity with only 26.91 GFLOPs and 8.05 M parameters, which is significantly less than Faster R-CNN (151.31 GFLOPs, 28.30 M) and Cascade (231.00 GFLOPs, 69.16 M). These results indicate that our method achieves a good trade-off between detection accuracy and computational cost, and&#160;it possesses strong generalization ability across different datasets and&#160;scenarios.</p><p>To provide a more comprehensive evaluation, we further examined the performance of CrackNet-Weather through precision&#8211;recall (PR) curves under different IoU thresholds. Specifically, we compared the baseline model with our proposed model across two thresholds and two datasets (rainy and snowy conditions) in&#160;order to highlight their differences in precision&#8211;recall&#160;trade-offs.</p><p>When the IoU threshold is 0.5, as&#160;shown in <xref rid="sensors-25-05587-f004" ref-type="fig">Figure 4</xref>, CrackNet-Weather shows clear improvements in recall across both rainy and snowy datasets. The&#160;PR curves consistently shift toward the top-right region compared with YOLOv12, which indicates that the model can capture a larger proportion of positive instances while sustaining comparable precision. Importantly, the&#160;gains are most evident for longitudinal and transverse cracks, which are more likely to appear faint and fragmented in rainy or snowy conditions. This suggests that the proposed model is more capable of suppressing missed detections for low-contrast crack types, which is a&#160;factor that directly enhances safety&#160;relevance.</p><p>When the IoU threshold is 0.5&#8211;0.95, as&#160;shown in <xref rid="sensors-25-05587-f005" ref-type="fig">Figure 5</xref>, the&#160;evaluation emphasizes not only whether the crack is detected but also whether the crack is accurately localized. Under&#160;this stricter criterion, CrackNet-Weather still maintains a larger area under the PR curve compared to YOLOv12 in most categories. This advantage is particularly evident in cracks and potholes, which have irregular shapes and blurred edges in inclement weather, making accurate localization difficult. CrackNet-Weather&#8217;s superior stability under these conditions highlights its strong feature representation and localization capabilities, ensuring reliable detection even under challenging alignment&#160;requirements.</p><p>The dual-level PR analysis confirms that CrackNet-Weather provides benefits beyond higher mAP scores. It improves the recall of fine and low-contrast cracks at lower IoU thresholds, reducing the likelihood of critical missed detections. At&#160;the same time, it sustains robust localization accuracy at stricter thresholds, reflecting a stronger generalization ability under diverse weather&#160;scenarios.</p></sec><sec id="sec4dot5-sensors-25-05587"><title>4.5. Model Robustness&#160;Verification</title><p>To rigorously evaluate the robustness and generalization ability of CrackNet-Weather, we constructed two balanced test subsets: a Normal Weather Dataset (NWD) and an Extreme Weather Dataset (EWD). Specifically, we randomly selected 2000 images from the original Japan dataset to represent the normal weather scenario, and&#160;then we selected another 2000 images from our synthesized snow datasets to represent extreme weather conditions. In&#160;both subsets, we ensured that the distribution of defect types was balanced, so that each major defect category is equally represented. This design guarantees an objective and fair comparison of model performance across different environmental&#160;conditions.</p><p>As illustrated in <xref rid="sensors-25-05587-f006" ref-type="fig">Figure 6</xref>, in&#160;addition to the comparison under normal and extreme weather subsets, we further verified the robustness of the models under parameterized visual degradations. Specifically, three levels of localized Gaussian blur (radius = 1, 1.5, 2) were applied to crack regions to simulate the partial blurring of pavement defects under rainy or snowy conditions, while brightness factors of 0.6, 1.0, and&#160;1.4 were used to emulate dark, normal, and&#160;bright illumination scenarios. It is worth noting that both the blur and brightness variations were applied on top of the normal rain subset to simulate additional imaging degradations. Furthermore, light and heavy rain/snow conditions were synthesized using our augmentation algorithms (Algorithms&#160;1 and 2) with explicit parameter settings: light rain (<italic toggle="yes">Nlayers</italic> = 1, <italic toggle="yes">Nlines</italic> = 400, <italic toggle="yes">kmb</italic> = 5, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.15), heavy rain (<italic toggle="yes">Nlayers</italic> = 3, <italic toggle="yes">Nlines</italic> = 1200, <italic toggle="yes">kmb</italic> = 9, <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.25), light snow (<italic toggle="yes">Nsnow</italic> = 3k, <italic toggle="yes">size</italic> = [1, 3], <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = 0.15, streak = 20%), and&#160;heavy snow (<italic toggle="yes">Nsnow</italic> = 10k, <italic toggle="yes">size</italic> = [3, 6], <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = 0.25, streak = 50%). These controlled perturbations enable a systematic and reproducible evaluation of model robustness across diverse degradation scenarios, ensuring that the observed performance differences are not limited to a single weather condition but generalize across a spectrum of visual&#160;interferences.</p><p><xref rid="sensors-25-05587-t006" ref-type="table">Table 6</xref> presents the performance of both models across different weather conditions. Under&#160;normal weather conditions (NWD), the&#160;baseline model YOLOv12 achieved an AP<sub>50&#8211;95</sub> value of 18.0% and an AP<sub>50</sub> value of 40.4%. In&#160;comparison, the&#160;proposed CrackNet achieved AP<sub>50&#8211;95</sub> and AP<sub>50</sub> values of 19.2% and 43.2%, respectively, indicating improved detection capability under ideal conditions. However, under&#160;extreme weather conditions (EWD), CrackNet demonstrated significant robustness advantages. Specifically, CrackNet achieved an AP<sub>50&#8211;95</sub> value of 16.8%, representing a 1.7% improvement over the baseline model. For&#160;the AP<sub>50</sub> metric, CrackNet reached 38.3%, outperforming the baseline model&#8217;s 34.3%. These results indicate that the proposed method can better handle visual interference and feature degradation caused by adverse weather such as rain and snow, and&#160;they demonstrate its potential for deployment in challenging real-world scenarios that require reliable pavement defect detection and automated road&#160;inspection.</p><p><xref rid="sensors-25-05587-t007" ref-type="table">Table 7</xref> summarizes CrackNet&#8217;s performance under a spectrum of weather- and imaging-induced degradations. A&#160;consistent trend emerges across conditions: as the severity of the degradation increases, detection performance decreases. In&#160;particular, heavier rain/snow introduces denser streaks and occlusions that suppress edge contrast along crack boundaries, leading to larger drops in both <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mrow><mml:mo>&#8211;</mml:mo></mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. For&#160;localized Gaussian blur applied on the Normal (Rain) subset, the&#160;metrics degrade monotonically with the blur radius, reflecting the model&#8217;s sensitivity to a loss of high-frequency cues that are critical for accurate crack localization. The brightness variation further shows that under-exposure (darker scenes) is more detrimental than over-exposure, which is consistent with the reduction of signal-to-noise ratio and diminished crack&#8211;background separability under low&#160;illumination.</p><p>Notably, <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mrow><mml:mo>&#8211;</mml:mo></mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> exhibits larger relative drops than <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> across all adverse settings, indicating that degradations affect precise localization at higher IoU thresholds more than coarse detection at 0.5 IoU. Despite these challenges, CrackNet maintains stable performance trends across diverse perturbations and preserves competitive <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msub><mml:mi>AP</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> values, evidencing the robustness of its feature representations and decision head. The&#160;consistent ranking of difficulty (normal &gt; light &gt; heavy; small <italic toggle="yes">r</italic> &gt; large <italic toggle="yes">r</italic>; normal/bright &gt; dark) and the controlled, parameterized construction of the subsets further suggest that the model&#8217;s behavior generalizes systematically rather than overfitting to a particular weather type. Taken together, the&#160;results indicate that CrackNet retains reliable detection capacity across parameterized adverse-weather and imaging degradations, supporting its use in non-ideal, real-world pavement&#160;inspection.</p><p>To quantitatively evaluate robustness under different weather severities and imaging degradations, we conducted graded tests on <italic toggle="yes">light/normal/heavy</italic> rain and snow, and&#160;we further tested under <italic toggle="yes">partial Gaussian blur</italic> and <italic toggle="yes">brightness variation</italic> (under-/over-exposure). <xref rid="sensors-25-05587-t006" ref-type="table">Table 6</xref> reports the AP<sub>50:95</sub> and AP<sub>50</sub> values on the RDD2022-Japan&#160;subset.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold> Pseudocode for Procedural Snowfall Augmentation</td></tr><tr><td align="left" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Input:</bold><break/><list list-type="bullet"><list-item><p>Original image <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Image directory <italic toggle="yes">D</italic> (for batch processing)</p></list-item><list-item><p>Number of snowflakes <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Mask transparency <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Snow particle size range <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Snow mask output path <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item></list><bold>Output:</bold><break/><list list-type="bullet"><list-item><p>Snow-augmented image <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item></list><bold>Step 1: Generate Snow Mask</bold><break/><list list-type="simple"><list-item><label>1.</label><p>Create an empty RGBA mask <italic toggle="yes">M</italic> with the same size as <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>2.</label><p>For <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: <list list-type="bullet"><list-item><p>Randomly sample position <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the image.</p></list-item><list-item><p>Randomly select particle type: grain or streak.</p></list-item><list-item><p>If&#160;grain: <list list-type="simple"><list-item><label>&#8211;</label><p>Sample radius <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>&#8211;</label><p>Sample alpha <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>&#8211;</label><p>Draw an irregular filled polygon at <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on <italic toggle="yes">M</italic>.</p></list-item></list></p></list-item><list-item><p>If&#160;streak: <list list-type="simple"><list-item><label>&#8211;</label><p>Sample length <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>2.5</mml:mn><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>7.0</mml:mn><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, width <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>1.4</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>&#8211;</label><p>Sample angle <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#960;</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>&#960;</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>&#8211;</label><p>Sample alpha <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><label>&#8211;</label><p>Draw a line at <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of length <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, width <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, angle <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on <italic toggle="yes">M</italic>.</p></list-item></list></p></list-item></list></p></list-item><list-item><label>3.</label><p>Apply Gaussian blur (e.g., <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) to <italic toggle="yes">M</italic> for realism.</p></list-item><list-item><label>4.</label><p>Save <italic toggle="yes">M</italic> as a snow mask to <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></list-item></list><bold>Step 2: Blend Snow Mask with Original Image</bold><break/><list list-type="simple"><list-item><label>1.</label><p>For each image <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in directory <italic toggle="yes">D</italic>: <list list-type="bullet"><list-item><p>Resize <italic toggle="yes">M</italic> to match <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> if needed.</p></list-item><list-item><p>For each pixel <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>: <list list-type="simple"><list-item><label>&#8211;</label><p>Compute blended pixel:</p><p><inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>G</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></p></list-item><list-item><p>Save <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p></list-item><list-item><label>2.</label><p>Output the snow-augmented images.</p></list-item></list></td></tr></tbody></array></p><array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2</bold> Pseudocode for Procedural Rainfall Augmentation</td></tr><tr><td align="left" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Input:</bold>
<break/>
<list list-type="bullet"><list-item><p>Original image <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Image directory <italic toggle="yes">D</italic> (for batch processing)</p></list-item><list-item><p>Number of rain layers <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Number of rain lines per layer <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Rain line length range <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Rain line angle mean <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and std <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Rain line thickness range <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Rain line brightness range <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Motion blur kernel size <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Transparency (per-layer alpha) range <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Output path <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item></list>
<bold>Output:</bold>
<break/>
<list list-type="bullet"><list-item><p>Rain-augmented image <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item></list>
<bold>Step 1: Generate and Overlay Rain Layers</bold>
<break/>
<list list-type="simple"><list-item><label>1.</label><p>Initialize <inline-formula><mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>2.</label><p><bold>For</bold>&#160;<inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: <list list-type="bullet"><list-item><p>Create empty mask <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with same size as <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><bold>For</bold>&#160;<inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: <list list-type="simple"><list-item><label>&#8211;</label><p>Randomly sample start point <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#8211;</label><p>Randomly sample length <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#8211;</label><p>Randomly sample angle <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#963;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#8211;</label><p>Compute end point <inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by <inline-formula><mml:math id="mm169" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm170" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#8211;</label><p>Randomly sample thickness <inline-formula><mml:math id="mm171" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#8211;</label><p>Randomly sample brightness <inline-formula><mml:math id="mm172" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#8211;</label><p>Draw line from <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm174" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with width <inline-formula><mml:math id="mm175" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, brightness <inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on <inline-formula><mml:math id="mm177" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item></list></p></list-item><list-item><p>Apply motion blur with kernel size <inline-formula><mml:math id="mm178" overflow="scroll"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> along <inline-formula><mml:math id="mm179" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm180" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Randomly sample transparency <inline-formula><mml:math id="mm181" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p>Blend <inline-formula><mml:math id="mm182" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> onto <inline-formula><mml:math id="mm183" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:</p><p><inline-formula><mml:math id="mm184" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item></list></p></list-item></list>
<bold>Step 2: Batch Processing and Output</bold>
<break/>
<list list-type="simple"><list-item><label>1.</label><p>Repeat the above for each <inline-formula><mml:math id="mm185" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in directory <italic toggle="yes">D</italic></p></list-item><list-item><label>2.</label><p>Save <inline-formula><mml:math id="mm186" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm187" overflow="scroll"><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>3.</label><p>Output the rain-augmented images</p></list-item></list>
</td></tr></tbody></array><p>To further characterize stability, we summarize each factor (rain, snow, blur, brightness) with two severity-aware metrics:<disp-formula id="FD23-sensors-25-05587"><label>(23)</label><mml:math id="mm188" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Normalized</mml:mi><mml:mspace width="4.pt"/><mml:mi>Drop</mml:mi><mml:mspace width="0.277778em"/><mml:mo>=</mml:mo><mml:mspace width="0.277778em"/><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Light</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Heavy</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Light</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD24-sensors-25-05587"><label>(24)</label><mml:math id="mm189" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Severity</mml:mi><mml:mspace width="4.pt"/><mml:mi>Slope</mml:mi><mml:mspace width="0.277778em"/><mml:mo>=</mml:mo><mml:mspace width="0.277778em"/><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Heavy</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Light</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mspace width="1.em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>pp</mml:mi><mml:mo>/</mml:mo><mml:mi>level</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm190" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Light</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm191" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Normal</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm192" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Heavy</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote the overall AP<sub>50:95</sub> measured on the Light, Normal, and Heavy subsets of the same factor; &#8220;pp&#8221; denotes percentage points. For brightness, severities are ordered as <italic toggle="yes">over-exposed (Light)</italic> &#8594; <italic toggle="yes">normal (Normal)</italic> &#8594; <italic toggle="yes">under-exposed (Heavy)</italic>.</p><p><xref rid="sensors-25-05587-t008" ref-type="table">Table 8</xref> condenses the severity-wise behavior of our detector using AP<sub>50:95</sub>. Across all factors, the <italic toggle="yes">Heavy</italic>-level AP remains a large fraction of the <italic toggle="yes">Light</italic>-level AP&#8212;85.5% for rain, 82.8% for snow, 90.7% for blur, and 87.7% for brightness&#8212;placing the worst-case loss within a narrow band (Normalized Drop 9.26&#8211;17.23%). The <italic toggle="yes">per-level</italic> degradation is also modest: the Severity Slope lies between <inline-formula><mml:math id="mm195" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>0.78</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm196" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1.49</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pp/level with <italic toggle="yes">snow</italic> being the hardest (17.23%, <inline-formula><mml:math id="mm197" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1.49</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) and <italic toggle="yes">blur</italic> the mildest (9.26%, <inline-formula><mml:math id="mm198" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>0.78</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>); <italic toggle="yes">rain</italic> sits in between (14.51%, <inline-formula><mml:math id="mm199" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1.335</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), while <italic toggle="yes">brightness</italic> is moderate (12.29%, which is <inline-formula><mml:math id="mm200" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1.045</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), consistent with the noted asymmetry that under-exposure harms more than over-exposure. Taken together&#8212;<italic toggle="yes">bounded worst-case loss</italic> and <italic toggle="yes">small, nearly constant per-level loss</italic>&#8212;these numbers show that accuracy degrades <italic toggle="yes">gradually and predictably</italic> as conditions worsen; i.e., the model is stable under graded adverse weather and imaging degradations.</p></sec></sec><sec id="sec5-sensors-25-05587"><title>5. Visualization</title><p>To further demonstrate the advantages of the proposed method, <xref rid="sensors-25-05587-f007" ref-type="fig">Figure 7</xref> shows a qualitative comparison of detection results under challenging weather conditions. In each example, the left image displays the predictions of the baseline YOLOv12 model, while the right image shows those of CrackNet-Weather on the same scene. Green bounding boxes denote pavement defects that are correctly detected by each model. Red bounding boxes indicate ground-truth defects that YOLOv12 failed to detect but were successfully identified by CrackNet-Weather.</p><p>As can be seen, CrackNet-Weather is able to accurately detect subtle, low-contrast, or partially occluded defects that are missed by YOLOv12, particularly under severe rain and snow. This qualitative comparison highlights the superior robustness and discriminative power of CrackNet-Weather for challenging real-world environments.</p></sec><sec sec-type="discussion" id="sec6-sensors-25-05587"><title>6. Discussion</title><p>In addition to the overall quantitative results, we further analyze representative error cases to better understand the model&#8217;s limitations under adverse weather conditions. <xref rid="sensors-25-05587-f005" ref-type="fig">Figure 5</xref> presents three typical examples: class confusion, false positives, and missed detections.</p><p>As shown in the left column of <xref rid="sensors-25-05587-f008" ref-type="fig">Figure 8</xref>, the model sometimes misclassifies potholes (class_3) as alligator cracks (class_2). This confusion is particularly common when potholes are partially filled with water, snow, or debris, which can alter their boundary shapes and visual textures. In such cases, the edge of the pothole becomes fragmented or network-like, resembling the patterns of alligator cracks. Adverse weather further exacerbates this issue by blurring boundaries and reducing feature distinctiveness. This phenomenon suggests that incorporating more discriminative features or additional context cues may help mitigate class confusion in future work.</p><p>The middle column of <xref rid="sensors-25-05587-f008" ref-type="fig">Figure 8</xref> presents a typical false positive case. In the ground truth, only a single longitudinal crack is annotated. However, the model incorrectly detects an additional defect, identifying a region of water accumulation on the road surface as a pothole (class_3). This type of misclassification frequently occurs under rainy conditions, where surface water can form reflective or irregular patterns that differ in appearance from the surrounding pavement. Factors such as light refraction, surface roughness, and varying water depth can create localized dark or distorted regions that visually resemble the typical signatures of potholes. To mitigate this issue, future work could incorporate multi-frame temporal cues, spectral information, or additional sensor modalities (such as polarization imaging or LiDAR) to more reliably differentiate between water accumulation and real structural defects.</p><p>The right column of <xref rid="sensors-25-05587-f008" ref-type="fig">Figure 8</xref> shows a missed detection, where an obvious longitudinal crack (class_0) is present in the ground truth but not detected by the model. Missed detections often occur in scenes with extremely low contrast or severe occlusion due to snow cover or wet road surfaces. In these conditions, the intensity difference between cracks and the surrounding pavement is diminished, and crack boundaries are partially hidden, making it challenging for the model to extract reliable features. Future efforts could explore multi-modal sensing (e.g., combining RGB with infrared or LiDAR data) or improved attention mechanisms to enhance the detection of subtle or occluded cracks.</p><p>Despite these promising results, CrackNet-Weather still exhibits certain limitations and areas for further improvement, as evidenced by the representative misclassification cases analyzed above. We refer to the following limitations specifically:</p><p>1. While the model shows robust performance overall, misclassification and missed detection still occur in challenging scenarios, particularly when environmental artifacts such as water accumulation or snow cover visually resemble true pavement defects, or when cracks are faint and difficult to distinguish under low-contrast or occluded conditions. These issues highlight the need for more advanced feature extraction, context modeling, and possibly the integration of multi-modal sensor information (e.g., infrared or LiDAR) to better distinguish between real defects and environmental noise.</p><p>2. The current training dataset is limited in scale and diversity. Future efforts should focus on further enriching the dataset to cover a wider range of regions, pavement types, and weather conditions, thereby improving the generalization and adaptability of the model.</p><p>3. Although CrackNet-Weather maintains a balance between detection accuracy and computational efficiency, there is still potential for further optimization in model size and inference speed. Approaches such as network pruning, quantization, and lightweight architecture design could facilitate real-time deployment on edge and mobile devices.</p><p>4. Another promising future direction is to incorporate semantic segmentation methods after accurate crack detection in order to isolate crack pixels and estimate crack widths. This would enable the identification of fine and narrow cracks that are difficult to capture with detection-based methods alone. Such an extension could greatly benefit the early detection of pavement defects and provide more detailed structural information for preventive road maintenance.</p><p>5. The current study relies solely on RGB imagery, which limits the ability to distinguish visually similar artefacts (e.g., water accumulation versus potholes) and to detect faint cracks under low-contrast conditions. Incorporating additional sensing modalities such as thermal infrared or depth could provide complementary cues, which represents an important direction for future work.</p><p>6. This study does not include inference speed or latency tests on embedded/edge hardware platforms (e.g., NVIDIA Jetson). While the proposed method achieves favorable complexity&#8211;accuracy trade-offs in simulation, its deployment performance on resource-constrained devices remains to be systematically evaluated in future work.</p><p>In summary, CrackNet-Weather offers an innovative and practical solution for pavement crack detection under adverse weather and complex pavement conditions. Nevertheless, ongoing optimization in model generalization, adaptability to extreme environments, and engineering deployment will be necessary to fully realize its potential in real-world applications. With continued advances in dataset diversity, model architecture, and multi-modal fusion, the proposed framework holds significant promise for large-scale intelligent road inspection, infrastructure health monitoring, and smart city development.</p></sec><sec sec-type="conclusions" id="sec7-sensors-25-05587"><title>7. Conclusions</title><p>In this paper, we proposed CrackNet-Weather, which is a robust and efficient pavement crack detection framework specifically designed to address the challenges posed by adverse weather conditions such as rain and snow. By systematically integrating the Haar Wavelet Downsampling Block (HWDB), Strip Pooling Bottleneck Block (SPBB), and Dynamic Sampling Upsampling Block (DSUB) into the YOLOv12 backbone, the proposed method effectively enhances the extraction and fusion of frequency, contextual, and structural information, significantly improving the detection accuracy of low-contrast, fine, and complex cracks.</p><p>Extensive experiments conducted on a diverse and challenging dataset&#8212;including both clear and weather-augmented images as well as a cross-domain test on the Norway dataset&#8212;demonstrate that CrackNet-Weather outperforms mainstream baseline models. The proposed framework not only achieves a higher mean Average Precision for all major defect categories but also maintains a favorable balance between detection accuracy, model size, and computational complexity. These results validate the effectiveness and practicality of CrackNet-Weather for real-world road inspection and large-scale deployment.</p><p>Despite these achievements, several issues remain for future research. In particular, expanding the dataset to cover more geographic regions and additional weather scenarios, as well as further optimizing the model for lightweight deployment, will be critical to improving generalization and adaptability. Incorporating multi-modal sensor information and exploring advanced feature enhancement strategies are also promising directions to further address challenging cases of misclassification and missed detection.</p><p>Overall, CrackNet-Weather provides an effective and scalable solution for pavement crack detection under adverse weather, holding substantial potential for application in intelligent transportation infrastructure monitoring and automated road maintenance systems.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, W.W.; methodology, X.Y.; software, B.J.; validation, Z.T.; formal analysis, W.Z.; investigation, S.W.; resources, W.W.; data curation, Y.X.; writing&#8212;original draft preparation, X.Y.; writing&#8212;review and editing, W.W.; visualization, S.L.; supervision, L.Y.; project administration, L.Y.; funding acquisition, W.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The dataset used in this study, RDD2022, is publicly available at the following GitHub repository: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/sekilab/RoadDamageDetector">https://github.com/sekilab/RoadDamageDetector</uri> (accessed on 1 June 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05587"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Fichera</surname><given-names>S.</given-names></name><name name-style="western"><surname>Paoletti</surname><given-names>P.</given-names></name><name name-style="western"><surname>Layzell</surname><given-names>L.</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>D.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>S.</given-names></name></person-group><article-title>Road surface defect detection&#8212;From image-based to non-image-based: A survey</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2024</year><volume>25</volume><fpage>10581</fpage><lpage>10603</lpage><pub-id pub-id-type="doi">10.1109/TITS.2024.3382837</pub-id></element-citation></ref><ref id="B2-sensors-25-05587"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>World Health Organization</collab></person-group><source>Global Status Report on Road Safety 2023: Summary</source><publisher-name>World Health Organization</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2023</year></element-citation></ref><ref id="B3-sensors-25-05587"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Theofilatos</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yannis</surname><given-names>G.</given-names></name></person-group><article-title>A review of the effect of traffic and weather characteristics on road safety</article-title><source>Accid. Anal. Prev.</source><year>2014</year><volume>72</volume><fpage>244</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.aap.2014.06.017</pub-id><pub-id pub-id-type="pmid">25086442</pub-id></element-citation></ref><ref id="B4-sensors-25-05587"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>W.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Shu</surname><given-names>J.</given-names></name></person-group><article-title>Crack detection and quantification for concrete structures using UAV and transformer</article-title><source>Autom. Constr.</source><year>2023</year><volume>152</volume><fpage>104929</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2023.104929</pub-id></element-citation></ref><ref id="B5-sensors-25-05587"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Majidifard</surname><given-names>H.</given-names></name><name name-style="western"><surname>Adu-Gyamfi</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Buttlar</surname><given-names>W.G.</given-names></name></person-group><article-title>Deep machine learning approach to develop a new asphalt pavement condition index</article-title><source>Constr. Build. Mater.</source><year>2020</year><volume>247</volume><fpage>118513</fpage><pub-id pub-id-type="doi">10.1016/j.conbuildmat.2020.118513</pub-id></element-citation></ref><ref id="B6-sensors-25-05587"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamishebahar</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Guan</surname><given-names>H.</given-names></name><name name-style="western"><surname>So</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jo</surname><given-names>J.</given-names></name></person-group><article-title>A comprehensive review of deep learning-based crack detection approaches</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><elocation-id>1374</elocation-id><pub-id pub-id-type="doi">10.3390/app12031374</pub-id></element-citation></ref><ref id="B7-sensors-25-05587"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>He</surname><given-names>Z.</given-names></name></person-group><article-title>Review of pavement defect detection methods</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>14531</fpage><lpage>14544</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2966881</pub-id></element-citation></ref><ref id="B8-sensors-25-05587"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>A deep learning approach for fast detection and classification of concrete damage</article-title><source>Autom. Constr.</source><year>2021</year><volume>128</volume><fpage>103785</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2021.103785</pub-id></element-citation></ref><ref id="B9-sensors-25-05587"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>Yolov3: An incremental improvement</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id><pub-id pub-id-type="arxiv">1804.02767</pub-id></element-citation></ref><ref id="B10-sensors-25-05587"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Weng</surname><given-names>K.</given-names></name><name name-style="western"><surname>Geng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ke</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>M.</given-names></name><name name-style="western"><surname>Nie</surname><given-names>W.</given-names></name><etal/></person-group><article-title>YOLOv6: A single-stage object detection framework for industrial applications</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arXiv.2209.02976</pub-id><pub-id pub-id-type="arxiv">2209.02976</pub-id></element-citation></ref><ref id="B11-sensors-25-05587"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Bochkovskiy</surname><given-names>A.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.-Y.M.</given-names></name></person-group><article-title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>7464</fpage><lpage>7475</lpage></element-citation></ref><ref id="B12-sensors-25-05587"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Yeh</surname><given-names>I.-H.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.-Y.M.</given-names></name></person-group><article-title>Yolov9: Learning what you want to learn using programmable gradient information</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date><fpage>1</fpage><lpage>21</lpage></element-citation></ref><ref id="B13-sensors-25-05587"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name></person-group><article-title>Yolov10: Real-time end-to-end object detection</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2024</year><volume>Volume 37</volume><fpage>107984</fpage><lpage>108011</lpage></element-citation></ref><ref id="B14-sensors-25-05587"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khanam</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>M.</given-names></name></person-group><article-title>Yolov11: An overview of the key architectural enhancements</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2410.17725</pub-id><pub-id pub-id-type="arxiv">2410.17725</pub-id></element-citation></ref><ref id="B15-sensors-25-05587"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Doermann</surname><given-names>D.</given-names></name></person-group><article-title>Yolov12: Attention-centric real-time object detectors</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2502.12524</pub-id></element-citation></ref><ref id="B16-sensors-25-05587"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lei</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>G.</given-names></name><name name-style="western"><surname>Du</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name></person-group><article-title>YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2506.17733</pub-id></element-citation></ref><ref id="B17-sensors-25-05587"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Branikas</surname><given-names>E.</given-names></name><name name-style="western"><surname>Murray</surname><given-names>P.</given-names></name><name name-style="western"><surname>West</surname><given-names>G.</given-names></name></person-group><article-title>A novel data augmentation method for improved visual crack detection using generative adversarial networks</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>22051</fpage><lpage>22059</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3251988</pub-id></element-citation></ref><ref id="B18-sensors-25-05587"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>N.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Miao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Hou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Data augmentation and intelligent recognition in pavement texture using a deep learning</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>25427</fpage><lpage>25436</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3140586</pub-id></element-citation></ref><ref id="B19-sensors-25-05587"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Rong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>S.</given-names></name></person-group><article-title>Automated highway pavement crack recognition under complex environment</article-title><source>Heliyon</source><year>2024</year><volume>10</volume><fpage>e26142</fpage><pub-id pub-id-type="doi">10.1016/j.heliyon.2024.e26142</pub-id><pub-id pub-id-type="pmid">38420379</pub-id><pub-id pub-id-type="pmcid">PMC10900953</pub-id></element-citation></ref><ref id="B20-sensors-25-05587"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cai</surname><given-names>S.</given-names></name><name name-style="western"><surname>Teng</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhong</surname><given-names>X.</given-names></name></person-group><article-title>Weather-Robust Spatial-Frequency Decoupling Transformer for Crack Segmentation</article-title><source>IEEE Sens. J.</source><year>2025</year><pub-id pub-id-type="doi">10.1109/JSEN.2025.3554650</pub-id></element-citation></ref><ref id="B21-sensors-25-05587"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>K.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ye</surname><given-names>J.</given-names></name></person-group><article-title>Object detection in 20 years: A survey</article-title><source>Proc. IEEE</source><year>2023</year><volume>111</volume><fpage>257</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2023.3238524</pub-id></element-citation></ref><ref id="B22-sensors-25-05587"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ouyang</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Fieguth</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Pietik&#228;inen</surname><given-names>M.</given-names></name></person-group><article-title>Deep learning for generic object detection: A survey</article-title><source>Int. J. Comput. Vis.</source><year>2020</year><volume>128</volume><fpage>261</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1007/s11263-019-01247-4</pub-id></element-citation></ref><ref id="B23-sensors-25-05587"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>H.-Y.M.</given-names></name></person-group><article-title>YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems</article-title><source>APSIPA Trans. Signal Inf. Process.</source><year>2024</year><volume>13</volume><fpage>e29</fpage><pub-id pub-id-type="doi">10.1561/116.20240058</pub-id></element-citation></ref><ref id="B24-sensors-25-05587"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pang</surname><given-names>R.</given-names></name><name name-style="western"><surname>Le</surname><given-names>Q.V.</given-names></name></person-group><article-title>Efficientdet: Scalable and efficient object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>10781</fpage><lpage>10790</lpage></element-citation></ref><ref id="B25-sensors-25-05587"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>A.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>A survey on vision transformer</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>45</volume><fpage>87</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3152247</pub-id><pub-id pub-id-type="pmid">35180075</pub-id></element-citation></ref><ref id="B26-sensors-25-05587"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Su</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name></person-group><article-title>Deformable detr: Deformable transformers for end-to-end object detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2010.04159</pub-id></element-citation></ref><ref id="B27-sensors-25-05587"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ali</surname><given-names>L.</given-names></name><name name-style="western"><surname>AlJassmi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Swavaf</surname><given-names>M.</given-names></name><name name-style="western"><surname>Khan</surname><given-names>W.</given-names></name><name name-style="western"><surname>Alnajjar</surname><given-names>F.</given-names></name></person-group><article-title>Rs-net: Residual Sharp U-Net architecture for pavement crack segmentation and severity assessment</article-title><source>J. Big Data</source><year>2024</year><volume>11</volume><fpage>116</fpage><pub-id pub-id-type="doi">10.1186/s40537-024-00981-y</pub-id></element-citation></ref><ref id="B28-sensors-25-05587"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>C.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>D.</given-names></name><name name-style="western"><surname>Duan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>D.</given-names></name></person-group><article-title>DA-RDD: Toward domain adaptive road damage detection across different countries</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>24</volume><fpage>3091</fpage><lpage>3103</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3221067</pub-id></element-citation></ref><ref id="B29-sensors-25-05587"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arya</surname><given-names>D.</given-names></name><name name-style="western"><surname>Maeda</surname><given-names>H.</given-names></name><name name-style="western"><surname>Ghosh</surname><given-names>S.K.</given-names></name><name name-style="western"><surname>Toshniwal</surname><given-names>D.</given-names></name><name name-style="western"><surname>Sekimoto</surname><given-names>Y.</given-names></name></person-group><article-title>RDD2022: A multi-national image dataset for automatic road damage detection</article-title><source>Geosci. Data J.</source><year>2024</year><volume>11</volume><fpage>846</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1002/gdj3.260</pub-id><pub-id pub-id-type="pmcid">PMC8166755</pub-id><pub-id pub-id-type="pmid">34095382</pub-id></element-citation></ref><ref id="B30-sensors-25-05587"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bossu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Hautiere</surname><given-names>N.</given-names></name><name name-style="western"><surname>Tarel</surname><given-names>J.-P.</given-names></name></person-group><article-title>Rain or snow detection in image sequences through use of a histogram of orientation of streaks</article-title><source>Int. J. Comput. Vis.</source><year>2011</year><volume>93</volume><fpage>348</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1007/s11263-011-0421-7</pub-id></element-citation></ref><ref id="B31-sensors-25-05587"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>A.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Min</surname><given-names>G.</given-names></name></person-group><article-title>Multi-Scene Auxiliary Network-Based Road Crack Detection under the Framework of Distributed Edge Intelligence</article-title><source>IEEE Internet Things J.</source><year>2025</year><volume>12</volume><fpage>4613</fpage><lpage>4628</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2025.3527233</pub-id></element-citation></ref><ref id="B32-sensors-25-05587"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Foroosh</surname><given-names>H.</given-names></name></person-group><article-title>Multiple adverse weather conditions adaptation for object detection via causal intervention</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>46</volume><fpage>1742</fpage><lpage>1756</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3166765</pub-id><pub-id pub-id-type="pmid">35412971</pub-id></element-citation></ref><ref id="B33-sensors-25-05587"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>C.</given-names></name><name name-style="western"><surname>He</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>X.</given-names></name></person-group><article-title>Haar wavelet downsampling: A simple but effective downsampling module for semantic segmentation</article-title><source>Pattern Recognit.</source><year>2023</year><volume>143</volume><fpage>109819</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2023.109819</pub-id></element-citation></ref><ref id="B34-sensors-25-05587"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>M.-M.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>J.</given-names></name></person-group><article-title>Strip pooling: Rethinking spatial pooling for scene parsing</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>4003</fpage><lpage>4012</lpage></element-citation></ref><ref id="B35-sensors-25-05587"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Fu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name></person-group><article-title>Learning to upsample by learning to sample</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>1&#8211;6 October 2023</conf-date><fpage>6027</fpage><lpage>6037</lpage></element-citation></ref><ref id="B36-sensors-25-05587"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>W.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Dang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>Detrs beat yolos on real-time object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>16965</fpage><lpage>16974</lpage></element-citation></ref><ref id="B37-sensors-25-05587"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Racah</surname><given-names>E.</given-names></name><name name-style="western"><surname>Prabhat</surname></name><name name-style="western"><surname>Correa</surname><given-names>J.</given-names></name><name name-style="western"><surname>Khosrowshahi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lavers</surname><given-names>D.</given-names></name><name name-style="western"><surname>Kunkel</surname><given-names>K.</given-names></name><name name-style="western"><surname>Wehner</surname><given-names>M.</given-names></name><name name-style="western"><surname>Collins</surname><given-names>W.</given-names></name></person-group><article-title>Application of deep convolutional neural networks for detecting extreme weather in climate datasets</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/arXiv.1605.01156</pub-id><pub-id pub-id-type="arxiv">1605.01156</pub-id></element-citation></ref><ref id="B38-sensors-25-05587"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>X.</given-names></name><name name-style="western"><surname>Belongie</surname><given-names>S.</given-names></name><name name-style="western"><surname>Yuille</surname><given-names>A.</given-names></name><name name-style="western"><surname>Torr</surname><given-names>P.H.S.</given-names></name><name name-style="western"><surname>Bai</surname><given-names>S.</given-names></name></person-group><article-title>Occluded video instance segmentation: A benchmark</article-title><source>Int. J. Comput. Vis.</source><year>2022</year><volume>130</volume><fpage>2022</fpage><lpage>2039</lpage><pub-id pub-id-type="doi">10.1007/s11263-022-01629-1</pub-id></element-citation></ref><ref id="B39-sensors-25-05587"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zakeri</surname><given-names>H.</given-names></name><name name-style="western"><surname>Nejad</surname><given-names>F.M.</given-names></name><name name-style="western"><surname>Fahimifar</surname><given-names>A.</given-names></name></person-group><article-title>Image based techniques for crack detection, classification and quantification in asphalt pavement: A review</article-title><source>Arch. Comput. Methods Eng.</source><year>2017</year><volume>24</volume><fpage>935</fpage><lpage>977</lpage><pub-id pub-id-type="doi">10.1007/s11831-016-9194-z</pub-id></element-citation></ref><ref id="B40-sensors-25-05587"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>El-Din Hemdan</surname><given-names>E.</given-names></name><name name-style="western"><surname>Al-Atroush</surname><given-names>M.E.</given-names></name></person-group><article-title>A review study of intelligent road crack detection: Algorithms and systems</article-title><source>Int. J. Pavement Res. Technol.</source><year>2025</year><fpage>1</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1007/s42947-025-00556-x</pub-id></element-citation></ref><ref id="B41-sensors-25-05587"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ragnoli</surname><given-names>A.</given-names></name><name name-style="western"><surname>De Blasiis</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Di Benedetto</surname><given-names>A.</given-names></name></person-group><article-title>Pavement distress detection methods: A review</article-title><source>Infrastructures</source><year>2018</year><volume>3</volume><elocation-id>58</elocation-id><pub-id pub-id-type="doi">10.3390/infrastructures3040058</pub-id></element-citation></ref><ref id="B42-sensors-25-05587"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gavil&#225;n</surname><given-names>M.</given-names></name><name name-style="western"><surname>Balcones</surname><given-names>D.</given-names></name><name name-style="western"><surname>Marcos</surname><given-names>O.</given-names></name><name name-style="western"><surname>Llorca</surname><given-names>D.F.</given-names></name><name name-style="western"><surname>Sotelo</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Parra</surname><given-names>I.</given-names></name><name name-style="western"><surname>Oca&#241;a</surname><given-names>M.</given-names></name><name name-style="western"><surname>Aliseda</surname><given-names>P.</given-names></name><name name-style="western"><surname>Yarza</surname><given-names>P.</given-names></name><name name-style="western"><surname>Am&#237;rola</surname><given-names>A.</given-names></name></person-group><article-title>Adaptive road crack detection system by pavement classification</article-title><source>Sensors</source><year>2011</year><volume>11</volume><fpage>9628</fpage><lpage>9657</lpage><pub-id pub-id-type="doi">10.3390/s111009628</pub-id><pub-id pub-id-type="pmid">22163717</pub-id><pub-id pub-id-type="pmcid">PMC3231249</pub-id></element-citation></ref><ref id="B43-sensors-25-05587"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>G.X.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>B.L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name></person-group><article-title>Pavement crack detection method based on deep learning models</article-title><source>Wirel. Commun. Mob. Comput.</source><year>2021</year><volume>2021</volume><fpage>5573590</fpage><pub-id pub-id-type="doi">10.1155/2021/5573590</pub-id></element-citation></ref><ref id="B44-sensors-25-05587"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>F.-Y.</given-names></name></person-group><article-title>Pavement cracks coupled with shadows: A new shadow-crack dataset and a shadow-removal-oriented crack detection approach</article-title><source>IEEE/CAA J. Autom. Sin.</source><year>2023</year><volume>10</volume><fpage>1593</fpage><lpage>1607</lpage><pub-id pub-id-type="doi">10.1109/JAS.2023.123447</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05587-f001" orientation="portrait"><label>Figure 1</label><caption><p>Representative challenging cases in pavement crack detection under adverse weather conditions. (<bold>Left</bold>) <bold>Occlusion</bold>: Environmental objects partially obscure crack regions. (<bold>Middle</bold>) <bold>Low-contrast</bold>: Rain or snow reduces the visibility of cracks against the pavement background. (<bold>Right</bold>) <bold>Complex crack</bold>: Irregular and fragmented crack patterns make detection and localization more difficult. The green bounding boxes indicate annotated defect regions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05587-g001.jpg"/></fig><fig position="float" id="sensors-25-05587-f002" orientation="portrait"><label>Figure 2</label><caption><p>Architecture of the proposed method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05587-g002.jpg"/></fig><fig position="float" id="sensors-25-05587-f003" orientation="portrait"><label>Figure 3</label><caption><p>Representative image samples of the four pavement defect categories in the dataset: (<bold>0</bold>) longitudinal cracks; (<bold>1</bold>) transverse cracks; (<bold>2</bold>) alligator cracks; (<bold>3</bold>) potholes. The green bounding boxes denote the annotated defect regions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05587-g003.jpg"/></fig><fig position="float" id="sensors-25-05587-f004" orientation="portrait"><label>Figure 4</label><caption><p>Precision&#8211;recall (PR) plot with an IoU of 0.5. The (<bold>top</bold>) half shows the PR plot for the rain dataset, while the (<bold>bottom</bold>) half shows the PR plot for the snow dataset. The (<bold>left</bold>) side shows the baseline model, and the (<bold>right</bold>) side shows CrackNet-Weather.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05587-g004.jpg"/></fig><fig position="float" id="sensors-25-05587-f005" orientation="portrait"><label>Figure 5</label><caption><p>Precision-recall (PR) plots for IoU ranges of 0.5-0.95. The (<bold>top</bold>) half shows the PR plot for the rain dataset, while the (<bold>bottom</bold>) half shows the PR plot for the snow dataset. The (<bold>left</bold>) side shows the baseline model, and the (<bold>right</bold>) side shows CrackNet-Weather.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05587-g005.jpg"/></fig><fig position="float" id="sensors-25-05587-f006" orientation="portrait"><label>Figure 6</label><caption><p>Examples of image degradation used for robustness verification. The first two rows illustrate Gaussian blur (radius = 1, 1.5, 2) and brightness variation (factors 0.6, 1.0, 1.4). The last two rows show synthetic rain and snow generated by our augmentation algorithms, where each pair of images represents light (<bold>left</bold>) and heavy (<bold>right</bold>) conditions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05587-g006.jpg"/></fig><fig position="float" id="sensors-25-05587-f007" orientation="portrait"><label>Figure 7</label><caption><p>Qualitative visualization of detection results for YOLOv12 (<bold>left</bold>) and CrackNet- Weather (<bold>right</bold>) under adverse weather conditions. Green bounding boxes denote correctly detected pavement defects by each model. Red bounding boxes indicate ground-truth defects that were missed by YOLOv12 but successfully detected by CrackNet-Weather. These results demonstrate the superior ability of CrackNet-Weather to identify subtle and challenging defects, especially under severe visual interference.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05587-g007.jpg"/></fig><fig position="float" id="sensors-25-05587-f008" orientation="portrait"><label>Figure 8</label><caption><p>Representative misclassification cases of CrackNet-Weather under adverse weather conditions. The first row shows the ground-truth labels for three scenes, while the second row shows the model&#8217;s predictions. (<bold>Left</bold>) Class confusion: a pothole is misclassified as an alligator crack. (<bold>Middle</bold>) False positive: water accumulation is incorrectly detected as a pothole. (<bold>Right</bold>) Missed detection: a longitudinal crack is present in the ground truth but not detected by the model.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05587-g008.jpg"/></fig><table-wrap position="float" id="sensors-25-05587-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05587-t001_Table 1</object-id><label>Table 1</label><caption><p>Statistics of the two datasets used in this study, including the number of training, validation, and&#160;test sets in each dataset and the number of annotated bounding boxes for each defect&#160;class.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Datasets</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Total</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Train</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Val</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Class + boxNum</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Longitudinal Cracks</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Transverse Cracks</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Alligator Cracks</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Potholes</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain/Snow</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2600</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2000</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1307</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1288</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2098</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">733</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Norway</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2914</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2039</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">875</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8570</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1730</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">468</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">461</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05587-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05587-t002_Table 2</object-id><label>Table 2</label><caption><p>Ablation study of each module under rainy and snowy&#160;conditions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Models</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">SPBB</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">HWDB</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">DSUB</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Longitudinal</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Transverse</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Alligator</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Potholes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">All</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="8" colspan="1">YOLOv12-s</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">13.41</td><td align="left" valign="middle" rowspan="1" colspan="1">13.13</td><td align="left" valign="middle" rowspan="1" colspan="1">8.62</td><td align="left" valign="middle" rowspan="1" colspan="1">7.54</td><td align="left" valign="middle" rowspan="1" colspan="1">26.42</td><td align="left" valign="middle" rowspan="1" colspan="1">25.12</td><td align="left" valign="middle" rowspan="1" colspan="1">15.74</td><td align="left" valign="middle" rowspan="1" colspan="1">14.81</td><td align="left" valign="middle" rowspan="1" colspan="1">16.02</td><td align="left" valign="middle" rowspan="1" colspan="1">15.16</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">14.13</td><td align="left" valign="middle" rowspan="1" colspan="1">13.22</td><td align="left" valign="middle" rowspan="1" colspan="1">8.83</td><td align="left" valign="middle" rowspan="1" colspan="1">8.24</td><td align="left" valign="middle" rowspan="1" colspan="1">28.83</td><td align="left" valign="middle" rowspan="1" colspan="1">27.72</td><td align="left" valign="middle" rowspan="1" colspan="1">16.15</td><td align="left" valign="middle" rowspan="1" colspan="1">15.17</td><td align="left" valign="middle" rowspan="1" colspan="1">17.02</td><td align="left" valign="middle" rowspan="1" colspan="1">16.34</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">14.31</td><td align="left" valign="middle" rowspan="1" colspan="1">13.25</td><td align="left" valign="middle" rowspan="1" colspan="1">8.33</td><td align="left" valign="middle" rowspan="1" colspan="1">7.02</td><td align="left" valign="middle" rowspan="1" colspan="1">27.75</td><td align="left" valign="middle" rowspan="1" colspan="1">28.57</td><td align="left" valign="middle" rowspan="1" colspan="1">16.14</td><td align="left" valign="middle" rowspan="1" colspan="1">15.45</td><td align="left" valign="middle" rowspan="1" colspan="1">17.33</td><td align="left" valign="middle" rowspan="1" colspan="1">16.24</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">13.43</td><td align="left" valign="middle" rowspan="1" colspan="1">13.07</td><td align="left" valign="middle" rowspan="1" colspan="1">10.12</td><td align="left" valign="middle" rowspan="1" colspan="1">7.31</td><td align="left" valign="middle" rowspan="1" colspan="1">28.75</td><td align="left" valign="middle" rowspan="1" colspan="1">27.42</td><td align="left" valign="middle" rowspan="1" colspan="1">15.36</td><td align="left" valign="middle" rowspan="1" colspan="1">15.33</td><td align="left" valign="middle" rowspan="1" colspan="1">16.94</td><td align="left" valign="middle" rowspan="1" colspan="1">15.91</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">14.53</td><td align="left" valign="middle" rowspan="1" colspan="1">13.25</td><td align="left" valign="middle" rowspan="1" colspan="1">9.26</td><td align="left" valign="middle" rowspan="1" colspan="1">8.53</td><td align="left" valign="middle" rowspan="1" colspan="1">29.12</td><td align="left" valign="middle" rowspan="1" colspan="1">28.77</td><td align="left" valign="middle" rowspan="1" colspan="1">16.54</td><td align="left" valign="middle" rowspan="1" colspan="1">15.79</td><td align="left" valign="middle" rowspan="1" colspan="1">17.63</td><td align="left" valign="middle" rowspan="1" colspan="1">16.51</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">14.42</td><td align="left" valign="middle" rowspan="1" colspan="1">13.27</td><td align="left" valign="middle" rowspan="1" colspan="1">9.93</td><td align="left" valign="middle" rowspan="1" colspan="1">8.17</td><td align="left" valign="middle" rowspan="1" colspan="1">28.83</td><td align="left" valign="middle" rowspan="1" colspan="1">28.87</td><td align="left" valign="middle" rowspan="1" colspan="1">16.82</td><td align="left" valign="middle" rowspan="1" colspan="1">15.86</td><td align="left" valign="middle" rowspan="1" colspan="1">17.54</td><td align="left" valign="middle" rowspan="1" colspan="1">16.32</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">14.51</td><td align="left" valign="middle" rowspan="1" colspan="1">13.32</td><td align="left" valign="middle" rowspan="1" colspan="1">9.63</td><td align="left" valign="middle" rowspan="1" colspan="1">8.12</td><td align="left" valign="middle" rowspan="1" colspan="1">29.24</td><td align="left" valign="middle" rowspan="1" colspan="1">27.85</td><td align="left" valign="middle" rowspan="1" colspan="1">16.22</td><td align="left" valign="middle" rowspan="1" colspan="1">15.43</td><td align="left" valign="middle" rowspan="1" colspan="1">17.28</td><td align="left" valign="middle" rowspan="1" colspan="1">16.21</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.85</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.32</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.24</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.76</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.53</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.43</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.84</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.92</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.88</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.85</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05587-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05587-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation study of computational complexity and inference speed for each&#160;module.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Models</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">SPBB</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HWDB</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DSUB</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FLOPs</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Inference (ms/img)</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="8" colspan="1"> YOLOv12-s</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">21.5</td><td align="left" valign="middle" rowspan="1" colspan="1">9.25</td><td align="center" valign="middle" rowspan="1" colspan="1">2.8</td><td align="left" valign="middle" rowspan="1" colspan="1">357</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">29.5</td><td align="left" valign="middle" rowspan="1" colspan="1">9.10</td><td align="center" valign="middle" rowspan="1" colspan="1">4.2</td><td align="left" valign="middle" rowspan="1" colspan="1">238</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">18.9</td><td align="left" valign="middle" rowspan="1" colspan="1">8.19</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8</td><td align="left" valign="middle" rowspan="1" colspan="1">208</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">21.3</td><td align="left" valign="middle" rowspan="1" colspan="1">9.25</td><td align="center" valign="middle" rowspan="1" colspan="1">4.6</td><td align="left" valign="middle" rowspan="1" colspan="1">217</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">26.9</td><td align="left" valign="middle" rowspan="1" colspan="1">8.03</td><td align="center" valign="middle" rowspan="1" colspan="1">4.6</td><td align="left" valign="middle" rowspan="1" colspan="1">217</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">29.6</td><td align="left" valign="middle" rowspan="1" colspan="1">9.12</td><td align="center" valign="middle" rowspan="1" colspan="1">4.6</td><td align="left" valign="middle" rowspan="1" colspan="1">217</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" rowspan="1" colspan="1">18.9</td><td align="left" valign="middle" rowspan="1" colspan="1">8.21</td><td align="center" valign="middle" rowspan="1" colspan="1">4.8</td><td align="left" valign="middle" rowspan="1" colspan="1">208</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#10003;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.9</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.6</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">217</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05587-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05587-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison with other mainstream methods under rainy and snowy&#160;conditions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Models</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Flops/G</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Parameters/M</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Longitudinal</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Transverse</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Alligator</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Potholes</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">All</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rain</th><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Snow</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8-s</td><td align="center" valign="middle" rowspan="1" colspan="1">14.3</td><td align="center" valign="middle" rowspan="1" colspan="1">11.1</td><td align="left" valign="middle" rowspan="1" colspan="1">14.61</td><td align="left" valign="middle" rowspan="1" colspan="1">12.42</td><td align="left" valign="middle" rowspan="1" colspan="1">8.12</td><td align="left" valign="middle" rowspan="1" colspan="1">7.74</td><td align="left" valign="middle" rowspan="1" colspan="1">25.42</td><td align="left" valign="middle" rowspan="1" colspan="1">25.36</td><td align="left" valign="middle" rowspan="1" colspan="1">19.25</td><td align="left" valign="middle" rowspan="1" colspan="1">19.12</td><td align="left" valign="middle" rowspan="1" colspan="1">16.91</td><td align="left" valign="middle" rowspan="1" colspan="1">16.06</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv10-s [<xref rid="B13-sensors-25-05587" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">24.8</td><td align="center" valign="middle" rowspan="1" colspan="1">8.07</td><td align="left" valign="middle" rowspan="1" colspan="1">12.41</td><td align="left" valign="middle" rowspan="1" colspan="1">11.42</td><td align="left" valign="middle" rowspan="1" colspan="1">7.33</td><td align="left" valign="middle" rowspan="1" colspan="1">6.62</td><td align="left" valign="middle" rowspan="1" colspan="1">24.92</td><td align="left" valign="middle" rowspan="1" colspan="1">24.13</td><td align="left" valign="middle" rowspan="1" colspan="1">14.75</td><td align="left" valign="middle" rowspan="1" colspan="1">14.28</td><td align="left" valign="middle" rowspan="1" colspan="1">14.81</td><td align="left" valign="middle" rowspan="1" colspan="1">14.14</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv11-s [<xref rid="B14-sensors-25-05587" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">10.8</td><td align="center" valign="middle" rowspan="1" colspan="1">9.46</td><td align="left" valign="middle" rowspan="1" colspan="1">12.12</td><td align="left" valign="middle" rowspan="1" colspan="1">13.33</td><td align="left" valign="middle" rowspan="1" colspan="1">8.21</td><td align="left" valign="middle" rowspan="1" colspan="1">7.06</td><td align="left" valign="middle" rowspan="1" colspan="1">27.67</td><td align="left" valign="middle" rowspan="1" colspan="1">26.55</td><td align="left" valign="middle" rowspan="1" colspan="1">17.22</td><td align="left" valign="middle" rowspan="1" colspan="1">14.74</td><td align="left" valign="middle" rowspan="1" colspan="1">16.31</td><td align="left" valign="middle" rowspan="1" colspan="1">15.46</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv12-s [<xref rid="B15-sensors-25-05587" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">21.5</td><td align="center" valign="middle" rowspan="1" colspan="1">9.25</td><td align="left" valign="middle" rowspan="1" colspan="1">13.41</td><td align="left" valign="middle" rowspan="1" colspan="1">13.13</td><td align="left" valign="middle" rowspan="1" colspan="1">7.52</td><td align="left" valign="middle" rowspan="1" colspan="1">6.34</td><td align="left" valign="middle" rowspan="1" colspan="1">27.02</td><td align="left" valign="middle" rowspan="1" colspan="1">26.42</td><td align="left" valign="middle" rowspan="1" colspan="1">16.04</td><td align="left" valign="middle" rowspan="1" colspan="1">14.51</td><td align="left" valign="middle" rowspan="1" colspan="1">16.02</td><td align="left" valign="middle" rowspan="1" colspan="1">15.16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv13-s [<xref rid="B16-sensors-25-05587" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">21.0</td><td align="center" valign="middle" rowspan="1" colspan="1">9.03</td><td align="left" valign="middle" rowspan="1" colspan="1">10.22</td><td align="left" valign="middle" rowspan="1" colspan="1">8.63</td><td align="left" valign="middle" rowspan="1" colspan="1">6.24</td><td align="left" valign="middle" rowspan="1" colspan="1">5.95</td><td align="left" valign="middle" rowspan="1" colspan="1">28.31</td><td align="left" valign="middle" rowspan="1" colspan="1">26.64</td><td align="left" valign="middle" rowspan="1" colspan="1">14.42</td><td align="left" valign="middle" rowspan="1" colspan="1">14.13</td><td align="left" valign="middle" rowspan="1" colspan="1">14.86</td><td align="left" valign="middle" rowspan="1" colspan="1">12.61</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.05</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.35</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.92</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.44</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.56</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.73</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.33</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.14</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.02</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.88</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.85</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05587-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05587-t005_Table 5</object-id><label>Table 5</label><caption><p>Performance comparison on the Norway pavement defect&#160;dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP<sub>50:95</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP<sub>50</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Flops/G</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters/M</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv8-s</td><td align="center" valign="middle" rowspan="1" colspan="1">8.13</td><td align="center" valign="middle" rowspan="1" colspan="1">20.13</td><td align="center" valign="middle" rowspan="1" colspan="1">14.27</td><td align="center" valign="middle" rowspan="1" colspan="1">11.14</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv11-s</td><td align="center" valign="middle" rowspan="1" colspan="1">6.13</td><td align="center" valign="middle" rowspan="1" colspan="1">16.34</td><td align="center" valign="middle" rowspan="1" colspan="1">10.81</td><td align="center" valign="middle" rowspan="1" colspan="1">9.46</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv12-s</td><td align="center" valign="middle" rowspan="1" colspan="1">7.39</td><td align="center" valign="middle" rowspan="1" colspan="1">18.82</td><td align="center" valign="middle" rowspan="1" colspan="1">21.51</td><td align="center" valign="middle" rowspan="1" colspan="1">9.25</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLOv13-s</td><td align="center" valign="middle" rowspan="1" colspan="1">7.52</td><td align="center" valign="middle" rowspan="1" colspan="1">19.35</td><td align="center" valign="middle" rowspan="1" colspan="1">21.02</td><td align="center" valign="middle" rowspan="1" colspan="1">8.05</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Faster-rcnn-r18</td><td align="center" valign="middle" rowspan="1" colspan="1">9.61</td><td align="center" valign="middle" rowspan="1" colspan="1">26.36</td><td align="center" valign="middle" rowspan="1" colspan="1">151.31</td><td align="center" valign="middle" rowspan="1" colspan="1">28.30</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cascade</td><td align="center" valign="middle" rowspan="1" colspan="1">11.02</td><td align="center" valign="middle" rowspan="1" colspan="1">26.71</td><td align="center" valign="middle" rowspan="1" colspan="1">231.00</td><td align="center" valign="middle" rowspan="1" colspan="1">69.16</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CrackNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.05</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05587-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05587-t006_Table 6</object-id><label>Table 6</label><caption><p>Model performance under normal and extreme weather&#160;conditions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="2" colspan="1">Model</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Normal Weather Dataset</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Extreme Weather Dataset</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AP<sub>50&#8211;95</sub></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AP<sub>50</sub></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AP<sub>50&#8211;95</sub></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AP<sub>50</sub></th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv12</td><td align="center" valign="middle" rowspan="1" colspan="1">18.01</td><td align="center" valign="middle" rowspan="1" colspan="1">40.42</td><td align="center" valign="middle" rowspan="1" colspan="1">15.16</td><td align="center" valign="middle" rowspan="1" colspan="1">34.32</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CrackNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.62</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05587-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05587-t007_Table 7</object-id><label>Table 7</label><caption><p>CrackNet performance under different degradation&#160;conditions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Condition</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP<sub>50&#8211;95</sub></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP<sub>50</sub></th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="12" colspan="1">CrackNet</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal (Rain)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.12</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal (Snow)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.62</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Light Rain</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.80</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Heavy Rain</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.21</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Light Snow</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.20</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Heavy Snow</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.99</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaussian Blur (radius = 1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.09</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaussian Blur (radius = 1.5)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.93</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaussian Blur (radius = 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.98</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brightness (factor = 0.6)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.21</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brightness (factor = 1.0)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.12</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brightness (factor = 1.4)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.48</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05587-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05587-t008_Table 8</object-id><label>Table 8</label><caption><p>Severity-wise summary on RDD2022-Japan (AP<sub>50:95</sub>). Retention <inline-formula><mml:math id="mm204" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Heavy</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>Light</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Factor</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm205" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">AP</mml:mi><mml:mi mathvariant="bold">Light</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm206" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">AP</mml:mi><mml:mi mathvariant="bold">Heavy</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Retention (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Normalized Drop (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Slope (pp/Level)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Rain</td><td align="center" valign="middle" rowspan="1" colspan="1">18.40</td><td align="center" valign="middle" rowspan="1" colspan="1">15.73</td><td align="center" valign="middle" rowspan="1" colspan="1">85.5</td><td align="center" valign="middle" rowspan="1" colspan="1">14.51</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm207" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1.335</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Snow</td><td align="center" valign="middle" rowspan="1" colspan="1">17.30</td><td align="center" valign="middle" rowspan="1" colspan="1">14.32</td><td align="center" valign="middle" rowspan="1" colspan="1">82.8</td><td align="center" valign="middle" rowspan="1" colspan="1">17.23</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm208" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1.490</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Blur</td><td align="center" valign="middle" rowspan="1" colspan="1">16.84</td><td align="center" valign="middle" rowspan="1" colspan="1">15.28</td><td align="center" valign="middle" rowspan="1" colspan="1">90.7</td><td align="center" valign="middle" rowspan="1" colspan="1">9.26</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm209" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>0.780</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brightness <sup>&#8224;</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm211" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1.045</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr></tbody></table><table-wrap-foot><fn><p><sup>&#8224;</sup> Brightness severities ordered as <italic toggle="yes">over-exposed (Light)</italic> &#8594; <italic toggle="yes">normal (Normal)</italic> &#8594; <italic toggle="yes">under-exposed (Heavy)</italic>; Normal value is <inline-formula><mml:math id="mm213" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>17.88</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for reference.</p></fn></table-wrap-foot></table-wrap></floats-group></article></pmc-articleset>