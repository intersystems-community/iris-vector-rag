<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431306</article-id><article-id pub-id-type="pmcid-ver">PMC12431306.1</article-id><article-id pub-id-type="pmcaid">12431306</article-id><article-id pub-id-type="pmcaiid">12431306</article-id><article-id pub-id-type="doi">10.3390/s25175254</article-id><article-id pub-id-type="publisher-id">sensors-25-05254</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Unsupervised Tablet Defect Detection Method Based on Diffusion Model</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-1063-2887</contrib-id><name name-style="western"><surname>Zhang</surname><given-names initials="M">Mengfan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9537-8767</contrib-id><name name-style="western"><surname>Liu</surname><given-names initials="W">Weifeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="c1-sensors-25-05254" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-0458-0361</contrib-id><name name-style="western"><surname>He</surname><given-names initials="L">Linqing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-2294-194X</contrib-id><name name-style="western"><surname>Wang</surname><given-names initials="D">Di</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Gillich</surname><given-names initials="GR">Gilbert-Rainer</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05254">School of Electrical and Control Engineering, Shaanxi University of Science and Technology, Xi&#8217;an 710016, China; <email>230612052@sust.edu.cn</email> (M.Z.); <email>230611024@sust.edu.cn</email> (L.H.); <email>230611017@sust.edu.cn</email> (D.W.)</aff><author-notes><corresp id="c1-sensors-25-05254"><label>*</label>Correspondence: <email>liuwf@sust.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5254</elocation-id><history><date date-type="received"><day>23</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>17</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>20</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>23</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05254.pdf"/><abstract><p>Reconstruction-based unsupervised detection methods have demonstrated strong generalization capabilities in the field of tablet anomaly detection, but there are still problems such as poor reconstruction effect and inaccurate positioning of abnormal areas. To address these problems, this paper proposes an unsupervised <bold>D</bold>iffusion-based <bold>T</bold>ablet <bold>D</bold>efect <bold>D</bold>etection (<bold>DTDD</bold>) method. This method uses an Assisted Reconstruction (AR) network to introduce original image information to assist in the reconstruction of abnormal areas, thereby improving the reconstruction effect of the diffusion model. It also uses a Scale Fusion (SF) network and an improved anomaly measurement method to improve the accuracy of abnormal area positioning. Finally, the effectiveness of the algorithm is verified on the tablet dataset. The experimental results show that the algorithm in this paper is superior to the algorithms in the same field, effectively improving the detection accuracy and abnormal positioning accuracy, and performing well in the tablet defect detection task.</p></abstract><kwd-group><kwd>tablet anomaly detection</kwd><kwd>diffusion model</kwd><kwd>feature extraction</kwd><kwd>anomaly location</kwd><kwd>unsupervised detection method</kwd></kwd-group><funding-group><award-group><funding-source>Shaanxi Qinchuangyuan &#8220;Scientist + Engineer&#8221; Team Building</funding-source><award-id>2023KXJ-118</award-id></award-group><award-group><funding-source>NSFC</funding-source><award-id>62376147</award-id></award-group><funding-statement>This research was funded by the Shaanxi Qinchuangyuan &#8220;Scientist + Engineer&#8221; Team Building (2023KXJ-118) and the NSFC (62376147).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05254"><title>1. Introduction</title><p>Drug quality is directly related to the health and safety of patients and the brand reputation of pharmaceutical companies. In recent years, drug safety issues [<xref rid="B1-sensors-25-05254" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05254" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05254" ref-type="bibr">3</xref>] have occurred frequently, and the number of drug safety accidents has continued to rise. Among them, drug surface defects [<xref rid="B4-sensors-25-05254" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05254" ref-type="bibr">5</xref>] are an important component, a key quality issue that needs to be detected in the production process, and the most intuitive manifestation of product quality being affected. Therefore, drug surface defect detection is of great significance to ensure drug quality, improve drug safety, and reduce the frequency of accidents [<xref rid="B6-sensors-25-05254" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05254" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05254" ref-type="bibr">8</xref>].</p><p>Tablets are the most common form of medicines. Tablet defect detection is generally aimed at detecting scratches, defects, foreign matter occlusion, color pollution, holes, and other defects on the surface of tablets [<xref rid="B9-sensors-25-05254" ref-type="bibr">9</xref>]. Traditional manual visual inspection methods are inefficient and highly subjective, and it is difficult to meet the requirements of high-speed production lines for accuracy and real-time performance. Traditional image processing technology achieves abnormality recognition through low-dimensional feature extraction and rule design, but its generalization ability for complex texture defects is insufficient and it is easily affected by illumination changes and background interference. In recent years, deep learning technology [<xref rid="B10-sensors-25-05254" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05254" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05254" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-05254" ref-type="bibr">13</xref>] has significantly improved the accuracy and robustness of tablet defect detection with its powerful feature learning ability and has gradually become the mainstream method. Among them, supervised methods represented by Siamese Network [<xref rid="B14-sensors-25-05254" ref-type="bibr">14</xref>], Fast R-CNN [<xref rid="B15-sensors-25-05254" ref-type="bibr">15</xref>], and YOLO [<xref rid="B16-sensors-25-05254" ref-type="bibr">16</xref>] have fully demonstrated their advantages of a high accuracy and good adaptability. TT Mac et al. [<xref rid="B17-sensors-25-05254" ref-type="bibr">17</xref>] proposed a method that combines pill image processing with an improved VGG16 convolutional neural network based on Adam optimization to ensure a high accuracy of defect detection and classification. Xue Q et al. [<xref rid="B18-sensors-25-05254" ref-type="bibr">18</xref>] applied YOLOV5 with attention mechanism and multi-scale feature fusion to the drug inspection scenario, providing a high-precision first-level real-time defect detection system. S Kim et al. [<xref rid="B19-sensors-25-05254" ref-type="bibr">19</xref>] proposed a pipeline consisting of a pill detection module and a defect detection module based on an autoencoder to detect defective pills in pill packaging. For the task of tablet defect detection, supervised methods require a large amount of defect data annotation, which cannot cope with the problem of a small number of defect samples and diverse defect forms generated during the tablet production process. In this regard, unsupervised methods represented by DRAEM [<xref rid="B20-sensors-25-05254" ref-type="bibr">20</xref>], PatchCore [<xref rid="B21-sensors-25-05254" ref-type="bibr">21</xref>], and LDM [<xref rid="B22-sensors-25-05254" ref-type="bibr">22</xref>] are more suitable, among which reconstruction-based methods are a major research hotspot [<xref rid="B23-sensors-25-05254" ref-type="bibr">23</xref>]. The core of the reconstruction-based method is that the model only learns feature distribution from normal images during the training phase and uses the trained model to reconstruct abnormal images into normal images during the test phase so as to determine the abnormal location by comparing the reconstructed image with the input image. Autoencoders [<xref rid="B24-sensors-25-05254" ref-type="bibr">24</xref>], Variational Autoencoders [<xref rid="B25-sensors-25-05254" ref-type="bibr">25</xref>] and Generative Adversarial Networks [<xref rid="B26-sensors-25-05254" ref-type="bibr">26</xref>] all belong to this category. In recent years, the diffusion model, which has performed outstandingly, has been widely studied and applied.</p><p>In 2015, Jascha Sohl-Dickstein et al. [<xref rid="B27-sensors-25-05254" ref-type="bibr">27</xref>] proposed a diffusion model based on the principles of non-equilibrium thermodynamics. The idea is to systematically and slowly destroy the structure of the original data distribution through an iterative forward diffusion process so that it gradually becomes a simple, analyzable distribution, and then learn a reverse diffusion process to restore the structure in the data, thereby obtaining a highly flexible and easy-to-handle generation model. In 2020, Ho J et al. [<xref rid="B28-sensors-25-05254" ref-type="bibr">28</xref>] proposed a model for image generation, and it has been widely used. In 2021, Robin Rombach et al. proposed the LDM architecture, which can greatly reduce the computational complexity and achieve better results by performing a diffusion process on the latent representation space. J. Wang et al. [<xref rid="B29-sensors-25-05254" ref-type="bibr">29</xref>] proposed a defect detection model based on a classifier-guided conditional diffusion model to detect defects and accurately identify defective areas; Hang Yao et al. [<xref rid="B30-sensors-25-05254" ref-type="bibr">30</xref>] proposed a global and local adaptive diffusion model that can achieve anomaly-free reconstruction while retaining normal information as much as possible. He H et al. [<xref rid="B31-sensors-25-05254" ref-type="bibr">31</xref>] proposed a diffusion-based anomaly detection architecture for multi-class anomaly detection, which can cope with the anomaly detection problem under multiple classifications. Zhang X et al. [<xref rid="B32-sensors-25-05254" ref-type="bibr">32</xref>] proposed a feature reconstruction network Realnet with real synthetic anomalies and adaptive feature selection to synthesize real and diverse anomaly samples. In addition, to address the limitations of feature extraction in anomaly detection tasks, Z He et al. [<xref rid="B33-sensors-25-05254" ref-type="bibr">33</xref>] introduced a cross-modal recurrent enhancement module and a frequency-aware alternating fusion module. These modules effectively suppress interference from low-quality depth maps and facilitate the aggregation of cross-modal features. By overcoming the similarity between the foreground and background in the frequency domain, this approach enables robust cross-scale fusion of integrated features. S Woo et al. [<xref rid="B34-sensors-25-05254" ref-type="bibr">34</xref>] proposed the CBAM attention mechanism, which infers attention maps along two independent dimensions, channel and space, and it is widely used to improve the representation ability of the network. J Chen et al. [<xref rid="B35-sensors-25-05254" ref-type="bibr">35</xref>] proposed a bidirectional cross-scale connection feature fusion network with a direct information connection layer and a shallow information fusion layer to address the difficulty and low accuracy of small target detection. Although the above methods have achieved certain results, they still face many challenges in the task of tablet defect detection: (1) small target defects such as tiny scratches and pits on the surface of tablets have a very low pixel ratio and extremely weak semantic information. Existing detection algorithms find it difficult to accurately capture their features, resulting in large positioning deviations and difficulty in designing an adaptive loss function to optimize the detection effect. (2) The defect morphology of tablets varies greatly. It is difficult for the feature fusion network to balance the feature extraction at different scales. The shallow network cannot deeply explore the details of small defects, and the deep network is prone to losing the overall structural information of large defects, resulting in limited detection accuracy. (3) Some tablets have complex defect backgrounds and textures, resulting in poor reconstruction effect and reconstruction quality of existing algorithms. In this regard, this paper proposes an unsupervised tablet defect detection method, DTDD, based on a diffusion model.</p><p>The contributions of this paper are as follows:</p><p>(1) An AR network is used to improve the quality of image reconstruction. This network introduces the semantic information of the original image into the diffusion model denoising network, ensuring the consistency of semantic information and assisting the diffusion model to correctly reconstruct the abnormal image, thereby improving the reconstruction effect.</p><p>(2) A semantically enhanced lightweight attention module (SELA) is designed to enhance the semantic expression and detail perception of target features, thereby improving the ability of the AR network to understand global semantic information and local detail textures.</p><p>(3) An SF network is used to achieve accurate positioning of abnormal areas. This method achieves hierarchical extraction of features of different scales through explicit task division, performs abnormality measurement and adaptive weighted fusion on features of each layer, avoids the computational overhead of cross-layer fusion, and enables the model to take into account both details and global information.</p></sec><sec id="sec2-sensors-25-05254"><title>2. Background</title><sec id="sec2dot1-sensors-25-05254"><title>2.1. Denoising Diffusion Probabilistic Model</title><p>DDPM models the data generation process as a reverse Markov chain, which is divided into two stages: noise addition (forward process) and data generation (reverse process). In the forward diffusion process, the model starts from the data and gradually adds Gaussian noise until the signal is completely covered by the noise. Assume that <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the original data, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the original data density, <italic toggle="yes">t</italic> represents the time of the diffusion step, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the preset variance schedule, which determines the amount of noise added, <italic toggle="yes">I</italic> represents the identity matrix of the same dimension as the input image <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:math></inline-formula> is the noise sampled from the standard Gaussian distribution, Definition <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#946;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8719;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, then the entire forward process <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed as follows:<disp-formula id="FD1-sensors-25-05254"><label>(1)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msqrt><mml:msub><mml:mover><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:msqrt><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-05254"><label>(2)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mover><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:msqrt><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the process of inverse denoising, the model learns to gradually recover the original noise-free data from the noisy data. Definition <italic toggle="yes">p</italic> is a parameterized Gaussian distribution, and its learning process can be expressed as follows:<disp-formula id="FD3-sensors-25-05254"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="" open="|" close="|"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mo>&#931;</mml:mo><mml:mi>&#952;</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05254"><label>(4)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>&#8719;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> means the joint probability distribution of the current moment after the next time step data is given. The data distribution of the current time step is Gaussian distribution, and its mean and covariance are determined by the model parameters. <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> means the joint probability distribution from the last time step to the first time step. In the inverse denoising process, the model uses the maximum likelihood objective to train the neural network and uses stochastic gradient descent to optimize the random term. The expression can be written as follows:<disp-formula id="FD5-sensors-25-05254"><label>(5)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>L</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8214;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mspace width="1.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8214;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Set the covariance to a fixed value and calculate the mean as follows:<disp-formula id="FD6-sensors-25-05254"><label>(6)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>&#183;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#946;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#956;</mml:mi></mml:mrow><mml:mi>&#963;</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>; this enables the model to more accurately reconstruct the original data from the noisy data, and the loss function is simplified to the following formula:<disp-formula id="FD7-sensors-25-05254"><label>(7)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:mi>&#952;</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:mi>&#1013;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msqrt><mml:msub><mml:mover><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:msqrt><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:mi>&#1013;</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot2-sensors-25-05254"><title>2.2. Latent Diffusion Model</title><p>LDM first uses a pre-trained variational autoencoder to encode high-dimensional image data into a low-dimensional latent space. The representation in this latent space is more compact and computationally efficient than the original image space. The encoder maps image <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to latent space <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, where LDM applies a diffusion model to generate data. The diffusion model transforms the data from the original distribution to a Gaussian distribution by gradually adding noise, and then it learns how to reverse the process. Finally, the decoder is used to reconstruct image <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> from the latent space. The expressions of the forward diffusion process and the reverse denoising process are shown in Equations (8) and (9).<disp-formula id="FD8-sensors-25-05254"><label>(8)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:msqrt><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-05254"><label>(9)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since LDM supports conditional generation, the given conditional information <italic toggle="yes">c</italic> such as text description and category label can be introduced into the inverse diffusion process through the cross-attention mechanism to generate images. Therefore, its loss function can be expressed as follows:<disp-formula id="FD10-sensors-25-05254"><label>(10)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>&#8722;</mml:mo></mml:mrow><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot3-sensors-25-05254"><title>2.3. Tablet Surface Defect Types</title><p>The types of surface defects of tablets mainly include the following, as shown in <xref rid="sensors-25-05254-f001" ref-type="fig">Figure 1</xref>.</p><p>Tablet defect. The basic outline of the tablet is incomplete, and some structures are changed, including cracks, missing corners, wear, etc. The cracks are manifested as the tablet splitting from the top or bottom to form one or more fragments; the missing corners are mostly caused by external force collision, resulting in the missing corners of the tablet. Wear is due to local loss caused by friction and other reasons, resulting in an uneven tablet surface or shape change.</p><p>Tablet contamination. Discoloration, contamination, or impurities on the tablet surface, mainly including color contamination and impurity contamination.</p><p>Tablet surface scratches. This type of defect is generally caused by uneven molds or scratches by sharp objects. Depending on the depth and length of the scratches, the damage to the tablet structure is also different.</p></sec></sec><sec id="sec3-sensors-25-05254"><title>3. Method</title><p>The DDTD network structure proposed in this paper consists of a latent diffusion model guided by the AR network and an SF network for feature extraction. The LDM-AR network is used to generate high-quality reconstructed images, and the SF network is responsible for extracting multi-scale features and comparative analysis of the input image and the reconstructed image, and then outputting accurate anomaly scores. In terms of input images, we integrated tablet data from open source datasets such as the MVTec anomaly detection dataset (MVTec AD) [<xref rid="B36-sensors-25-05254" ref-type="bibr">36</xref>] to construct a tablet defect detection dataset containing multi-scale defects and complex backgrounds. Combined with the field images collected by the self-built test bench, the model&#8217;s field detection performance was tested on it.</p><sec id="sec3dot1-sensors-25-05254"><title>3.1. Network Architecture</title><p>In the image reconstruction task, in order to balance computational efficiency and image reconstruction quality, this paper introduces LDM as the core framework. The input image <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8476;</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is first encoded into a latent space vector <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>&#949;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through a pre-trained encoder, where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mo>&#8476;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, in the latent space, the vector is gradually noisy to become a random Gaussian noise vector <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, completing the forward diffusion process of the diffusion model. The output <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the forward diffusion process can be represented as follows:<disp-formula id="FD11-sensors-25-05254"><label>(11)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mover><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:msqrt><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover><mml:mi>&#945;</mml:mi><mml:mo stretchy="false">&#175;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In order to further enhance the model&#8217;s ability to understand the semantic information of the original image, this paper designs an AR network in the reverse denoising stage to guide the denoising network to reconstruct high-quality images. In this process, the random Gaussian noise vector <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is input into the denoising network of the diffusion model. At the same time, the AR network performs deep feature processing on the original image and the random Gaussian noise vector <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and fuses the calculation results with the denoising network output. Through this collaborative mechanism, the LDM is effectively guided to generate a reconstructed image <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> with richer details and more accurate semantics.</p><p>In the detection and location tasks of abnormal areas, we introduce the difference comparison between the two into the feature space. After obtaining the reconstructed image <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, it is input into the SF network together with the input image <italic toggle="yes">x</italic>. Through the multi-scale feature extraction strategy, the texture, shape, and structural differences of defects at multiple scales are effectively captured, thereby enhancing the recognition accuracy of various defects such as subtle scratches, tiny defects, and complex pollution. Taking into full consideration the characteristics of tablet defects, this paper adopts the combination of Euclidean distance and cosine similarity to design the anomaly score. By comparing the anomaly scores of input image <italic toggle="yes">x</italic> and reconstructed image <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> on feature maps of different scales, the abnormal area can be obtained. Finally, the abnormal area is visualized through heatmap generation to obtain the detection result. The DTDD network architecture is shown in <xref rid="sensors-25-05254-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec3dot2-sensors-25-05254"><title>3.2. Assisted Reconstruction Network</title><p>In order to solve the problems of detail loss and semantic deviation in traditional diffusion models when reconstructing images, this paper designs an Assisted Reconstruction network. In the AR network, the input consists of two parts: input image <italic toggle="yes">x</italic> and random Gaussian noise vector <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The input image is first reduced to the same dimension as a random Gaussian noise vector through convolution. The sum of <italic toggle="yes">x</italic> and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is then fed into the encoding block for continuous downsampling. Finally, the Assisted Reconstruction Middle (ARM) block extracts contextual information, gradually extracting and refining features and exploring deep connections between features, enabling the model to better understand the image content. The structure of the four encoding blocks and the ARM is shown in <xref rid="sensors-25-05254-f003" ref-type="fig">Figure 3</xref>.</p><p>The encoding block is mainly composed of a residual block with time-embedded conditional input, a standard attention mechanism, and a downsampling block. The current scale features extracted by each encoding block are saved and input into the downsampling block to achieve information transfer with the stable diffusion network. ARM mainly includes two residual layers and a semantically enhanced lightweight attention mechanism (SELA). The design of the double residual layer helps to extract deep features, and SELA can efficiently enhance the model&#8217;s ability to capture key semantic features.</p><p>The output of the ARM is superimposed with the middle block of the denoising network. The output of each encoding block is spliced with the output of the denoising network decoding block of the corresponding scale using jump connections, guiding the SD network from both global and local perspectives. In this way, the reconstructed output <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> covers both latent space features and original semantic features, which can fully focus on multi-scale information and achieve an accurate reconstruction of the tablet image.</p><p>After the denoising process is completed, the reconstructed output <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is restored to the reconstructed image <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> through the pre-trained decoder, and the loss function can be expressed as follows:<disp-formula id="FD12-sensors-25-05254"><label>(12)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>&#1013;</mml:mi><mml:mo>&#8764;</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="[" close="]"><mml:msubsup><mml:mfenced separators="" open="&#x2225;" close="&#x2225;"><mml:mi>&#1013;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#1013;</mml:mi><mml:mi>&#952;</mml:mi></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mfenced><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the auxiliary reconstruction information generated by the AR network.</p></sec><sec id="sec3dot3-sensors-25-05254"><title>3.3. Semantic-Enhanced Lightweight Attention</title><p>In order to make the AR network pay attention to global semantic information and accurately capture local detail textures, this paper designs a semantically enhanced lightweight attention (SELA) module to enhance the semantic expression and detail perception capabilities of the target features. The module structure and process are shown in <xref rid="sensors-25-05254-f004" ref-type="fig">Figure 4</xref>. The SELA attention module adopts a parallel dual-branch structure. The semantic channel branch (left) and the spatial detail branch (right) enhance the feature expression capabilities from different dimensions and finally fuse to generate a semantically enhanced feature map. The semantic channel branch generates channel-level statistical features through average pooling and maximum pooling operations in the spatial dimension, and then generates SC attention weights through lightweight convolution to focus on channels with semantic significance. The spatial detail branch mines intra-channel associations through grouped convolutions and uses depthwise separable convolutions to enhance spatial detail features, generating SD attention weights to highlight the spatial area where the semantic information is located. The weights generated by the two branches weight the original feature maps respectively, and finally the enhanced results of the two branches are fused through splicing and 1 &#215; 1 convolution.</p><p>Define the input feature map as <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and the global maximum pooling and global average pooling results as <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>&#957;</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>; then, the SC attention weight generated by the semantic channel branch is expressed as follows:<disp-formula id="FD13-sensors-25-05254"><label>(13)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>&#957;</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>C</mml:mi><mml:mi>r</mml:mi></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Sigmoid function, <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the RELU activation function, <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the 1 &#215; 1 convolution operation, and the number of output channels is <italic toggle="yes">k</italic>; <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the pooling splicing operation, and <italic toggle="yes">r</italic> is the channel compression ratio, which is set to 4 in this paper.</p><p>Define the group convolution of <italic toggle="yes">X</italic> to be divided into <italic toggle="yes">G</italic> groups, <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and the depth-wise separable convolution operation DWConv can be defined as <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, the SD attention weight calculated by the spatial detail branch is expressed as follows:<disp-formula id="FD14-sensors-25-05254"><label>(14)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The output <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> of the SELA module is expressed as follows:<disp-formula id="FD15-sensors-25-05254"><label>(15)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo>&#8857;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot4-sensors-25-05254"><title>3.4. Scale Fusion Network</title><p>We feed the reconstructed image <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and the input image <italic toggle="yes">x</italic> into the Scale Fusion (SF) network for feature extraction and calculate the anomaly score and locate the abnormal area based on the feature differences between the two types of images.</p><p>The SF network proposed in this paper designs four hierarchical feature extraction blocks (HFE Block) for the task of tablet anomaly detection. It realizes the hierarchical extraction of features of different scales through explicit task division, avoiding the computational overhead of cross-layer fusion. The network framework is shown in <xref rid="sensors-25-05254-f005" ref-type="fig">Figure 5</xref>.</p><p>Among them, after sampling the Patchify (4 &#215; 4) table, the initial features are extracted by 7 &#215; 7 deep separable convolution to retain the macro texture of the tablet surface; HFE Block2 enhances the sensitivity to subtle defects by downsampling through Patch Merging and stacking 3 times of 3 &#215; 3 deep separable convolutions; HFE Block3 introduces dilated convolution to expand the receptive field and adapt to defects of different sizes; HFE Block4 realizes global semantic modeling of the overall shape of the tablet through downsampling, 7 &#215; 7 deep separable convolution, and global average pooling. Taking the input image X as an example, the calculation process of its multi-scale feature map <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is as follows:<disp-formula id="FD16-sensors-25-05254"><label>(16)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD17-sensors-25-05254"><label>(17)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:msup><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-sensors-25-05254"><label>(18)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="FD19-sensors-25-05254"><label>(19)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2.em"/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution, <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a three-dimensional separable convolution stack, <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is an expansion convolution (expansion rate <italic toggle="yes">d</italic> = 2), <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a 2 &#215; 2 downsampling operation, <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is layer normalization, <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a Gaussian error linear unit activation function, <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a 1 &#215; 1 pointwise convolution, and <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is global average pooling. The structural diagram of the four blocks is shown in <xref rid="sensors-25-05254-f006" ref-type="fig">Figure 6</xref>.</p><p>The feature maps output at each stage have different resolutions and semantic information. We measure the abnormal distance between the input image feature map <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the reconstructed image feature map <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the corresponding scale, and we obtain the abnormal feature map <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> at different scales. Through the adaptive weighted fusion strategy, we use the learnable weights to upsample and fuse the feature maps of different scales, so that the model takes into account both details and global information. Compared with traditional models, the SF network can capture tiny defects and handle overall shape anomalies in the task of tablet anomaly detection, achieving a balance between accuracy and speed.</p></sec><sec id="sec3dot5-sensors-25-05254"><title>3.5. Anomaly Scores and Visualization</title><p>In the design of anomaly scores, cosine similarity, Mahalanobis distance, and Euclidean norm are the most commonly used calculation methods. Among them, Euclidean distance can measure the straight-line distance between two points in space, which is suitable for capturing the geometric shape, size, and other features of tablet images, thereby measuring such differences. Cosine similarity can measure the directional similarity of two vectors, which is suitable for capturing the texture, color distribution, and other features of tablets. Both are aimed at defect features in different directions and are simpler to calculate than Mahalanobis distance, which can improve the calculation speed. Considering the complex surface texture of tablets, many types of defects, and small individuals, we adopt a weighted combination of cosine similarity and Euclidean distance to construct anomaly scores. We calculate the anomaly scores of the input image features <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and reconstructed image features <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of different scales extracted by the SF network, and the expression of the Euclidean distance <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> of the i-th scale is<disp-formula id="FD20-sensors-25-05254"><label>(20)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In order to achieve the weighting of cosine similarity and Euclidean distance, cosine similarity <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> should be converted into cosine distance <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the formula of the cosine distance <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> of the <italic toggle="yes">i</italic>-th scale is as follows:<disp-formula id="FD21-sensors-25-05254"><label>(21)</label><mml:math id="mm81" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>&#8739;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>&#8739;</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, the expressions of the abnormal feature map <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and abnormal score <italic toggle="yes">S</italic> of the <italic toggle="yes">i</italic>-th scale are as follows:<disp-formula id="FD22-sensors-25-05254"><label>(22)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#955;</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#955;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD23-sensors-25-05254"><label>(23)</label><mml:math id="mm84" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>&#969;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#947;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the upsampling operation, <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#947;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the upsampling factor of the <italic toggle="yes">i</italic>-th scale, <italic toggle="yes">n</italic> is the number of feature maps involved in the fusion, and <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the weight coefficients of the distance metric and the multi-scale feature map, respectively. In order to display the calculated anomaly score results more concisely and clearly, and visually locate the defect area at the same time, we first use the anomaly score to generate anomaly maps and perform normalization and color mapping to obtain a more understandable visualization heat map. Similar operations are performed on the input image, and the heat map and the original image are superimposed, with the weights set to 0.6 and 0.4. The generated visualization results can display the abnormal area while retaining the original image information.</p></sec></sec><sec id="sec4-sensors-25-05254"><title>4. Experiment</title><sec id="sec4dot1-sensors-25-05254"><title>4.1. Experimental Dataset</title><p>This study used multiple open-source datasets, including MVTec (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.mvtec.com/company/research/datasets/mvtec-ad">https://www.mvtec.com/company/research/datasets/mvtec-ad</uri>, accessed on 19 August 2025) and PILL (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://aistudio.baidu.com/datasetdetail/121965">https://aistudio.baidu.com/datasetdetail/121965</uri>, accessed on 19 August 2025), containing 3623 images of normal tablets and 1344 images of tablets with various defects, such as scratches, defects, stains, mold, bumps, and adhesions, mimicking actual industrial production scenarios. Of the 4967 tablet images in the dataset, the training set consists of 3586 normal tablet images, and the test set consists of 1381 images, including both normal and abnormal samples.</p></sec><sec id="sec4dot2-sensors-25-05254"><title>4.2. Implementation Details</title><p>We conducted experiments on a Linux deep learning server using the Ubuntu 20.04 operating system; the CPU model was 14 vCPU Intel (R) Xeon (R) Platinum 8362 CPU@2.80GHz, the GPU model was RTX3090, the video memory size was 48 GB, and the memory size was 64 GB. All models used were implemented based on Pytorch 3.9 using the PYtorch 1.12.1 framework, the Cuda version was 11.3, the image size was uniformly set to 256 &#215; 256, the training rounds were 500 times, the batch size was 16, and all parameters were kept consistent in the experiments. All experimental results in this paper are presented as the mean &#177; standard deviation of five independent runs, and the significance of the improvements was verified by a <italic toggle="yes">t</italic>-test.</p></sec><sec id="sec4dot3-sensors-25-05254"><title>4.3. Evaluation Metrics</title><p>The evaluation indicators of anomaly detection usually include recall rate, precision, F1 score, AUROC, average precision, and the per-region-overlap score of each region, which are used to evaluate detection efficiency and accuracy. Unsupervised anomaly detection is characterized by a scarcity of anomaly samples, a high proportion of normal samples, class imbalance, and ambiguous anomaly definitions. The task requires both classification and localization. Therefore, evaluation metrics must meet the following requirements: be robust to imbalanced data, measure discriminative ability, and support region-level evaluation.</p><p>A single metric alone cannot meet these requirements. Therefore, this paper uses a combination of AUROC, PRO, F1, and AP as its evaluation metric. AUROC is insensitive to imbalanced data, eliminates the need for manual thresholding, and measures overall discriminative ability. AP is a comprehensive metric that combines ranking and localization but is sensitive to the ratio of positive and negative samples. F1 balances precision and recall, assessing overall performance at a practical threshold. PRO is a key metric for region-level anomaly localization, complementing the previous three metrics that ignore &#8220;regional details&#8221;. The formula is shown as follows:</p><p>The AUROC evaluation model is used as Formula (24) to evaluate the overall ability of the model to distinguish normal and abnormal samples under different thresholds. The closer the value is to 1, the better the model distinguishes between positive and negative samples.<disp-formula id="FD24-sensors-25-05254"><label>(24)</label><mml:math id="mm89" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The PRO index shown in Formula (25) is used to evaluate the overlap between the abnormal area predicted by the model and the actual abnormal area, and to measure the accuracy of locating the abnormal position.<disp-formula id="FD25-sensors-25-05254"><label>(25)</label><mml:math id="mm90" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>&#8745;</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> score shown in Formula (26) is used to comprehensively measure the accuracy and recall of the model in detecting anomalies and to more comprehensively determine the accuracy of the algorithm.<disp-formula id="FD26-sensors-25-05254"><label>(26)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>AP is used to judge the detection accuracy of the model for abnormal samples. By calculating the average precision under different recall rates, the accuracy of the model in detecting anomalies is reflected. Its expression is shown in Formula (27).<disp-formula id="FD27-sensors-25-05254"><label>(27)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec4dot4-sensors-25-05254"><title>4.4. Experimental Results</title><sec id="sec4dot4dot1-sensors-25-05254"><title>4.4.1. Comparison with Baseline Model</title><p>The improvement of the algorithmic framework in the image reconstruction part is a targeted innovation based on the LDM algorithm process and the requirements of the tablet defect detection task. Therefore, this paper first compares the improved algorithm with the LDM algorithm to verify the advanced nature of the AR network. The experimental results are shown in <xref rid="sensors-25-05254-t001" ref-type="table">Table 1</xref>.</p><p>It can be seen from the table data that the algorithm proposed in this paper performs excellently across all metrics. After verification by the <italic toggle="yes">t</italic>-test method, the results show that, compared with the LDM model, each metric achieves a significant improvement. The AP index increases by 11.15%, the <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> score increases by 9.61%, the AUROC increases by 14.61%, and the PRO increases by 15.73%. This shows that the AR network plays an important role in improving the overall accuracy of the model and guiding the model to perform image reconstruction.</p></sec><sec id="sec4dot4dot2-sensors-25-05254"><title>4.4.2. Comparative Experiment</title><p>The improved algorithm is improved and designed for the task of tablet defect detection and can cope with various defect types and small sample challenges. The visualization results of the test are shown in <xref rid="sensors-25-05254-f007" ref-type="fig">Figure 7</xref>. To further verify the superiority of the algorithm in the task of tablet defect detection, the algorithm in this paper is compared with five algorithms: UniAD [<xref rid="B37-sensors-25-05254" ref-type="bibr">37</xref>], PaDiM [<xref rid="B38-sensors-25-05254" ref-type="bibr">38</xref>], RD4AD [<xref rid="B39-sensors-25-05254" ref-type="bibr">39</xref>], DRAEM, and YOLOV8. The experimental results are shown in <xref rid="sensors-25-05254-t002" ref-type="table">Table 2</xref>.</p><p>As shown in the table, the proposed algorithm achieves the best detection performance. Compared with the supervised algorithm YOLOV8, our method yields comparable results in AP and F1 metrics but demonstrates significant improvements in AUROC and PRO. Compared to the best results among other unsupervised algorithms, AP, F1, AUROC, and PRO are improved by 2.85%, 2.55%, 4.31%, and 2.36%, respectively. Therefore, our algorithm outperforms UniAD, PaDiM, RD4AD, DRAEM, and YOLOV8 in tablet defect detection tasks. To provide a more intuitive comparison between models, the results are presented in the form of bar charts in <xref rid="sensors-25-05254-f008" ref-type="fig">Figure 8</xref>. <xref rid="sensors-25-05254-f009" ref-type="fig">Figure 9</xref> illustrates the performance of each algorithm on different tablet detection tasks within the dataset. As shown in the figure, the proposed algorithm exhibits superior reconstruction performance and sensitivity to anomalous regions, enabling precise detection of small defects of various types and shapes.</p></sec><sec id="sec4dot4dot3-sensors-25-05254"><title>4.4.3. Ablation Experiment</title><p>In addition to the AR network, this paper mainly makes innovative improvements in the three aspects of attention mechanism, feature extraction network, and anomaly score measurement, so ablation experiments were conducted on these three aspects to prove the effectiveness of the improvements.</p><p>The improvement of this paper in the attention mechanism is that it innovatively proposes the SELA attention mechanism to efficiently enhance the model&#8217;s ability to capture key semantic features. It is now compared with the three commonly used attention mechanisms of Self-Attention [<xref rid="B40-sensors-25-05254" ref-type="bibr">40</xref>], CBAM, and ECA-Net [<xref rid="B41-sensors-25-05254" ref-type="bibr">41</xref>]. The four attentions were used to train the model under the same parameters and to test the indicators. The data in <xref rid="sensors-25-05254-t003" ref-type="table">Table 3</xref> are as follows:</p><p>From the data in <xref rid="sensors-25-05254-t003" ref-type="table">Table 3</xref>, it can be seen that the SELA attention proposed in this paper has better performance. Compared with other attentions, it has an improvement of 2.14%, 3.96%, and 2.01% in AP, AUROC, and PRO, respectively. Among them, SELA is slightly worse than ECA in <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> score, which is related to the strong morphological texture feature capture ability of the ECA local cross-channel interaction structure. In comparison, SELA has a certain gap, but it has more advantages in distinguishing boundary samples and multi-class defects.</p><p>The improvement made in this paper in the feature extraction part is that it adopts an SF network with stronger feature extraction ability and multi-scale target processing ability. In order to verify the effectiveness of this network, different feature extraction networks were combined with the model of this paper, and experiments were carried out separately. The experiment replaced the SF network with representative models of ResNet, VGG, Efficient net, and Inception net series, and it added the combined structural comparison of ResNet50+FPN to increase the persuasiveness of the experiment. The ablation experiment results are shown in <xref rid="sensors-25-05254-t004" ref-type="table">Table 4</xref>, which are organized into a bar chart format as shown in <xref rid="sensors-25-05254-f010" ref-type="fig">Figure 10</xref>.</p><p>From the data in the table, we can see that although the combination of ResNet+FPN has a certain improvement compared with other networks, its computational complexity is larger, which will cause a large waste of computing power. The algorithm in this paper not only has a lower computational complexity than the combination of ResNet+FPN, but also outperforms other feature extraction networks in various indicators and has better results in tablet defect detection. Among the four indicators, AP is 2.28% higher than the best data in other algorithms, <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is 2.34% higher than the best data in other algorithms, AUROC is 2.10% higher than the best data in other algorithms, and PRO is 2.33% higher than the best data in other algorithms, which further proves the advantages of the algorithm in this paper in the task of tablet defect detection.</p><p>The anomaly score measurement indicators used in this paper are weighted Euclidean distance and cosine distance, which are more suitable for the tablet defect detection task than a single indicator. In the calculation formula of the anomaly score, the weight <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the anomaly map of different scales is obtained by network training, while the weighted values of Euclidean distance and cosine distance are obtained by the grid search method. <xref rid="sensors-25-05254-t005" ref-type="table">Table 5</xref> lists the data with <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> as 0 (cosine measurement), 1 (Euclidean measurement), and 0.85 (optimal value of grid search method); <xref rid="sensors-25-05254-f011" ref-type="fig">Figure 11</xref> is a trend chart of the dual indicators AUROC and <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> changing with <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula>, which is used to show the results of the grid search method. The best value obtained in this experiment is 0.85.</p></sec><sec id="sec4dot4dot4-sensors-25-05254"><title>4.4.4. Physical Verification</title><p>To verify the effectiveness of the algorithm in practical production scenarios, a complete data acquisition system was built using an MV-CS050-10UC camera, various LED light sources, and an XCY-ICS400-V2 line scan test rig platform. This system was used to construct a comprehensive tablet defect detection dataset that includes samples of normal, defective, contaminated, and scratched tablets. This dataset is designed in the MVTec AD format, and the dataset contains 1000 normal tablets and more than 1500 abnormal defect photos. For this dataset, the algorithm in this paper is compared with the four algorithms of UniAD, PaDiM, RD4AD, and DRAEM to fully verify the performance of this method in the task of tablet defect detection. The experimental results are shown in <xref rid="sensors-25-05254-t006" ref-type="table">Table 6</xref>, and the visualization results are shown in <xref rid="sensors-25-05254-f012" ref-type="fig">Figure 12</xref>.</p><p>As shown in <xref rid="sensors-25-05254-t006" ref-type="table">Table 6</xref>, in the field validation dataset, the proposed algorithm is still superior to the other four algorithms in various indicators, and compared with the best data, AP, AUROC, F1, and PRO improved by 1.54%, 3.26%, 5.60%, and 2.7 8%, respectively. It can also be seen from <xref rid="sensors-25-05254-f012" ref-type="fig">Figure 12</xref> that when faced with interference factors such as complex tablet textures and diverse defect shapes, DRAEM missed detections, oversimplified abnormal expressions of abnormal areas, and had weak overall detection robustness. UniAD and other algorithms had different degrees of insufficient thermal differentiation between the background and defects and were prone to misjudgment in complex defect scenes. In contrast, the background area of the proposed method was thermally clean; the abnormal area was thermally focused, had clear boundaries, and had better abnormality detection effects.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05254"><title>5. Conclusions</title><p>In order to solve the problems of diversified defects, few defect samples, the poor reconstruction effect of unsupervised methods based on reconstruction, and the inaccurate positioning of abnormal areas in the tablet production process, this paper proposes an unsupervised tablet defect detection method (DTDD) based on a diffusion model. We propose an Assisted Reconstruction (AR) network and Semantic-Enhanced Lightweight Attention to help the diffusion model achieve better reconstruction results, and we use the Scale Fusion (SF) network in the feature matching part to improve the matching accuracy. Finally, this paper designs anomaly scores and visualization heat maps for tablet defect detection tasks to comprehensively and reasonably evaluate and display the anomaly detection effect. A large number of experimental results on tablet defect datasets show that the algorithm in this paper is superior to advanced methods in the same field, and both positioning and detection performance are greatly improved. In the future, we will use more data to train the model and expand the training target to more types of pharmaceutical products so that it can play a greater role in the pharmaceutical testing industry.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.Z., W.L. and L.H.; methodology, W.L.; software, M.Z.; validation, M.Z. and D.W.; formal analysis, M.Z.; data curation, M.Z.; writing&#8212;original draft preparation, M.Z.; writing&#8212;review and editing, W.L. and L.H.; visualization, W.L.; supervision, W.L.; project administration, W.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Dataset available on request from the authors.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05254"><label>1.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jose</surname><given-names>J.</given-names></name><name name-style="western"><surname>Cox</surname><given-names>A.R.</given-names></name><name name-style="western"><surname>Bate</surname><given-names>A.</given-names></name></person-group><article-title>Introduction to Drug Safety and Pharmacovigilance</article-title><source>Principles and Practice of Pharmacovigilance and Drug Safety</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2024</year><fpage>3</fpage><lpage>30</lpage></element-citation></ref><ref id="B2-sensors-25-05254"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mishra</surname><given-names>H.P.</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>R.</given-names></name></person-group><article-title>Leveraging Generative AI for Drug Safety and Pharmacovigilance</article-title><source>Curr. Rev. Clin. Exp. Pharmacol.</source><year>2025</year><volume>20</volume><fpage>89</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.2174/0127724328311400240823062829</pub-id><pub-id pub-id-type="pmid">39238375</pub-id></element-citation></ref><ref id="B3-sensors-25-05254"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Trifir&#242;</surname><given-names>G.</given-names></name><name name-style="western"><surname>Crisafulli</surname><given-names>S.</given-names></name></person-group><article-title>A new era of pharmacovigilance: Future challenges and opportunities</article-title><source>Front. Drug Saf. Regul.</source><year>2022</year><volume>2</volume><elocation-id>866898</elocation-id><pub-id pub-id-type="doi">10.3389/fdsfr.2022.866898</pub-id></element-citation></ref><ref id="B4-sensors-25-05254"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>Improved U2Net-Based Surface Defect Detection Method for Blister Tablets</article-title><source>Algorithms</source><year>2024</year><volume>17</volume><elocation-id>429</elocation-id><pub-id pub-id-type="doi">10.3390/a17100429</pub-id></element-citation></ref><ref id="B5-sensors-25-05254"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Ding</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>F.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>L.</given-names></name></person-group><article-title>Surface defect detection methods for industrial products: A review</article-title><source>Appl. Sci.</source><year>2021</year><volume>11</volume><elocation-id>7657</elocation-id><pub-id pub-id-type="doi">10.3390/app11167657</pub-id></element-citation></ref><ref id="B6-sensors-25-05254"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bihan</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lebrun-Vignes</surname><given-names>B.</given-names></name><name name-style="western"><surname>Funck-Brentano</surname><given-names>C.</given-names></name><name name-style="western"><surname>Salem</surname><given-names>J.E.</given-names></name></person-group><article-title>Uses of pharmacovigilance databases: An overview</article-title><source>Therapies</source><year>2020</year><volume>75</volume><fpage>591</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1016/j.therap.2020.02.022</pub-id><pub-id pub-id-type="pmid">32169289</pub-id></element-citation></ref><ref id="B7-sensors-25-05254"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>L.</given-names></name><name name-style="western"><surname>Prakash</surname><given-names>A.</given-names></name><name name-style="western"><surname>Medhi</surname><given-names>B.</given-names></name></person-group><article-title>Ensuring medication and patient safety for better quality healthcare</article-title><source>Indian J. Pharmacol.</source><year>2024</year><volume>56</volume><fpage>375</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.4103/ijp.ijp_109_25</pub-id><pub-id pub-id-type="pmid">39973825</pub-id><pub-id pub-id-type="pmcid">PMC11913332</pub-id></element-citation></ref><ref id="B8-sensors-25-05254"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Al-Worafi</surname><given-names>Y.M.</given-names></name></person-group><article-title>Quality indicators for medications safety</article-title><source>Drug safety in developing countries</source><publisher-name>Elsevier</publisher-name><publisher-loc>Amsterdam, The Netherlands</publisher-loc><year>2020</year><fpage>229</fpage><lpage>242</lpage></element-citation></ref><ref id="B9-sensors-25-05254"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kittikunakorn</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sorman</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Marsh</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mongeau</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pich&#233;</surname><given-names>N.</given-names></name><name name-style="western"><surname>Williams</surname><given-names>R.O.</given-names><suffix>III</suffix></name><name name-style="western"><surname>Skomski</surname><given-names>D.</given-names></name></person-group><article-title>Application of deep learning convolutional neural networks for internal tablet defect detection: High accuracy, throughput, and adaptability</article-title><source>J. Pharm. Sci.</source><year>2020</year><volume>109</volume><fpage>1547</fpage><lpage>1557</lpage><pub-id pub-id-type="doi">10.1016/j.xphs.2020.01.014</pub-id><pub-id pub-id-type="pmid">31982393</pub-id></element-citation></ref><ref id="B10-sensors-25-05254"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>X.</given-names></name><name name-style="western"><surname>Kittikunakorn</surname><given-names>N.</given-names></name><name name-style="western"><surname>Sorman</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xi</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>A.</given-names></name><name name-style="western"><surname>Marsh</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mongeau</surname><given-names>A.</given-names></name><name name-style="western"><surname>Pich&#233;</surname><given-names>N.</given-names></name><name name-style="western"><surname>Williams</surname><given-names>R.</given-names><suffix>III</suffix></name><name name-style="western"><surname>Skomski</surname><given-names>D.</given-names></name></person-group><article-title>Deep learning convolutional neural networks for pharmaceutical tablet defect detection</article-title><source>Microsc. Microanal.</source><year>2020</year><volume>26</volume><fpage>1606</fpage><lpage>1609</lpage><pub-id pub-id-type="doi">10.1017/S1431927620018693</pub-id><pub-id pub-id-type="pmid">31982393</pub-id></element-citation></ref><ref id="B11-sensors-25-05254"><label>11.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Quan</surname><given-names>H.T.</given-names></name><name name-style="western"><surname>Huy</surname><given-names>D.D.</given-names></name><name name-style="western"><surname>Hoan</surname><given-names>N.T.</given-names></name><name name-style="western"><surname>Duc</surname><given-names>N.T.</given-names></name></person-group><article-title>Deep learning-based automatic detection of defective tablets in pharmaceutical manufacturing</article-title><source>Proceedings of the 8th International Conference on the Development of Biomedical Engineering in Vietnam: Proceedings of BME 8, 2020, Vietnam: Healthcare Technology for Smart City in Low-and Middle-Income Countries</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2022</year><fpage>789</fpage><lpage>801</lpage></element-citation></ref><ref id="B12-sensors-25-05254"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ettalibi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Elouadi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mansour</surname><given-names>A.</given-names></name></person-group><article-title>AI and computer vision-based real-time quality control: A review of industrial applications</article-title><source>Procedia Comput. Sci.</source><year>2024</year><volume>231</volume><fpage>212</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2023.12.195</pub-id></element-citation></ref><ref id="B13-sensors-25-05254"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kwon</surname><given-names>H.J.</given-names></name><name name-style="western"><surname>Kim</surname><given-names>H.G.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>S.H.</given-names></name></person-group><article-title>Pill detection model for medicine inspection based on deep learning</article-title><source>Chemosensors</source><year>2021</year><volume>10</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3390/chemosensors10010004</pub-id></element-citation></ref><ref id="B14-sensors-25-05254"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Jackel</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bottou</surname><given-names>L.</given-names></name><name name-style="western"><surname>Brunot</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cortes</surname><given-names>C.</given-names></name><name name-style="western"><surname>Denker</surname><given-names>J.</given-names></name><name name-style="western"><surname>Drucker</surname><given-names>H.</given-names></name><name name-style="western"><surname>Guyon</surname><given-names>I.</given-names></name><name name-style="western"><surname>Muller</surname><given-names>U.</given-names></name><name name-style="western"><surname>Sackinger</surname><given-names>E.</given-names></name><etal/></person-group><article-title>Comparison of learning algorithms for handwritten digit recognition</article-title><source>Proceedings of the International Conference on Artificial Neural Networks</source><conf-loc>Perth, Australia</conf-loc><conf-date>27 November&#8211;1 December 1995</conf-date><volume>Volume 60</volume><fpage>53</fpage><lpage>60</lpage></element-citation></ref><ref id="B15-sensors-25-05254"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name><name name-style="western"><surname>He</surname><given-names>K.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2016</year><volume>39</volume><fpage>1137</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id></element-citation></ref><ref id="B16-sensors-25-05254"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J.</given-names></name><name name-style="western"><surname>Divvala</surname><given-names>S.</given-names></name><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style="western"><surname>Farhadi</surname><given-names>A.</given-names></name></person-group><article-title>You only look once: Unified, real-time object detection</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><publisher-name>IEEE Computer Society</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2016</year><fpage>779</fpage><lpage>788</lpage></element-citation></ref><ref id="B17-sensors-25-05254"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Mac</surname><given-names>T.T.</given-names></name></person-group><article-title>Pill Defect Detection Using an Improved Convolutional Neural Network</article-title><source>Proceedings of the International Conference on Mechatronics and Control Engineering</source><conf-loc>Virtual</conf-loc><conf-date>26&#8211;28 July 2021</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year><fpage>55</fpage><lpage>60</lpage></element-citation></ref><ref id="B18-sensors-25-05254"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xue</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Jun</surname><given-names>L.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Xiaoyu</surname><given-names>Q.</given-names></name></person-group><article-title>Real-time detection method of pill surface defects based on YOLOV5</article-title><source>Inf. Technol. Netw. Secur. Jishu Wangluo Anquan</source><year>2021</year><volume>40</volume><pub-id pub-id-type="doi">10.19358/j.issn.2096-5133.2021.12.008</pub-id></element-citation></ref><ref id="B19-sensors-25-05254"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>S.</given-names></name><name name-style="western"><surname>Jo</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Cho</surname><given-names>J.</given-names></name><name name-style="western"><surname>Song</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>M.</given-names></name></person-group><article-title>Spatially variant convolutional autoencoder based on patch division for pill defect detection</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>216781</fpage><lpage>216792</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3041790</pub-id></element-citation></ref><ref id="B20-sensors-25-05254"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zavrtanik</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kristan</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sko&#269;aj</surname><given-names>D.</given-names></name></person-group><article-title>Draem-a discriminatively trained reconstruction embedding for surface anomaly detection</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, BC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>8330</fpage><lpage>8339</lpage></element-citation></ref><ref id="B21-sensors-25-05254"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Roth</surname><given-names>K.</given-names></name><name name-style="western"><surname>Pemula</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zepeda</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sch&#246;lkopf</surname><given-names>B.</given-names></name><name name-style="western"><surname>Brox</surname><given-names>T.</given-names></name><name name-style="western"><surname>Gehler</surname><given-names>P.</given-names></name></person-group><article-title>Towards total recall in industrial anomaly detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>14318</fpage><lpage>14328</lpage></element-citation></ref><ref id="B22-sensors-25-05254"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Rombach</surname><given-names>R.</given-names></name><name name-style="western"><surname>Blattmann</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lorenz</surname><given-names>D.</given-names></name><name name-style="western"><surname>Esser</surname><given-names>P.</given-names></name><name name-style="western"><surname>Ommer</surname><given-names>B.</given-names></name></person-group><article-title>High-resolution image synthesis with latent diffusion models</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>10684</fpage><lpage>10695</lpage></element-citation></ref><ref id="B23-sensors-25-05254"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>H.</given-names></name><name name-style="western"><surname>Noh</surname><given-names>J.</given-names></name><name name-style="western"><surname>Ham</surname><given-names>B.</given-names></name></person-group><article-title>Learning memory-guided normality for anomaly detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#8211;19 June 2020</conf-date><fpage>14372</fpage><lpage>14381</lpage></element-citation></ref><ref id="B24-sensors-25-05254"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>G.E.</given-names></name><name name-style="western"><surname>Salakhutdinov</surname><given-names>R.R.</given-names></name></person-group><article-title>Reducing the dimensionality of data with neural networks</article-title><source>Science</source><year>2006</year><volume>313</volume><fpage>504</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1126/science.1127647</pub-id><pub-id pub-id-type="pmid">16873662</pub-id></element-citation></ref><ref id="B25-sensors-25-05254"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group><article-title>Auto-Encoding Variational Bayes</article-title><source>Stat</source><year>2014</year><volume>1050</volume><fpage>1</fpage></element-citation></ref><ref id="B26-sensors-25-05254"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goodfellow</surname><given-names>I.</given-names></name><name name-style="western"><surname>Pouget-Abadie</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mirza</surname><given-names>M.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Warde-Farley</surname><given-names>D.</given-names></name><name name-style="western"><surname>Ozair</surname><given-names>S.</given-names></name><name name-style="western"><surname>Courville</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Generative adversarial networks</article-title><source>Commun. ACM</source><year>2020</year><volume>63</volume><fpage>139</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1145/3422622</pub-id></element-citation></ref><ref id="B27-sensors-25-05254"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Sohl-Dickstein</surname><given-names>J.</given-names></name><name name-style="western"><surname>Weiss</surname><given-names>E.</given-names></name><name name-style="western"><surname>Maheswaranathan</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ganguli</surname><given-names>S.</given-names></name></person-group><article-title>Deep unsupervised learning using nonequilibrium thermodynamics</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Lile, France</conf-loc><conf-date>6&#8211;11 July 2015</conf-date><fpage>2256</fpage><lpage>2265</lpage></element-citation></ref><ref id="B28-sensors-25-05254"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ho</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jain</surname><given-names>A.</given-names></name><name name-style="western"><surname>Abbeel</surname><given-names>P.</given-names></name></person-group><article-title>Denoising diffusion probabilistic models</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>6840</fpage><lpage>6851</lpage></element-citation></ref><ref id="B29-sensors-25-05254"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Z.</given-names></name></person-group><article-title>A Surface Defect Detection Method Based on Image Guided Conditional Diffusion Model</article-title><source>Proceedings of the 2024 43rd Chinese Control Conference (CCC)</source><conf-loc>Kunming, China</conf-loc><conf-date>28&#8211;31 July 2024</conf-date><fpage>8541</fpage><lpage>8546</lpage></element-citation></ref><ref id="B30-sensors-25-05254"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yin</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hong</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zuo</surname><given-names>W.</given-names></name></person-group><article-title>GLAD: Towards better reconstruction with global and local adaptive diffusion models for unsupervised anomaly detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2024</year><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="B31-sensors-25-05254"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name><name name-style="western"><surname>Xie</surname><given-names>L.</given-names></name></person-group><article-title>A diffusion-based framework for multi-class anomaly detection</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>26&#8211;27 February 2024</conf-date><volume>Volume 38</volume><fpage>8472</fpage><lpage>8480</lpage></element-citation></ref><ref id="B32-sensors-25-05254"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X.</given-names></name></person-group><article-title>Realnet: A feature selection network with realistic synthetic anomaly for anomaly detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><fpage>16699</fpage><lpage>16708</lpage></element-citation></ref><ref id="B33-sensors-25-05254"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Li</surname><given-names>G.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z.</given-names></name></person-group><article-title>RGB-D Rail Surface Defect Inspection Driven by Conditional Diffusion Architecture and Frequency Knowledge</article-title><source>IEEE Sens. J.</source><year>2025</year><volume>25</volume><fpage>18334</fpage><lpage>18343</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2025.3554715</pub-id></element-citation></ref><ref id="B34-sensors-25-05254"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Woo</surname><given-names>S.</given-names></name><name name-style="western"><surname>Park</surname><given-names>J.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.Y.</given-names></name><name name-style="western"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>Cbam: Convolutional block attention module</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2018</year><fpage>3</fpage><lpage>19</lpage></element-citation></ref><ref id="B35-sensors-25-05254"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Mai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Luo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>X.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>K.</given-names></name></person-group><article-title>Effective feature fusion network in BIFPN for small object detection</article-title><source>Proceedings of the 2021 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>19&#8211;22 September 2021</conf-date><fpage>699</fpage><lpage>703</lpage></element-citation></ref><ref id="B36-sensors-25-05254"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Bergmann</surname><given-names>P.</given-names></name><name name-style="western"><surname>Fauser</surname><given-names>M.</given-names></name><name name-style="western"><surname>Sattlegger</surname><given-names>D.</given-names></name><name name-style="western"><surname>Steger</surname><given-names>C.</given-names></name></person-group><article-title>MVTec AD&#8211;A comprehensive real-world dataset for unsupervised anomaly detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>16&#8211;17 June 2019</conf-date><fpage>9592</fpage><lpage>9600</lpage></element-citation></ref><ref id="B37-sensors-25-05254"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Cui</surname><given-names>L.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>K.</given-names></name><name name-style="western"><surname>Lu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Le</surname><given-names>X.</given-names></name></person-group><article-title>A unified model for multi-class anomaly detection</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>4571</fpage><lpage>4584</lpage></element-citation></ref><ref id="B38-sensors-25-05254"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Defard</surname><given-names>T.</given-names></name><name name-style="western"><surname>Setkov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Loesch</surname><given-names>A.</given-names></name><name name-style="western"><surname>Audigier</surname><given-names>R.</given-names></name></person-group><article-title>Padim: A patch distribution modeling framework for anomaly detection and localization</article-title><source>Proceedings of the International Conference on Pattern Recognition</source><conf-loc>Xiamen, China</conf-loc><conf-date>24&#8211;26 September 2021</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year><fpage>475</fpage><lpage>489</lpage></element-citation></ref><ref id="B39-sensors-25-05254"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>Anomaly detection via reverse distillation from one-class embedding</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>9737</fpage><lpage>9746</lpage></element-citation></ref><ref id="B40-sensors-25-05254"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style="western"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style="western"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style="western"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Jones</surname><given-names>L.</given-names></name><name name-style="western"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style="western"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style="western"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS 2017)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#8211;9 December 2017</conf-date></element-citation></ref><ref id="B41-sensors-25-05254"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>P.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name><name name-style="western"><surname>Zuo</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hu</surname><given-names>Q.</given-names></name></person-group><article-title>ECA-Net: Efficient channel attention for deep convolutional neural networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>11534</fpage><lpage>11542</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05254-f001" orientation="portrait"><label>Figure 1</label><caption><p>Tablet defect types.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g001.jpg"/></fig><fig position="float" id="sensors-25-05254-f002" orientation="portrait"><label>Figure 2</label><caption><p>Model framework.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g002.jpg"/></fig><fig position="float" id="sensors-25-05254-f003" orientation="portrait"><label>Figure 3</label><caption><p>Implementation details of encoder in AR network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g003.jpg"/></fig><fig position="float" id="sensors-25-05254-f004" orientation="portrait"><label>Figure 4</label><caption><p>Schematic diagram of SELA.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g004.jpg"/></fig><fig position="float" id="sensors-25-05254-f005" orientation="portrait"><label>Figure 5</label><caption><p>Schematic diagram of SF network.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g005.jpg"/></fig><fig position="float" id="sensors-25-05254-f006" orientation="portrait"><label>Figure 6</label><caption><p>Schematic diagram of HFE blocks.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g006.jpg"/></fig><fig position="float" id="sensors-25-05254-f007" orientation="portrait"><label>Figure 7</label><caption><p>Visualization result.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g007.jpg"/></fig><fig position="float" id="sensors-25-05254-f008" orientation="portrait"><label>Figure 8</label><caption><p>Comparative experimental results bar graph.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g008.jpg"/></fig><fig position="float" id="sensors-25-05254-f009" orientation="portrait"><label>Figure 9</label><caption><p>Qualitative illustration on Tablet dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g009.jpg"/></fig><fig position="float" id="sensors-25-05254-f010" orientation="portrait"><label>Figure 10</label><caption><p>Ablation experiment results bar graph.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g010.jpg"/></fig><fig position="float" id="sensors-25-05254-f011" orientation="portrait"><label>Figure 11</label><caption><p>Line graph of AUROC and <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> changes.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g011.jpg"/></fig><fig position="float" id="sensors-25-05254-f012" orientation="portrait"><label>Figure 12</label><caption><p>Qualitative illustration on physical verification dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05254-g012.jpg"/></fig><table-wrap position="float" id="sensors-25-05254-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05254-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparative experiment with baseline model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Algorithmic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AUROC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PRO (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">LDM</td><td align="center" valign="middle" rowspan="1" colspan="1">86.27 &#177; 3.75</td><td align="center" valign="middle" rowspan="1" colspan="1">85.62 &#177; 3.38</td><td align="center" valign="middle" rowspan="1" colspan="1">80.13 &#177; 2.42</td><td align="center" valign="middle" rowspan="1" colspan="1">75.91 &#177; 5.93</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.42 &#177; 1.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.23 &#177; 1.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.74 &#177; 1.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.64 &#177; 1.53</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05254-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05254-t002_Table 2</object-id><label>Table 2</label><caption><p>Quantitative results on Tablet dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Algorithmic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AUROC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PRO (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">UniAD</td><td align="center" valign="middle" rowspan="1" colspan="1">93.52 &#177; 3.02</td><td align="center" valign="middle" rowspan="1" colspan="1">92.07 &#177; 2.43</td><td align="center" valign="middle" rowspan="1" colspan="1">89.14 &#177; 1.29</td><td align="center" valign="middle" rowspan="1" colspan="1">87.73 &#177; 3.58</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PaDiM</td><td align="center" valign="middle" rowspan="1" colspan="1">91.41 &#177; 1.76</td><td align="center" valign="middle" rowspan="1" colspan="1">90.82 &#177; 1.38</td><td align="center" valign="middle" rowspan="1" colspan="1">87.55 &#177; 0.79</td><td align="center" valign="middle" rowspan="1" colspan="1">84.91 &#177; 2.47</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RD4AD</td><td align="center" valign="middle" rowspan="1" colspan="1">94.57 &#177; 1.54</td><td align="center" valign="middle" rowspan="1" colspan="1">92.68 &#177; 1.97</td><td align="center" valign="middle" rowspan="1" colspan="1">90.43 &#177; 1.06</td><td align="center" valign="middle" rowspan="1" colspan="1">89.28 &#177; 2.78</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DRAEM</td><td align="center" valign="middle" rowspan="1" colspan="1">90.86 &#177; 3.32</td><td align="center" valign="middle" rowspan="1" colspan="1">88.32 &#177; 3.01</td><td align="center" valign="middle" rowspan="1" colspan="1">86.19 &#177; 1.65</td><td align="center" valign="middle" rowspan="1" colspan="1">83.36 &#177; 4.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOV8</td><td align="center" valign="middle" rowspan="1" colspan="1">96.72 &#177; 4.35</td><td align="center" valign="middle" rowspan="1" colspan="1">96.01 &#177; 4.03</td><td align="center" valign="middle" rowspan="1" colspan="1">91.31 &#177; 2.87</td><td align="center" valign="middle" rowspan="1" colspan="1">84.62 &#177; 6.32</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.42 &#177; 1.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.23 &#177; 1.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.74 &#177; 1.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.64 &#177; 1.53</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05254-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05254-t003_Table 3</object-id><label>Table 3</label><caption><p>Ablation studies of SELA.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Algorithmic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AUROC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PRO (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Self-Attention</td><td align="center" valign="middle" rowspan="1" colspan="1">90.38 &#177; 1.80</td><td align="center" valign="middle" rowspan="1" colspan="1">89.76 &#177; 1.75</td><td align="center" valign="middle" rowspan="1" colspan="1">85.52 &#177; 1.62</td><td align="center" valign="middle" rowspan="1" colspan="1">85.07 &#177; 1.85</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CBAM</td><td align="center" valign="middle" rowspan="1" colspan="1">94.92 &#177; 1.53</td><td align="center" valign="middle" rowspan="1" colspan="1">93.22 &#177; 1.45</td><td align="center" valign="middle" rowspan="1" colspan="1">90.78 &#177; 1.35</td><td align="center" valign="middle" rowspan="1" colspan="1">89.63 &#177; 1.67</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ECA-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">95.28 &#177; 1.42</td><td align="center" valign="middle" rowspan="1" colspan="1">95.73 &#177; 1.31</td><td align="center" valign="middle" rowspan="1" colspan="1">89.89 &#177; 1.28</td><td align="center" valign="middle" rowspan="1" colspan="1">88.88 &#177; 1.59</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.42 &#177; 1.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.23 &#177; 1.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.74 &#177; 1.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.64 &#177; 1.53</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05254-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05254-t004_Table 4</object-id><label>Table 4</label><caption><p>Ablation studies of SFN.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Algorithmic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AUROC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PRO (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">94.38 &#177; 1.52</td><td align="center" valign="middle" rowspan="1" colspan="1">92.22 &#177; 1.41</td><td align="center" valign="middle" rowspan="1" colspan="1">92.04 &#177; 1.33</td><td align="center" valign="middle" rowspan="1" colspan="1">88.41 &#177; 1.25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet101</td><td align="center" valign="middle" rowspan="1" colspan="1">93.57 &#177; 1.63</td><td align="center" valign="middle" rowspan="1" colspan="1">92.35 &#177; 1.32</td><td align="center" valign="middle" rowspan="1" colspan="1">91.32 &#177; 1.42</td><td align="center" valign="middle" rowspan="1" colspan="1">87.19 &#177; 1.34</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" rowspan="1" colspan="1">90.68 &#177; 1.14</td><td align="center" valign="middle" rowspan="1" colspan="1">89.48 &#177; 1.23</td><td align="center" valign="middle" rowspan="1" colspan="1">89.25 &#177; 1.15</td><td align="center" valign="middle" rowspan="1" colspan="1">82.75 &#177; 1.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VGG19</td><td align="center" valign="middle" rowspan="1" colspan="1">91.07 &#177; 1.35</td><td align="center" valign="middle" rowspan="1" colspan="1">89.81 &#177; 1.44</td><td align="center" valign="middle" rowspan="1" colspan="1">87.97 &#177; 1.22</td><td align="center" valign="middle" rowspan="1" colspan="1">83.41 &#177; 1.16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Efficient net B0</td><td align="center" valign="middle" rowspan="1" colspan="1">93.43 &#177; 1.46</td><td align="center" valign="middle" rowspan="1" colspan="1">90.27 &#177; 1.35</td><td align="center" valign="middle" rowspan="1" colspan="1">89.83 &#177; 1.34</td><td align="center" valign="middle" rowspan="1" colspan="1">85.68 &#177; 1.27</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Efficient net B4</td><td align="center" valign="middle" rowspan="1" colspan="1">93.92 &#177; 1.55</td><td align="center" valign="middle" rowspan="1" colspan="1">90.53 &#177; 1.43</td><td align="center" valign="middle" rowspan="1" colspan="1">90.46 &#177; 1.26</td><td align="center" valign="middle" rowspan="1" colspan="1">86.14 &#177; 1.32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Inception net V3</td><td align="center" valign="middle" rowspan="1" colspan="1">91.98 &#177; 1.24</td><td align="center" valign="middle" rowspan="1" colspan="1">91.46 &#177; 1.33</td><td align="center" valign="middle" rowspan="1" colspan="1">91.11 &#177; 1.25</td><td align="center" valign="middle" rowspan="1" colspan="1">85.97 &#177; 1.17</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Inception net V4</td><td align="center" valign="middle" rowspan="1" colspan="1">92.67 &#177; 1.33</td><td align="center" valign="middle" rowspan="1" colspan="1">91.40 &#177; 1.42</td><td align="center" valign="middle" rowspan="1" colspan="1">90.88 &#177; 1.34</td><td align="center" valign="middle" rowspan="1" colspan="1">86.52 &#177; 1.24</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet+FPN</td><td align="center" valign="middle" rowspan="1" colspan="1">95.14 &#177; 1.78</td><td align="center" valign="middle" rowspan="1" colspan="1">92.89 &#177; 1.53</td><td align="center" valign="middle" rowspan="1" colspan="1">92.64 &#177; 1.56</td><td align="center" valign="middle" rowspan="1" colspan="1">89.31 &#177; 2.06</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.42 &#177; 1.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.23 &#177; 1.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.74 &#177; 1.02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.64 &#177; 1.53</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05254-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05254-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation studies of anomaly score.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">&#955;</mml:mi></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AUROC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PRO (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">94.31 &#177; 1.43</td><td align="center" valign="middle" rowspan="1" colspan="1">93.34 &#177; 1.42</td><td align="center" valign="middle" rowspan="1" colspan="1">87.58 &#177; 1.23</td><td align="center" valign="middle" rowspan="1" colspan="1">89.37 &#177; 1.62</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.85</td><td align="center" valign="middle" rowspan="1" colspan="1">97.42 &#177; 1.47</td><td align="center" valign="middle" rowspan="1" colspan="1">95.23 &#177; 1.34</td><td align="center" valign="middle" rowspan="1" colspan="1">94.74 &#177; 1.02</td><td align="center" valign="middle" rowspan="1" colspan="1">91.64 &#177; 1.53</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.23 &#177; 1.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.58 &#177; 1.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.74 &#177; 1.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.73 &#177; 1.71</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05254-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05254-t006_Table 6</object-id><label>Table 6</label><caption><p>Quantitative comparisons on physical verification dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Algorithmic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AP (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">AUROC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PRO (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">UniAD</td><td align="center" valign="middle" rowspan="1" colspan="1">91.68 &#177; 3.21</td><td align="center" valign="middle" rowspan="1" colspan="1">90.55 &#177; 2.59</td><td align="center" valign="middle" rowspan="1" colspan="1">88.08 &#177; 1.45</td><td align="center" valign="middle" rowspan="1" colspan="1">85.52 &#177; 3.71</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">PaDiM</td><td align="center" valign="middle" rowspan="1" colspan="1">89.27 &#177; 1.93</td><td align="center" valign="middle" rowspan="1" colspan="1">88.74 &#177; 1.55</td><td align="center" valign="middle" rowspan="1" colspan="1">85.73 &#177; 0.96</td><td align="center" valign="middle" rowspan="1" colspan="1">82.89 &#177; 2.63</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RD4AD</td><td align="center" valign="middle" rowspan="1" colspan="1">94.89 &#177; 1.71</td><td align="center" valign="middle" rowspan="1" colspan="1">91.01 &#177; 2.13</td><td align="center" valign="middle" rowspan="1" colspan="1">88.21 &#177; 1.23</td><td align="center" valign="middle" rowspan="1" colspan="1">87.27 &#177; 2.93</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DRAEM</td><td align="center" valign="middle" rowspan="1" colspan="1">88.51 &#177; 3.55</td><td align="center" valign="middle" rowspan="1" colspan="1">86.29 &#177; 4.19</td><td align="center" valign="middle" rowspan="1" colspan="1">83.96 &#177; 1.81</td><td align="center" valign="middle" rowspan="1" colspan="1">80.44 &#177; 4.91</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.43 &#177; 1.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.27 &#177; 1.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.81 &#177; 1.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.05 &#177; 1.68</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>