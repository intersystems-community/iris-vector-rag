<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12430997</article-id><article-id pub-id-type="pmcid-ver">PMC12430997.1</article-id><article-id pub-id-type="pmcaid">12430997</article-id><article-id pub-id-type="pmcaiid">12430997</article-id><article-id pub-id-type="doi">10.3390/s25175546</article-id><article-id pub-id-type="publisher-id">sensors-25-05546</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Automated Rice Seedling Segmentation and Unsupervised Health Assessment Using Segment Anything Model with Multi-Modal Feature Analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-6850-914X</contrib-id><name name-style="western"><surname>Rezvan</surname><given-names initials="H">Hassan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05546" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4325-8741</contrib-id><name name-style="western"><surname>Valadan Zoej</surname><given-names initials="MJ">Mohammad Javad</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05546" ref-type="aff">1</xref><xref rid="c1-sensors-25-05546" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7265-3866</contrib-id><name name-style="western"><surname>Youssefi</surname><given-names initials="F">Fahimeh</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><xref rid="af1-sensors-25-05546" ref-type="aff">1</xref><xref rid="af2-sensors-25-05546" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5165-1773</contrib-id><name name-style="western"><surname>Ghaderpour</surname><given-names initials="E">Ebrahim</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af3-sensors-25-05546" ref-type="aff">3</xref><xref rid="c1-sensors-25-05546" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Ampatzidis</surname><given-names initials="Y">Yiannis</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name name-style="western"><surname>Pasinetti</surname><given-names initials="S">Simone</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05546"><label>1</label>Department of Photogrammetry and Remote Sensing, K. N. Toosi University of Technology, Tehran 19967-15433, Iran; <email>h.rezvan@email.kntu.ac.ir</email> (H.R.); or <email>youssefi@usx.edu.cn</email> (F.Y.)</aff><aff id="af2-sensors-25-05546"><label>2</label>Institute of Artificial Intelligence, USX, Shaoxing University, 508 West Huancheng Road, Yuecheng District, Shaoxing 312000, China</aff><aff id="af3-sensors-25-05546"><label>3</label>Department of Earth Sciences &amp; CERI Research Centre, Sapienza University of Rome, P.le Aldo Moro, 5, 00185 Rome, Italy</aff><author-notes><corresp id="c1-sensors-25-05546"><label>*</label>Correspondence: <email>valadanzouj@kntu.ac.ir</email> (M.J.V.Z.); <email>ebrahim.ghaderpour@uniroma1.it</email> (E.G.)</corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5546</elocation-id><history><date date-type="received"><day>23</day><month>7</month><year>2025</year></date><date date-type="rev-recd"><day>27</day><month>8</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>05</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05546.pdf"/><abstract><p>This research presents a fully automated two-step method for segmenting rice seedlings and assessing their health by integrating spectral, morphological, and textural features. Driven by the global need for increased food production, the proposed method enhances monitoring and control in agricultural processes. Seedling locations are first identified by the excess green minus excess red index, which enables automated point-prompt inputs for the segment anything model to achieve precise segmentation and masking. Morphological features are extracted from the generated masks, while spectral and textural features are derived from corresponding red&#8211;green&#8211;blue imagery. Health assessment is conducted through anomaly detection using a one-class support vector machine, which identifies seedlings exhibiting abnormal morphology or spectral signatures suggesting stress. The proposed method is validated by visual inspection and Silhouette score, confirming effective separation of anomalies. For segmentation, the proposed method achieved mean dice scores ranging from 72.6 to 94.7. For plant health assessment, silhouette scores ranged from 0.31 to 0.44 across both datasets and various growth stages. Applied across three consecutive rice growth stages, the framework facilitates temporal monitoring of seedling health. The findings highlight the potential of advanced segmentation and anomaly detection techniques to support timely interventions, such as pruning or replacing unhealthy seedlings, to optimize crop yield.</p></abstract><kwd-group><kwd>smart agriculture</kwd><kwd>crop growth monitoring</kwd><kwd>food security</kwd><kwd>remote sensing</kwd><kwd>feature fusion</kwd><kwd>deep learning</kwd><kwd>segment anything model</kwd></kwd-group><funding-group><award-group><funding-source>PROGETTI MEDI ATENEO 2024 by Sapienza University of Rome</funding-source><award-id>SAP_RICERCA_2024_DEFORMEARTH_GHADER_E_01</award-id></award-group><funding-statement>The APC of this research was supported in part by PROGETTI MEDI ATENEO 2024 by Sapienza University of Rome: ATENEO_2024_Ghaderpour_DEFORMEARTH-Uncertainty Analysis and Ground Deformation Monitoring via Earth Observation Data and Advanced Data Analytics Methods. Code: SAP_RICERCA_2024_DEFORMEARTH_GHADER_E_01.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05546"><title>1. Introduction</title><p>The global population is projected to approach 10 billion in the coming decades, intensifying the demand for food production and placing significant pressure on the agricultural sector to enhance productivity and sustainability [<xref rid="B1-sensors-25-05546" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-05546" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-05546" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-05546" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-05546" ref-type="bibr">5</xref>]. Despite progress in reducing hunger and food insecurity worldwide, these efforts remain uneven, with an estimated 582 million people likely to face chronic undernourishment by 2030 if current trends continue [<xref rid="B1-sensors-25-05546" ref-type="bibr">1</xref>]. Rice, a staple food for over half of the world&#8217;s population, especially in Asia, plays a critical role in global food security and poverty alleviation efforts [<xref rid="B3-sensors-25-05546" ref-type="bibr">3</xref>,<xref rid="B6-sensors-25-05546" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-05546" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05546" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05546" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05546" ref-type="bibr">10</xref>].</p><p>To address the growing demand for food, smart farming and precision agriculture have emerged as essential approaches for enhancing rice production efficiency [<xref rid="B4-sensors-25-05546" ref-type="bibr">4</xref>,<xref rid="B9-sensors-25-05546" ref-type="bibr">9</xref>]. These technologies enable precise monitoring of crop health, identification of environmental stressors, quantification of plant stress, seedling counting, and early detection of defective plants [<xref rid="B3-sensors-25-05546" ref-type="bibr">3</xref>,<xref rid="B7-sensors-25-05546" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05546" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05546" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05546" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05546" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05546" ref-type="bibr">12</xref>]. Data-driven insights from these approaches support optimized decisions on planting density, irrigation, and resource management, ultimately contributing to higher yields and more sustainable practices [<xref rid="B3-sensors-25-05546" ref-type="bibr">3</xref>,<xref rid="B9-sensors-25-05546" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05546" ref-type="bibr">10</xref>].</p><p>Crop and seedling monitoring typically aims to (i) detect and localize individual plants and (ii) assess plant health or stress levels. Remote sensing, including optical and radar sensors, has been widely used for crop mapping [<xref rid="B13-sensors-25-05546" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05546" ref-type="bibr">14</xref>]. However, unmanned aerial vehicle (UAV) imagery offers superior utility due to its higher spatial resolution for these tasks [<xref rid="B12-sensors-25-05546" ref-type="bibr">12</xref>,<xref rid="B14-sensors-25-05546" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05546" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05546" ref-type="bibr">16</xref>]. Deep learning and object detection models, such as fully convolutional networks, visual geometry group (VGG-16), scalable and efficient object detection (EfficientDet), MobileNetV2, Faster region-based convolutional neural network (R-CNN), and you only look once (YOLO) variants have been extensively applied for this rice seedling detection [<xref rid="B3-sensors-25-05546" ref-type="bibr">3</xref>,<xref rid="B5-sensors-25-05546" ref-type="bibr">5</xref>,<xref rid="B7-sensors-25-05546" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-05546" ref-type="bibr">8</xref>,<xref rid="B10-sensors-25-05546" ref-type="bibr">10</xref>,<xref rid="B17-sensors-25-05546" ref-type="bibr">17</xref>]. Similar methods have also been used for other crops such as sorghum and maize [<xref rid="B15-sensors-25-05546" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05546" ref-type="bibr">16</xref>].</p><p>Beyond detection, health and stress assessment are essential for mitigating yield losses and ensuring crop quality [<xref rid="B4-sensors-25-05546" ref-type="bibr">4</xref>,<xref rid="B9-sensors-25-05546" ref-type="bibr">9</xref>,<xref rid="B18-sensors-25-05546" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05546" ref-type="bibr">19</xref>]. Early detection of stress symptoms enables timely interventions to optimize growth conditions and resource use [<xref rid="B4-sensors-25-05546" ref-type="bibr">4</xref>,<xref rid="B9-sensors-25-05546" ref-type="bibr">9</xref>]. Various optical technologies, including red&#8211;green&#8211;blue (RGB), multispectral and hyperspectral imaging, thermography and 3D scanning, have been applied to detect plant diseases and stress factors such as nutrient deficiencies, drought and pathogens [<xref rid="B11-sensors-25-05546" ref-type="bibr">11</xref>,<xref rid="B18-sensors-25-05546" ref-type="bibr">18</xref>,<xref rid="B20-sensors-25-05546" ref-type="bibr">20</xref>]. Structural attributes (e.g., plant height, leaf area, canopy structure) [<xref rid="B20-sensors-25-05546" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-05546" ref-type="bibr">21</xref>], spectral indices (e.g., normalized difference vegetation index, soil adjusted vegetation index, excess green index) [<xref rid="B22-sensors-25-05546" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-05546" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-05546" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-05546" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-05546" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-05546" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-05546" ref-type="bibr">28</xref>], and textural features further enrich health assessments [<xref rid="B11-sensors-25-05546" ref-type="bibr">11</xref>,<xref rid="B19-sensors-25-05546" ref-type="bibr">19</xref>,<xref rid="B29-sensors-25-05546" ref-type="bibr">29</xref>]. Comprehensive approaches that integrate these diverse features improve detection accuracy and reliability.</p><p>The segment anything model (SAM), a recent innovation in image segmentation, offers zero-shot generalization to segment unfamiliar objects without additional training [<xref rid="B30-sensors-25-05546" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-05546" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-05546" ref-type="bibr">32</xref>]. SAM has been applied across diverse domains, including public health, habitat detection [<xref rid="B32-sensors-25-05546" ref-type="bibr">32</xref>], medical imaging [<xref rid="B33-sensors-25-05546" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-05546" ref-type="bibr">34</xref>], and autonomous robotics [<xref rid="B35-sensors-25-05546" ref-type="bibr">35</xref>]. SAM operates effectively with minimal human input, such as bounding boxes, points, or text-based prompts [<xref rid="B32-sensors-25-05546" ref-type="bibr">32</xref>]. In agriculture, however, most health assessment tasks still depend on large, labeled datasets, which are costly, time-consuming to generate, and often lack scalability across diverse and dynamic agricultural landscapes [<xref rid="B36-sensors-25-05546" ref-type="bibr">36</xref>].</p><p>Unsupervised techniques, such as clustering and anomaly detection, provide viable alternatives by enabling data analysis without labeled samples. Approaches like fuzzy c-means clustering, one-class support vector machine (OCSVM), and Isolation Forest have been used for tasks ranging from crop type mapping to stress detection and sensor fault identification in smart farming systems [<xref rid="B37-sensors-25-05546" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-05546" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-05546" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-05546" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-05546" ref-type="bibr">41</xref>,<xref rid="B42-sensors-25-05546" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-05546" ref-type="bibr">43</xref>]. These methods are particularly valuable in agricultural contexts where labeled data are scarce.</p><p>Despite advancements in remote sensing and computer vision, significant gaps persist in rice seedling monitoring. Current methods often rely on complex, black-box deep learning architectures with limited focus on single growth stages and lack integration between segmentation outputs and actionable agronomic insights. These limitations hinder practical deployment in real-world agricultural scenarios.</p><p>To address these challenges, this study introduces a novel, label-free framework for automated rice seedling segmentation and stress assessment across three growth stages using UAV-acquired RGB imagery. The method integrates SAM for segmentation, extracts morphological, spectral, and textural features, and employs OCSVM for anomaly detection to identify potentially stressed or unhealthy seedlings. This study builds upon the authors&#8217; previous work [<xref rid="B44-sensors-25-05546" ref-type="bibr">44</xref>], where the applicability of the SAM was evaluated for rice seedling segmentation under three prompting scenarios: point prompts, bounding-box prompts, and automatic mask generation, all relying on user-provided inputs from the source dataset. In contrast, the present manuscript introduces a fully automated framework that eliminates the need for manual inputs. Additionally, this work advances beyond segmentation by analyzing seedling health. To check the scalability of the approach, the entire process was also applied to a second, independently curated dataset.</p><p>The key contributions of this study include</p><list list-type="bullet"><list-item><p>Proposing a lightweight, training-free framework for automated seedling segmentation.</p></list-item><list-item><p>Developing an interpretable model combining spectral and morphological, and textural features.</p></list-item><list-item><p>Implementing a stage-wise monitoring approach to capture temporal dynamics in seedling health.</p></list-item><list-item><p>Bridging AI models with practical agricultural applications by linking segmentation outputs to field-based actions.</p></list-item></list><p>The remainder of this paper is organized as follows: <xref rid="sec2-sensors-25-05546" ref-type="sec">Section 2</xref> details the datasets, methodology, including segmentation, feature extraction, and health/stress assessment. <xref rid="sec3-sensors-25-05546" ref-type="sec">Section 3</xref> presents the results and evaluation. <xref rid="sec4-sensors-25-05546" ref-type="sec">Section 4</xref> discusses the implications and limitations, followed by conclusions in <xref rid="sec5-sensors-25-05546" ref-type="sec">Section 5</xref>.</p></sec><sec id="sec2-sensors-25-05546"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-25-05546"><title>2.1. Study Regions and Rice Seedling Datasets</title><p>This study was conducted using two geographically and climatically distinct rice cultivation regions. The first study area is located in Wufeng District, Taichung, Taiwan, where UAV-based field surveys were carried out by the Department of Civil Engineering and the Innovation and Development Center of Sustainable Agriculture at National Chung Hsing University, in collaboration with the Taiwan Agricultural Research Institute (TARI) [<xref rid="B7-sensors-25-05546" ref-type="bibr">7</xref>,<xref rid="B10-sensors-25-05546" ref-type="bibr">10</xref>,<xref rid="B45-sensors-25-05546" ref-type="bibr">45</xref>]. The images were taken using two sensors, DJI Phantom 4 Pro and DJI Zenmuse X7. The second study area is situated in Heilongjiang Province, in the northeastern region of China, characterized by lower annual temperatures and a high sensitivity of rice seedlings to sunlight and rainfall fluctuations [<xref rid="B46-sensors-25-05546" ref-type="bibr">46</xref>]. These differences in environmental conditions provide diverse settings for validating the robustness of the proposed segmentation and monitoring frameworks. The location and elevation maps of these regions are illustrated in <xref rid="sensors-25-05546-f001" ref-type="fig">Figure 1</xref>.</p><p>Two benchmark datasets were used for model development and evaluation. The first dataset, collected in Taiwan, comprises 600 high-resolution RGB images acquired via a multi-rotor UAV that followed a pre-defined scouting route. UAV acquisitions were scheduled at fixed calendar dates to represent distinct growth stages, a practical compromise in the absence of detailed phenological measurements. Therefore, data were gathered across three distinct rice seedling growth stages: (1) early seedling stage (7 August 2018), featuring sparse, small seedlings; (2) mid-growth stage (14 August 2018), marked by denser canopy formation; and (3) mature stage (23 August 2018), characterized by full canopy coverage. The dataset includes both close-up images of individual seedlings and orthophotos of entire fields, supporting a range of tasks from detection to classification.</p><p>The second dataset, from China, was collected between 9 May and 16 June 2022, using RGB cameras installed at 11 meteorological stations. Images were captured seven times daily under consistent natural light conditions, using vertically mounted cameras 2.4 m above ground with a 90&#176; field of view. Each image covers an area of 4.4 &#215; 2.5 m. While this dataset includes both RGB and near-infrared (NIR) spectral data, only RGB images were utilized in this study [<xref rid="B46-sensors-25-05546" ref-type="bibr">46</xref>]. One image per day was selected based on consistent lighting and time of capture, and the 17-day collection was grouped into three growth stages for analysis, while phenology data is not available. A summary of the information about the datasets is shown in <xref rid="sensors-25-05546-t001" ref-type="table">Table 1</xref>.</p><p>Sample images of the different growth stages of rice seedlings used in this study are presented in <xref rid="sensors-25-05546-f002" ref-type="fig">Figure 2</xref>. Notably, in the second dataset, the rice field is visibly flooded during the initial growth stage.</p></sec><sec sec-type="methods" id="sec2dot2-sensors-25-05546"><title>2.2. Methodology</title><p>This section outlines the proposed methodology for rice seedling detection and unsupervised health/abnormality assessment. The framework consists of three main steps: seedling segmentation, feature extraction, and anomaly detection, as shown in <xref rid="sensors-25-05546-f003" ref-type="fig">Figure 3</xref>.</p><p>First, seedling locations are extracted using excess green minus excess red (ExGR) index, which enhances vegetation contrast in RGB images. Based on the extracted seedling positions, bounding boxes are automatically drawn to localize each seedling. These bounding boxes serve as point prompts for the SAM, which performs precise segmentation and masking of individual seedlings without requiring labeled data.</p><p>Following segmentation, a comprehensive set of features is extracted:<list list-type="bullet"><list-item><p>Morphological features are derived from the generated masks to capture shape and structural characteristics.</p></list-item><list-item><p>Spectral features are computed from the corresponding RGB imagery to assess color and reflectance properties.</p></list-item><list-item><p>Textural features are extracted to quantify surface patterns and fine-grained variations that may indicate stress or disease.</p></list-item></list></p><p>These combined features serve as input to an OCSVM, an unsupervised anomaly detection algorithm. The OCSVM identifies seedlings with morphological or spectral deviations from the norm, flagging them as potentially stressed, unhealthy, or abnormal&#8212;without relying on pre-labeled datasets.</p><p>This integrated, label-free approach enables automated seedling monitoring and health assessment across multiple growth stages, facilitating early detection of issues for precision agriculture interventions.</p><sec id="sec2dot2dot1-sensors-25-05546"><title>2.2.1. Automatic SAM for Rice Seedling Detection</title><p>To segment rice seedlings from UAV-acquired RGB imagery, a hybrid approach is implemented, which combines vegetation index enhancement with prompt-based segmentation. Initially, ExGR index in Equation (1) was computed to emphasize green regions corresponding to seedlings. Multi-Otsu thresholding and connected component analysis were then applied to automatically delineate bounding boxes around individual seedlings. The geometric center of each bounding box was extracted as a hint point to perform SAM, capable of producing high-quality instance masks without additional training. This workflow was applied across three growth stages to accommodate varying seedling morphologies with minimal manual input. The final output for each image was a binary mask highlighting segmented seedlings. <xref rid="sensors-25-05546-t002" ref-type="table">Table 2</xref> shows the SAM parameters.<disp-formula id="FD1-sensors-25-05546"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>2.4</mml:mn><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Segmentation performance was assessed using standard metrics, including mean Dice (mDice) and mean Intersection over Union (mIoU), and mean False Positive Rate (mFPR), alongside seedling count accuracy as a practical, agriculture-relevant indicator. The formulas of these metrics are as follows.<disp-formula id="FD2-sensors-25-05546"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD3-sensors-25-05546"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">U</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-05546"><label>(4)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where TPs (True Positives) are pixels correctly identified as seedlings, while TNs (True Negatives) are pixels correctly identified as background. FPs (False Positives) occur when background pixels are incorrectly labeled as seedlings, and FNs (False Negatives) occur when actual seedling pixels are missed and labeled as background. Algorithm 1 shows this step. The visual diagram of Algorithm 1 is illustrated in <xref rid="sensors-25-05546-f004" ref-type="fig">Figure 4</xref>.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1.</bold> Automated Rice Seedling Detection</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1: Input: RGB image I of a rice seedling patch<break/>2: Output: Binary mask M with segmented seedlings<break/>3: Compute ExGR index from I to enhance green regions<break/>4: Apply multi-Otsu thresholding on ExGR to generate a binary map B<break/>5: <bold>for each</bold> connected component C in B <bold>do</bold>:<break/>5:&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Compute bounding box BB_C around component C<break/>6:&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Extract center point P_C of BB_C<break/>7:&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Append P_C to the prompt list L<break/>8: Apply the SAM to I using point prompts in L<break/>9: Obtain segmentation mask M from SAM<break/>10: <bold>Return</bold> M</td></tr></tbody></array></p></sec><sec id="sec2dot2dot2-sensors-25-05546"><title>2.2.2. Morphological-Spectral-Textural OCSVM-Based Anomaly Detection</title><p>From each segmented seedling mask, a set of morphologic, spectral, and textural features was extracted. Geometric features included Area, Perimeter, Solidity, Eccentricity, and Circularity, while spectral features comprised the intensities of the Red, Green, and Blue channels, along with the excess green (ExG) index. Moreover, contrast, dissimilarity, homogeneity, energy, correlation, and second moment computed from the gray-level co-occurrence matrix (GLCM) are represented as textural features. In the present research, textural features are extracted from GLCM using Scikit-image in Python 3.11. These extracted features collectively served as the input variables for the clustering analysis and subsequent anomaly detection processes. The formulas to calculate solidity, eccentricity, circularity and ExG index are as follows.</p><p>Solidity represents the ratio of a seedling&#8217;s area to its convex hull, reflecting shape compactness, where lower values may signal gaps or structural weakness. Eccentricity quantifies elongation, with abnormal values often linked to irregular growth. Circularity measures similarity to a circle and is highly sensitive to boundary irregularities such as notches or serrations. ExG emphasizes vegetation, with low values suggesting potential stress or poor health.<disp-formula id="FD5-sensors-25-05546"><label>(5)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD6-sensors-25-05546"><label>(6)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-05546"><label>(7)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi mathvariant="sans-serif">&#960;</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-05546"><label>(8)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where Area is the number of pixels inside the seedling mask, Convex_area is the smallest convex polygon that encloses the seedling mask, Perimeter is the length of the boundary of the seedling mask, and Major_ and Minor_axis lengths are the lengths of the longest and shortest axes of the best-fitting ellipse to the seedling shape.</p><p>An OCSVM was employed, trained solely on the extracted feature vectors under the assumption that most seedlings belonged to the healthy or normal class. OCSVM was chosen for its capability to detect anomalies within high-dimensional feature spaces while requiring minimal prior assumptions&#8212;particularly advantageous given the absence of labeled training data. A grid search optimization was performed to determine the best kernel type (polynomial, radial basis function, linear) and tune key hyperparameters, &#957; and &#947;. We utilized the silhouette score during the grid search to guide parameter selection and reduce overfitting, providing a robust unsupervised alternative in the absence of ground-truth labels. The final model classified each seedling as either an inlier (+1) or outlier (&#8722;1), with outliers interpreted as potentially stressed, damaged, or unhealthy seedlings.</p><p>To assess the performance of the proposed approach, several validation strategies were applied. First, anomaly seedlings were overlaid on UAV imagery to visualize the spatial distribution of detected outliers, allowing direct inspection of their physical characteristics. This qualitative step was carried out by the research team to verify whether detected anomalies corresponded to visibly distinct seedlings. Additionally, the Bhattacharyya distance was calculated to quantify the degree of overlap between clusters, with higher values indicating better separability [<xref rid="B47-sensors-25-05546" ref-type="bibr">47</xref>]. The silhouette score, a widely used clustering metric [<xref rid="B48-sensors-25-05546" ref-type="bibr">48</xref>,<xref rid="B49-sensors-25-05546" ref-type="bibr">49</xref>], was also employed to evaluate the overall compactness and separation between inlier and outlier groups, where values approaching 1 reflect well-defined cluster boundaries. The process of this detection is shown in Algorithm 2:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2.</bold> Morphologic&#8211;Spectral&#8211;Textural OCSVM-Based Anomaly Detection</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1: Inputs: Set of segmented seedling masks, Corresponding RGB images<break/>2: Output: Outlier labels <italic toggle="yes">O</italic> (&#8722;1 for anomaly, +1 for normal) for each seedling<break/>3: Extract morphologic features (Area, Perimeter, Solidity, Eccentricity, Circularity)<break/>4: Extract spectral features (Red, Green, Blue intensities and ExG)<break/>5: Extract textural features (Contrast, Dissimilarity, Homogeneity, Energy, Correlation, <break/>6: Second Moment)<break/>7: Form combined feature vectors F = {<italic toggle="yes">f</italic><sub>1</sub>, <italic toggle="yes">f</italic><sub>2</sub>, &#8230;, <italic toggle="yes">f<sub>n</sub>}</italic> &#8712; &#8477;<italic toggle="yes"><sup>d</sup></italic><break/>8: Perform grid search to optimize OCSVM parameters:<break/>9:&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Kernel &#8712; {linear, radial basis function, polynomial}<break/>10:&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<italic toggle="yes">&#957;</italic> (nu) &#8712; [0.01, 0.05, 0.1]<break/>11:&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<italic toggle="yes">&#947;</italic> (gamma) &#8712; [0.01, 0.1, auto, scale]<break/>12: Train OCSVM on F using best (kernel, <italic toggle="yes">&#957;</italic>, <italic toggle="yes">&#947;</italic>)<break/>13: Predict anomaly labels L &#8712; {+1, &#8722;1} for each seedling<break/>14: Visual Validation and statistical metrics<break/>15: Return <italic toggle="yes">O</italic></td></tr></tbody></array></p></sec></sec></sec><sec sec-type="results" id="sec3-sensors-25-05546"><title>3. Results</title><p>This section presents the outcomes of the proposed framework, including the performance of the automatic segmentation using the SAM and the results of the unsupervised health and abnormality assessment of seedlings. Supporting analyses and validation metrics are also provided.</p><sec id="sec3dot1-sensors-25-05546"><title>3.1. The Results of Automatic SAM for Rice Seedling Detection</title><sec id="sec3dot1dot1-sensors-25-05546"><title>3.1.1. Segmentation Results</title><p>The segmentation results generated by the proposed method across the three rice growth stages are visually presented in <xref rid="sensors-25-05546-f005" ref-type="fig">Figure 5</xref>.</p><p>In addition to visual interpretation, the evaluation results using metrics are summarized in <xref rid="sensors-25-05546-t003" ref-type="table">Table 3</xref>.</p></sec><sec id="sec3dot1dot2-sensors-25-05546"><title>3.1.2. Rice Seedling Counting</title><p>Counting seedlings poses a significant challenge in precision and smart agriculture. As part of the analysis, in addition to conventional segmentation performance metrics, seedlings identified by the proposed method were counted across all three growth stages to evaluate segmentation accuracy. The counted results were then compared with the actual number of seedlings using linear regression analysis, and the coefficient of determination (R<sup>2</sup>) was calculated to measure agreement. The resulting regression plots are shown in <xref rid="sensors-25-05546-f006" ref-type="fig">Figure 6</xref>.</p></sec></sec><sec id="sec3dot2-sensors-25-05546"><title>3.2. Morphological-Spectral-Textural OCSVM-Based Anomaly Detection</title><p>After performing OCSVM, t-SNE is used to project features into 2D and colored points by OCSVM labels to visualize separation. The scatter plots are shown in <xref rid="sensors-25-05546-f007" ref-type="fig">Figure 7</xref>.</p><p>Also, plots of OCSVM decision function show how far points are from the decision boundary. These plots for each growth stage are illustrated in <xref rid="sensors-25-05546-f008" ref-type="fig">Figure 8</xref>.</p><p>For analyzing cluster-wise feature distribution, the Bhattacharyya distance calculated for each feature during the three growth stages is summarized in <xref rid="sensors-25-05546-t004" ref-type="table">Table 4</xref>.</p><p>Finally, for validation, outliers in UAV images are visually inspected and evaluated. Some of the samples are shown in <xref rid="sensors-25-05546-f009" ref-type="fig">Figure 9</xref>.</p><p>Also, the results of evaluating the quality of clusters using silhouette score are summarized in <xref rid="sensors-25-05546-t005" ref-type="table">Table 5</xref>.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-05546"><title>4. Discussion</title><p>The SAM has been widely adopted across various fields for its flexible, prompt-based segmentation capabilities. As previously discussed, SAM typically relies on minimal human input. In this study, a fully automated seedling detection pipeline is developed that extracts representative hint points algorithmically, converting the segmentation task into a point-prompt framework without manual intervention. Both statistical evaluations and visual assessments confirmed the effectiveness of this approach. As illustrated in <xref rid="sensors-25-05546-f005" ref-type="fig">Figure 5</xref>, the proposed method successfully delineated rice seedlings and accurately segmented their boundaries across the early and mid-growth stages in both datasets. Although segmentation performance declined somewhat in the third stage due to dense canopy cover and overlapping seedlings along planting lines (the upper and lower boundaries in the first dataset and the right and left boundaries in second dataset) remained clearly distinguishable from the background, demonstrating the robustness and practical utility of the proposed framework even in challenging conditions.</p><p>In addition to visual evaluations, a comprehensive quantitative assessment is done using established segmentation metrics, including mIoU, mDice, and mFPR, to account for both segmentation accuracy and error rates (<xref rid="sensors-25-05546-t003" ref-type="table">Table 3</xref>). Consistent with the visual observations, these metrics demonstrated higher performance for the first and second growth stages, with notably lower false positive rates and stronger boundary delineation. Performance degradation was observed in the third growth period, where increased canopy density and overlapping seedlings led to a higher rate of false positives and reduced segmentation accuracy. Beyond pixel-based metrics, seedling counting was also employed as a practical indicator of the model&#8217;s capability to detect and separate individual seedlings. As anticipated, the predicted seedling counts aligned closely with ground truth values in the first and second stages, reflected by higher R<sup>2</sup> values from the regression analyses. However, this agreement diminished considerably in the third stage, as the merging of closely positioned seedlings along cultivated lines resulted in underestimation and a drop in the regression performance. These findings highlight both the strengths and current limitations of the proposed method in varying phenological conditions.</p><p>The selection of spectral, morphological, and textural features in this study was grounded in both agronomic relevance and their sensitivity to phenotypic variations associated with plant health. Mean values of the Red, Green, and Blue channels were included as healthy rice seedlings typically exhibit higher green reflectance, while stressed or diseased plants tend to display yellowing (low green, high red) or browning (low green with high red and blue). The ExG index further enhances this distinction, as lower ExG values often correspond to poor plant health. Morphological features were chosen to capture geometrical cues linked to plant health and structure. The area of each seedling mask reflects overall size, with smaller areas potentially indicating stunted or underdeveloped seedlings. Perimeter values provide insight into shape regularity, where healthy seedlings generally maintain smooth, compact contours, whereas stressed or abnormal plants often present with irregular, fragmented edges leading to larger perimeters (e.g., [<xref rid="B50-sensors-25-05546" ref-type="bibr">50</xref>]). Eccentricity was included in detecting elongation or asymmetry arising from abnormal growth, while circularity measures how closely a shape approximates a circle, making it sensitive to serrations or notches at the seedling boundary. Solidity assesses structural integrity, with lower values suggesting gaps, deformities, or incomplete canopy cover. As shown in <xref rid="sensors-25-05546-t004" ref-type="table">Table 4</xref>, these selected features proved relatively effective in separating normal seedlings from abnormal ones. Notably, circularity emerged as a strong discriminator during the second and third growth stages, reflected by favorable Bhattacharyya distance values. In contrast, during the first growth stage, textural attributes such as contrast and dissimilarity, together with some spectral features (e.g., Red and Green channels), exhibited higher discriminative power, reflecting the stronger color and texture differences between seedlings and background at early stages. Interestingly, in the third stage of the first dataset, the second moment also reached its maximum discriminative value, further supporting the stage-dependent feature shift. These results confirm the agronomic relevance and practical utility of the proposed feature set in monitoring seedling health.</p><p>The performance of the OCSVM anomaly detection model was assessed across both datasets, demonstrating its suitability for unsupervised monitoring in imbalanced seedling populations. As shown in <xref rid="sensors-25-05546-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-05546-f008" ref-type="fig">Figure 8</xref>, the first dataset achieved Silhouette scores ranging from 0.44 to 0.41, with concentrated and well-separated score distributions in the early and mid-growth stages, indicating stable crop conditions and effective anomaly separation. Although a slight decrease in the Silhouette score was observed in the final stage, likely due to increased canopy overlap and stress heterogeneity, the anomalies remained distinctly separated from the normal seedlings near the decision boundary. In the second dataset, Silhouette scores were slightly lower, ranging from 0.34 to 0.31. The relatively lower scores can be attributed to more heterogeneous backgrounds and potential differences in UAV imaging distance, which collectively reduced the separability of feature distributions. Despite these lower values, the silhouette scores remain within an acceptable range for unsupervised clustering tasks. The anomaly score distributions initially exhibited moderate separation but became progressively wider and more dispersed in the later stages, reflecting heightened physiological variability under fluctuating environmental conditions. Despite these challenges, the OCSVM model, by the selected spectral, geometric, and textural features, consistently provided a reliable and moderately effective framework for temporal anomaly detection across both datasets without the need for labeled training data.</p><p>Since the approach was unsupervised, excluding features without ground-truth labels risks discarding potentially relevant anomaly-related information; therefore, all spectral and geometric features were retained to capture complementary variability. In addition, the silhouette scores remained stable when testing different feature subsets, indicating that the model performance was not overly sensitive to potential feature redundancy.</p><p>The t-SNE embeddings (<xref rid="sensors-25-05546-f007" ref-type="fig">Figure 7</xref>) provided a visual representation of the OCSVM&#8217;s ability to distinguish healthy seedlings from anomalies across growth stages in both datasets. In the first dataset, early-stage embeddings showed a few outliers positioned at the edges of dense clusters, reflecting predominantly healthy seedlings with minimal stress variation. Outliers increased and clustered in localized regions during the mid-growth stage, suggesting the emergence of homogeneous stress patterns. By the final stage, although outliers remained relatively sparse, they were more widely scattered as feature variability increased with canopy development. A similar trend was observed in the second dataset, where initial outliers appeared along cluster boundaries, followed by a gradual rise in scattered anomalies through the second and third stages, indicating growing physiological variability under fluctuating field conditions. These patterns align with expected plant stress dynamics as growth progresses.</p><p>According to the visual assessment of the samples of seedlings identified as unhealthy and stressed, and as illustrated in <xref rid="sensors-25-05546-f009" ref-type="fig">Figure 9</xref>, these seedlings often appear yellow or brown at different growth stages or exhibit notably smaller sizes at maturity compared to their healthy counterparts. This observation aligns with the findings in <xref rid="sensors-25-05546-t004" ref-type="table">Table 4</xref>, where spectral and area-based morphological features show the greatest discriminatory power. Moreover, the temporal consistency in the health status of detected seedlings&#8212;with no sign of fluctuating classification across stages&#8212;suggests that the model captures persistent stress conditions rather than noise. The increasing number of identified anomalies across the three growth periods may therefore reflect actual physiological stress, growth variation, or responses to environmental factors.</p><p>The proposed framework demonstrates strong potential for scalability and practical deployment in agricultural monitoring. A key advantage is its unsupervised design, which does not rely heavily on annotated datasets that are often costly and labor-intensive to generate. By employing OCSVM for anomaly detection and the zero-shot capability of SAM for segmentation, the approach can be applied in a patch-wise manner, allowing efficient processing of large UAV datasets and enabling extension to broader spatial scales. Importantly, this zero-shot strategy preserves scalability, whereas fine-tuning SAM on task-specific data could improve performance but at the cost of reduced generalizability and higher dependency on annotated training datasets. At the same time, real-world challenges such as illumination variability, soil background heterogeneity, and crop phenological differences can influence segmentation and anomaly detection performance. To partially address these factors, the methodology was tested on two UAV datasets with distinct background conditions (bare soil and flooded fields) and under different illumination scenarios, demonstrating its robustness across variable environments. These results suggest that the framework is not only adaptable but also holds promise for wider application in diverse field conditions.</p><p>A limitation of this study is that SAM was applied with its default ViT-H backbone and point prompts, without model-level fine-tuning or domain-specific architectural adaptations to test the generalization capacity of foundation models. While this baseline implementation demonstrates the feasibility of SAM in rice seedling detection, future work will explore optimized prompting strategies and task-specific adaptations (e.g., fine-tuning) to improve robustness under complex conditions such as canopy overlap. While the proposed geometric-spectral-textural anomaly detection method using OCSVM effectively identifies stressed or unhealthy seedlings without requiring labeled data, several limitations must be acknowledged. First, the model is sensitive to the choice of hyperparameters such as the outlier fraction (&#957;) and to feature scaling, which can influence detection accuracy. Second, the absence of ground truth labels or field-based validation data limits the ability to confirm whether detected anomalies truly reflect biological or environmental stress, highlighting the need for future integration with in-field observations. Finally, due to the unsupervised nature of the model, there is a risk of misclassifying rare-but-healthy seedlings as anomalies.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-05546"><title>5. Conclusions</title><p>As the global population continues to grow and food demand rises, the need for efficient and scalable agricultural monitoring has become increasingly critical to maximize productivity and ensure food security. Traditional field monitoring, reliant on manual inspection, remains labor-intensive, time-consuming and costly. In contrast, the integration of remote sensing technologies with automated, data-driven approaches offers a practical and scalable solution for crop monitoring throughout the growing season. In the present study, a two-stage unsupervised framework for rice seedling detection and health assessment is proposed, leveraging both geometric and spectral features. The first stage automated SAM using point-prompt to achieve precise segmentation of individual rice seedlings from RGB imagery. In the second stage, an OCSVM model was applied to a comprehensive set of extracted morphological, spectral and textural features to identify anomalous seedlings&#8212;potentially representing those under stress, damaged, or otherwise unhealthy. The integration of morphological features with spectral and textural indicators allowed for a comprehensive characterization of each seedling. The OCSVM model effectively detected anomalies, validated through visual inspection and silhouette scores, demonstrating robust performance in unsupervised seedling health assessment. The framework is fully automated, as both seedling extraction and health assessment are handled entirely through algorithmic procedures without human input. Specifically, ExGR thresholding was automatically determined using multi-Otsu, while OCSVM hyperparameters were optimized via an automated grid search, ensuring that the pipeline runs without any need for manual parameter adjustment. Furthermore, the application of this method across three distinct rice growth stages provided valuable insights into temporal patterns of plant development and stress responses, emphasizing the dynamic nature of crop health.</p><p>Key contributions of this work include</p><list list-type="bullet"><list-item><p>The automation of SAM for efficient seedling segmentation without manual annotations.</p></list-item><list-item><p>The integration of multi-modal features (morphological, spectral, and textural) for comprehensive anomaly detection.</p></list-item><list-item><p>The application of unsupervised learning for time-resolved monitoring of crop health in a scalable and interpretable framework.</p></list-item></list><p>These findings underscore the potential of advanced image segmentation and machine learning methods to support precision agriculture by enabling early detection of plant stress and guiding timely interventions to optimize crop yields.</p><p>Future research can build on this framework by incorporating more modalities, such as NIR and red-edge, thermal bands, 3D point clouds or chlorophyll fluorescence to extract physiological and structural indicators, such as chlorophyll content. RGB imagery effectively captures visible traits like size, shape, and greenness but fails to detect early, subtle physiological changes. Stress indicators, such as chlorophyll loss or early water stress, often remain invisible in RGB data. Furthermore, alternative unsupervised models, such as Isolation Forest or Autoencoders, can be evaluated and compared against OCSVM. Also, feature importance analysis or dimensionality reduction techniques could be incorporated to refine the feature space and enhance interpretability. Physiological measurements or field-validated training data may also be introduced to enable supervised or semi-supervised deep learning models for more refined classification and more rigorous validation. Lastly, this pipeline can be generalized and tested on other crops, broadening its applicability in smart agriculture and crop management systems.</p></sec></body><back><ack><title>Acknowledgments</title><p>Authors thank Sapienza University of Rome and PROGETTI MEDI ATENEO 2024 for financial support. The authors also thank the reviewers for their time and insightful comments.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, H.R., F.Y., M.J.V.Z. and E.G.; methodology, H.R. and F.Y.; software, H.R.; validation, H.R.; formal analysis, H.R. and F.Y.; investigation, H.R.; resources, M.J.V.Z.; data curation, H.R.; writing&#8212;original draft preparation, H.R.; writing&#8212;review and editing, H.R., F.Y., M.J.V.Z. and E.G.; visualization, H.R.; supervision, M.J.V.Z.; project administration, H.R., M.J.V.Z. and E.G. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The first dataset used in this study was obtained from Rice Seedling Dataset repository, originally published in &#8220;A UAV Open Dataset of Rice Paddies for Deep Learning Practice&#8221; by Yang et al., 2021 [<xref rid="B7-sensors-25-05546" ref-type="bibr">7</xref>]. The dataset is publicly available in a GitHub repository and can be accessed at the following link: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/aipal-nchu/RiceSeedlingDataset">https://github.com/aipal-nchu/RiceSeedlingDataset</uri> (accessed on 30 August 2024). The second dataset was obtained from repository of &#8220;Image Dataset of Wheat, Corn, and Rice Seedlings in Heilongjiang Province&#8221;, originally published in 2022 by Qin Jia Le and Guo Leifeng [<xref rid="B46-sensors-25-05546" ref-type="bibr">46</xref>]. The dataset is publicly available in the Science Data Bank repository and can be accessed at the following link: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.scidb.cn/en/detail?dataSetId=a511f28b23444235b5378953c76c47c6#p4">https://www.scidb.cn/en/detail?dataSetId=a511f28b23444235b5378953c76c47c6#p4</uri> (accessed on 10 June 2025).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EfficientDet</td><td align="left" valign="middle" rowspan="1" colspan="1">Scalable and Efficient Object Detection</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EVI</td><td align="left" valign="middle" rowspan="1" colspan="1">Enhanced Vegetation Index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ExG</td><td align="left" valign="middle" rowspan="1" colspan="1">Excess Green</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ExGR</td><td align="left" valign="middle" rowspan="1" colspan="1">Excess Green minus Excess Red</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FAO</td><td align="left" valign="middle" rowspan="1" colspan="1">Food and Agriculture Organization</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FN</td><td align="left" valign="middle" rowspan="1" colspan="1">False Negative</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FP</td><td align="left" valign="middle" rowspan="1" colspan="1">False Positive</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GLCM</td><td align="left" valign="middle" rowspan="1" colspan="1">Gray-Level Co-occurrence Matrix</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mDice</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Dice</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mIoU</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Intersection over Union</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mFPR</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean False Positive Rate</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NDVI</td><td align="left" valign="middle" rowspan="1" colspan="1">Normalized Difference Vegetation Index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NIR</td><td align="left" valign="middle" rowspan="1" colspan="1">Near-infrared</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OCSVM</td><td align="left" valign="middle" rowspan="1" colspan="1">One-Class Support Vector Machine</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAM</td><td align="left" valign="middle" rowspan="1" colspan="1">Segment Anything Model</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAVI</td><td align="left" valign="middle" rowspan="1" colspan="1">Soil Adjusted Vegetation Index</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TARI</td><td align="left" valign="middle" rowspan="1" colspan="1">Taiwan Agricultural Research Institute</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TN</td><td align="left" valign="middle" rowspan="1" colspan="1">True Negative</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TP</td><td align="left" valign="middle" rowspan="1" colspan="1">True Positive</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UAV</td><td align="left" valign="middle" rowspan="1" colspan="1">Unmanned Aerial Vehicle</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VGG</td><td align="left" valign="middle" rowspan="1" colspan="1">Visual Geometry Group</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ViT</td><td align="left" valign="middle" rowspan="1" colspan="1">Vision Transformer</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">YOLO</td><td align="left" valign="middle" rowspan="1" colspan="1">You Only Look Once</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-05546"><label>1.</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>FAO</collab><collab>IFAD</collab><collab>UNICEF</collab><collab>WFP</collab><collab>WHO</collab></person-group><source>The State of Food Security and Nutrition in the World 2024</source><comment>eBooks</comment><publisher-name>FAO, IFAD, UNICEF, WFP, WHO</publisher-name><publisher-loc>Rome, Italy</publisher-loc><year>2024</year><pub-id pub-id-type="doi">10.4060/cd1254en</pub-id></element-citation></ref><ref id="B2-sensors-25-05546"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hashemi</surname><given-names>F.S.</given-names></name><name name-style="western"><surname>Valadan Zoej</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Youssefi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name><name name-style="western"><surname>Shafian</surname><given-names>S.</given-names></name><name name-style="western"><surname>Farnaghi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pirasteh</surname><given-names>S.</given-names></name></person-group><article-title>Integrating RS data with fuzzy decision systems for innovative crop water needs assessment</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2024</year><volume>136</volume><fpage>104338</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2024.104338</pub-id></element-citation></ref><ref id="B3-sensors-25-05546"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Anuar</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Halin</surname><given-names>A.A.</given-names></name><name name-style="western"><surname>Perumal</surname><given-names>T.</given-names></name><name name-style="western"><surname>Kalantar</surname><given-names>B.</given-names></name></person-group><article-title>Aerial Imagery Paddy Seedlings Inspection Using Deep Learning</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>274</elocation-id><pub-id pub-id-type="doi">10.3390/rs14020274</pub-id></element-citation></ref><ref id="B4-sensors-25-05546"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>N.</given-names></name><name name-style="western"><surname>Rashid</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Pasandideh</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ray</surname><given-names>B.</given-names></name><name name-style="western"><surname>Moore</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kadel</surname><given-names>R.</given-names></name></person-group><article-title>A Review of Applications and Communication Technologies for Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) Based Sustainable Smart Farming</article-title><source>Sustainability</source><year>2021</year><volume>13</volume><elocation-id>1821</elocation-id><pub-id pub-id-type="doi">10.3390/su13041821</pub-id></element-citation></ref><ref id="B5-sensors-25-05546"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tseng</surname><given-names>H.-H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>M.-D.</given-names></name><name name-style="western"><surname>Saminathan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>Y.-C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>D.-H.</given-names></name></person-group><article-title>Rice Seedling Detection in UAV Images Using Transfer Learning and Machine Learning</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>2837</elocation-id><pub-id pub-id-type="doi">10.3390/rs14122837</pub-id></element-citation></ref><ref id="B6-sensors-25-05546"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J.</given-names></name><name name-style="western"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>W.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gan</surname><given-names>X.</given-names></name></person-group><article-title>CVRP, A rice image dataset with high-quality annotations for image segmentation and plant phenomics research</article-title><source>Plant Phenomics</source><year>2025</year><volume>7</volume><fpage>100025</fpage><pub-id pub-id-type="doi">10.1016/j.plaphe.2025.100025</pub-id></element-citation></ref><ref id="B7-sensors-25-05546"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>M.-D.</given-names></name><name name-style="western"><surname>Tseng</surname><given-names>H.-H.</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>Y.-C.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>C.-Y.</given-names></name><name name-style="western"><surname>Lai</surname><given-names>M.-H.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>D.-H.</given-names></name></person-group><article-title>A UAV Open Dataset of Rice Paddies for Deep Learning Practice</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>1358</elocation-id><pub-id pub-id-type="doi">10.3390/rs13071358</pub-id></element-citation></ref><ref id="B8-sensors-25-05546"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Han</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name></person-group><article-title>Automatic Counting of in situ Rice Seedlings from UAV Images Based on a Deep Fully Convolutional Neural Network</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>691</elocation-id><pub-id pub-id-type="doi">10.3390/rs11060691</pub-id></element-citation></ref><ref id="B9-sensors-25-05546"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hashim</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ali</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>Mahadi</surname><given-names>M.R.</given-names></name><name name-style="western"><surname>Abdullah</surname><given-names>A.F.</given-names></name><name name-style="western"><surname>Wayayok</surname><given-names>A.</given-names></name><name name-style="western"><surname>Kassim</surname><given-names>M.S.M.</given-names></name><name name-style="western"><surname>Jamaluddin</surname><given-names>A.</given-names></name></person-group><article-title>Smart farming for sustainable rice production: An insight into applications, challenges and future prospects</article-title><source>Rice Sci.</source><year>2023</year><volume>31</volume><fpage>47</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.rsci.2023.08.004</pub-id></element-citation></ref><ref id="B10-sensors-25-05546"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Anandakrishnan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Sangaiah</surname><given-names>A.K.</given-names></name><name name-style="western"><surname>Darmawan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Son</surname><given-names>N.K.</given-names></name><name name-style="western"><surname>Lin</surname><given-names>Y.-B.</given-names></name><name name-style="western"><surname>Alenazi</surname><given-names>M.J.F.</given-names></name></person-group><article-title>Precise Spatial Prediction of Rice Seedlings from Large Scale Airborne Remote Sensing Data Using Optimized Li-YOLOv9</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2024</year><volume>18</volume><fpage>2226</fpage><lpage>2238</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2024.3505964</pub-id></element-citation></ref><ref id="B11-sensors-25-05546"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mahlein</surname><given-names>A.-K.</given-names></name></person-group><article-title>Plant Disease Detection by Imaging Sensors&#8212;Parallels and Specific Demands for Precision Agriculture and Plant Phenotyping</article-title><source>Plant Dis.</source><year>2016</year><volume>100</volume><fpage>241</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1094/PDIS-03-15-0340-FE</pub-id><pub-id pub-id-type="pmid">30694129</pub-id></element-citation></ref><ref id="B12-sensors-25-05546"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Lightweight Deep Learning Models for High-Precision Rice Seedling Segmentation from UAV-Based Multispectral Images</article-title><source>Plant Phenomics</source><year>2023</year><volume>5</volume><fpage>0123</fpage><pub-id pub-id-type="doi">10.34133/plantphenomics.0123</pub-id><pub-id pub-id-type="pmid">38047001</pub-id><pub-id pub-id-type="pmcid">PMC10688663</pub-id></element-citation></ref><ref id="B13-sensors-25-05546"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Majnoun Hosseini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Valadan Zoej</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Taheri Dehkordi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ghaderpour</surname><given-names>E.</given-names></name></person-group><article-title>Cropping intensity mapping in Sentinel-2 and Landsat-8/9 remote sensing data using temporal transfer of a stacked ensemble machine learning model within google earth engine</article-title><source>Geocarto Int.</source><year>2024</year><volume>39</volume><fpage>2387786</fpage><pub-id pub-id-type="doi">10.1080/10106049.2024.2387786</pub-id></element-citation></ref><ref id="B14-sensors-25-05546"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arabi Aliabad</surname><given-names>F.</given-names></name><name name-style="western"><surname>Ghafarian Malamiri</surname><given-names>H.</given-names></name><name name-style="western"><surname>Sarsangi</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sekertekin</surname><given-names>A.</given-names></name><name name-style="western"><surname>Ghaderpour</surname><given-names>E.</given-names></name></person-group><article-title>Identifying and Monitoring Gardens in Urban Areas Using Aerial and Satellite Imagery</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>4053</elocation-id><pub-id pub-id-type="doi">10.3390/rs15164053</pub-id></element-citation></ref><ref id="B15-sensors-25-05546"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>H.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cen</surname><given-names>F.</given-names></name><name name-style="western"><surname>He</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Z.</given-names></name></person-group><article-title>Estimation of sorghum seedling number from drone image based on support vector machine and YOLO algorithms</article-title><source>Front. Plant Sci.</source><year>2024</year><volume>15</volume><elocation-id>1399872</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2024.1399872</pub-id><pub-id pub-id-type="pmid">39391781</pub-id><pub-id pub-id-type="pmcid">PMC11464359</pub-id></element-citation></ref><ref id="B16-sensors-25-05546"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Geng</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>X.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>R.</given-names></name><name name-style="western"><surname>Li</surname><given-names>P.</given-names></name></person-group><article-title>Research on Segmentation Method of Maize Seedling Plant Instances Based on UAV Multispectral Remote Sensing Images</article-title><source>Plants</source><year>2024</year><volume>13</volume><elocation-id>1842</elocation-id><pub-id pub-id-type="doi">10.3390/plants13131842</pub-id><pub-id pub-id-type="pmid">38999682</pub-id><pub-id pub-id-type="pmcid">PMC11244470</pub-id></element-citation></ref><ref id="B17-sensors-25-05546"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ispiryan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Grigoriev</surname><given-names>I.</given-names></name><name name-style="western"><surname>Zu Castell</surname><given-names>W.</given-names></name><name name-style="western"><surname>Sch&#228;ffner</surname><given-names>A.R.</given-names></name></person-group><article-title>A segmentation procedure using colour features applied to images of Arabidopsis thaliana</article-title><source>Funct. Plant Biol.</source><year>2013</year><volume>40</volume><fpage>1065</fpage><lpage>1075</lpage><pub-id pub-id-type="doi">10.1071/FP12323</pub-id><pub-id pub-id-type="pmid">32481174</pub-id></element-citation></ref><ref id="B18-sensors-25-05546"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baret</surname><given-names>F.</given-names></name><name name-style="western"><surname>Houles</surname><given-names>V.</given-names></name><name name-style="western"><surname>Guerif</surname><given-names>M.</given-names></name></person-group><article-title>Quantification of plant stress using remote sensing observations and crop models: The case of nitrogen management</article-title><source>J. Exp. Bot.</source><year>2006</year><volume>58</volume><fpage>869</fpage><lpage>880</lpage><pub-id pub-id-type="doi">10.1093/jxb/erl231</pub-id><pub-id pub-id-type="pmid">17220515</pub-id></element-citation></ref><ref id="B19-sensors-25-05546"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Islam</surname><given-names>S.</given-names></name><name name-style="western"><surname>Reza</surname><given-names>M.N.</given-names></name><name name-style="western"><surname>Ahmed</surname><given-names>S.</given-names></name><name name-style="western"><surname>Samsuzzaman Cho</surname><given-names>Y.J.</given-names></name><name name-style="western"><surname>Noh</surname><given-names>D.H.</given-names></name><name name-style="western"><surname>Chung</surname><given-names>S.-O.</given-names></name></person-group><article-title>Seedling Growth Stress Quantification Based on Environmental Factors Using Sensor Fusion and Image Processing</article-title><source>Horticulturae</source><year>2024</year><volume>10</volume><elocation-id>186</elocation-id><pub-id pub-id-type="doi">10.3390/horticulturae10020186</pub-id></element-citation></ref><ref id="B20-sensors-25-05546"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Nolan</surname><given-names>T.M.</given-names></name><name name-style="western"><surname>Bao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Elmore</surname><given-names>M.</given-names></name><name name-style="western"><surname>Tuel</surname><given-names>T.</given-names></name><name name-style="western"><surname>Gai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shah</surname><given-names>D.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Huser</surname><given-names>N.M.</given-names></name><name name-style="western"><surname>Hurd</surname><given-names>A.M.</given-names></name><etal/></person-group><article-title>Robotic Assay for Drought (RoAD): An automated phenotyping system for brassinosteroid and drought responses</article-title><source>Plant J.</source><year>2021</year><volume>107</volume><fpage>1837</fpage><lpage>1853</lpage><pub-id pub-id-type="doi">10.1111/tpj.15401</pub-id><pub-id pub-id-type="pmid">34216161</pub-id></element-citation></ref><ref id="B21-sensors-25-05546"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lausch</surname><given-names>A.</given-names></name><name name-style="western"><surname>Lausch</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bastian</surname><given-names>O.</given-names></name><name name-style="western"><surname>Klotz</surname><given-names>S.</given-names></name><name name-style="western"><surname>Leit&#227;o</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>Leit&#227;o</surname><given-names>P.J.</given-names></name><name name-style="western"><surname>Jung</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rocchini</surname><given-names>D.</given-names></name><name name-style="western"><surname>Schaepman</surname><given-names>M.E.</given-names></name><name name-style="western"><surname>Skidmore</surname><given-names>A.K.</given-names></name><etal/></person-group><article-title>Understanding and assessing vegetation health by in situ species and remote-sensing approaches</article-title><source>Methods Ecol. Evol.</source><year>2018</year><volume>9</volume><fpage>1799</fpage><lpage>1809</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.13025</pub-id></element-citation></ref><ref id="B22-sensors-25-05546"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cvetkovi&#263;</surname><given-names>N.</given-names></name><name name-style="western"><surname>&#272;okovi&#263;</surname><given-names>A.</given-names></name><name name-style="western"><surname>Dobrota</surname><given-names>M.</given-names></name><name name-style="western"><surname>Radoji&#269;i&#263;</surname><given-names>M.</given-names></name></person-group><article-title>New Methodology for Corn Stress Detection Using Remote Sensing and Vegetation Indices</article-title><source>Sustainability</source><year>2023</year><volume>15</volume><elocation-id>5487</elocation-id><pub-id pub-id-type="doi">10.3390/su15065487</pub-id></element-citation></ref><ref id="B23-sensors-25-05546"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bendig</surname><given-names>J.</given-names></name><name name-style="western"><surname>Bolten</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bennertz</surname><given-names>S.</given-names></name><name name-style="western"><surname>Broscheit</surname><given-names>J.</given-names></name><name name-style="western"><surname>Eichfuss</surname><given-names>S.</given-names></name><name name-style="western"><surname>Bareth</surname><given-names>G.</given-names></name></person-group><article-title>Estimating Biomass of Barley Using Crop Surface Models (CSMs) Derived from UAV-Based RGB Imaging</article-title><source>Remote Sens.</source><year>2014</year><volume>6</volume><fpage>10395</fpage><lpage>10412</lpage><pub-id pub-id-type="doi">10.3390/rs61110395</pub-id></element-citation></ref><ref id="B24-sensors-25-05546"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>B.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Bian</surname><given-names>C.</given-names></name><name name-style="western"><surname>Jin</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>The estimation of crop emergence in potatoes by UAV RGB imagery</article-title><source>Plant Methods</source><year>2019</year><volume>15</volume><fpage>15</fpage><pub-id pub-id-type="doi">10.1186/s13007-019-0399-7</pub-id><pub-id pub-id-type="pmid">30792752</pub-id><pub-id pub-id-type="pmcid">PMC6371461</pub-id></element-citation></ref><ref id="B25-sensors-25-05546"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhen</surname><given-names>S.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Y.</given-names></name></person-group><article-title>Estimating Leaf Chlorophyll Content of Buffaloberry Using Normalized Difference Vegetation Index Sensors</article-title><source>HortTechnology</source><year>2021</year><volume>31</volume><fpage>297</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.21273/HORTTECH04808-21</pub-id></element-citation></ref><ref id="B26-sensors-25-05546"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Candiago</surname><given-names>S.</given-names></name><name name-style="western"><surname>Remondino</surname><given-names>F.</given-names></name><name name-style="western"><surname>De Giglio</surname><given-names>M.</given-names></name><name name-style="western"><surname>Dubbini</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gattelli</surname><given-names>M.</given-names></name></person-group><article-title>Evaluating Multispectral Images and Vegetation Indices for Precision Farming Applications from UAV Images</article-title><source>Remote Sens.</source><year>2015</year><volume>7</volume><fpage>4026</fpage><lpage>4047</lpage><pub-id pub-id-type="doi">10.3390/rs70404026</pub-id></element-citation></ref><ref id="B27-sensors-25-05546"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lawley</surname><given-names>V.</given-names></name><name name-style="western"><surname>Lewis</surname><given-names>M.</given-names></name><name name-style="western"><surname>Clarke</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ostendorf</surname><given-names>B.</given-names></name></person-group><article-title>Site-based and remote sensing methods for monitoring indicators of vegetation condition: An Australian review</article-title><source>Ecol. Indic.</source><year>2016</year><volume>60</volume><fpage>1273</fpage><lpage>1283</lpage><pub-id pub-id-type="doi">10.1016/j.ecolind.2015.03.021</pub-id></element-citation></ref><ref id="B28-sensors-25-05546"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.</given-names></name></person-group><article-title>Combining Canopy Coverage and Plant Height from UAV-Based RGB Images to Estimate Spraying Volume on Potato</article-title><source>Sustainability</source><year>2022</year><volume>14</volume><elocation-id>6473</elocation-id><pub-id pub-id-type="doi">10.3390/su14116473</pub-id></element-citation></ref><ref id="B29-sensors-25-05546"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>R.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>B.</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>X.</given-names></name><name name-style="western"><surname>Peng</surname><given-names>C.</given-names></name><name name-style="western"><surname>Lv</surname><given-names>C.</given-names></name></person-group><article-title>High-Performance Grape Disease Detection Method Using Multimodal Data and Parallel Activation Functions</article-title><source>Plants</source><year>2024</year><volume>13</volume><elocation-id>2720</elocation-id><pub-id pub-id-type="doi">10.3390/plants13192720</pub-id><pub-id pub-id-type="pmid">39409590</pub-id><pub-id pub-id-type="pmcid">PMC11478535</pub-id></element-citation></ref><ref id="B30-sensors-25-05546"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Osco</surname><given-names>L.P.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Lemos</surname><given-names>E.L.</given-names></name><name name-style="western"><surname>Gon&#231;alves</surname><given-names>W.N.</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>A.P.M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name><name name-style="western"><surname>Marcato</surname><given-names>J.</given-names></name></person-group><article-title>The Segment Anything Model (SAM) for remote sensing applications: From zero to one shot</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2023</year><volume>124</volume><fpage>103540</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2023.103540</pub-id></element-citation></ref><ref id="B31-sensors-25-05546"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kirillov</surname><given-names>A.</given-names></name><name name-style="western"><surname>Mintun</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ravi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mao</surname><given-names>H.</given-names></name><name name-style="western"><surname>Rolland</surname><given-names>C.</given-names></name><name name-style="western"><surname>Gustafson</surname><given-names>L.</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Whitehead</surname><given-names>S.</given-names></name><name name-style="western"><surname>Berg</surname><given-names>A.C.</given-names></name><name name-style="western"><surname>Lo</surname><given-names>W.Y.</given-names></name><etal/></person-group><article-title>Segment anything</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.02643</pub-id></element-citation></ref><ref id="B32-sensors-25-05546"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rezvan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Valadan Zoej</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Hassanpour</surname><given-names>G.</given-names></name><name name-style="western"><surname>Youssefi</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hanafi-Bojd</surname><given-names>A.A.</given-names></name></person-group><article-title>Identification of Aquatic Habitats of Anopheles Mosquito Using Time-series Analysis of Sentinel-1 data through Google Earth Engine</article-title><source>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2024</year><volume>XLVIII-3/W3-2024</volume><fpage>63</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.5194/isprs-archives-XLVIII-3-W3-2024-63-2024</pub-id></element-citation></ref><ref id="B33-sensors-25-05546"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Abaxi</surname><given-names>S.M.D.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>H.</given-names></name><name name-style="western"><surname>Lo</surname><given-names>F.P.-W.</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>W.</given-names></name></person-group><article-title>Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation</article-title><source>Diagnostics</source><year>2023</year><volume>13</volume><elocation-id>1947</elocation-id><pub-id pub-id-type="doi">10.3390/diagnostics13111947</pub-id><pub-id pub-id-type="pmid">37296799</pub-id><pub-id pub-id-type="pmcid">PMC10252742</pub-id></element-citation></ref><ref id="B34-sensors-25-05546"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mazurowski</surname><given-names>M.A.</given-names></name><name name-style="western"><surname>Dong</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gu</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Konz</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Segment anything model for medical image analysis: An experimental study</article-title><source>Med. Image Anal.</source><year>2023</year><volume>89</volume><fpage>102918</fpage><pub-id pub-id-type="doi">10.1016/j.media.2023.102918</pub-id><pub-id pub-id-type="pmid">37595404</pub-id><pub-id pub-id-type="pmcid">PMC10528428</pub-id></element-citation></ref><ref id="B35-sensors-25-05546"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>S.</given-names></name><name name-style="western"><surname>Fei</surname><given-names>C.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>M.</given-names></name><name name-style="western"><surname>Han</surname><given-names>J.</given-names></name><name name-style="western"><surname>Song</surname><given-names>W.</given-names></name></person-group><article-title>Zero-Shot Kidney Stone Segmentation Based on Segmentation Anything Model for Robotic-Assisted Endoscope Navigation</article-title><source>Intelligent Robotics and Applications</source><comment>Lecture Notes in Computer Science</comment><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2023</year><fpage>80</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1007/978-981-99-6489-5_7</pub-id></element-citation></ref><ref id="B36-sensors-25-05546"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rivera</surname><given-names>A.J.</given-names></name><name name-style="western"><surname>Perez-Godoy</surname><given-names>M.D.</given-names></name><name name-style="western"><surname>Elizondo</surname><given-names>D.</given-names></name><name name-style="western"><surname>Deka</surname><given-names>L.</given-names></name><name name-style="western"><surname>del Jesus</surname><given-names>M.J.</given-names></name></person-group><article-title>Analysis of clustering methods for crop type mapping using satellite imagery</article-title><source>Neurocomputing</source><year>2022</year><volume>492</volume><fpage>91</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2022.04.002</pub-id></element-citation></ref><ref id="B37-sensors-25-05546"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>G.</given-names></name><name name-style="western"><surname>Dai</surname><given-names>H.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Xu</surname><given-names>B.</given-names></name><name name-style="western"><surname>Feng</surname><given-names>H.</given-names></name><name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name></person-group><article-title>Fuzzy Clustering of Maize Plant-Height Patterns Using Time Series of UAV Remote-Sensing Images and Variety Traits</article-title><source>Front. Plant Sci.</source><year>2019</year><volume>10</volume><elocation-id>926</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2019.00926</pub-id><pub-id pub-id-type="pmid">31379905</pub-id><pub-id pub-id-type="pmcid">PMC6652214</pub-id></element-citation></ref><ref id="B38-sensors-25-05546"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tits</surname><given-names>L.</given-names></name><name name-style="western"><surname>Somers</surname><given-names>B.</given-names></name><name name-style="western"><surname>Coppin</surname><given-names>P.</given-names></name></person-group><article-title>The Potential and Limitations of a Clustering Approach for the Improved Efficiency of Multiple Endmember Spectral Mixture Analysis in Plant Production System Monitoring</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2012</year><volume>50</volume><fpage>2273</fpage><lpage>2286</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2011.2173696</pub-id></element-citation></ref><ref id="B39-sensors-25-05546"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marino</surname><given-names>S.</given-names></name><name name-style="western"><surname>Alvino</surname><given-names>A.</given-names></name></person-group><article-title>Vegetation Indices Data Clustering for Dynamic Monitoring and Classification of Wheat Yield Crop Traits</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>541</elocation-id><pub-id pub-id-type="doi">10.3390/rs13040541</pub-id></element-citation></ref><ref id="B40-sensors-25-05546"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Catalano</surname><given-names>C.</given-names></name><name name-style="western"><surname>Paiano</surname><given-names>L.</given-names></name><name name-style="western"><surname>Calabrese</surname><given-names>F.</given-names></name><name name-style="western"><surname>Cataldo</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mancarella</surname><given-names>L.</given-names></name><name name-style="western"><surname>Tommasi</surname><given-names>F.</given-names></name></person-group><article-title>Anomaly detection in smart agriculture systems</article-title><source>Comput. Ind.</source><year>2022</year><volume>143</volume><fpage>103750</fpage><pub-id pub-id-type="doi">10.1016/j.compind.2022.103750</pub-id></element-citation></ref><ref id="B41-sensors-25-05546"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leygonie</surname><given-names>R.</given-names></name><name name-style="western"><surname>Lobry</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wendling</surname><given-names>L.</given-names></name></person-group><article-title>Can we detect plant diseases without prior knowledge of their existence?</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2024</year><volume>134</volume><fpage>104192</fpage><pub-id pub-id-type="doi">10.1016/j.jag.2024.104192</pub-id></element-citation></ref><ref id="B42-sensors-25-05546"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moso</surname><given-names>J.C.</given-names></name><name name-style="western"><surname>Cormier</surname><given-names>S.</given-names></name><name name-style="western"><surname>de Runz</surname><given-names>C.</given-names></name><name name-style="western"><surname>Fouchal</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wandeto</surname><given-names>J.M.</given-names></name></person-group><article-title>Anomaly Detection on Data Streams for Smart Agriculture</article-title><source>Agriculture</source><year>2021</year><volume>11</volume><elocation-id>1083</elocation-id><pub-id pub-id-type="doi">10.3390/agriculture11111083</pub-id></element-citation></ref><ref id="B43-sensors-25-05546"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gkountakos</surname><given-names>K.</given-names></name><name name-style="western"><surname>Ioannidis</surname><given-names>K.</given-names></name><name name-style="western"><surname>Demestichas</surname><given-names>K.</given-names></name><name name-style="western"><surname>Vrochidis</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kompatsiaris</surname><given-names>I.</given-names></name></person-group><article-title>A Comprehensive Review of Deep Learning-Based Anomaly Detection Methods for Precision Agriculture</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>197715</fpage><lpage>197733</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3522248</pub-id></element-citation></ref><ref id="B44-sensors-25-05546"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rezvan</surname><given-names>H.</given-names></name><name name-style="western"><surname>Valadan Zoej</surname><given-names>M.J.</given-names></name><name name-style="western"><surname>Youssefi</surname><given-names>F.</given-names></name></person-group><article-title>A Segment Anything Model Approach for Rice Seedlings Detection Based on UAV Images</article-title><source>ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2025</year><volume>X-G-2025</volume><fpage>713</fpage><lpage>719</lpage><pub-id pub-id-type="doi">10.5194/isprs-annals-X-G-2025-713-2025</pub-id></element-citation></ref><ref id="B45-sensors-25-05546"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eladl</surname><given-names>S.G.</given-names></name><name name-style="western"><surname>Haikal</surname><given-names>A.Y.</given-names></name><name name-style="western"><surname>Saafan</surname><given-names>M.M.</given-names></name><name name-style="western"><surname>ZainEldin</surname><given-names>H.Y.</given-names></name></person-group><article-title>A proposed plant classification framework for smart agricultural applications using UAV images and artificial intelligence techniques</article-title><source>Alex. Eng. J.</source><year>2024</year><volume>109</volume><fpage>466</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1016/j.aej.2024.08.076</pub-id></element-citation></ref><ref id="B46-sensors-25-05546"><label>46.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Qin</surname><given-names>J.L.</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L.</given-names></name></person-group><article-title>Image Dataset of Wheat, Corn, and Rice Seedlings in Heilongjiang Province in 2022</article-title><publisher-name>Science Data Bank</publisher-name><year>2024</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://cstr.cn/17058.11.sciencedb.agriculture.00092" ext-link-type="uri">https://cstr.cn/17058.11.sciencedb.agriculture.00092</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-06-10">(accessed on 10 June 2025)</date-in-citation></element-citation></ref><ref id="B47-sensors-25-05546"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>X.-Y.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>D.-S.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>F.-L.</given-names></name></person-group><article-title>A review on blind detection for image steganography</article-title><source>Signal Process.</source><year>2008</year><volume>88</volume><fpage>2138</fpage><lpage>2157</lpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2008.03.016</pub-id></element-citation></ref><ref id="B48-sensors-25-05546"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mao</surname><given-names>X.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>B.</given-names></name></person-group><article-title>Comparative study on methods for computing electrical distance</article-title><source>Int. J. Electr. Power Energy Syst.</source><year>2021</year><volume>130</volume><fpage>106923</fpage><pub-id pub-id-type="doi">10.1016/j.ijepes.2021.106923</pub-id></element-citation></ref><ref id="B49-sensors-25-05546"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Allaoui</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kherfi</surname><given-names>M.L.</given-names></name><name name-style="western"><surname>Cheriet</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bouchachia</surname><given-names>A.</given-names></name></person-group><article-title>Unified embedding and clustering</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>238</volume><fpage>121923</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2023.121923</pub-id></element-citation></ref><ref id="B50-sensors-25-05546"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ahmadi</surname><given-names>P.</given-names></name><name name-style="western"><surname>Mansor</surname><given-names>S.</given-names></name><name name-style="western"><surname>Farjad</surname><given-names>B.</given-names></name><name name-style="western"><surname>Ghaderpour</surname><given-names>E.</given-names></name></person-group><article-title>Unmanned Aerial Vehicle (UAV)-Based Remote Sensing for Early-Stage Detection of Ganoderma</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>1239</elocation-id><pub-id pub-id-type="doi">10.3390/rs14051239</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05546-f001" orientation="portrait"><label>Figure 1</label><caption><p>Study area: (<bold>a</bold>) First dataset, located in Taichung province in Taiwan; (<bold>b</bold>) Second dataset, located in Heilongjiang in China.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g001.jpg"/></fig><fig position="float" id="sensors-25-05546-f002" orientation="portrait"><label>Figure 2</label><caption><p>Examples of growth stages of rice seedlings dataset: (<bold>a</bold>) First growth stage; (<bold>b</bold>) Second growth stage; (<bold>c</bold>) Third growth stage. Above and below rows are the images for the first and second datasets, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g002.jpg"/></fig><fig position="float" id="sensors-25-05546-f003" orientation="portrait"><label>Figure 3</label><caption><p>Flowchart of the proposed methodology.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g003.jpg"/></fig><fig position="float" id="sensors-25-05546-f004" orientation="portrait"><label>Figure 4</label><caption><p>The visual process of Algorithm 1.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g004.jpg"/></fig><fig position="float" id="sensors-25-05546-f005" orientation="portrait"><label>Figure 5</label><caption><p>Segmentation results of rice seedlings for varying growth stages: First dataset: (<bold>a</bold>) 7 August 2018; (<bold>b</bold>) 14 August 2018; (<bold>c</bold>) 23 August 2018; Second dataset: (<bold>d</bold>) 1 June 2022; (<bold>e</bold>) 6 June 2022; (<bold>f</bold>) 11 June 2022.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g005.jpg"/></fig><fig position="float" id="sensors-25-05546-f006" orientation="portrait"><label>Figure 6</label><caption><p>Predicted counts versus the ground truth counts for each growth stage: (<bold>a</bold>&#8211;<bold>c</bold>) first, second and third growth stage for the first dataset; (<bold>d</bold>&#8211;<bold>f</bold>) first, second and third growth stage for the second dataset, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g006.jpg"/></fig><fig position="float" id="sensors-25-05546-f007" orientation="portrait"><label>Figure 7</label><caption><p>Separability of seedlings in reduced space (2D feature space) obtained from t-SNE, where Component 1 (<italic toggle="yes">x</italic>-axis) and Component 2 (<italic toggle="yes">y</italic>-axis) represent abstract embedding dimensions derived from high-dimensional features: (<bold>a</bold>&#8211;<bold>c</bold>) first dataset: 7 August 2018, 14 August 2018, 23 August 2018 and (<bold>d</bold>&#8211;<bold>f</bold>) second dataset: 30 May 2022, 7 June 2022, 14 June 2022, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g007.jpg"/></fig><fig position="float" id="sensors-25-05546-f008" orientation="portrait"><label>Figure 8</label><caption><p>Decision function plot and distance of samples from separating hyperplane: (<bold>a</bold>&#8211;<bold>c</bold>) first dataset: 7 August 2018, 14 August 2018, 23 August 2018 and (<bold>d</bold>&#8211;<bold>f</bold>) second dataset: 30 May 2022, 7 June 2022, 14 June 2022, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g008.jpg"/></fig><fig position="float" id="sensors-25-05546-f009" orientation="portrait"><label>Figure 9</label><caption><p>Visual validation of seedlings: (<bold>1</bold>) first growth stage, (<bold>2</bold>) second growth stage, and (<bold>3</bold>) third growth stage. Also, (<bold>a</bold>,<bold>c</bold>,<bold>e</bold>) detected anomalies as damaged or unhealthy seedlings, and (<bold>b</bold>,<bold>d</bold>,<bold>f</bold>) normal seedlings.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05546-g009.jpg"/></fig><table-wrap position="float" id="sensors-25-05546-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05546-t001_Table 1</object-id><label>Table 1</label><caption><p>A description of datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spatial Resolution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Temporal Resolution</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">DOI (Link) of the Dataset</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">First dataset</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.4 &#956;m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7 days</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/aipal-nchu/RiceSeedlingDataset">https://github.com/aipal-nchu/RiceSeedlingDataset</uri> (accessed on 30 August 2024)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Second dataset</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 mm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Daily</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://doi.org/10.57760/sciencedb.agriculture.00092">https://doi.org/10.57760/sciencedb.agriculture.00092</uri> (accessed on 10 June 2025)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05546-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05546-t002_Table 2</object-id><label>Table 2</label><caption><p>SAM parameters. ViT is short for vision transformer.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">model_type</td><td align="center" valign="middle" rowspan="1" colspan="1">ViT_H</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">checkpoint</td><td align="center" valign="middle" rowspan="1" colspan="1">sam_vit_h_4b8939.pth</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Automatic</td><td align="center" valign="middle" rowspan="1" colspan="1">False</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Predictor</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SAM Predictor</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05546-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05546-t003_Table 3</object-id><label>Table 3</label><caption><p>Evaluation of metrics for the proposed automated SAM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Growth Stage</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mDice</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIoU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mFPR</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">First dataset</td><td align="center" valign="middle" rowspan="1" colspan="1">Early stage (7 August 2018)</td><td align="center" valign="middle" rowspan="1" colspan="1">94.7</td><td align="center" valign="middle" rowspan="1" colspan="1">90.1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.038</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mid-growth stage (14 August 2018)</td><td align="center" valign="middle" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" rowspan="1" colspan="1">84.0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.069</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mature stage (23 August 2018)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.219</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Second dataset</td><td align="center" valign="middle" rowspan="1" colspan="1">Early stage (29 May 2022 to 3 June 2022)</td><td align="center" valign="middle" rowspan="1" colspan="1">93.0</td><td align="center" valign="middle" rowspan="1" colspan="1">87.0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.047</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mid-growth stage (4&#8211;9 June 2022)</td><td align="center" valign="middle" rowspan="1" colspan="1">85.0</td><td align="center" valign="middle" rowspan="1" colspan="1">73.8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.054</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mature stage (10&#8211;14 June 2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.075</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05546-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05546-t004_Table 4</object-id><label>Table 4</label><caption><p>Bhattacharyya distance during growth stages.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">First Dataset</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Second Dataset</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Features</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stage 1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stage 2</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stage 3</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stage 1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stage 2</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stage 3</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Area</td><td align="center" valign="middle" rowspan="1" colspan="1">0.222</td><td align="center" valign="middle" rowspan="1" colspan="1">0.361</td><td align="center" valign="middle" rowspan="1" colspan="1">0.326</td><td align="center" valign="middle" rowspan="1" colspan="1">0.115</td><td align="center" valign="middle" rowspan="1" colspan="1">0.244</td><td align="center" valign="middle" rowspan="1" colspan="1">0.222</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Perimeter</td><td align="center" valign="middle" rowspan="1" colspan="1">0.250</td><td align="center" valign="middle" rowspan="1" colspan="1">0.360</td><td align="center" valign="middle" rowspan="1" colspan="1">0.215</td><td align="center" valign="middle" rowspan="1" colspan="1">0.076</td><td align="center" valign="middle" rowspan="1" colspan="1">0.418</td><td align="center" valign="middle" rowspan="1" colspan="1">0.230</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solidity</td><td align="center" valign="middle" rowspan="1" colspan="1">0.084</td><td align="center" valign="middle" rowspan="1" colspan="1">0.206</td><td align="center" valign="middle" rowspan="1" colspan="1">0.189</td><td align="center" valign="middle" rowspan="1" colspan="1">0.170</td><td align="center" valign="middle" rowspan="1" colspan="1">0.206</td><td align="center" valign="middle" rowspan="1" colspan="1">0.118</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Eccentricity</td><td align="center" valign="middle" rowspan="1" colspan="1">0.063</td><td align="center" valign="middle" rowspan="1" colspan="1">0.141</td><td align="center" valign="middle" rowspan="1" colspan="1">0.152</td><td align="center" valign="middle" rowspan="1" colspan="1">0.210</td><td align="center" valign="middle" rowspan="1" colspan="1">0.115</td><td align="center" valign="middle" rowspan="1" colspan="1">0.067</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Circularity</td><td align="center" valign="middle" rowspan="1" colspan="1">0.488</td><td align="center" valign="middle" rowspan="1" colspan="1">0.807</td><td align="center" valign="middle" rowspan="1" colspan="1">0.574</td><td align="center" valign="middle" rowspan="1" colspan="1">0.092</td><td align="center" valign="middle" rowspan="1" colspan="1">0.360</td><td align="center" valign="middle" rowspan="1" colspan="1">0.578</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Red</td><td align="center" valign="middle" rowspan="1" colspan="1">0.193</td><td align="center" valign="middle" rowspan="1" colspan="1">0.251</td><td align="center" valign="middle" rowspan="1" colspan="1">0.160</td><td align="center" valign="middle" rowspan="1" colspan="1">0.275</td><td align="center" valign="middle" rowspan="1" colspan="1">0.290</td><td align="center" valign="middle" rowspan="1" colspan="1">0.200</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Green</td><td align="center" valign="middle" rowspan="1" colspan="1">0.169</td><td align="center" valign="middle" rowspan="1" colspan="1">0.214</td><td align="center" valign="middle" rowspan="1" colspan="1">0.122</td><td align="center" valign="middle" rowspan="1" colspan="1">0.314</td><td align="center" valign="middle" rowspan="1" colspan="1">0.339</td><td align="center" valign="middle" rowspan="1" colspan="1">0.244</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Blue</td><td align="center" valign="middle" rowspan="1" colspan="1">0.179</td><td align="center" valign="middle" rowspan="1" colspan="1">0.105</td><td align="center" valign="middle" rowspan="1" colspan="1">0.205</td><td align="center" valign="middle" rowspan="1" colspan="1">0.287</td><td align="center" valign="middle" rowspan="1" colspan="1">0.254</td><td align="center" valign="middle" rowspan="1" colspan="1">0.377</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ExG</td><td align="center" valign="middle" rowspan="1" colspan="1">0.201</td><td align="center" valign="middle" rowspan="1" colspan="1">0.068</td><td align="center" valign="middle" rowspan="1" colspan="1">0.178</td><td align="center" valign="middle" rowspan="1" colspan="1">0.184</td><td align="center" valign="middle" rowspan="1" colspan="1">0.197</td><td align="center" valign="middle" rowspan="1" colspan="1">0.195</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Contrast</td><td align="center" valign="middle" rowspan="1" colspan="1">0.250</td><td align="center" valign="middle" rowspan="1" colspan="1">0.167</td><td align="center" valign="middle" rowspan="1" colspan="1">0.218</td><td align="center" valign="middle" rowspan="1" colspan="1">0.397</td><td align="center" valign="middle" rowspan="1" colspan="1">0.246</td><td align="center" valign="middle" rowspan="1" colspan="1">0.320</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Dissimilarity</td><td align="center" valign="middle" rowspan="1" colspan="1">0.264</td><td align="center" valign="middle" rowspan="1" colspan="1">0.176</td><td align="center" valign="middle" rowspan="1" colspan="1">0.230</td><td align="center" valign="middle" rowspan="1" colspan="1">0.339</td><td align="center" valign="middle" rowspan="1" colspan="1">0.096</td><td align="center" valign="middle" rowspan="1" colspan="1">0.314</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Homogeneity</td><td align="center" valign="middle" rowspan="1" colspan="1">0.224</td><td align="center" valign="middle" rowspan="1" colspan="1">0.128</td><td align="center" valign="middle" rowspan="1" colspan="1">0.196</td><td align="center" valign="middle" rowspan="1" colspan="1">0.313</td><td align="center" valign="middle" rowspan="1" colspan="1">0.050</td><td align="center" valign="middle" rowspan="1" colspan="1">0.147</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Energy</td><td align="center" valign="middle" rowspan="1" colspan="1">0.199</td><td align="center" valign="middle" rowspan="1" colspan="1">0.305</td><td align="center" valign="middle" rowspan="1" colspan="1">0.376</td><td align="center" valign="middle" rowspan="1" colspan="1">0.302</td><td align="center" valign="middle" rowspan="1" colspan="1">0.129</td><td align="center" valign="middle" rowspan="1" colspan="1">0.067</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Correlation</td><td align="center" valign="middle" rowspan="1" colspan="1">0.115</td><td align="center" valign="middle" rowspan="1" colspan="1">0.147</td><td align="center" valign="middle" rowspan="1" colspan="1">0.148</td><td align="center" valign="middle" rowspan="1" colspan="1">0.354</td><td align="center" valign="middle" rowspan="1" colspan="1">0.189</td><td align="center" valign="middle" rowspan="1" colspan="1">0.177</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Second Moment</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.236</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.325</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.925</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.330</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.221</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.099</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05546-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05546-t005_Table 5</object-id><label>Table 5</label><caption><p>Silhouette scores during growth stages.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Growth Stage</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Silhouette Score</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">First</td><td align="center" valign="middle" rowspan="1" colspan="1">Stage 1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Stage 2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.44</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stage 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.41</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Second</td><td align="center" valign="middle" rowspan="1" colspan="1">Stage 1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.34</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Stage 2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stage 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.31</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>