<?xml version="1.0"  ?><!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd"><pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="pmc-domain-id">1660</journal-id><journal-id journal-id-type="pmc-domain">sensors</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12431438</article-id><article-id pub-id-type="pmcid-ver">PMC12431438.1</article-id><article-id pub-id-type="pmcaid">12431438</article-id><article-id pub-id-type="pmcaiid">12431438</article-id><article-id pub-id-type="doi">10.3390/s25175428</article-id><article-id pub-id-type="publisher-id">sensors-25-05428</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Smartphone-Based Markerless Motion Capture for Accessible Rehabilitation: A Computer Vision Study</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8661-3080</contrib-id><name name-style="western"><surname>Cunha</surname><given-names initials="B">Bruno</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-05428" ref-type="aff">1</xref><xref rid="af2-sensors-25-05428" ref-type="aff">2</xref><xref rid="c1-sensors-25-05428" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ma&#231;&#227;es</surname><given-names initials="J">Jos&#233;</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#8211; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af3-sensors-25-05428" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6102-6165</contrib-id><name name-style="western"><surname>Amorim</surname><given-names initials="I">Ivone</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#8211; review &amp; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af2-sensors-25-05428" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Mercaldo</surname><given-names initials="F">Francesco</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-05428"><label>1</label>CINTESIS@RISE, CINTESIS.UPT, Department of Science and Technology, Portucalense University, Rua Dr. Ant&#243;nio Bernardino de Almeida 541, 4200-072 Porto, Portugal</aff><aff id="af2-sensors-25-05428"><label>2</label>Porto Research, Technology &amp; Innovation Center, Polytechnic of Porto (IPP), Rua Arquitecto Lob&#227;o Vital, 172, 4200-375 Porto, Portugal; <email>ifa@isep.ipp.pt</email></aff><aff id="af3-sensors-25-05428"><label>3</label>FEUP&#8212;Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, 4200-465 Porto, Portugal</aff><author-notes><corresp id="c1-sensors-25-05428"><label>*</label>Correspondence: <email>cun@isep.ipp.pt</email> or <email>bruno.cunha@upt.pt</email></corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><volume>25</volume><issue>17</issue><issue-id pub-id-type="pmc-issue-id">496815</issue-id><elocation-id>5428</elocation-id><history><date date-type="received"><day>26</day><month>6</month><year>2025</year></date><date date-type="rev-recd"><day>28</day><month>7</month><year>2025</year></date><date date-type="accepted"><day>18</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>13</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-13 17:25:36.317"><day>13</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pmc-pdf" xlink:href="sensors-25-05428.pdf"/><abstract><p>Physical rehabilitation is crucial for injury recovery, offering pain relief and faster healing. However, traditional methods rely heavily on in-person professional feedback, which can be time-consuming, expensive, and prone to human error, limiting accessibility and effectiveness. As a result, patients are often encouraged to perform exercises at home; however, due to the lack of professional guidance, motivation dwindles and adherence becomes a challenge. To address this, this paper proposes a smartphone-based solution that enables patients to receive exercise feedback independently. This paper reviews current Computer Vision systems for assessing rehabilitation exercises and introduces an intelligent system designed to assist patients in their recovery. Our proposed system uses motion tracking based on Computer Vision, analyzing videos recorded with a smartphone. With accessibility as a priority, the system is evaluated against the advanced Qualysis Motion Capture System using a dataset labeled by expert physicians. The framework focuses on human pose detection and movement quality assessment, aiming to reduce recovery times, minimize human error, and make rehabilitation more accessible. This proof-of-concept study was conducted as a pilot evaluation involving 15 participants, consistent with earlier work in the field, and serves to assess feasibility before scaling to larger datasets. This innovative approach has the potential to transform rehabilitation, providing accurate feedback and support to patients without the need for in-person supervision or specialized equipment.</p></abstract><kwd-group><kwd>rehabilitation</kwd><kwd>computer vision</kwd><kwd>artificial intelligence</kwd><kwd>accessibility</kwd><kwd>machine learning</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-05428"><title>1. Introduction</title><p>Rehabilitation plays a vital role in injury recovery and in restoring the physical capabilities of individuals who have disabilities or other health conditions. Traditional rehabilitation approaches often rely on in-person sessions supervised by healthcare professionals, which can be costly, time-consuming, and&#160;may lack accessibility for certain populations [<xref rid="B1-sensors-25-05428" ref-type="bibr">1</xref>]. Since 2020, there has been a growing interest in integrating advanced technologies to augment rehabilitation processes and improve patient outcomes [<xref rid="B2-sensors-25-05428" ref-type="bibr">2</xref>]. One area of technology that holds significant promise in the field of rehabilitation is Computer Vision (CV). CV allows computers to interpret visual data through image processing and Machine Learning algorithms. By capturing and analyzing the movements of individuals, these systems should be able to provide feedback without depending on the availability of a healthcare professional. They have the potential to revolutionize rehabilitation by enabling increased access to rehabilitation&#160;services.</p><p>In this paper, we focus on the application of CV for accessible physical rehabilitation. To accomplish this, we explore the use of markerless motion tracking techniques, which eliminate the need for external sensors or physical markers. This paper analyzes the key stages of markerless Computer Vision rehabilitation systems (data acquisition, pose estimation, and&#160;motion assessment) and introduces a smartphone-based prototype built for accessible home&#160;use.</p><p>The motivation behind this research comes from recognizing the significant challenges and limitations faced by individuals who require rehabilitation services. Simply put, rehabilitation is not accessible enough in terms of time, money, and&#160;availability. Developing a CV rehabilitation system aims to overcome these barriers and make rehabilitation more accessible to a wider population. This approach empowers individuals to perform rehabilitation exercises at home, removing the need for specialized equipment or constant supervision. This affordability and autonomy can extend to individuals with limited access to healthcare facilities or those facing financial constraints. This work builds upon prior clinical collaborations and is intended as a pilot feasibility study, designed to assess whether smartphone-based markerless analysis is viable for rehabilitation assessment at a low cost. The&#160;chosen sample size reflects logistical constraints and is in line with our previous exploratory studies in the field, including Lopes&#160;et&#160;al. [<xref rid="B3-sensors-25-05428" ref-type="bibr">3</xref>].</p><p>Overall, this paper aims to contribute to the growing body of knowledge in the field of CV systems for rehabilitation by developing a motion tracking system that can improve the accessibility of rehabilitation processes. Through our work, we hope to empower individuals to take control of their rehabilitation journey, enabling them to enhance their overall&#160;well-being.</p><p>The remaining sections of this paper are organized as follows: In <xref rid="sec2-sensors-25-05428" ref-type="sec">Section 2</xref>, the&#160;state of the art and the literature review are detailed. In <xref rid="sec3-sensors-25-05428" ref-type="sec">Section 3</xref>, the&#160;implementation of the solution is described. <xref rid="sec4-sensors-25-05428" ref-type="sec">Section 4</xref> presents and analyses the assessment of the system evaluation and the collected results. <xref rid="sec5-sensors-25-05428" ref-type="sec">Section 5</xref> contains the conclusion and identifies the directions for future&#160;works.</p></sec><sec id="sec2-sensors-25-05428"><title>2. State of the&#160;Art</title><p>The advent of intelligent technology has sparked great interest in the field of rehabilitation. Its integration enables the creation of new systems that have the potential to improve the efficiency of rehabilitation by helping to ensure exercises are performed correctly. They can also provide personalized assessments and progress tracking, among&#160;other benefits. Furthermore, this integration could increase the accessibility of rehabilitation, as&#160;these intelligent technologies can be autonomous and not require continuous professional supervision. This section reviews the state-of-the-art on the application of CV to rehabilitation. Firstly, we present a historical overview of vision-based motion tracking systems for rehabilitation. After that, there is a literature review on the specific topic of motion tracking systems for intelligent&#160;rehabilitation.</p><sec id="sec2dot1-sensors-25-05428"><title>2.1. Historical&#160;Overview</title><p>According to the review by Colyer&#160;et&#160;al. [<xref rid="B4-sensors-25-05428" ref-type="bibr">4</xref>], the&#160;first attempts to build vision-based motion tracking systems for rehabilitation purposes were based on manual digitization, which consisted of manually localizing points of interest, most typically joints, in&#160;each sequential image. This process is very time-consuming, demanding significant effort from experts to identify and track the relevant anatomical landmarks accurately, and&#160;can be subject to human errors, introducing potential inaccuracies in the analysis of exercises. The inherent drawbacks served as motivation to search for automated solutions. Automatic systems aim to automate the process of point localization and tracking, alleviating the burden of manual intervention and minimizing human errors. They also offer the advantage of reducing the time required for exercise&#160;analysis.</p><p>The first automated approach was automatic marker-based motion tracking systems [<xref rid="B4-sensors-25-05428" ref-type="bibr">4</xref>]. As the name suggests, these systems relied on the placement of passive markers on the subject&#8217;s body, in&#160;strategic places such as joints, to&#160;make it easier to detect the position of these points of interest. This can be used to deduce the pose of the subject. The&#160;main advantages of these systems are automation and all that comes with it, such as eliminating the need for human interference and consuming less time, and&#160;the accuracy they present. Despite that, the&#160;requirement for physical markers causes many drawbacks. Markers take time to place and prepare; they are difficult to place in their correct positions, and&#160;that placement can shift from day to day; as they are not completely fixed to a joint, their position can differ from the joint&#8217;s position; they limit and constrain the subject&#8217;s movements. These limitations were the driving force for the next step: exploring markerless&#160;solutions.</p><p>Nowadays, research has focused intensively on markerless motion tracking systems [<xref rid="B5-sensors-25-05428" ref-type="bibr">5</xref>], as&#160;they present numerous advantages, namely not requiring preparation of the subject, not being invasive or constraining the subject, and&#160;being much more accessible [<xref rid="B4-sensors-25-05428" ref-type="bibr">4</xref>]. Although&#160;Colyer&#160;et&#160;al. [<xref rid="B4-sensors-25-05428" ref-type="bibr">4</xref>] provides a useful synthesis, additional works have explored hybrid and semi-automated approaches, including early use of inertial measurement units (IMUs) and stereo vision systems in rehabilitation monitoring. These alternative technologies offer distinct advantages and trade-offs in terms of cost, portability, and&#160;clinical adoption. In&#160;addition to vision-based systems, these other rehabilitation monitoring solutions have leveraged IMUs, stereo vision cameras, and&#160;structured-light depth sensors, which remain active areas of research despite higher equipment complexity or cost [<xref rid="B6-sensors-25-05428" ref-type="bibr">6</xref>]. This direction allows for more intelligent methods to be used, namely CV methods, that can detect pose and assess motion quality comparatively. The next sections detail the current state of research on markerless motion tracking systems for intelligent&#160;rehabilitation.</p></sec><sec id="sec2dot2-sensors-25-05428"><title>2.2. Accessible Motion Tracking Systems for Intelligent&#160;Rehabilitation</title><p>In recent years, as&#160;this is an open research area, some motion tracking systems have been developed for intelligent rehabilitation for research purposes. Following the structure proposed by [<xref rid="B7-sensors-25-05428" ref-type="bibr">7</xref>], in&#160;the domain of motion tracking systems for rehabilitation, systems are typically assembled in steps following a given flow. This flow tends to start with Data Acquisition, meaning the capture of a raw video of an exercise being performed by a given sensor. Then, <italic toggle="yes">Feature Engineering</italic> takes place, that is, meaningful features are extracted, and&#160;after that, these features are encoded, which is done by changing the way the features are represented. In this domain, the&#160;representation tends to be a time sequence of a group of joint positions or angles or other kinematic parameters. After that, there is a <italic toggle="yes">Comparison and Assessment</italic> step, where this sequence is compared to another, typically a reference or a desired sequence, in&#160;some way to derive an assessment of the quality of the performed exercise that will be the feedback the user will receive. This can be represented by the diagram in <xref rid="sensors-25-05428-f001" ref-type="fig">Figure 1</xref>.</p><p>We have identified and analyzed 13 scientific articles focusing on CV for rehabilitation, which are listed in <xref rid="sensors-25-05428-t001" ref-type="table">Table 1</xref>. In&#160;this table we also categorize the articles based on their choices in terms of <italic toggle="yes">Data Acquisition</italic> methods, <italic toggle="yes">Feature Engineering</italic> techniques, and <italic toggle="yes">Comparison and Assessment</italic> methods. These 13 works were selected based on a structured review of English-language publications since 2018, using the terms &#8216;computer vision,&#8217; &#8216;rehabilitation,&#8217; and &#8216;motion tracking&#8217; in article databases. The&#160;final selection focused on studies with explicit technical breakdowns of feature extraction and motion assessment. The articles that do not perform any of these steps or fail to mention it are under &#8220;Not specified&#8221;, and&#160;if those steps are performed but fall out of the scope of this review (i.e., the&#160;last step is not focused on rehabilitation), the&#160;cell will be marked &#8220;N/A&#8221;. &#160;&#160;&#160;</p><p>In the following sections, we detail each step present in <xref rid="sensors-25-05428-f001" ref-type="fig">Figure 1</xref> and analyze how it was addressed by each analyzed work. A&#160;commercial solution will also be&#160;presented.</p></sec><sec id="sec2dot3-sensors-25-05428"><title>2.3. Data&#160;Acquisition</title><p>From our analysis of existing literature on accessible technology, we observed that there are two main options for data acquisition: <italic toggle="yes">Microsoft Kinect</italic> and <italic toggle="yes">RGB Cameras</italic>.</p><p><italic toggle="yes">Microsoft Kinect</italic> was widely adopted for CV research in rehabilitation. Numerous reviewed articles in this field have relied on <italic toggle="yes">Microsoft Kinect</italic> as their primary data collection tool [<xref rid="B8-sensors-25-05428" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-05428" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-05428" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-05428" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-05428" ref-type="bibr">12</xref>]. After its introduction in 2010, <italic toggle="yes">Microsoft Kinect</italic> rapidly gained popularity, and&#160;one of the key factors contributing to its widespread adoption is its affordability, as&#160;it was a cost-effective solution for capturing RGB-D (colour and depth) images. Unlike traditional RGB cameras, besides&#160;colour and brightness information, <italic toggle="yes">Microsoft Kinect</italic> provides depth information for each pixel, which denotes the distance of that point from the sensor, according to [<xref rid="B4-sensors-25-05428" ref-type="bibr">4</xref>]. <italic toggle="yes">Microsoft Kinect</italic>&#8217;s accuracy was reviewed and found to be superior to RGB-only systems [<xref rid="B21-sensors-25-05428" ref-type="bibr">21</xref>]. The study conducted by Mousavi Hondori and Kademi deemed <italic toggle="yes">Microsoft Kinect</italic> acceptable for rehabilitation purposes, affirming its suitability as a reliable sensor in this domain. In addition to its low cost and effective RGB-D imaging capabilities, it is important to highlight that <italic toggle="yes">Microsoft Kinect</italic> also included an SDK. This SDK provided researchers with a comprehensive set of tools and libraries that facilitated the development of CV applications tailored explicitly for <italic toggle="yes">Microsoft Kinect</italic>. This made <italic toggle="yes">Microsoft Kinect</italic> much more valuable for&#160;researchers.</p><p>However, it is important to mention that <italic toggle="yes">Microsoft Kinect</italic> has drawbacks worth considering. These drawbacks include, but&#160;are not limited to, the&#160;following: first, Microsoft has discontinued production of the <italic toggle="yes">Microsoft Kinect</italic> [<xref rid="B22-sensors-25-05428" ref-type="bibr">22</xref>]. This means that researchers are forced to switch to another device or go in a different direction. Additionally, although <italic toggle="yes">Microsoft Kinect</italic> was a low-cost solution compared to some alternative motion capture systems, it, as&#160;any other depth-sensor-based system, still represents an additional equipment requirement. Its utilization requires a dedicated <italic toggle="yes">Microsoft Kinect</italic> sensor, which moves away from the original goal of obtaining an as-accessible-as-possible&#160;solution.</p><p>The main alternative to <italic toggle="yes">Microsoft Kinect</italic> in the studied articles is the <italic toggle="yes">RGB Camera</italic>, which was the approach adopted by most of the reviewed articles [<xref rid="B13-sensors-25-05428" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-05428" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-05428" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-05428" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-05428" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-05428" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-05428" ref-type="bibr">19</xref>]. In this set, we include standard, computer, and&#160;smartphone cameras, with&#160;the latter being the most accessible&#160;option.</p><p>Smartphones have become ubiquitous and readily available for a significant part of the population. Leveraging smartphones&#8217; built-in cameras offers a more accessible approach to video capture, as&#160;the need for any extra equipment would not limit&#160;patients.</p><p>While the main limitation of an <italic toggle="yes">RGB Camera</italic> is its lack of precision compared to a system with a depth sensor, smartphone cameras have nonetheless been deemed suitable for clinical use [<xref rid="B23-sensors-25-05428" ref-type="bibr">23</xref>]. In fact, Lam&#160;et&#160;al. [<xref rid="B24-sensors-25-05428" ref-type="bibr">24</xref>] have concluded that smartphones will play a crucial role in the future of the application of CV for rehabilitation, contributing to accessibility. In&#160;<xref rid="sensors-25-05428-t001" ref-type="table">Table 1</xref>, a&#160;summary of the different approaches for the Data Acquisition component is&#160;presented.</p></sec><sec id="sec2dot4-sensors-25-05428"><title>2.4. Feature&#160;Engineering</title><p>Feature engineering is the second step in the flow of a motion tracking system. It can encompass many smaller parts, including, but&#160;not limited to, feature extraction and feature encoding. Feature encoding is almost never detailed in the studied articles, as&#160;features are typically encoded as a time sequence. Thus, we will focus on feature extraction. Here, the&#160;CV task to be addressed is pose estimation. Typically, information deemed relevant is joint positions and angles. Obtaining this information involves tracking and extracting the spatial coordinates of key joints or measuring the angles between body segments during exercises. The extracted features are temporal sequences of the mentioned key points based on their evolution over time. Additionally, various measures can be incorporated, such as the range of an angle or the velocity of a joint, to&#160;provide more comprehensive information about movement&#160;dynamics.</p><p>Our study of the existing literature found that authors vary in their choice of tools for feature extraction. These choices include <italic toggle="yes">OpenPose</italic> [<xref rid="B25-sensors-25-05428" ref-type="bibr">25</xref>], <italic toggle="yes">BlazePose</italic> [<xref rid="B26-sensors-25-05428" ref-type="bibr">26</xref>], <italic toggle="yes">OpenNI</italic> [<xref rid="B27-sensors-25-05428" ref-type="bibr">27</xref>], <italic toggle="yes">FaceMesh</italic>, <italic toggle="yes">OpenFace</italic>, <italic toggle="yes">PoseNet</italic> [<xref rid="B28-sensors-25-05428" ref-type="bibr">28</xref>] and <italic toggle="yes">Convolutional Pose Machines</italic> [<xref rid="B29-sensors-25-05428" ref-type="bibr">29</xref>].</p><p>When <italic toggle="yes">Microsoft Kinect</italic> is used for data acquisition, the&#160;provided SDK offers tools that allow developers to access pre-computed skeletal joint positions, orientations, and&#160;other relevant parameters. This means there is no need for explicit feature extraction. Therefore, almost all the studied articles that resort to <italic toggle="yes">Microsoft Kinect</italic> do not perform explicit feature extraction, making use of <italic toggle="yes">Microsoft Kinect</italic>&#8217;s SDK. The one exception is the work of Chen&#160;et&#160;al. [<xref rid="B8-sensors-25-05428" ref-type="bibr">8</xref>], as&#160;they employ a custom process involving a series of image transformations using <italic toggle="yes">OpenNI</italic> [<xref rid="B27-sensors-25-05428" ref-type="bibr">27</xref>] to extract spatial information, transforming this visual data into seven key points that they deem required for their rehabilitation&#160;system.</p><p>When it comes to raw images, feature extraction has to be performed. In the case of rehabilitation, as&#160;mentioned before, the&#160;task to be addressed is pose estimation. <italic toggle="yes">OpenPose</italic> [<xref rid="B25-sensors-25-05428" ref-type="bibr">25</xref>] is the most popular choice, being used by Francisco and Rodrigues [<xref rid="B14-sensors-25-05428" ref-type="bibr">14</xref>] and Ferrer-Mallol&#160;et&#160;al. [<xref rid="B18-sensors-25-05428" ref-type="bibr">18</xref>], but&#160;it has the drawback of being computationally expensive, at&#160;least in comparison to other options, as&#160;will be seen in the next paragraph. Another pose estimation library, <italic toggle="yes">BlazePose</italic> [<xref rid="B26-sensors-25-05428" ref-type="bibr">26</xref>], was employed by Yang&#160;et&#160;al. [<xref rid="B16-sensors-25-05428" ref-type="bibr">16</xref>], offering similar results as <italic toggle="yes">OpenPose</italic> [<xref rid="B25-sensors-25-05428" ref-type="bibr">25</xref>] but with significantly improved computational efficiency, making it apt to use with a smartphone. <italic toggle="yes">BlazePose</italic> [<xref rid="B26-sensors-25-05428" ref-type="bibr">26</xref>] is favored for its rapid execution and is integrated into Google&#8217;s MediaPipe pose detector: OpenPose offers high accuracy but is computationally expensive, whereas BlazePose offers near-real-time performance suitable for mobile devices. More advanced feature encoding, such as dimensionality reduction or latent representations, remains underexplored in this context.</p><p>Abbas&#160;et&#160;al. [<xref rid="B17-sensors-25-05428" ref-type="bibr">17</xref>] used <italic toggle="yes">OpenFace</italic> [<xref rid="B30-sensors-25-05428" ref-type="bibr">30</xref>], a&#160;mobile-oriented facial behavior analysis toolkit. OpenFace focuses on head pose and facial expressions, making it valuable for rehabilitation assessments but limited to specific&#160;scenarios.</p><p>For real-time human pose estimation, Leechaikul and Charoenseang [<xref rid="B15-sensors-25-05428" ref-type="bibr">15</xref>] employed <italic toggle="yes">PoseNet</italic> [<xref rid="B28-sensors-25-05428" ref-type="bibr">28</xref>], a&#160;lightweight deep learning model. While it sacrifices some accuracy due to its lightweight nature, it serves the purpose efficiently. Li&#160;et&#160;al. [<xref rid="B19-sensors-25-05428" ref-type="bibr">19</xref>] utilized <italic toggle="yes">Convolutional Pose Machines</italic> (CPMs) [<xref rid="B29-sensors-25-05428" ref-type="bibr">29</xref>] in their work. CPMs leverage a Convolutional Neural Network&#160;(CNN) architecture with an iterative approach, generating heatmaps at each stage to estimate skeleton key&#160;points.</p><p>The fact that most reviewed articles used different tools showcases the diversity of the feature engineering domain, as&#160;can be seen in <xref rid="sensors-25-05428-t001" ref-type="table">Table 1</xref>.</p><p>The present study focuses on positional coordinates, but&#160;future work could incorporate kinematic features such as velocity, acceleration, and&#160;movement smoothness to capture additional clinical&#160;nuances.</p></sec><sec id="sec2dot5-sensors-25-05428"><title>2.5. Comparison and&#160;Assessment</title><p>Once the relevant features, such as joint positions or angles, have been extracted and represented, they need to be compared and evaluated to assess the quality and effectiveness of the performed exercises. Feature comparison involves the analysis of the extracted features to determine how closely they align with reference movement patterns. Various techniques can be employed for feature comparison, including distance-based, model-less metrics, statistical-model-based metrics, or&#160;even Deep Learning. These approaches aim to quantify the similarity between the observed movement patterns and the desired&#160;ones.</p><p>Reviewing the existing literature on this topic, we can conclude that this portion of the process is the less standardized one, with&#160;most articles proposing their own version of a solution by combining several methods. That being said, one proposed option is a direct distance-based technique, Dynamic Time-Warping (DTW). However, most methods are model-based, be it via DL or by Hidden Markov Models (HMMs). Fuzzy logic is also present in more than one&#160;article.</p><p>Chen&#160;et&#160;al. [<xref rid="B8-sensors-25-05428" ref-type="bibr">8</xref>] developed a vision-based rehabilitation system with a primary focus on action identification, discerning if a subject is performing a rehabilitation exercise and identifying the specific exercise. They employed DTW to measure the similarity between sequences of varying lengths, enabling comparison of different exercise paces. Their system achieved impressive results, boasting a 98.1% accuracy rate for action&#160;identification.</p><p>Su&#160;et&#160;al. [<xref rid="B9-sensors-25-05428" ref-type="bibr">9</xref>] employed DTW for comparison and implemented an Adaptive Neuro-Fuzzy Inference System (ANFIS) for evaluation. ANFIS combines Neural Networks&#160;(NNs) and fuzzy logic principles. Their evaluation module includes a trajectory evaluator based on a Sugeno-type ANFIS, a&#160;speed evaluator, and&#160;an overall performance evaluator based on a Mamdani fuzzy inference model. The system matched physicians&#8217; scores at an 80.1% rate, demonstrating its&#160;effectiveness.</p><p>Capecci&#160;et&#160;al. [<xref rid="B10-sensors-25-05428" ref-type="bibr">10</xref>] devised an innovative approach to evaluate rehabilitation exercises using a Hidden Semi-Markov Model (HSMM). Unlike traditional HMMs, HSMM models state durations as distributions. Collaborating with clinicians, they trained the HSMM with data from the seven top-rated subjects. The&#160;HSMM calculates a comprehensive score based on observation likelihood. Through hyperparameter tuning, they optimized the HSMM to match clinician ratings. This resulted in a significant correlation for two of the five exercises. Compared to DTW, the&#160;HSMM outperformed DTW in correlating with clinician scores for four of the five&#160;exercises.</p><p>Ciabattoni&#160;et&#160;al. [<xref rid="B13-sensors-25-05428" ref-type="bibr">13</xref>] collaborated with physiotherapists to establish target values, incorporating tolerances for five exercises. They designed a score function based on these targets and measured values for specific joint angles. The overall exercise score is computed as the average or maximum of individual scores. They used a Takagi-Sugeno Fuzzy Inference System to determine the subject&#8217;s global score, offering flexibility in weighting exercise targets according to physiotherapist&#160;recommendations.</p><p>Francisco and Rodrigues [<xref rid="B14-sensors-25-05428" ref-type="bibr">14</xref>] utilize a Modular NN for exercise assessment. They begin by extracting and storing joint angles, focusing on four angles. After that, their system is divided into their detection module, which identifies what exercise is being performed, and&#160;their measure module, which employs an NN to assess exercise correctness. The initial network architecture was straightforward and comprised three layers. Multiple configurations were tested to obtain the best architecture and evaluated using accuracy and the area under the Receiver Operating Characteristic (ROC) curve. Ultimately, the&#160;most effective architecture ended up featuring eight hidden layers and achieving 94.54% accuracy and an area under the ROC curve ranging from 0.8 to 1, depending on the&#160;exercise.</p><p>Liao&#160;et&#160;al. [<xref rid="B20-sensors-25-05428" ref-type="bibr">20</xref>] employ a Gaussian Mixture Model (GMM) log-likelihood performance metric for rehabilitation, leveraging its ability to effectively capture the inherent variability in human movements. This metric is incorporated into a Deep-Learning model comprised of a complex deep NN that includes convolutional layers, recurrent layers, and&#160;temporal pyramids. It outputs movement quality scores for input exercises. In their evaluation, they compared the GMM log-likelihood metric to Euclidean distance, Mahalanobis distance, and&#160;DTW distance, focusing on its capacity to distinguish correct from incorrect movements. The results showcased the superiority of the proposed metric, although&#160;DTW distance and Euclidean distance displayed noteworthy results. Additionally, they assessed their Deep-Learning model against other NN models, using absolute deviation as the performance metric. The model demonstrated overall superior performance, although&#160;specific models outperformed it in certain exercises within the&#160;dataset.</p><p><xref rid="sensors-25-05428-t001" ref-type="table">Table 1</xref> also showcases the diversity in Comparison and Assessment methods. In summary, model-based approaches such as HMMs and neural networks offer flexibility and robustness but require large datasets. DTW, while simpler, is more interpretable and easier to deploy. The&#160;lack of standardization in this step reflects the diversity of rehabilitation exercises, absence of benchmarking datasets, and&#160;the tension between clinical interpretability and computational&#160;complexity.</p></sec><sec id="sec2dot6-sensors-25-05428"><title>2.6. Commercial&#160;Systems</title><p>In addition to research efforts, there are also commercial solutions available in the field of CV for rehabilitation. One such solution is Exer Health [<xref rid="B31-sensors-25-05428" ref-type="bibr">31</xref>], a&#160;patient mobile app designed to enhance the rehabilitation process. Exer Health aims to keep patients engaged and gather critical health assessment data while they perform exercises at home. The app claims to measure range of motion, counts repetitions, recommends form adjustments, and&#160;provides real-time feedback to patients. This feedback helps patients adhere to their recovery protocols, and&#160;the data collected can be used by professionals to evaluate progress throughout the rehabilitation journey. Moreover, Exer Health offers professionals an intuitive mobile app that facilitates the creation of high-touch, closed-loop recovery protocols. This gives patients a richer experience while providers focus on delivering optimal&#160;care.</p><p>Regarding technology, Exer Health claims to employ a proprietary motion-AI platform that powers all of its digital health software. The platform is said to run &#8220;on the edge,&#8221; utilizing common laptops, phones, and&#160;tablets. This approach ensures accessibility and ease of use for patients and professionals, allowing seamless technology integration into rehabilitation. Although they do not give any details, they mention the use of NNs, Machine Learning, and&#160;CV.</p><p><xref rid="sensors-25-05428-f002" ref-type="fig">Figure 2</xref> shows an example of how the Exer Health application&#160;works.</p><p>The main problem with Exer&#8217;s solution is that they do not reveal much about it, with&#160;their explanations being very generic when it comes to the technology used and how they used it. Our work differs from theirs in that sense, as&#160;our objective is to make this research public and available for anyone to understand. On&#160;this topic, we would like to also highlight the mobile system presented by Pereira&#160;et&#160;al. [<xref rid="B32-sensors-25-05428" ref-type="bibr">32</xref>]. Although&#160;it is in an early development stage, the&#160;proposed integration of intelligent analysis for exercise compression is very&#160;promising.</p></sec><sec id="sec2dot7-sensors-25-05428"><title>2.7. Available Datasets for Modeling Computer Vision&#160;Systems</title><p>The availability of high-quality and diverse datasets is very important in training CV models for rehabilitation applications. Whereas in other motion modeling applications based on CV, there is a wide range of large public datasets; in the case of rehabilitation data, authors tend to collect their own data, obtaining relatively small datasets [<xref rid="B7-sensors-25-05428" ref-type="bibr">7</xref>]. In <xref rid="sensors-25-05428-t002" ref-type="table">Table 2</xref>, we compiled the most relevant datasets found during our&#160;research.</p><p>However, these datasets all mention that their data is captured by an RGB-D camera such as <italic toggle="yes">Microsoft Kinect</italic>. This is also true for other datasets found during the literature review, as&#160;well as others that mention the use of body-worn sensors. The dominance of RGB-D datasets limits the applicability of models to RGB-only systems, which are more suitable for smartphone deployment. At&#160;present, there is a gap in public datasets for smartphone-based rehabilitation, which restricts external validation and hampers generalization across&#160;platforms.</p></sec><sec id="sec2dot8-sensors-25-05428"><title>2.8. Takeaways and&#160;Conclusions</title><p>The previous sections have explored the intersection of CV and intelligent rehabilitation systems, examining the relevant literature and gaining insights into this field.</p><p>We observed that data acquisition methods have evolved over time. While <italic toggle="yes">Microsoft Kinect</italic> was once a prominent choice, its discontinuation has led to the emergence of smartphone cameras as accessible and reliable alternatives for rehabilitation&#160;purposes.</p><p>In terms of feature engineering techniques, OpenPose is the most popular choice. Although&#160;it is widely used for pose estimation, it is computationally expensive, so we highlighted an alternative, BlazePose, that offers greater efficiency and&#160;speed.</p><p>The comparison and assessment of exercises involve diverse techniques, including DTW, fuzzy inference systems, HSMMs, and&#160;deep NNs. These approaches aim to quantify the similarity between observed and desired movement patterns. Every studied article mentioned satisfactory&#160;results.</p><p>What we conclude by comparing the different studies is that DTW seems to be a standard method. It could be interesting to use as an initial method and a term for comparison of different methods, serving as a benchmark. We also found that the trend is the use of DL, as&#160;in most fields of computer science. Fuzzy logic seems to be regarded as an effective tool for this task, as&#160;well as HMMs. We also discussed Exer Healt [<xref rid="B31-sensors-25-05428" ref-type="bibr">31</xref>], a&#160;commercial solution with a promising premise. Unfortunately, they are not very transparent: they state that they employ a proprietary motion-AI platform, and&#160;we found mentions of NNs, Machine Learning, and&#160;CV, but&#160;no concrete details are given in terms of the specific techniques used, instead opting for very generic explanations. Another important aspect mentioned is the available datasets. As detailed before, there are not many large public datasets, which is something researchers have to deal with. Typically, the&#160;solution is for authors to collect their own data. Building on the insights from this review, our work takes the discontinuation of <italic toggle="yes">Microsoft Kinect</italic> into consideration and explores the potential benefits of smartphone cameras. While methods such as DTW are frequently used in research, they are not clinical standards. Deep Learning is increasingly applied in recent literature [<xref rid="B2-sensors-25-05428" ref-type="bibr">2</xref>], but&#160;model transparency and data scarcity remain challenges. This study addresses the gap in evaluating smartphone-only, markerless rehabilitation systems and provides an open, reproducible benchmark for future&#160;work.</p></sec></sec><sec id="sec3-sensors-25-05428"><title>3. Implementation</title><sec id="sec3dot1-sensors-25-05428"><title>3.1. Architecture</title><p>The proposed system follows the structure presented in <xref rid="sec2-sensors-25-05428" ref-type="sec">Section 2</xref>, meaning it involves three components: Data Acquisition, Feature Engineering, and&#160;Comparison and Assessment. Therefore, we had to make technological and architectural choices for each one of these components. The following subsections will detail the choices made for each of the components. A smartphone camera was chosen due to its ubiquity and integration into daily life, ensuring feasibility for at-home rehabilitation use. BlazePose, part of Google&#8217;s MediaPipe framework, offers real-time pose estimation on mobile devices and has proven effective in similar tasks. DTW, a&#160;distance-based alignment technique, was selected for its simplicity and clinical interpretability, despite the availability of more complex model-based&#160;metrics.</p><sec id="sec3dot1dot1-sensors-25-05428"><title>3.1.1. Data Acquisition: Smartphone&#160;Camera</title><p>In the Data Acquisition component, we had to choose between <italic toggle="yes">Microsoft Kinect</italic> and the RGB camera, as&#160;is explained in <xref rid="sec2dot3-sensors-25-05428" ref-type="sec">Section 2.3</xref>. Deriving from what is mentioned in the aforementioned section, we opted for the RGB camera, more specifically, the&#160;smartphone camera, due to the ubiquity of the smartphone, aligning with the focus on accessibility. Furthermore, it should allow for a more straightforward and user-friendly approach to data&#160;collection.</p></sec><sec id="sec3dot1dot2-sensors-25-05428"><title>3.1.2. Feature Engineering: MediaPipe&#8217;s Pose Landmarker (BlazePose)</title><p>Regarding Feature Engineering, the&#160;range of choices was much broader, as&#160;detailed in <xref rid="sec2dot4-sensors-25-05428" ref-type="sec">Section 2.4</xref>. The most popular choice, OpenPose, had the drawback of being computationally expensive, which could pose problems given the choice of the smartphone, a&#160;less computationally powerful device. One of the alternatives was BlazePose, which presented results similar to OpenPose but was much less computationally expensive, making it suitable for a smartphone. One framework that implements the BlazePose algorithm is the MediaPipe Pose Landmarker. MediaPipe [<xref rid="B36-sensors-25-05428" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-05428" ref-type="bibr">37</xref>] is an open-source framework created by Google that provides a toolkit for the development of Machine Learning applications, mainly focused on helping people design and implement solutions for vision, audio, and&#160;text-based Machine Learning tasks. In terms of audio and text, it provides developers with tools suitable for various classification, detection, and&#160;embedding tasks. When it comes to vision-based tasks, it offers tools to perform Object Detection, Image Classification, Hand Landmark, Hand Gesture Recognition, Image Segmentation, Interactive Segmentation, Face Detection, Face Landmark Detection, and&#160;the one we are interested in, Pose Landmark&#160;Detection.</p><p>The MediaPipe Pose Landmarker [<xref rid="B38-sensors-25-05428" ref-type="bibr">38</xref>] is a component within the broader MediaPipe framework designed for human pose estimation. This specific module is focused on identifying and tracking key points on a person&#8217;s body, enabling applications to access an estimate of the human pose. It receives as input still images, decoded video frames, or&#160;live video feed and outputs pose landmarks in normalized image coordinates or pose landmarks in world coordinates. MediaPipe includes a pose landmarker model that outputs 33 key landmarks. <xref rid="sensors-25-05428-f003" ref-type="fig">Figure 3</xref> shows the position of these 33 landmarks in the human body.</p><p>The output of the MediaPipe Pose Landmarker contains X, Y, and&#160;Z coordinates for each landmark and a visibility factor, representing how likely it is that the landmark is visible. As mentioned before, the&#160;MediaPipe Pose Landmarker can output pose landmarks in either normalized image coordinates or world coordinates. If the output is normalized image coordinates, X and Y are normalized between 0 and 1 in relation to the image&#8217;s width and height. The Z coordinate is the landmark depth, with&#160;the hips as the origin. It has the same magnitude as X, and&#160;the smaller the value is, the&#160;closer it is to the camera. If the output is world coordinates, X, Y, and&#160;Z are real-world three-dimensional coordinates in meters, also with the hips as the origin. An example of the output of a landmark is presented in <xref rid="sensors-25-05428-f004" ref-type="fig">Figure 4</xref>.</p></sec><sec id="sec3dot1dot3-sensors-25-05428"><title>3.1.3. Comparison and Assessment: Dynamic Time&#160;Warping</title><p>To perform the Comparison and Assessment part of the proposed system, we opted for DTW. DTW is an algorithm used to assess how similar two time series are by measuring the distance between those two series. Let us use as an example two time series <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic>. The simplest way to measure the distance between two time series would be to use Euclidean distance, which is computed as the square root of the sum of the squares of the differences between <italic toggle="yes">A</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">B</italic><sub><italic toggle="yes">i</italic></sub>, where <italic toggle="yes">A</italic><sub><italic toggle="yes">i</italic></sub> represents the value of time series <italic toggle="yes">A</italic> at index <italic toggle="yes">i</italic> and <italic toggle="yes">B</italic><sub><italic toggle="yes">i</italic></sub> represents the value of time series <italic toggle="yes">B</italic> at the same index. Equation&#160;(1) introduces the Euclidean distance between the time series <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic>.<disp-formula id="FD1-sensors-25-05428"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The problem with this measure is that it does not account for shifts along the time axis. For example, if&#160;two time series are equal but one of them is shifted in time, Euclidean distance would indicate a difference between both&#160;series.</p><p>For example, let us consider the following sequences: <italic toggle="yes">A</italic><sup>&#8242;</sup> = [0, 1, 2, 3, 0, 0] and <italic toggle="yes">B</italic><sup>&#8242;</sup> = [0, 0, 1, 2, 3, 0]. We can understand that the sequences represent the same evolution, but&#160;sequence <italic toggle="yes">B</italic><sup>&#8242;</sup> starts later than sequence <italic toggle="yes">A</italic><sup>&#8242;</sup>. If we calculate the Euclidean distance between these series, we would obtain a result of <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msqrt><mml:mn>12</mml:mn></mml:msqrt><mml:mo>&#8776;</mml:mo><mml:mn>3.46</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, when the desirable result would indicate a smaller&#160;difference.</p><p>With the aim of solving this problem, DTW was developed [<xref rid="B39-sensors-25-05428" ref-type="bibr">39</xref>]. The idea behind DTW is the computation of a path that warps points of the two series to one another in order to obtain a more intuitive relationship between the series. Warping two points means creating a correspondence between them. This path includes every index of both time series, meaning that every point in one sequence is warped to a point in the other sequence. The goal is to compute the optimal warp path, i.e.,&#160;the one with the minimum distance. To do so, we must first compute a two-dimensional distance matrix, <italic toggle="yes">D</italic>. Given the same time series <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic>, <italic toggle="yes">D</italic> is of size <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the lengths of <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic>, respectively. Therefore, each axis represents one of the time series. The process of computing the matrix is done dynamically, starting with <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and finishing with <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mo>,</mml:mo><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> The value of each cell in <italic toggle="yes">D</italic> is computed by the following formula:<disp-formula id="FD2-sensors-25-05428"><label>(2)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8804;</mml:mo><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8804;</mml:mo><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In every calculation, we add the distance between the two points in each axis at the current index, <italic toggle="yes">Dist</italic>(<italic toggle="yes">A</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">B</italic><sub><italic toggle="yes">j</italic></sub>), to&#160;the minimum value of the three cells that precede the current one. This is because we know that the optimal path must pass through one of those cells and that one of them contains the minimum possible distance, so we add the current distance to the smallest of those three values. <italic toggle="yes">Dist</italic>(<italic toggle="yes">A</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">B</italic><sub><italic toggle="yes">j</italic></sub>) can be calculated by various distance measures, such as Euclidean distance or Manhattan distance. A comparative study between different distance measures for DTW showed that Euclidean distance obtains the best results [<xref rid="B40-sensors-25-05428" ref-type="bibr">40</xref>]. After we fill the matrix, the&#160;value of the DTW distance between the two sequences is the value at <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mo>,</mml:mo><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo>|</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For example, let us consider the same sequences <italic toggle="yes">A</italic><sup>&#8242;</sup> and <italic toggle="yes">B</italic><sup>&#8242;</sup> defined above. <xref rid="sensors-25-05428-f005" ref-type="fig">Figure 5</xref> shows the filled distance matrix for the calculation of DTW between those sequences.</p><p>It can be observed that we obtain the optimal path from <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mo>,</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and are left with the value 0 at <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mo>,</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This means that the DTW distance between the two sequences is&#160;0.</p><p>The choice for DTW stems from the fact that it is simple, being a distance-based metric, but&#160;presents impressive results measuring the similarity between sequences of movements. And&#160;we also had scalability into account: the current prototype focuses on offline analysis but real-time execution is a key goal. MediaPipe inference operates at around 30 fps on modern smartphones, and&#160;future system iterations will leverage framewise feedback to support real-time movement&#160;correction.</p></sec></sec><sec id="sec3dot2-sensors-25-05428"><title>3.2. Dataset</title><p>Collaborating with Escola Superior de Sa&#250;de (ESS) of the Instituto Polit&#233;cnico do Porto was instrumental in accessing a pre-existing dataset of exercises; and, also, their healthcare professionals were involved in the selection of clinically relevant joints and validation of movement ranges based on their rehabilitation protocols. The data that is included was collected with the primary aim of examining rehabilitation exercises. Subsequently, it has been utilized in various works, including the study conducted by Lopes&#160;et&#160;al. [<xref rid="B3-sensors-25-05428" ref-type="bibr">3</xref>]. Fifteen patients, labeled with IDs 1 to 15, were asked to perform two distinct exercises, <italic toggle="yes">Diagonal</italic> and <italic toggle="yes">Rotation</italic>, each repeated three times. The&#160;limited cohort size was a deliberate choice reflecting the pilot nature of this study. Our previous work with ESS with similar experimental constraints adopted comparable sample sizes (Lopes&#160;et&#160;al. [<xref rid="B3-sensors-25-05428" ref-type="bibr">3</xref>]) and our goal was to assess technical viability and alignment with clinical scores before pursuing broader deployment or generalization. Those exercises were recorded with a smartphone camera, and&#160;those videos are what constitute our dataset, as&#160;well as two reference videos performed by healthcare professionals&#8212;one for each video. <xref rid="sensors-25-05428-f006" ref-type="fig">Figure 6</xref> shows frames of a video from the&#160;dataset.</p><p>That is a total of 90 examples, 45 for each exercise, plus the two reference videos. The dataset has been evaluated by an advanced state-of-the-art system, the&#160;Qualisys Motion Capture System (QTM) [<xref rid="B41-sensors-25-05428" ref-type="bibr">41</xref>], that can evaluate Range of Motion (ROM), being scored from 0 to 100. The&#160;specifics of the conversion from a QTM evaluation to such a score are further detailed in another publication by Lopes&#160;et&#160;al. [<xref rid="B3-sensors-25-05428" ref-type="bibr">3</xref>]; but, in&#160;sum, it involves identifying key points of movement and measuring the maximum and minimum angles at these points. However, the&#160;reference scores are not attributed to each example in the dataset but to each patient, making a total of 30 scores: one for each of the two exercises performed by the 15 patients. This score represents a movement quality index based on comparing ROM between instances of subjects performing Guided Exercises (GEs) and Non-Guided Exercises (NGEs) and is calculated by the following equation:<disp-formula id="FD3-sensors-25-05428"><label>(3)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>Q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>%</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The selected dataset contains outliers, which have been explicitly identified by the physiotherapists responsible for its collection. <xref rid="sec4dot2-sensors-25-05428" ref-type="sec">Section 4.2</xref> provides an in-depth exploration of the strategies employed to deal with these outliers. As a final note, it is important to notice that public datasets were considered; however, there was none that matched our criteria of RGB-only, smartphone-acquired, rehabilitation-specific data synchronized with QTM&#160;scores.</p></sec><sec id="sec3dot3-sensors-25-05428"><title>3.3. System&#160;Development</title><sec id="sec3dot3dot1-sensors-25-05428"><title>3.3.1. Comparing Two&#160;Videos</title><p>In order to obtain a system that compares two videos, we devised the following plan:<list list-type="order"><list-item><p>Start by focusing on a single exercise and identify the relevant joints for that exercise.</p></list-item><list-item><p>Process one input&#160;video:
<list list-type="simple"><list-item><label>(a)</label><p>Read the video.</p></list-item><list-item><label>(b)</label><p>Extract values of relevant joint positions for each frame.</p></list-item><list-item><label>(c)</label><p>Create and save a sequence of these values.</p></list-item></list></p></list-item><list-item><p>Process another input video in the same fashion.</p></list-item><list-item><p>Calculate the DTW distance between two sequences.</p></list-item></list></p><p>Thus, we started by selecting one of the two mentioned exercises, <italic toggle="yes">Diagonal</italic> and <italic toggle="yes">Rotation</italic>. There were no substantial differentiating factors between the two, so we opted for <italic toggle="yes">Diagonal</italic>. We then wanted to identify which joints were relevant for the exercise. In order to do that in a clinically sound manner, we made use of our collaboration with Escola Superior de Sa&#250;de. We asked them which joints were relevant for comparing this exercise. They mentioned that they divided joints into three major groups for their comparison: Head, Trunk, and&#160;Shoulder. They also mentioned which landmarks of the MediaPipe Pose Landmarker Model should be included in each joint group. This meant that the relevant joints were organized as follows:<list list-type="bullet"><list-item><p>Head: Landmarks 0 to 10.</p></list-item><list-item><p>Trunk: Landmarks 11, 12, 23, 24.</p></list-item><list-item><p>Shoulder: Landmarks 12, 14, 16.</p></list-item></list></p><p>The next step was to process an input video. This involved developing a Python script where videos could be processed, and&#160;relevant data could be retrieved, particularly the positions of the relevant landmarks for each frame. In order to process the video, we resort to the OpenCV library [<xref rid="B42-sensors-25-05428" ref-type="bibr">42</xref>], which is an open-source software library designed for Computer Vision and Machine Learning applications. It offers a comprehensive set of image and video processing tools, including modules for image manipulation, object detection, feature extraction, and&#160;more. It is widely utilized in Computer Vision applications. We then made use of the MediaPipe Pose Landmarker to perform pose estimation and obtain the X, Y, and&#160;Z coordinates of each landmark present in the relevant joint groups for each frame. At this point, we faced a choice between using MediaPipe Pose Landmarker&#8217;s <monospace>Landmarks</monospace> or <monospace>WorldLandmarks</monospace>. We opted for <monospace>WorldLandmarks</monospace> because they are relative to the subject&#8217;s body. <monospace>Landmarks</monospace> are sensitive to the position of the subject in the image, which can cause problems when comparing two videos. These coordinates were saved after each frame, and&#160;we created a sequence of coordinates for each relevant landmark. Each sequence was essentially a list that had as elements smaller lists, one for each frame, that had as elements the coordinates of that landmark at that frame. After processing a video, we were left with a set of lists, one for each landmark, representing the evolution of the position of that landmark over&#160;time.</p><p>A generic sequence for a landmark would be represented as:<disp-formula id="FD4-sensors-25-05428"><label>(4)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this sequence, each list represents the X, Y, and&#160;Z coordinates of the landmark for each frame. After we had processed two input videos and obtained the sequences that represented the motion in terms of the evolution of the position of the relevant joints, we were ready to compare them and assess how similar those sequences were and, consequently, how similar the exercises being performed were. To compare the two exercises, we opted to calculate the distance between each relevant landmark&#8217;s evolution in one video to the corresponding one in the other video and then sum up those distances according to the grouping suggested before, meaning the distances between landmarks in the head group would be summed up to obtain the overall distance measure for the head. In&#160;order to calculate the distance between two sequences, we used&#160;DTW.</p><p>There are many libraries that implement DTW in Python (version 3.10.11 used), as&#160;is the case of <monospace>dtaidistance</monospace>, <monospace>TSLearn</monospace>, or&#160;<monospace>FastDTW</monospace>. <monospace>FastDTW</monospace> is the most commonly used one [<xref rid="B43-sensors-25-05428" ref-type="bibr">43</xref>], but&#160;it is actually an approximation, designed to perform the DTW calculation faster than the original algorithm [<xref rid="B44-sensors-25-05428" ref-type="bibr">44</xref>], whereas both <monospace>dtaidistance</monospace> and <monospace>TSLearn</monospace> perform the full DTW&#160;algorithm.</p><p>However, it has been claimed that <monospace>FastDTW</monospace> might actually not be faster and, because&#160;it is an approximation, the&#160;trade-off between accuracy and speed might not be worth it [<xref rid="B43-sensors-25-05428" ref-type="bibr">43</xref>].</p><p>In order to make a choice, we compared the execution of the three algorithms as well as a generic ad hoc DTW implementation. To compare the execution time of each algorithm, we ran each one on the same two videos ten times, registering the average of their execution times.</p><p>As we can see in <xref rid="sensors-25-05428-f007" ref-type="fig">Figure 7</xref>, <italic toggle="yes">dtaidistance</italic> had the smallest execution time, so that was the final choice. This meant we could now compare two videos and obtain distance measures for the three landmark groups, concluding the initial part of our development plan. <xref rid="sensors-25-05428-f008" ref-type="fig">Figure 8</xref> shows the pipeline followed to develop the part of the system that compares two videos and obtains a distance between them. It takes two videos as input, processes them using OpenCV, captures the landmarks of both using the MediaPipe Pose Landmarker, obtaining the landmark time sequences, which are then compared using DTW, resulting in a measure of the distance between the two videos.</p></sec><sec id="sec3dot3dot2-sensors-25-05428"><title>3.3.2. Generalizing for the Whole&#160;Dataset</title><p>After we had built a system capable of comparing two videos and obtaining a measure of the difference between the videos, our intention was to generalize and obtain that measure for each exercise in the dataset. Thus, we wrote a Python script that would do this for us. First, it processed the reference video for one exercise and saved the landmark sequences. Then, it iterated through the dataset, reading each video of that exercise and obtaining the landmark sequences, comparing them, using DTW, to the reference. This gave us a distance score for the three landmark groups in each video, which were then saved in a data frame and, subsequently, in a CSV file. This process was repeated for the second exercise, leaving us with two files with the distances recorded between the time sequences of the reference exercise and each of the examples in the dataset. Each file was divided into distances for the Head, Trunk, and Shoulder landmark groups. These files were then used as input for the evaluation of the proposed system, comparing the results obtained to those obtained by the QTM mentioned in <xref rid="sec3dot2-sensors-25-05428" ref-type="sec">Section 3.2</xref>.</p><p><xref rid="sensors-25-05428-t003" ref-type="table">Table 3</xref> shows a portion of the CSV file that contains the distances between the <italic toggle="yes">Diagonal</italic> exercise examples in the dataset and the <italic toggle="yes">Diagonal</italic> reference exercise, divided into the Head, Trunk, and Shoulder landmark groups.</p><p><xref rid="sensors-25-05428-f009" ref-type="fig">Figure 9</xref> shows the pipeline followed to develop the part of the system that iterates through the dataset, comparing every example video to the reference and obtaining a distance for each one.</p><p>This section details how the CV-based motion tracking system was implemented, including the architecture of the proposed system and the choices that were made at each step of the system flow, the dataset that was used and the technologies that were selected. While this study involved manual preprocessing steps, future iterations aim to integrate full automation of video segmentation, joint extraction, and scoring for scalability.</p></sec></sec></sec><sec id="sec4-sensors-25-05428"><title>4. System Evaluation and&#160;Results</title><p>This section is focused on the evaluation of the proposed system, the presentation and discussion of the obtained results. The first section explains the concept of data normalization, why it is necessary, and how we opted to apply it in our data. After that comes a section on the details of the proposed system evaluation, where we discuss the metrics used for evaluation and how we handled outliers. Then, there is a section presenting the results and a following section discussing them. After that, there is a section on statistical analysis. Finally, there is a small summary of the section.</p><sec id="sec4dot1-sensors-25-05428"><title>4.1. Data&#160;Normalization</title><p>To evaluate the performance of the proposed system, we will compare the results obtained by the QTM, mentioned in <xref rid="sec3dot2-sensors-25-05428" ref-type="sec">Section 3.2</xref>. However, the results obtained are a distance measure, whereas the dataset is labeled with scores on a scale of 0 to 100. To ensure a meaningful comparison, we need to make sure that both sets of results are on the same scale, which means we need to transform our distance measures into a 0 to 100 score. One way to achieve this is through data normalization.</p><p>Data normalization is a process in which data is scaled to a specific range. It is usually performed as a preprocessing step before the comparison of two sets of data and allows the comparison of data with different ranges. There are many different ways of performing data normalization, including, for example, Z-score normalization or min-max scaling.</p><p>Lima et al. [<xref rid="B45-sensors-25-05428" ref-type="bibr">45</xref>] conducted a comprehensive study comparing various normalization options, more specifically for the normalization of time series. They mention that Z-score is by far the most popular option, with many authors not even considering the alternatives. It is particularly well-suited for normally distributed data. However, they conclude that it may not always be the best option and recommend at least exploring one more option before using just Z-score&#8212;more specifically, maximum absolute scaling.</p><p>Z-score normalization transforms data to have a mean of 0 and a standard deviation of 1, with each transformed value being calculated with the following equation, where <italic toggle="yes">&#956;</italic> is the mean of the data, and <italic toggle="yes">&#963;</italic> is the absolute deviation:<disp-formula id="FD5-sensors-25-05428"><label>(5)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#956;</mml:mi></mml:mrow><mml:mi>&#963;</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Min-max scaling scales the data to a specific range, often 0 to 1, but in our case, 0 to 100, based on specified minimum and maximum values. These values are usually the maximum and minimum values of the dataset, but we could select values that correspond to what would be considered, in the original scale, a score of 0 and a score of 100. For example, in our case, the minimum would be 0, as the ideal performance of an exercise corresponds to a distance of 0 when comparing the two time sequences, and the maximum would be the values obtained by comparing two very different videos. One crucial detail to consider is that, in our case, larger distances correspond to lower scores and vice versa, which means that, after scaling is done, we invert the scale by subtracting each score from 100. The transformed values are calculated via the following equation:<disp-formula id="FD6-sensors-25-05428"><label>(6)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Maximum absolute scaling works in a very similar way but just considers the maximum absolute value. Each value is obtained with the following equation.<disp-formula id="FD7-sensors-25-05428"><label>(7)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>x</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Because all our data is positive and we know the minimum value is 0, maximum absolute scaling and min-max scaling have the same behavior. From this point on, we will only refer to min-max scaling.</p><p>Before choosing the normalization technique, we must first analyze our data.</p><p><xref rid="sensors-25-05428-f010" ref-type="fig">Figure 10</xref> and <xref rid="sensors-25-05428-f011" ref-type="fig">Figure 11</xref> help us visualize the distribution of our data. They include histograms for the three joint groups for each exercise. Examining the presented graphs, it becomes evident that the distribution of the data does not have the characteristics of a normal distribution, as there are clear deviations from the expected bell-shaped curve. This tells us that the data does not conform to a normal distribution and that Z-score normalization would not necessarily be suitable for our data. However, due to its immense popularity, and given most authors will not even consider not using Z-score, we decided to compare the effectiveness of these normalization techniques, applying both Z-score and min-max scaling to our data. Finally, we calculate the average scores for three exercises per ID, providing an overall performance score for each exercise. This aggregated score is what we compare to the QTM score. An example of the aggregated scores can be seen in <xref rid="sensors-25-05428-t004" ref-type="table">Table 4</xref>.</p></sec><sec id="sec4dot2-sensors-25-05428"><title>4.2. System&#160;Evaluation</title><p>The evaluation process is crucial for assessing the performance and practical utility of the system. A thorough evaluation can not only validate a system but also identify areas for improvement. As mentioned before, to evaluate the proposed system, we will compare the results obtained to those obtained by the QTM. However, it is important to clarify that, to the best of our knowledge, no other approaches utilize the same data structure as ours, which involves manually annotated QTM evaluations by physicians combined with a mediapipe/DTW intelligent analysis.</p><sec id="sec4dot2dot1-sensors-25-05428"><title>4.2.1. Why Range-of-Motion and Trajectory Similarity&#160;Works</title><p>Although ROM similarity and landmark-trajectory similarity via DTW arise from different mathematical definitions, both metrics ultimately quantify a subject&#8217;s deviation from an ideal movement pattern. ROM similarity condenses the motion of a joint into a single amplitude measure&#8212;that is, the difference between observed and reference extremal angles&#8212;while DTW on joint trajectories evaluates the temporal alignment and shape of the entire movement cycle. In both cases, higher similarity implies closer adherence to the clinically guided exercise. By framing both ROM and DTW as proxies for &#8220;movement fidelity,&#8221; we can directly contrast our smartphone-based, markerless CV approach against QTM data, which clinicians already trust.</p><p>Selecting these two metrics was driven by our interdisciplinary team&#8217;s combined expertise and by the goal of demonstrating feasibility under real-world constraints. Our health-science collaborators are most comfortable interpreting ROM deviations, since that is standard in clinical motion-capture reports, whereas our AI team had experience implementing DTW for time-series analysis of pose-estimation outputs. By pairing established ROM benchmarks with trajectory-based DTW, we leveraged complementary strengths: ROM provided a simple, clinically interpretable score, and DTW captured the full spatiotemporal profile of each movement. This dual-metric strategy let us validate our markerless CV system both in terms that clinicians recognize and in the richer, sequence-level information that modern AI methods afford.</p><p>We acknowledge that a fully rigorous validation would compare identical variables&#8212;either both ROM measures or both trajectory measures&#8212;across systems. That deeper head-to-head comparison lies beyond this study&#8217;s scope. Instead, our proof-of-concept shows that low-cost, smartphone-based pose estimation can reproduce both the amplitude-based insights clinicians use (ROM) and the dynamic, cycle-by-cycle fidelity captured by DTW. Future work will extend this validation by collecting simultaneous IMU, video-only, and QTM data, applying a unified set of metrics to each.</p></sec><sec id="sec4dot2dot2-sensors-25-05428"><title>4.2.2. Evaluation&#160;Metrics</title><p>In order to evaluate such a system, we must first consider the task we are performing and what evaluation metrics better suit it. In this case, we are predicting a score for the quality of a given rehabilitation exercise based on measuring distance. In more practical terms, we are predicting a continuous value for each entry in our dataset, which, despite not being a regression problem, can be evaluated with typical regression metrics. With that in mind, we selected Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and the Pearson Correlation Coefficient (CC) to evaluate the proposed system. Although data were non-normally distributed, Pearson correlation was chosen for comparability with previous works. Future studies will incorporate non-parametric alternatives such as Spearman to capture non-linear relationships.</p><p>MAE measures the average absolute difference between the predicted scores and the actual scores. It provides an interpretable metric for understanding the overall accuracy of the proposed system&#8217;s predictions. Equation (8) shows how to calculate MAE, with <italic toggle="yes">N</italic> being the number of predictions, <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub> being the actual values, and <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> being the predicted values.<disp-formula id="FD8-sensors-25-05428"><label>(8)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>RMSE calculates the square root of the average squared difference between the predicted scores and the actual scores. It penalizes larger errors more significantly than MAE and provides insight into the magnitude of prediction errors. Equation (9) is the RMSE equation, with <italic toggle="yes">N</italic>, <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>, and <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> having the same meaning as in the previous section.<disp-formula id="FD9-sensors-25-05428"><label>(9)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>CC assesses the linear relationship between the predicted scores and the actual scores. It quantifies the strength and direction of the linear association, providing a measure of how well the variation in one variable explains the variation in another. In our case, it tells us how well the variation in the predicted scores explains the variation in the actual scores. Equation (10) presents the formula to compute CC, where <italic toggle="yes">N</italic>, <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> keep their meaning and <italic toggle="yes">m</italic> and <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are the mean of <italic toggle="yes">y</italic> and <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, respectively.<disp-formula id="FD10-sensors-25-05428"><label>(10)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec4dot2dot3-sensors-25-05428"><title>4.2.3. Outliers</title><p>Before we show the results obtained with the chosen metrics, mentioning outliers in our dataset is very important. As mentioned in <xref rid="sec3dot2-sensors-25-05428" ref-type="sec">Section 3.2</xref>, the dataset used in our work contains outliers that have been labeled as such by the researchers who collected the data. This means we have to handle them correctly, as they can have strong implications in our evaluation process.</p><p>One way to handle outliers is to remove them from the data directly [<xref rid="B46-sensors-25-05428" ref-type="bibr">46</xref>], making it so that they would not affect our evaluation. However, our data is multidimensional, as we have a dimension for each joint group, Head, Trunk, and Shoulder, and the outliers appear only in one dimension. In the case of the <italic toggle="yes">Diagonal</italic> Exercise, the Head measurement of patient ID15 was considered an outlier. For the <italic toggle="yes">Rotation</italic> Exercise, the Head measurements of patients ID14 and ID15 were considered outliers. If we opt to remove the whole exercise from the dataset, we remove the outlier successfully, but we also remove valid data, more specifically, the measurements of the other joint groups of the mentioned patients.</p><p>Given this scenario, we could impute the value of the outliers. Imputation is a technique used to fill in missing data or handle outliers in a dataset by, for example, replacing them with the mean of the other values [<xref rid="B47-sensors-25-05428" ref-type="bibr">47</xref>]. This would allow us to evaluate the complete dataset. Another alternative would be to remove the values just from the mentioned joint groups and evaluate the other joint groups for those patients. We ended up evaluating the joint groups separately, so it made sense to remove just those values instead of imputing them.</p></sec></sec><sec sec-type="results" id="sec4dot3-sensors-25-05428"><title>4.3. Results</title><p>After analyzing our data, normalizing it, and selecting the best metrics, we are ready to evaluate the proposed system. This section presents the results of the evaluation, in which the system&#8217;s performance is assessed through a comparison with the QTM. This comparison allows us to evaluate the proposed system&#8217;s efficacy against a very powerful tracking methodology. The results will be presented in two separate sections, one for each exercise. Each section has three subsections, one for each joint group. Each subsection includes scores normalized using Z-score normalization and the scores normalized using min-max scaling, as mentioned in <xref rid="sec4dot1-sensors-25-05428" ref-type="sec">Section 4.1</xref>.</p><sec id="sec4dot3dot1-sensors-25-05428"><title>4.3.1. <italic toggle="yes">Diagonal</italic> Exercise</title><p>This section contains the results of the <italic toggle="yes">Diagonal</italic> exercise, divided into the three joint groups. For each joint group, we present the results obtained from the selected evaluation metrics: MAE, RMSE, and CC.</p><p>
<list list-type="simple"><list-item><p><bold>Head</bold>: The values obtained by the metrics for both the Head joint group of the <italic toggle="yes">Diagonal</italic> exercise are shown in <xref rid="sensors-25-05428-t005" ref-type="table">Table 5</xref>, and <xref rid="sensors-25-05428-f012" ref-type="fig">Figure 12</xref> shows the error distribution for the min-max predictions and the Z-score predictions.</p></list-item></list>
</p><p>Using min-max scaling, the Head joint group for the <italic toggle="yes">Diagonal</italic> exercise obtained an MAE of 13.29, an RMSE of 15.52, and a CC of 0.36. The value of the MAE tells us how big the errors were in our prediction, on average, in the scale of the results obtained, that is, 0 to 100. This means that the Head joint group&#8217;s predicted score was, on average, 13.29 points away from the score obtained by the QTM. The RMSE value calculates the difference between the squares of the errors, penalizing larger errors. In our case, the values are not too different, meaning that most errors are small and that there was little influence by large errors. The CC of 0.36 indicates a positive but weak correlation, as 0.36 is positive but small compared to the ideal value of 1. The interpretation of this value can be that the variation in our predictions is in the same direction as the actual values, meaning when our predictions increase, the actual values tend to increase as well, and vice-versa, but that relationship is not strong, meaning not much of the variation in the actual values can be explained by the variation in our predictions. When it comes to the Z-score results&#8217; evaluation, if we compare the MAE and the RMSE to those of the min-max results, we can immediately detect that Z-score caused more errors, as the MAE is more than double, and larger errors, with an even larger difference of RMSE. This is expected, given what was explained in <xref rid="sec4dot1-sensors-25-05428" ref-type="sec">Section 4.1</xref>: Z-score normalization is not necessarily suitable for our data. The difference in errors can be visually understood with <xref rid="sensors-25-05428-f012" ref-type="fig">Figure 12</xref>. The CC is the same.</p><p>
<list list-type="simple"><list-item><p><bold>Trunk</bold>: The values obtained by the metrics for the Trunk joint group of the <italic toggle="yes">Diagonal</italic> exercise are shown in <xref rid="sensors-25-05428-t006" ref-type="table">Table 6</xref>. In <xref rid="sensors-25-05428-f013" ref-type="fig">Figure 13</xref>, we can see the error distribution for both sets of predictions. With min-max scaling, the Trunk joint group obtained an MAE of 17.10, an RMSE of 19.12, and a CC of 0.47. The MAE of 17.10 reveals that, on average, the predicted scores for the Trunk joint group deviated by 17.10 points from the QTM scores. As for the Head, while there are some larger errors, the majority are smaller, as indicated by similar values of MAE and RMSE. The CC of 0.47 indicates a positive correlation, and it is relatively stronger than observed in the Head joint group. The variation in our predictions aligns with the variation in actual values, suggesting a reasonably strong directional relationship. In contrast, the Z-score results for the Trunk joint group show a higher MAE of 31.11 and a larger RMSE of 36.67 compared to min-max scaling. This notable increase in errors in magnitude and quantity reveals the difference between min-max and Z-score and can be better understood when we look at <xref rid="sensors-25-05428-f013" ref-type="fig">Figure 13</xref>. Again, the CC value is the same.</p></list-item></list>
</p><p>
<list list-type="simple"><list-item><p><bold>Shoulder</bold>: The evaluation metrics for the Shoulder joint group during the <italic toggle="yes">Diagonal</italic> exercise are illustrated in <xref rid="sensors-25-05428-t007" ref-type="table">Table 7</xref>, and the error distribution for both sets of predictions is represented in <xref rid="sensors-25-05428-f013" ref-type="fig">Figure 13</xref>. Under min-max scaling, the Shoulder joint group obtained an MAE of 14.44, an RMSE of 15.59, and a CC of &#8722;0.37. The MAE is similar to the one obtained by the Head and represents an average deviation of 14.44 points from the QTM scores. The values of MAE and RMSE suggest a reasonable level of accuracy, with few large errors, given how close the two values are. The CC of &#8722;0.37 indicates a negative correlation, which means that when the values in our predictions increase, the actual values decrease, which is not ideal. However, this correlation is weak, which means not much of the variation in the actual values can be explained by the variation in our predictions. Following the trend of the other two joint groups, the Z-score results for the Shoulder joint group show a substantially higher MAE of 57.59 and a larger RMSE of 62.97. In this case, the difference is even greater, meaning a considerable increase in errors, as can be seen in <xref rid="sensors-25-05428-f014" ref-type="fig">Figure 14</xref>.</p></list-item></list>
</p></sec><sec id="sec4dot3dot2-sensors-25-05428"><title>4.3.2. <italic toggle="yes">Rotation</italic> Exercise</title><p>This section contains the results of the Rotation exercise, divided into the three joint groups. For each joint group, we present the results obtained from the selected evaluation metrics: MAE, RMSE, and CC.</p><p>
<list list-type="simple"><list-item><p><bold>Head</bold>: The values obtained by the metrics for the Head joint group of the <italic toggle="yes">Rotation</italic> exercise are shown in <xref rid="sensors-25-05428-t008" ref-type="table">Table 8</xref>. <xref rid="sensors-25-05428-f015" ref-type="fig">Figure 15</xref> shows the error distribution for the Head joint group during the <italic toggle="yes">Rotation</italic> exercise. The MAE of 20.41 implies that, on average, the predicted scores for the Head joint group deviated by 20.41 points from the QTM scores. The RMSE, measuring the square root of the average squared errors, is slightly larger, indicating the presence of some large errors. The CC of 0.22 indicates a positive but weak correlation. This suggests a directional relationship between our predictions and the actual values, but the correlation strength is low. Utilizing Z-score normalization, the Head joint group achieved an MAE of 18.86 and an RMSE of 27.11. While the MAE shows a slight improvement in prediction accuracy compared to min-max scaling, the RMSE reflects an increase in the magnitude of errors. In <xref rid="sensors-25-05428-f015" ref-type="fig">Figure 15</xref>, we can see that Z-score shows, in general, smaller errors than min-max, explaining the lower MAE, but, as we can see, it also shows one very large error, penalized by RMSE. As mentioned in the previous sections, the CC is the same for Z-score and min-max results.</p></list-item></list>
</p><p>
<list list-type="simple"><list-item><p><bold>Trunk</bold>:&#160;<xref rid="sensors-25-05428-t009" ref-type="table">Table 9</xref> displays the evaluation metrics for the Trunk joint group during the <italic toggle="yes">Rotation</italic> exercise, and in <xref rid="sensors-25-05428-f016" ref-type="fig">Figure 16</xref>, we can see the error distribution for the Trunk joint group during the <italic toggle="yes">Rotation</italic> exercise. With min-max scaling, the Trunk joint group achieved an MAE of 32.30, an RMSE of 33.69, and a CC of 0.51. The MAE of 32.30 is the largest of any joint group with min-max, indicating larger errors on average. A similar value for RMSE reveals that the largest error must not be much larger than the other errors. However, because the MAE is already large, the RMSE could still indicate the presence of large errors. The CC of 0.51 reveals a positive correlation with a moderate strength. This suggests a reasonably strong directional relationship between our predictions and the actual values for the Trunk joint group. Switching to Z-score normalization, the Trunk joint group achieved an MAE of 27.30 and an RMSE of 32.25. Notably, Z-score normalization resulted in a decrease in the MAE, indicating an improvement in prediction accuracy, while the value of the RMSE being close to the one obtained with the min-max results implies a comparable distribution of errors. <xref rid="sensors-25-05428-f016" ref-type="fig">Figure 16</xref> shows that Z-score presents many more errors close to 0, but there are still some very large errors, while min-max&#8217;s errors are generally larger. Once more, the CC is the same.</p></list-item></list>
</p><p>
<list list-type="simple"><list-item><p><bold>Shoulder</bold>: The evaluation metrics for the Shoulder joint group during the &#8217;Rotation&#8217; exercise are illustrated in <xref rid="sensors-25-05428-t010" ref-type="table">Table 10</xref>. <xref rid="sensors-25-05428-f017" ref-type="fig">Figure 17</xref> displays the error distribution for the Trunk joint group during the <italic toggle="yes">Rotation</italic> exercise. When employing min-max scaling, the Shoulder joint group obtained an MAE of 21.45, an RMSE of 23.19, and a CC of &#8722;0.37. Once again, the value of the MAE indicates that, on average, the predicted scores for the Shoulder joint group deviated by 21.45 points from the QTM scores. The RMSE suggests reasonably large errors, as it is larger than MAE. As for the Shoulder group of the <italic toggle="yes">Diagonal</italic> exercise, the CC of &#8722;0.37 indicates a negative correlation, revealing an inverse relationship between our predictions and the actual values for the Shoulder joint group. However, this correlation is weak, with a relatively low magnitude. Upon employing Z-score normalization, the Shoulder joint group achieved an MAE of 30.30 and an RMSE of 36.83. Z-score normalization resulted in an increase in both the MAE, indicating a decrease in prediction accuracy, and the RMSE, revealing the presence of larger errors. <xref rid="sensors-25-05428-f017" ref-type="fig">Figure 17</xref> clarifies the difference between MAE and RMSE values. Min-max shows not only smaller values on average but also larger values that are smaller than the ones obtained by Z-score. Once again, the value of the CC is not affected by normalization.</p></list-item></list>
</p></sec></sec><sec sec-type="results" id="sec4dot4-sensors-25-05428"><title>4.4. Results&#160;Discussion</title><p>The presented results shed light on the performance of the proposed system in predicting the quality scores of rehabilitation exercises compared to the QTM. In this section, we discuss key findings and potential implications of the obtained results. Before we take a longer look at the results, however, it is relevant to state that the choice of normalization method significantly impacts the results obtained. Min-max scaling consistently outperforms Z-score normalization, showcasing its suitability for our dataset, as we had clear minimum and maximum values. While Z-score normalization is widely adopted, our findings emphasize the importance of exploring alternative normalization techniques, as suggested by Lima and Souza [<xref rid="B45-sensors-25-05428" ref-type="bibr">45</xref>]. It is also important to mention outlier handling and how crucial it is. Our decision was to remove values from specific joint groups rather than entire exercises, as this approach preserves valuable data while addressing the influence of outliers on specific joint measurements.</p><sec id="sec4dot4dot1-sensors-25-05428"><title>4.4.1. Overall Performance&#160;Trends</title><p>Across both the <italic toggle="yes">Diagonal</italic> and <italic toggle="yes">Rotation</italic> exercises, the system demonstrates varying levels of accuracy and correlation for different joint groups. That is noticeable when we look at the ranges of values obtained for each calculated error metric. When it comes to the min-max scores, MAE ranged from 13.29 to 32.30, and RMSE ranged from 15.52 to 33.69. In both metrics, a difference of more than double is present. Z-score MAE ranged from 18.86 to 57.7, and RMSE ranged from 27.11 to 62.97. This tells us that performances were not consistent over different joint groups and exercises. In terms of correlation, we obtained reasonable results overall, with most of the correlations being positive, including the Trunk group, which obtained CCs around the 0.5 mark. However, the Shoulder group evaluation resulted in negative coefficients, which indicates an inverse relationship between the predicted and the actual values. This means that, overall, there was a mixture of results, which means we must compare results between exercises and joint groups and discuss whether there are any indications of one exercise outperforming the other or one joint group outperforming the others.</p></sec><sec id="sec4dot4dot2-sensors-25-05428"><title>4.4.2. Exercise-Specific&#160;Observations</title><p>This subsection includes observations for each exercise and joint group, allowing for comparison between them and a better understanding of the results obtained.</p><p>
<list list-type="simple"><list-item><p><bold><italic toggle="yes">Diagonal</italic> Exercise</bold> Considering the min-max results for the <italic toggle="yes">Diagonal</italic> exercise, the Head group presents modest errors and exhibits a positive correlation, which, despite being weak, suggests a consistent directional relationship. The Trunk group&#8217;s errors are slightly larger and include a positive correlation, highlighting reasonable prediction accuracy. The Shoulder groups&#8217; evaluation metrics&#8217; results are between those of the Head and the Trunk groups. However, a negative correlation suggests that predictions may not be related to actual values. Overall, the min-max scores for this exercise obtained reasonable results, with an average MAE of 14.94 and an average RMSE of 16.74. Although the Head had fewer errors, the values are too close to state a clear difference between any of the joint groups. With the Z-score scores, the errors were larger. While the Head and Trunk groups obtained MAE and RMSE values close to 30, which would already be worse than the min-max scores, the Shoulder group obtained an MAE and an RMSE close to 60, revealing that the average prediction was off by almost 60 points.</p></list-item><list-item><p><bold><italic toggle="yes">Rotation</italic> Exercise</bold> Regarding the min-max results for the <italic toggle="yes">Rotation</italic> exercise, the Head and Shoulder groups obtained similar error metrics, and the Trunk group obtained a higher degree of error. In terms of correlation, the trend from the <italic toggle="yes">Diagonal</italic> exercise continues, with the Head obtaining a weak positive correlation, the Trunk a reasonably high positive correlation, and the Shoulder a weak negative correlation. Compared with the <italic toggle="yes">Diagonal</italic> exercise, the min-max scores for this exercise obtained worse results, with an average MAE of 24.72 and an average RMSE of 27.16, practically 10 points higher than the respective metrics for the <italic toggle="yes">Diagonal</italic> exercise. Interestingly, the Z-score scores obtained very similar, if not better, performances to those of the min-max scores in both the Head and the Trunk groups, obtaining slightly lower MAEs and similar RMSEs.</p></list-item></list>
</p></sec><sec sec-type="discussion" id="sec4dot4dot3-sensors-25-05428"><title>4.4.3. Discussion&#160;Summary</title><p>In summary, the evaluation of the rehabilitation exercise prediction system offers key insights into its performance across the <italic toggle="yes">Diagonal</italic> and <italic toggle="yes">Rotation</italic> exercises, focusing on joint groups &#8211; Head, Trunk, and Shoulder. The results highlight the impact of data normalization, emphasizing the superior performance of min-max scaling over Z-score normalization. The Shoulder joint group showed weaker correlation, likely due to occlusion, depth estimation errors, or limited reliability in upper-body tracking during seated exercises. And while the current implementation uses uniform joint weighting in DTW, future extensions may incorporate adaptive or learned weighting schemes to prioritize clinically relevant joints and improve sensitivity to movement nuances. The chosen evaluation metrics&#8212;MAE, RMSE, and CC&#8212;help us understand the accuracy of the performance of the proposed system and how related our predictions may be with the actual scores, unveiling performance trends across joint groups and exercises, such as the overall better performance of the proposed system during the <italic toggle="yes">Diagonal</italic> exercise and the higher correlation results yielded from the Trunk group. Although there is currently no established clinical threshold for these metrics in rehabilitation, preliminary discussions with clinicians from Escola de Sa&#250;de suggest this level of deviation may be tolerable for at-home progress monitoring. Our vision-based and the Qualisys scores differ by at most &#8764;0.1 units (with <italic toggle="yes">p</italic> &#8804; 0.001). These very small differences indicate high agreement with the (at the moment) gold-standard system. However, and while this system shows promising alignment with a gold-standard QTM assessment, it is not yet clinically validated for deployment. As this was a pilot study with only two exercises and 15 participants, the results may not generalize across populations, body types, or rehabilitation protocols. Future validation will require larger, more diverse cohorts and task variety to mitigate the risk of overfitting.</p></sec></sec><sec id="sec4dot5-sensors-25-05428"><title>4.5. Statistical&#160;Analysis</title><p>After computing traditional evaluation metrics, we wanted to better understand the results obtained through hypothesis testing. Given the nature of our data, characterized by a relatively small population and the inability to assume a specific distribution, we opted for non-parametric tests. More specifically, we opted for the Wilcoxon Signed-Rank Test [<xref rid="B48-sensors-25-05428" ref-type="bibr">48</xref>], which is particularly suitable for paired samples, in our case the predicted and actual scores, providing a robust analysis of whether there is a significant difference between them.</p><sec><title>Wilcoxon Signed-Rank&#160;Test</title><p>The Wilcoxon Signed-Rank Test [<xref rid="B48-sensors-25-05428" ref-type="bibr">48</xref>] is a non-parametric statistical test used to assess whether there is a significant difference between paired samples. The basic idea is to rank the absolute differences between the pairs of observations, which in our case are the errors, ignore the signs, and sum the ranks of the positive or negative differences. The null hypothesis assumes that the distribution of these differences is symmetric around zero. The test statistic is then calculated based on these ranks [<xref rid="B49-sensors-25-05428" ref-type="bibr">49</xref>]. The steps for performing this test are as follows:<list list-type="order"><list-item><p>For each pair of observations, calculate the absolute difference between the predicted and the actual score.</p></list-item><list-item><p>Rank these absolute differences from smallest to largest, ignoring the signs.</p></list-item><list-item><p>Taking the signs back into account, sum all the positive ranks (<italic toggle="yes">R</italic><sub>&#8722;</sub>) and all the negative ranks (<italic toggle="yes">R</italic><sub>+</sub>).</p></list-item><list-item><p>Calculate the smallest value between <italic toggle="yes">R</italic><sub>&#8722;</sub> and <italic toggle="yes">R</italic><sub>+</sub>: <italic toggle="yes">R</italic> = min(<italic toggle="yes">R</italic><sub>&#8722;</sub>, <italic toggle="yes">R</italic><sub>+</sub>).</p></list-item></list></p><p>The final value, <italic toggle="yes">R</italic>, is the test statistic and can be compared with tabled critical values to obtain the <italic toggle="yes">p</italic>-value. That <italic toggle="yes">p</italic>-value is used to assess the significance of the difference found between the two sets of scores. If it is under a given significance value, commonly 0.05, we can reject the null hypothesis and state that there is a significant statistical difference between the two sets of scores. If it is over that value, we cannot confirm that that statistical difference is significant. Thus, we performed the Wilcoxon Signed-Rank Test for the differences between the min-max scores and the actual scores and the differences between the Z-score scores and the actual scores, divided into the three joint groups. We performed the test once for each exercise. <xref rid="sensors-25-05428-t011" ref-type="table">Table 11</xref> presents the results for the Wilcoxon Signed-Rank Test for the <italic toggle="yes">Rotation</italic> exercise.</p><p>One problem with this test was noticed immediately, as the P-values of the Trunk joint group of the <italic toggle="yes">Rotation</italic> exercise for both the min-max and the Z-score scores are 0, as well as the P-value of the Shoulder group of the <italic toggle="yes">Diagonal</italic> exercise for the min-max scores. This is due to the fact that, for those particular scores, all of the predictions were lower than the actual score. This can be confirmed by interpreting <xref rid="sensors-25-05428-f018" ref-type="fig">Figure 18</xref> and <xref rid="sensors-25-05428-f019" ref-type="fig">Figure 19</xref>, which display a scatter plot of the min-max and Z-score predictions against the actual scores.</p><p>As we can see, all the predictions in <xref rid="sensors-25-05428-f018" ref-type="fig">Figure 18</xref> are under the red dashed line that indicates a correct prediction, meaning that all the differences are negative. The same thing happens for the min-max predictions in <xref rid="sensors-25-05428-f019" ref-type="fig">Figure 19</xref>.</p><p>Going back to how the Wilcoxon Signed-Rank Test works, it is easy to understand that if the differences are all positive or all negative, one of <italic toggle="yes">R</italic><sub>&#8722;</sub> or <italic toggle="yes">R</italic><sub>+</sub> will be 0, and thus <italic toggle="yes">R</italic> will always be 0. In these cases, the Wilcoxon Signed-Rank Test does not provide meaningful information. However, the other values are of interest, and we can interpret them following what was mentioned earlier. For the <italic toggle="yes">Diagonal</italic> exercise, the min-max Head value is more than 0.05, indicating that, for that pair of sets of scores, we cannot reject the null hypothesis. That is, we cannot confirm that there is a significant statistical difference between the sets of scores that generated those <italic toggle="yes">p</italic>-values. The rest of the sets of scores had <italic toggle="yes">p</italic>-values less than 0.05, confirming that the difference between the paired sets is not by chance and holds statistical significance.</p><p>For the Shoulder min-max value, which is less than 0.05, we reject the null hypothesis and confirm the statistical significance of the differences between the sets of scores.</p><p>An important note: no correction for multiple comparisons was applied, so <italic toggle="yes">p</italic>-values should be interpreted cautiously. Future work will adopt stricter controls such as Bonferroni correction to reduce Type I error risk.</p></sec></sec><sec id="sec4dot6-sensors-25-05428"><title>4.6. Summary</title><p>This section focused on evaluating our CV-based motion tracking system for rehabilitation. We began by discussing the importance of data normalization, specifically for comparing results obtained from a distance-based system with a ground truth dataset labeled on a 0 to 100 scale. Three normalization methods were presented: Z-score normalization, min-max scaling, and maximum absolute scaling. We mentioned that we opted to use min-max scaling, based on the characteristics of the dataset, but also Z-score normalization, as it is very commonly used. We then explored the system evaluation process, emphasizing the significance of choosing appropriate metrics. MAE, RMSE, and CC were selected to assess the system&#8217;s prediction accuracy. We also addressed the outliers in the dataset and how we handled them by removing specific joint group measurements rather than entire exercises. Then, we presented the results for the <italic toggle="yes">Diagonal</italic> and <italic toggle="yes">Rotation</italic> exercises, focusing on the Head, Trunk, and Shoulder joint groups. We discussed the results, highlighting the impact of data normalization and how min-max scaling outperformed Z-score normalization for the most part. We touched on overall performance trends that showed varying levels of accuracy across the different exercises and joint groups. Thus, we included exercise-specific observations, revealing that the system performed better during the <italic toggle="yes">Diagonal</italic> exercise and that the Trunk joint group obtained higher overall correlation results. After evaluation, we employed hypothesis testing to gain insights into the evaluation metrics obtained earlier. We introduced parametric and non-parametric tests and explained the difference between the two. In our case, there was a preference for non-parametric methods due to the dataset&#8217;s characteristics, such as a small population and the absence of assumptions about data distribution. The Wilcoxon Signed-Rank Test was specifically chosen for its applicability to paired samples, comparing predicted and actual scores. The test was explained, and the steps for performing it were outlined. However, a limitation with the test arose, as when all predictions are consistently lower than actual scores, the test is rendered ineffective. Despite this limitation, the section provides valuable insights into the statistical significance of differences between predicted and actual scores for various exercise and joint group combinations.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-05428"><title>5. Conclusions</title><p>In this paper, we have explored the fields of Computer Vision systems and rehabilitation and how the former can be applied to improve the latter and make it more accessible. Our main goal was to develop a CV-based system that allows patients to perform rehabilitation exercises independently, using accessible technology, while receiving necessary and adequate feedback. In order to do that, we had to research the two main fields involved: Rehabilitation and Computer Vision.</p><p>This study presents a preliminary but promising comparison between a smartphone-based, markerless pose estimation system and a gold-standard QTM motion analysis. Although the accuracy metrics are not yet sufficient for full clinical substitution, they are within the range of acceptable error margins for certain rehabilitation contexts. This supports the hypothesis that such systems can provide useful patient feedback or augment remote clinical supervision, particularly where access to expensive motion capture setups is limited. This study lays the groundwork for accessible, data-driven rehabilitation support, paving the way for future clinical trials and system deployment at scale.</p><sec id="sec5dot1-sensors-25-05428"><title>5.1. Summary of Main&#160;Takeaways</title><p>In the course of this research, we made pivotal observations and conclusions, from which we want to highlight the most important ones. Firstly, we reviewed the current state of the art in Computer Vision systems for assessing physical rehabilitation exercises. Then, we successfully built a CV-based motion tracking system while prioritizing accessibility. That means we successfully demonstrated that there is accessible technology currently available to fill this gap. This could indicate a shift toward developing rehabilitation systems based on accessible technology. One of the aspects of our work that greatly supports that shift is the fact that the smartphone can be used to successfully capture rehabilitation-relevant data. As has been mentioned before, we agree with Lam et al. [<xref rid="B24-sensors-25-05428" ref-type="bibr">24</xref>] that smartphones will play a pivotal role in the future of rehabilitation. Another important takeaway is that DTW can be used to compare two videos. Despite the trend being the integration of Deep Learning, we further validate in our research that DTW can be used as a benchmark. Finally, we also confirm that Z-score normalization does not suit non-normally distributed data. Our findings underscore the significance of choosing appropriate normalization methods, with min-max scaling emerging as a preferred alternative in scenarios where there is a clear maximum and minimum defined. These takeaways encapsulate the key achievements and insights derived from our research, serving as a brief summary of what can be concluded from this work.</p></sec><sec id="sec5dot2-sensors-25-05428"><title>5.2. Opportunities for Improvement and Future&#160;Work</title><p>The final result of this paper can still be improved, which is shown by the results being worse than desired. One of the main opportunities for improvement is the lack of exploration of alternatives for the <italic toggle="yes">Comparison and Assessment</italic> component of the proposed system. More specifically, as understood from <xref rid="sec2-sensors-25-05428" ref-type="sec">Section 2</xref>, while DTW shows interesting results, the trend is to explore Deep Learning. Moving forward, this should be the main focus of future work. Its integration into the project was hindered by time constraints and the fact that the dataset available was not particularly big. We also believe that conducting an intraclass correlation analysis to assess the agreement between the QTM and mediapipe/DTW methods would be beneficial. This approach could yield more appropriate and interpretable results.</p><p>At present, our study establishes a foundational proof-of-concept, demonstrating that low-cost, smartphone-based pose estimation can approximate both clinicians&#8217; traditional ROM assessments and the richer, sequence-level fidelity captured by DTW. Building on this groundwork, future work should pursue a unified validation framework in which identical metrics are computed from synchronized QTM, smartphone video, and IMU recordings. Such a head-to-head design would eliminate questions about cross-metric correspondence and provide a clearer picture of how markerless CV stacks up against each gold-standard modality. This effort could also explore hybrid metrics that blend angle extrema with temporal alignment scores, potentially yielding an even more robust movement-quality index.</p><p>Beyond metric unification, there are opportunities to broaden the system&#8217;s applicability and resilience. Integrating real-time feedback loops&#8212;where the smartphone app not only analyzes but also guides patients through corrective cues&#8212;would transform the tool from a passive recorder into an active rehabilitation coach. Expanding the study population to include individuals with diverse movement impairments and across multiple exercise types would further test the generalizability of our approach. Finally, exploring emerging lightweight neural architectures for on-device inference could reduce reliance on cloud processing, enhancing privacy and minimizing latency for truly ubiquitous, at-home rehabilitation support.</p><p>Another aspect to consider is the ethical outlines of this work. In a possible integration in an application made available to patients, a comprehensive exploration of ethical guidelines should be performed, prioritizing patient privacy. This could include collaboration with professionals to align technological advancement with ethical considerations. Efforts should be made to keep the patients safe. Acknowledging limitations serves as a stepping stone for future advancements. The outlined areas for future work preview how the system could be improved, aligning it with research trends and upholding ethical standards.</p></sec></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, B.C., I.A. and J.M.; methodology, B.C. and I.A.; software, J.M.; validation, I.A., B.C. and J.M.; formal analysis, B.C. and I.A.; investigation, B.C. and I.A.; resources, B.C.; data curation, B.C.; writing&#8212;original draft preparation, J.M.; writing&#8212;review and editing, B.C. and I.A.; visualization, B.C., I.A. and J.M.; supervision, B.C. and I.A.; project administration, B.C. and I.A. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>This study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of the School of Health of the Polytechnic Institute of Porto (protocol code CE0108C and date of approval 15 March 2023) for studies involving humans.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in this study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-05428"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Machlin</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chevan</surname><given-names>J.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>W.</given-names></name><name name-style="western"><surname>Zodet</surname><given-names>M.</given-names></name></person-group><article-title>Determinants of Utilization and Expenditures for Episodes of Ambulatory Physical Therapy Among Adults</article-title><source>Phys. Ther.</source><year>2011</year><volume>91</volume><fpage>1018</fpage><lpage>1029</lpage><pub-id pub-id-type="doi">10.2522/ptj.20100343</pub-id><pub-id pub-id-type="pmid">21566066</pub-id></element-citation></ref><ref id="B2-sensors-25-05428"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mehmood</surname><given-names>F.</given-names></name><name name-style="western"><surname>Mumtaz</surname><given-names>N.</given-names></name><name name-style="western"><surname>Mehmood</surname><given-names>A.</given-names></name></person-group><article-title>Next-Generation Tools for Patient Care and Rehabilitation: A Review of Modern Innovations</article-title><source>Actuators</source><year>2025</year><volume>14</volume><elocation-id>133</elocation-id><pub-id pub-id-type="doi">10.3390/act14030133</pub-id></element-citation></ref><ref id="B3-sensors-25-05428"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lopes</surname><given-names>M.</given-names></name><name name-style="western"><surname>Melo</surname><given-names>A.S.C.</given-names></name><name name-style="western"><surname>Cunha</surname><given-names>B.</given-names></name><name name-style="western"><surname>Sousa</surname><given-names>A.S.P.</given-names></name></person-group><article-title>Smartphone-Based Video Analysis for Guiding Shoulder Therapeutic Exercises: Concurrent Validity for Movement Quality Control</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>12282</elocation-id><pub-id pub-id-type="doi">10.3390/app132212282</pub-id></element-citation></ref><ref id="B4-sensors-25-05428"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Colyer</surname><given-names>S.L.</given-names></name><name name-style="western"><surname>Evans</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cosker</surname><given-names>D.P.</given-names></name><name name-style="western"><surname>Salo</surname><given-names>A.I.T.</given-names></name></person-group><article-title>A Review of the Evolution of Vision-Based Motion Analysis and the Integration of Advanced Computer Vision Methods Towards Developing a Markerless System</article-title><source>Sport. Med.-Open</source><year>2018</year><volume>4</volume><fpage>24</fpage><pub-id pub-id-type="doi">10.1186/s40798-018-0139-y</pub-id><pub-id pub-id-type="pmid">29869300</pub-id><pub-id pub-id-type="pmcid">PMC5986692</pub-id></element-citation></ref><ref id="B5-sensors-25-05428"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hellsten</surname><given-names>T.</given-names></name><name name-style="western"><surname>Karlsson</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shamsuzzaman</surname><given-names>M.</given-names></name><name name-style="western"><surname>Pulkkis</surname><given-names>G.</given-names></name></person-group><article-title>The Potential of Computer Vision-Based Marker-Less Human Motion Analysis for Rehabilitation</article-title><source>Rehabil. Process Outcome</source><year>2021</year><volume>10</volume><fpage>117957272110223</fpage><pub-id pub-id-type="doi">10.1177/11795727211022330</pub-id><pub-id pub-id-type="pmcid">PMC8492027</pub-id><pub-id pub-id-type="pmid">34987303</pub-id></element-citation></ref><ref id="B6-sensors-25-05428"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lobo</surname><given-names>P.</given-names></name><name name-style="western"><surname>Morais</surname><given-names>P.</given-names></name><name name-style="western"><surname>Murray</surname><given-names>P.</given-names></name><name name-style="western"><surname>Vila&#231;a</surname><given-names>J.L.</given-names></name></person-group><article-title>Trends and Innovations in Wearable Technology for Motor Rehabilitation, Prediction, and Monitoring: A Comprehensive Review</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>7973</elocation-id><pub-id pub-id-type="doi">10.3390/s24247973</pub-id><pub-id pub-id-type="pmid">39771710</pub-id><pub-id pub-id-type="pmcid">PMC11679760</pub-id></element-citation></ref><ref id="B7-sensors-25-05428"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Debnath</surname><given-names>B.</given-names></name><name name-style="western"><surname>O&#8217;Brien</surname><given-names>M.</given-names></name><name name-style="western"><surname>Yamaguchi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Behera</surname><given-names>A.</given-names></name></person-group><article-title>A review of computer vision-based approaches for physical rehabilitation and assessment</article-title><source>Multimed. Syst.</source><year>2022</year><volume>28</volume><fpage>209</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1007/s00530-021-00815-4</pub-id></element-citation></ref><ref id="B8-sensors-25-05428"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>Y.L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>C.H.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>C.W.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>P.</given-names></name><name name-style="western"><surname>Kuo</surname><given-names>Y.W.</given-names></name></person-group><article-title>An Upper Extremity Rehabilitation System Using Efficient Vision-Based Action Identification Techniques</article-title><source>Appl. Sci.</source><year>2018</year><volume>8</volume><elocation-id>1161</elocation-id><pub-id pub-id-type="doi">10.3390/app8071161</pub-id></element-citation></ref><ref id="B9-sensors-25-05428"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>C.J.</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>C.Y.</given-names></name><name name-style="western"><surname>Huang</surname><given-names>J.Y.</given-names></name></person-group><article-title>Kinect-enabled home-based rehabilitation system using Dynamic Time Warping and fuzzy logic</article-title><source>Appl. Soft Comput.</source><year>2014</year><volume>22</volume><fpage>652</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2014.04.020</pub-id></element-citation></ref><ref id="B10-sensors-25-05428"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Capecci</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ceravolo</surname><given-names>M.G.</given-names></name><name name-style="western"><surname>Ferracuti</surname><given-names>F.</given-names></name><name name-style="western"><surname>Iarlori</surname><given-names>S.</given-names></name><name name-style="western"><surname>Kyrki</surname><given-names>V.</given-names></name><name name-style="western"><surname>Monteri&#249;</surname><given-names>A.</given-names></name><name name-style="western"><surname>Romeo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Verdini</surname><given-names>F.</given-names></name></person-group><article-title>A Hidden Semi-Markov Model based approach for rehabilitation exercise assessment</article-title><source>J. Biomed. Inform.</source><year>2018</year><volume>78</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.jbi.2017.12.012</pub-id><pub-id pub-id-type="pmid">29277330</pub-id></element-citation></ref><ref id="B11-sensors-25-05428"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dorado</surname><given-names>J.</given-names></name><name name-style="western"><surname>del Toro Garcia</surname><given-names>X.</given-names></name><name name-style="western"><surname>Santofimia</surname><given-names>M.</given-names></name><name name-style="western"><surname>Parre&#241;o-Torres</surname><given-names>A.</given-names></name><name name-style="western"><surname>Cantarero</surname><given-names>R.</given-names></name><name name-style="western"><surname>Rubio Ruiz</surname><given-names>A.</given-names></name><name name-style="western"><surname>L&#243;pez</surname><given-names>J.C.</given-names></name></person-group><article-title>A computer-vision-based system for at-home rheumatoid arthritis rehabilitation</article-title><source>Int. J. Distrib. Sens. Netw.</source><year>2019</year><volume>15</volume><fpage>155014771987564</fpage><pub-id pub-id-type="doi">10.1177/1550147719875649</pub-id></element-citation></ref><ref id="B12-sensors-25-05428"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhi</surname><given-names>Y.X.</given-names></name><name name-style="western"><surname>Lukasik</surname><given-names>M.</given-names></name><name name-style="western"><surname>Li</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Dolatabadi</surname><given-names>E.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>R.H.</given-names></name><name name-style="western"><surname>Taati</surname><given-names>B.</given-names></name></person-group><article-title>Automatic Detection of Compensation During Robotic Stroke Rehabilitation Therapy</article-title><source>IEEE J. Transl. Eng. Health Med.</source><year>2018</year><volume>6</volume><fpage>2100107</fpage><pub-id pub-id-type="doi">10.1109/JTEHM.2017.2780836</pub-id><pub-id pub-id-type="pmid">29404226</pub-id><pub-id pub-id-type="pmcid">PMC5788403</pub-id></element-citation></ref><ref id="B13-sensors-25-05428"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Ciabattoni</surname><given-names>L.</given-names></name><name name-style="western"><surname>Ferracuti</surname><given-names>F.</given-names></name><name name-style="western"><surname>Iarlori</surname><given-names>S.</given-names></name><name name-style="western"><surname>Longhi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Romeo</surname><given-names>L.</given-names></name></person-group><article-title>A novel computer vision based e-rehabilitation system: From gaming to therapy support</article-title><source>Proceedings of the 2016 IEEE International Conference on Consumer Electronics (ICCE)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>7&#8211;11 January 2016</conf-date><fpage>43</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1109/ICCE.2016.7430515</pub-id></element-citation></ref><ref id="B14-sensors-25-05428"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Francisco</surname><given-names>J.A.</given-names></name><name name-style="western"><surname>Rodrigues</surname><given-names>P.S.</given-names></name></person-group><article-title>Computer Vision Based on a Modular Neural Network for Automatic Assessment of Physical Therapy Rehabilitation Activities</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2022</year><volume>31</volume><fpage>2174</fpage><lpage>2183</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2022.3226459</pub-id><pub-id pub-id-type="pmid">36459598</pub-id></element-citation></ref><ref id="B15-sensors-25-05428"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Leechaikul</surname><given-names>N.</given-names></name><name name-style="western"><surname>Charoenseang</surname><given-names>S.</given-names></name></person-group><source>Computer Vision Based Rehabilitation Assistant System</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>408</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-68017-6_61</pub-id></element-citation></ref><ref id="B16-sensors-25-05428"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y.</given-names></name></person-group><article-title>Rehabilitation Training Evaluation and Correction System Based on BlazePose</article-title><source>Proceedings of the 2022 IEEE 4th Eurasia Conference on IOT, Communication and Engineering (ECICE)</source><conf-loc>Yunlin, Taiwan</conf-loc><conf-date>28&#8211;30 October 2022</conf-date><fpage>27</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1109/ECICE55674.2022.10042886</pub-id></element-citation></ref><ref id="B17-sensors-25-05428"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abbas</surname><given-names>A.</given-names></name><name name-style="western"><surname>Yadav</surname><given-names>V.</given-names></name><name name-style="western"><surname>Smith</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ramjas</surname><given-names>E.</given-names></name><name name-style="western"><surname>Rutter</surname><given-names>S.</given-names></name><name name-style="western"><surname>Benavidez</surname><given-names>C.</given-names></name><name name-style="western"><surname>Koesmahargyo</surname><given-names>V.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style="western"><surname>Guan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Rosenfield</surname><given-names>P.</given-names></name><etal/></person-group><article-title>Computer Vision-Based Assessment of Motor Functioning in Schizophrenia: Use of Smartphones for Remote Measurement of Schizophrenia Symptomatology</article-title><source>Digit. Biomark.</source><year>2021</year><volume>5</volume><fpage>29</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1159/000512383</pub-id><pub-id pub-id-type="pmid">33615120</pub-id><pub-id pub-id-type="pmcid">PMC7879301</pub-id></element-citation></ref><ref id="B18-sensors-25-05428"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ferrer-Mallol</surname><given-names>E.</given-names></name><name name-style="western"><surname>Matthews</surname><given-names>C.</given-names></name><name name-style="western"><surname>Stoodley</surname><given-names>M.</given-names></name><name name-style="western"><surname>Gaeta</surname><given-names>A.</given-names></name><name name-style="western"><surname>George</surname><given-names>E.</given-names></name><name name-style="western"><surname>Reuben</surname><given-names>E.</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>A.</given-names></name><name name-style="western"><surname>Davies</surname><given-names>E.H.</given-names></name></person-group><article-title>Patient-led development of digital endpoints and the use of computer vision analysis in assessment of motor function in rare diseases</article-title><source>Front. Pharmacol.</source><year>2022</year><volume>13</volume><elocation-id>916714</elocation-id><pub-id pub-id-type="doi">10.3389/fphar.2022.916714</pub-id><pub-id pub-id-type="pmid">36172196</pub-id><pub-id pub-id-type="pmcid">PMC9510779</pub-id></element-citation></ref><ref id="B19-sensors-25-05428"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>M.H.</given-names></name><name name-style="western"><surname>Mestre</surname><given-names>T.A.</given-names></name><name name-style="western"><surname>Fox</surname><given-names>S.H.</given-names></name><name name-style="western"><surname>Taati</surname><given-names>B.</given-names></name></person-group><article-title>Vision-based assessment of parkinsonism and levodopa-induced dyskinesia with pose estimation</article-title><source>J. NeuroEng. Rehabil.</source><year>2018</year><volume>15</volume><fpage>97</fpage><pub-id pub-id-type="doi">10.1186/s12984-018-0446-z</pub-id><pub-id pub-id-type="pmid">30400914</pub-id><pub-id pub-id-type="pmcid">PMC6219082</pub-id></element-citation></ref><ref id="B20-sensors-25-05428"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liao</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Vakanski</surname><given-names>A.</given-names></name><name name-style="western"><surname>Xian</surname><given-names>M.</given-names></name></person-group><article-title>A Deep Learning Framework for Assessing Physical Rehabilitation Exercises</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2020</year><volume>28</volume><fpage>468</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2020.2966249</pub-id><pub-id pub-id-type="pmid">31940544</pub-id><pub-id pub-id-type="pmcid">PMC7032994</pub-id></element-citation></ref><ref id="B21-sensors-25-05428"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mousavi Hondori</surname><given-names>H.</given-names></name><name name-style="western"><surname>Khademi</surname><given-names>M.</given-names></name></person-group><article-title>A Review on Technical and Clinical Impact of Microsoft Kinect on Physical Therapy and Rehabilitation</article-title><source>J. Med Eng.</source><year>2014</year><volume>2014</volume><fpage>846514</fpage><pub-id pub-id-type="doi">10.1155/2014/846514</pub-id><pub-id pub-id-type="pmid">27006935</pub-id><pub-id pub-id-type="pmcid">PMC4782741</pub-id></element-citation></ref><ref id="B22-sensors-25-05428"><label>22.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Microsoft</collab></person-group><article-title>Kinect</article-title><year>2022</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://learn.microsoft.com/en-us/windows/apps/design/devices/kinect-for-windows" ext-link-type="uri">https://learn.microsoft.com/en-us/windows/apps/design/devices/kinect-for-windows</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2023-05-30">(accessed on 30 May 2023)</date-in-citation></element-citation></ref><ref id="B23-sensors-25-05428"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mourcou</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Fleury</surname><given-names>A.</given-names></name><name name-style="western"><surname>Diot</surname><given-names>B.</given-names></name><name name-style="western"><surname>Franco</surname><given-names>C.</given-names></name><name name-style="western"><surname>Vuillerme</surname><given-names>N.</given-names></name></person-group><article-title>Mobile Phone-Based Joint Angle Measurement for Functional Assessment and Rehabilitation of Proprioception</article-title><source>BioMed Res. Int.</source><year>2015</year><volume>2015</volume><elocation-id>328142</elocation-id><pub-id pub-id-type="doi">10.1155/2015/328142</pub-id><pub-id pub-id-type="pmid">26583101</pub-id><pub-id pub-id-type="pmcid">PMC4637026</pub-id></element-citation></ref><ref id="B24-sensors-25-05428"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lam</surname><given-names>W.W.T.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>Y.M.</given-names></name><name name-style="western"><surname>Fong</surname><given-names>K.N.K.</given-names></name></person-group><article-title>A systematic review of the applications of markerless motion capture (MMC) technology for clinical measurement in rehabilitation</article-title><source>J. NeuroEng. Rehabil.</source><year>2023</year><volume>20</volume><fpage>57</fpage><pub-id pub-id-type="doi">10.1186/s12984-023-01186-9</pub-id><pub-id pub-id-type="pmid">37131238</pub-id><pub-id pub-id-type="pmcid">PMC10155325</pub-id></element-citation></ref><ref id="B25-sensors-25-05428"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Hidalgo</surname><given-names>G.</given-names></name><name name-style="western"><surname>Simon</surname><given-names>T.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>S.E.</given-names></name><name name-style="western"><surname>Sheikh</surname><given-names>Y.</given-names></name></person-group><article-title>OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1812.08008</pub-id><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2929257</pub-id><pub-id pub-id-type="pmid">31331883</pub-id></element-citation></ref><ref id="B26-sensors-25-05428"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bazarevsky</surname><given-names>V.</given-names></name><name name-style="western"><surname>Grishchenko</surname><given-names>I.</given-names></name><name name-style="western"><surname>Raveendran</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Grundmann</surname><given-names>M.</given-names></name></person-group><article-title>BlazePose: On-device Real-time Body Pose tracking</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.2006.10204</pub-id><pub-id pub-id-type="arxiv">2006.10204</pub-id></element-citation></ref><ref id="B27-sensors-25-05428"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Falahati</surname><given-names>S.</given-names></name></person-group><source>OpenNI Cookbook</source><publisher-name>Packt Publishing</publisher-name><publisher-loc>Birmingham, UK</publisher-loc><year>2013</year></element-citation></ref><ref id="B28-sensors-25-05428"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kendall</surname><given-names>A.</given-names></name><name name-style="western"><surname>Grimes</surname><given-names>M.</given-names></name><name name-style="western"><surname>Cipolla</surname><given-names>R.</given-names></name></person-group><article-title>PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/arXiv.1505.07427</pub-id><pub-id pub-id-type="arxiv">1505.07427</pub-id></element-citation></ref><ref id="B29-sensors-25-05428"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>S.E.</given-names></name><name name-style="western"><surname>Ramakrishna</surname><given-names>V.</given-names></name><name name-style="western"><surname>Kanade</surname><given-names>T.</given-names></name><name name-style="western"><surname>Sheikh</surname><given-names>Y.</given-names></name></person-group><article-title>Convolutional Pose Machines</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/arXiv.1602.00134</pub-id><pub-id pub-id-type="arxiv">1602.00134</pub-id></element-citation></ref><ref id="B30-sensors-25-05428"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Baltrusaitis</surname><given-names>T.</given-names></name><name name-style="western"><surname>Robinson</surname><given-names>P.</given-names></name><name name-style="western"><surname>Morency</surname><given-names>L.P.</given-names></name></person-group><article-title>OpenFace: An open source facial behavior analysis toolkit</article-title><source>Proceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Lake Placid, NY, USA</conf-loc><conf-date>7&#8211;10 March 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2016</year><pub-id pub-id-type="doi">10.1109/wacv.2016.7477553</pub-id></element-citation></ref><ref id="B31-sensors-25-05428"><label>31.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Exer</collab></person-group><article-title>Exer Health</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.exer.ai/product/health" ext-link-type="uri">https://www.exer.ai/product/health</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2023-05-30">(accessed on 30 May 2023)</date-in-citation></element-citation></ref><ref id="B32-sensors-25-05428"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pereira</surname><given-names>B.</given-names></name><name name-style="western"><surname>Cunha</surname><given-names>B.</given-names></name><name name-style="western"><surname>Viana</surname><given-names>P.</given-names></name><name name-style="western"><surname>Lopes</surname><given-names>M.</given-names></name><name name-style="western"><surname>Melo</surname><given-names>A.S.C.</given-names></name><name name-style="western"><surname>Sousa</surname><given-names>A.S.P.</given-names></name></person-group><article-title>A Machine Learning App for Monitoring Physical Therapy at Home</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>158</elocation-id><pub-id pub-id-type="doi">10.3390/s24010158</pub-id><pub-id pub-id-type="pmid">38203019</pub-id><pub-id pub-id-type="pmcid">PMC10781250</pub-id></element-citation></ref><ref id="B33-sensors-25-05428"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Miron</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sadawi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Ismail</surname><given-names>W.</given-names></name><name name-style="western"><surname>Hussain</surname><given-names>H.</given-names></name><name name-style="western"><surname>Grosan</surname><given-names>C.</given-names></name></person-group><article-title>IntelliRehabDS (IRDS)&#8212;A Dataset of Physical Rehabilitation Movements</article-title><source>Data</source><year>2021</year><volume>6</volume><elocation-id>46</elocation-id><pub-id pub-id-type="doi">10.3390/data6050046</pub-id></element-citation></ref><ref id="B34-sensors-25-05428"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vakanski</surname><given-names>A.</given-names></name><name name-style="western"><surname>Jun</surname><given-names>H.P.</given-names></name><name name-style="western"><surname>Paul</surname><given-names>D.</given-names></name><name name-style="western"><surname>Baker</surname><given-names>R.</given-names></name></person-group><article-title>A Data Set of Human Body Movements for Physical Rehabilitation Exercises</article-title><source>Data</source><year>2018</year><volume>3</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.3390/data3010002</pub-id><pub-id pub-id-type="pmid">29354641</pub-id><pub-id pub-id-type="pmcid">PMC5773117</pub-id></element-citation></ref><ref id="B35-sensors-25-05428"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Capecci</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ceravolo</surname><given-names>M.G.</given-names></name><name name-style="western"><surname>Ferracuti</surname><given-names>F.</given-names></name><name name-style="western"><surname>Iarlori</surname><given-names>S.</given-names></name><name name-style="western"><surname>Monteri&#249;</surname><given-names>A.</given-names></name><name name-style="western"><surname>Romeo</surname><given-names>L.</given-names></name><name name-style="western"><surname>Verdini</surname><given-names>F.</given-names></name></person-group><article-title>The KIMORE Dataset: KInematic Assessment of MOvement and Clinical Scores for Remote Monitoring of Physical REhabilitation</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2019</year><volume>27</volume><fpage>1436</fpage><lpage>1448</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2019.2923060</pub-id><pub-id pub-id-type="pmid">31217121</pub-id></element-citation></ref><ref id="B36-sensors-25-05428"><label>36.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Google</collab></person-group><article-title>MediaPipe</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://developers.google.com/mediapipe" ext-link-type="uri">https://developers.google.com/mediapipe</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2023-12-21">(accessed on 21 December 2023)</date-in-citation></element-citation></ref><ref id="B37-sensors-25-05428"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Lugaresi</surname><given-names>C.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name><name name-style="western"><surname>Nash</surname><given-names>H.</given-names></name><name name-style="western"><surname>McClanahan</surname><given-names>C.</given-names></name><name name-style="western"><surname>Uboweja</surname><given-names>E.</given-names></name><name name-style="western"><surname>Hays</surname><given-names>M.</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>F.</given-names></name><name name-style="western"><surname>Chang</surname><given-names>C.L.</given-names></name><name name-style="western"><surname>Yong</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name><etal/></person-group><article-title>MediaPipe: A Framework for Perceiving and Processing Reality</article-title><source>Proceedings of the Third Workshop on Computer Vision for AR/VR at IEEE Computer Vision and Pattern Recognition (CVPR) 2019</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>17 June 2019</conf-date></element-citation></ref><ref id="B38-sensors-25-05428"><label>38.</label><element-citation publication-type="webpage"><person-group person-group-type="author"><collab>Google</collab></person-group><article-title>Pose Landmark Detection Guide</article-title><year>2023</year><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://developers.google.com/mediapipe/solutions/vision/pose_landmarker" ext-link-type="uri">https://developers.google.com/mediapipe/solutions/vision/pose_landmarker</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2023-12-21">(accessed on 21 December 2023)</date-in-citation></element-citation></ref><ref id="B39-sensors-25-05428"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sakoe</surname><given-names>H.</given-names></name><name name-style="western"><surname>Chiba</surname><given-names>S.</given-names></name></person-group><article-title>Dynamic programming algorithm optimization for spoken word recognition</article-title><source>IEEE Trans. Acoust. Speech Signal Process.</source><year>1978</year><volume>26</volume><fpage>43</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1109/TASSP.1978.1163055</pub-id></element-citation></ref><ref id="B40-sensors-25-05428"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulkarni</surname><given-names>N.</given-names></name></person-group><article-title>Effect of Dynamic Time Warping using different Distance Measures on Time Series Classification</article-title><source>Int. J. Comput. Appl.</source><year>2017</year><volume>179</volume><fpage>34</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.5120/ijca2017915974</pub-id></element-citation></ref><ref id="B41-sensors-25-05428"><label>41.</label><element-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Qualisys</surname><given-names>A.</given-names></name></person-group><source>Qualisys Track Manager User Manual</source><publisher-name>Qualisys AB</publisher-name><publisher-loc>Gothenburg, Sweden</publisher-loc><year>2006</year></element-citation></ref><ref id="B42-sensors-25-05428"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bradski</surname><given-names>G.R.</given-names></name><name name-style="western"><surname>Kaehler</surname><given-names>A.</given-names></name></person-group><article-title>OpenCV</article-title><source>Dr. Dobb&#8217;s J. Softw. Tools</source><year>2000</year><volume>120</volume><fpage>122</fpage><lpage>125</lpage></element-citation></ref><ref id="B43-sensors-25-05428"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Keogh</surname><given-names>E.</given-names></name></person-group><article-title>FastDTW is Approximate and Generally Slower Than the Algorithm it Approximates</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2020</year><volume>34</volume><fpage>3779</fpage><lpage>3785</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2020.3033752</pub-id></element-citation></ref><ref id="B44-sensors-25-05428"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Salvador</surname><given-names>S.</given-names></name><name name-style="western"><surname>Chan</surname><given-names>P.</given-names></name></person-group><article-title>FastDTW: Toward accurate dynamic time warping in linear time and space</article-title><source>Proceedings of the KDD Workshop on Mining Temporal and Sequential Data</source><conf-loc>Seattle, DC, USA</conf-loc><conf-date>22&#8211;25 August 2004</conf-date><volume>Volume 6</volume><fpage>70</fpage><lpage>80</lpage></element-citation></ref><ref id="B45-sensors-25-05428"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lima</surname><given-names>F.T.</given-names></name><name name-style="western"><surname>Souza</surname><given-names>V.M.A.</given-names></name></person-group><article-title>A Large Comparison of Normalization Methods on Time Series</article-title><source>Big Data Res.</source><year>2023</year><volume>34</volume><fpage>100407</fpage><pub-id pub-id-type="doi">10.1016/j.bdr.2023.100407</pub-id></element-citation></ref><ref id="B46-sensors-25-05428"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aguinis</surname><given-names>H.</given-names></name><name name-style="western"><surname>Gottfredson</surname><given-names>R.K.</given-names></name><name name-style="western"><surname>Joo</surname><given-names>H.</given-names></name></person-group><article-title>Best-Practice Recommendations for Defining, Identifying, and Handling Outliers</article-title><source>Organ. Res. Methods</source><year>2013</year><volume>16</volume><fpage>270</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1177/1094428112470848</pub-id></element-citation></ref><ref id="B47-sensors-25-05428"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chambers</surname><given-names>R.L.</given-names></name><name name-style="western"><surname>Ren</surname><given-names>R.</given-names></name></person-group><article-title>Chambers, R.L.; Ren, R. Outlier robust imputation of survey data</article-title><source>Proc. Am. Stat. Assoc.</source><year>2004</year><fpage>3336</fpage><lpage>3344</lpage></element-citation></ref><ref id="B48-sensors-25-05428"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wilcoxon</surname><given-names>F.</given-names></name></person-group><article-title>Individual Comparisons by Ranking Methods</article-title><source>Biom. Bull.</source><year>1945</year><volume>1</volume><fpage>80</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.2307/3001968</pub-id></element-citation></ref><ref id="B49-sensors-25-05428"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Whitley</surname><given-names>E.</given-names></name><name name-style="western"><surname>Ball</surname><given-names>J.</given-names></name></person-group><article-title>Statistics review 6: Nonparametric methods</article-title><source>Crit. Care</source><year>2002</year><volume>6</volume><fpage>509</fpage><pub-id pub-id-type="doi">10.1186/cc1820</pub-id><pub-id pub-id-type="pmid">12493072</pub-id><pub-id pub-id-type="pmcid">PMC153434</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-05428-f001" orientation="portrait"><label>Figure 1</label><caption><p>Typical structure.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g001.jpg"/></fig><fig position="float" id="sensors-25-05428-f002" orientation="portrait"><label>Figure 2</label><caption><p>Exer Health App. Source: Exex Health [<xref rid="B31-sensors-25-05428" ref-type="bibr">31</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g002.jpg"/></fig><fig position="float" id="sensors-25-05428-f003" orientation="portrait"><label>Figure 3</label><caption><p>MediaPipe Pose Landmarker Model. Source: Google Mediapipe [<xref rid="B38-sensors-25-05428" ref-type="bibr">38</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g003.jpg"/></fig><fig position="float" id="sensors-25-05428-f004" orientation="portrait"><label>Figure 4</label><caption><p>Example of landmark output.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g004.jpg"/></fig><fig position="float" id="sensors-25-05428-f005" orientation="portrait"><label>Figure 5</label><caption><p>Distance matrix for DTW calculation.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g005.jpg"/></fig><fig position="float" id="sensors-25-05428-f006" orientation="portrait"><label>Figure 6</label><caption><p>Frames of the <italic toggle="yes">Diagonal</italic> exercise. Source: Pereira et al. [<xref rid="B32-sensors-25-05428" ref-type="bibr">32</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g006.jpg"/></fig><fig position="float" id="sensors-25-05428-f007" orientation="portrait"><label>Figure 7</label><caption><p>Comparison of times of execution for the different DTW implementations.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g007.jpg"/></fig><fig position="float" id="sensors-25-05428-f008" orientation="portrait"><label>Figure 8</label><caption><p>Pipeline for comparing 2 videos.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g008.jpg"/></fig><fig position="float" id="sensors-25-05428-f009" orientation="portrait"><label>Figure 9</label><caption><p>Pipeline for comparing the entire dataset to the reference.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g009.jpg"/></fig><fig position="float" id="sensors-25-05428-f010" orientation="portrait"><label>Figure 10</label><caption><p>Histograms for the distances obtained for the <italic toggle="yes">Rotation</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g010.jpg"/></fig><fig position="float" id="sensors-25-05428-f011" orientation="portrait"><label>Figure 11</label><caption><p>Histograms for the distances obtained for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g011.jpg"/></fig><fig position="float" id="sensors-25-05428-f012" orientation="portrait"><label>Figure 12</label><caption><p>Error distribution for the Head joint group for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g012.jpg"/></fig><fig position="float" id="sensors-25-05428-f013" orientation="portrait"><label>Figure 13</label><caption><p>Error distribution for the Trunk joint group for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g013.jpg"/></fig><fig position="float" id="sensors-25-05428-f014" orientation="portrait"><label>Figure 14</label><caption><p>Error distribution for the Shoulder joint group for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g014.jpg"/></fig><fig position="float" id="sensors-25-05428-f015" orientation="portrait"><label>Figure 15</label><caption><p>Error distribution for the Head joint group for the <italic toggle="yes">Rotation</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g015.jpg"/></fig><fig position="float" id="sensors-25-05428-f016" orientation="portrait"><label>Figure 16</label><caption><p>Error distribution for the Trunk joint group for the <italic toggle="yes">Rotation</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g016.jpg"/></fig><fig position="float" id="sensors-25-05428-f017" orientation="portrait"><label>Figure 17</label><caption><p>Error distribution for the Shoulder joint group for the <italic toggle="yes">Rotation</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g017.jpg"/></fig><fig position="float" id="sensors-25-05428-f018" orientation="portrait"><label>Figure 18</label><caption><p>Scatter plot of predictions for the Trunk joint group for the <italic toggle="yes">Rotation</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g018.jpg"/></fig><fig position="float" id="sensors-25-05428-f019" orientation="portrait"><label>Figure 19</label><caption><p>Scatter plot of predictions for the Shoulder joint group for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" position="float" orientation="portrait" xlink:href="sensors-25-05428-g019.jpg"/></fig><table-wrap position="float" id="sensors-25-05428-t001" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t001_Table 1</object-id><label>Table 1</label><caption><p>Scientific articles on motion tracking systems for&#160;rehabilitation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Article</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data Acquisition</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feature Engineering</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Comparison and Assessment</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B8-sensors-25-05428" ref-type="bibr">8</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Microsoft Kinect</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OpenNI</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DTW</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B9-sensors-25-05428" ref-type="bibr">9</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Microsoft Kinect</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kinect SDK</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ANFIS for performance and speed components and Fuzzy Logic to combine those into a score</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B10-sensors-25-05428" ref-type="bibr">10</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Microsoft Kinect</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kinect SDK</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HSMM</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B11-sensors-25-05428" ref-type="bibr">11</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Microsoft Kinect</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kinect SDK</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B12-sensors-25-05428" ref-type="bibr">12</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Microsoft Kinect</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kinect SDK</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B13-sensors-25-05428" ref-type="bibr">13</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Custom scoring function and Fuzzy Logic to obtain an overall score</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B14-sensors-25-05428" ref-type="bibr">14</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OpenPose</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Modular NN</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B15-sensors-25-05428" ref-type="bibr">15</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PoseNet</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not Specified</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B16-sensors-25-05428" ref-type="bibr">16</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BlazePose</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B17-sensors-25-05428" ref-type="bibr">17</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OpenFace</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B18-sensors-25-05428" ref-type="bibr">18</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OpenPose</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B19-sensors-25-05428" ref-type="bibr">19</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB Camera</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional Pose Machines</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"> [<xref rid="B20-sensors-25-05428" ref-type="bibr">20</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep NN and GMM log-likelihood performance metric</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t002" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t002_Table 2</object-id><label>Table 2</label><caption><p>Relevant datasets for CV-based&#160;rehabilitation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of Individuals</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of Exercises</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IntelliRehabDS [<xref rid="B33-sensors-25-05428" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UI-PRMD [<xref rid="B34-sensors-25-05428" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KIMORE [<xref rid="B35-sensors-25-05428" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t003" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t003_Table 3</object-id><label>Table 3</label><caption><p>First 15 rows of the distances file for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Head</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Trunk</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Shoulder</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID01_1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">220.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">103.13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID01_2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">258.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">109.38</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID01_3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">257.50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">122.51</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID02_1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">272.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.85</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID02_2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">212.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.76</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID02_3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">310.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.19</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID03_1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">159.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.23</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID03_2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">152.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">102.72</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID03_3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">176.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.32</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID04_1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">235.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.61</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID04_2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">188.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID04_3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">203.35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.92</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID05_1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">308.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">117.46</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID05_2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">301.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">106.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID05_3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">316.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.68</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">113.23</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t004" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t004_Table 4</object-id><label>Table 4</label><caption><p>Scores file for the <italic toggle="yes">Rotation</italic> exercise using min-max scaling.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ID</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Head</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Trunk</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Shoulder</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.31</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID02</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.73</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.14</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.47</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.64</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.74</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.68</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.31</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.01</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.93</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.14</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.37</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.31</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ID15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.19</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t005" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t005_Table 5</object-id><label>Table 5</label><caption><p>Evaluation metrics for the Head joint group for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Normalization</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CC</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min-max</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.52</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.36</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Z-score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.88</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.36</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t006" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t006_Table 6</object-id><label>Table 6</label><caption><p>Evaluation metrics for the Trunk joint group for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Normalization</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CC</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min-max</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.12</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.47</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Z-score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.67</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t007" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t007_Table 7</object-id><label>Table 7</label><caption><p>Evaluation metrics for the Shoulder joint group for the <italic toggle="yes">Diagonal</italic> exercise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Normalization</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CC</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min-max</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.59</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#8722;0.37</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Z-score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.97</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t008" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t008_Table 8</object-id><label>Table 8</label><caption><p>Evaluation metrics for the Head joint group for the <italic toggle="yes">Rotation</italic> exercise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Normalization</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CC</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min-max</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.60</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.22</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Z-score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.86</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.11</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t009" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t009_Table 9</object-id><label>Table 9</label><caption><p>Evaluation metrics for the Trunk joint group for the <italic toggle="yes">Rotation</italic> exercise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Normalization</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CC</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">min-max</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.69</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">0.51</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Z-score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.25</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t010" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t010_Table 10</object-id><label>Table 10</label><caption><p>Evaluation metrics for the Shoulder joint group for the <italic toggle="yes">Rotation</italic> exercise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Normalization</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MAE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">CC</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min-max</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.19</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">&#8722;0.37</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Z-score</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.83</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-05428-t011" orientation="portrait"><object-id pub-id-type="pii">sensors-25-05428-t011_Table 11</object-id><label>Table 11</label><caption><p>Wilcoxon Signed-Rank Test results for min-max and Z-score scores for the <italic toggle="yes">Rotation</italic> and <italic toggle="yes">Diagonal</italic> exercises.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Exercise</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Joint Group</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-Value Min-Max</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-Value Z-Score</th></tr></thead><tbody><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Diagonal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Head</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.057</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.005</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trunk</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.001</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.002</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shoulder</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.000</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Rotation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Head</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.057</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.334</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trunk</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.000</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shoulder</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.035</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.107</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>