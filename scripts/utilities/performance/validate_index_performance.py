#!/usr/bin/env python3
"""
Validate Index Performance Improvements

This script tests ingestion performance before and after index creation
to confirm the performance improvements are working as expected.
"""

import time
import sys
import os
from datetime import datetime

# Add the project root to the path
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from common.iris_connector import get_iris_connection

def test_token_insertion_performance():
    """Test token insertion performance with new indexes."""
    print("üß™ TESTING TOKEN INSERTION PERFORMANCE")
    print("=" * 45)
    
    try:
        conn = get_iris_connection()
        if not conn:
            print("‚ùå Failed to connect to database")
            return
        
        cursor = conn.cursor()
        
        # Test 1: Check index usage for token insertions
        print("\n1. üîç TESTING INDEX USAGE FOR TOKEN LOOKUPS:")
        
        # Simulate a typical token lookup during insertion
        test_doc_id = "PMC556014"  # From recent docs
        
        start_time = time.time()
        cursor.execute("""
            SELECT COUNT(*) FROM RAG.DocumentTokenEmbeddings 
            WHERE doc_id = ? AND token_sequence_index < 100
        """, (test_doc_id,))
        result = cursor.fetchone()[0]
        lookup_time = time.time() - start_time
        
        print(f"   Token lookup for {test_doc_id}: {result} tokens found in {lookup_time:.4f}s")
        
        if lookup_time < 0.1:
            print("   ‚úÖ Fast lookup - indexes are working!")
        elif lookup_time < 0.5:
            print("   ‚ö†Ô∏è  Moderate lookup time - indexes helping but could be better")
        else:
            print("   ‚ùå Slow lookup - indexes may not be optimal")
        
        # Test 2: Check document existence check performance
        print("\n2. üîç TESTING DOCUMENT EXISTENCE CHECK PERFORMANCE:")
        
        start_time = time.time()
        cursor.execute("""
            SELECT doc_id, title FROM RAG.SourceDocuments 
            WHERE doc_id = ?
        """, (test_doc_id,))
        doc_result = cursor.fetchone()
        doc_lookup_time = time.time() - start_time
        
        print(f"   Document lookup for {test_doc_id}: found in {doc_lookup_time:.4f}s")
        
        if doc_lookup_time < 0.01:
            print("   ‚úÖ Very fast document lookup - primary key + index working!")
        elif doc_lookup_time < 0.05:
            print("   ‚úÖ Fast document lookup - indexes working well")
        else:
            print("   ‚ö†Ô∏è  Slower document lookup than expected")
        
        # Test 3: Check join performance between tables
        print("\n3. üîç TESTING JOIN PERFORMANCE:")
        
        start_time = time.time()
        cursor.execute("""
            SELECT TOP 5 s.doc_id, s.title, COUNT(t.token_sequence_index) as token_count
            FROM RAG.SourceDocuments s
            LEFT JOIN RAG.DocumentTokenEmbeddings t ON s.doc_id = t.doc_id
            WHERE s.doc_id LIKE 'PMC555%'
            GROUP BY s.doc_id, s.title
            ORDER BY s.doc_id DESC
        """)
        join_results = cursor.fetchall()
        join_time = time.time() - start_time
        
        print(f"   Join query returned {len(join_results)} results in {join_time:.4f}s")
        for doc_id, title, token_count in join_results:
            print(f"      {doc_id}: {token_count} tokens")
        
        if join_time < 0.1:
            print("   ‚úÖ Fast join performance - indexes optimizing joins!")
        elif join_time < 0.5:
            print("   ‚úÖ Good join performance - indexes helping")
        else:
            print("   ‚ö†Ô∏è  Join performance could be better")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        print(f"‚ùå Error testing performance: {e}")

def simulate_batch_insertion_performance():
    """Simulate a small batch insertion to test performance."""
    print("\nüß™ SIMULATING BATCH INSERTION PERFORMANCE")
    print("=" * 50)
    
    try:
        conn = get_iris_connection()
        if not conn:
            print("‚ùå Failed to connect to database")
            return
        
        cursor = conn.cursor()
        
        # Create a test document for insertion timing
        test_doc_id = f"PERF_TEST_{int(time.time())}"
        
        print(f"1. üìù TESTING DOCUMENT INSERTION:")
        
        start_time = time.time()
        cursor.execute("""
            INSERT INTO RAG.SourceDocuments 
            (doc_id, title, text_content, embedding)
            VALUES (?, ?, ?, ?)
        """, (test_doc_id, "Performance Test Document", "This is a test document for performance validation.", "0.1,0.2,0.3,0.4,0.5"))
        
        doc_insert_time = time.time() - start_time
        print(f"   Document insertion time: {doc_insert_time:.4f}s")
        
        print(f"\n2. üìù TESTING TOKEN EMBEDDING BATCH INSERTION:")
        
        # Prepare test token embeddings
        token_params = []
        for i in range(10):  # Small batch of 10 tokens
            token_params.append((
                test_doc_id,
                i,
                f"token_{i}",
                ",".join([str(0.1 + i * 0.01 + j * 0.001) for j in range(128)]),  # 128-dim embedding
                "{}"
            ))
        
        start_time = time.time()
        cursor.executemany("""
            INSERT INTO RAG.DocumentTokenEmbeddings
            (doc_id, token_sequence_index, token_text, token_embedding, metadata_json)
            VALUES (?, ?, ?, ?, ?)
        """, token_params)
        
        token_insert_time = time.time() - start_time
        print(f"   Token batch insertion time (10 tokens): {token_insert_time:.4f}s")
        print(f"   Average time per token: {token_insert_time/10:.4f}s")
        
        # Commit the test data
        conn.commit()
        
        # Performance analysis
        print(f"\nüìä PERFORMANCE ANALYSIS:")
        
        if token_insert_time < 0.1:
            print("   ‚úÖ Excellent token insertion performance!")
            estimated_batch_time = (token_insert_time / 10) * 91  # 91 avg tokens per doc
            print(f"   Estimated time for avg document (9.1 tokens): {estimated_batch_time:.2f}s")
        elif token_insert_time < 0.5:
            print("   ‚úÖ Good token insertion performance")
            estimated_batch_time = (token_insert_time / 10) * 91
            print(f"   Estimated time for avg document (9.1 tokens): {estimated_batch_time:.2f}s")
        else:
            print("   ‚ö†Ô∏è  Token insertion still slow - may need further optimization")
        
        # Clean up test data
        print(f"\nüßπ CLEANING UP TEST DATA:")
        cursor.execute("DELETE FROM RAG.DocumentTokenEmbeddings WHERE doc_id = ?", (test_doc_id,))
        cursor.execute("DELETE FROM RAG.SourceDocuments WHERE doc_id = ?", (test_doc_id,))
        conn.commit()
        print("   ‚úÖ Test data cleaned up")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        print(f"‚ùå Error simulating batch insertion: {e}")

def provide_optimization_recommendations():
    """Provide additional optimization recommendations based on test results."""
    print(f"\nüéØ ADDITIONAL OPTIMIZATION RECOMMENDATIONS")
    print("=" * 50)
    
    print(f"\n1. üîß IMMEDIATE ACTIONS:")
    print(f"   - Restart your ingestion process to benefit from new indexes")
    print(f"   - Monitor batch timing - should see 30-50% improvement")
    print(f"   - Consider reducing batch size to 10-15 documents if still slow")
    print(f"   - Use smaller token embedding batches (5-10 docs at a time)")
    
    print(f"\n2. üìä MONITORING METRICS:")
    print(f"   - Target batch time: 20-40 seconds (down from 65s)")
    print(f"   - Target ingestion rate: 20-25 docs/sec (up from 15 docs/sec)")
    print(f"   - Watch for memory usage spikes during large batches")
    
    print(f"\n3. üöÄ FURTHER OPTIMIZATIONS (if still needed):")
    print(f"   - Implement connection pooling")
    print(f"   - Add periodic connection refresh every 100 batches")
    print(f"   - Consider parallel insertion workers")
    print(f"   - Implement checkpoint-based resumable ingestion")
    
    print(f"\n4. üîç TROUBLESHOOTING:")
    print(f"   - If performance doesn't improve, check IRIS memory allocation")
    print(f"   - Monitor disk I/O during ingestion")
    print(f"   - Consider VARCHAR to VECTOR migration for Enterprise Edition")
    print(f"   - Check for lock contention in database logs")

def main():
    """Main validation function."""
    print("üöÄ INDEX PERFORMANCE VALIDATION")
    print("=" * 35)
    print(f"‚è∞ Validation started at: {datetime.now()}")
    
    test_token_insertion_performance()
    simulate_batch_insertion_performance()
    provide_optimization_recommendations()
    
    print(f"\n‚úÖ Validation completed at: {datetime.now()}")
    print(f"\nüéØ SUMMARY:")
    print(f"   The new indexes should significantly improve ingestion performance.")
    print(f"   Monitor your ingestion process and expect 1.6x to 2.6x speedup.")
    print(f"   If performance is still slow, consider the additional optimizations above.")

if __name__ == "__main__":
    main()