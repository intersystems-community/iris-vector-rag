#!/usr/bin/env python3
"""
Enterprise Validation with Hybrid iFind RAG

This script validates the complete enterprise RAG system including all 7 techniques:
1. BasicRAG
2. HyDE  
3. CRAG
4. ColBERT (Optimized)
5. NodeRAG
6. GraphRAG
7. Hybrid iFind+Graph+Vector RAG
"""

import os
import sys
import time
import logging
import json
from typing import Dict, Any, List

# Add the project root directory to Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) # Assuming scripts is in project root
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from common.iris_connector import get_iris_connection # Updated import
from common.utils import get_embedding_func, get_llm_func # Updated import
from iris_rag.pipelines.basic import BasicRAGPipeline # Updated import
from iris_rag.pipelines.colbert import ColBERTRAGPipeline # Updated import
from iris_rag.pipelines.hyde import HyDERAGPipeline # Updated import
from iris_rag.pipelines.crag import CRAGPipeline # Updated import
from iris_rag.pipelines.noderag import NodeRAGPipeline # Updated import
from iris_rag.pipelines.graphrag import GraphRAGPipeline # Updated import
from iris_rag.pipelines.hybrid_ifind import HybridIFindRAGPipeline # Updated import

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_mock_colbert_encoder(embedding_dim: int = 128):
    """Create a mock ColBERT encoder for testing."""
    def mock_encoder(text: str) -> List[List[float]]:
        import numpy as np
        words = text.split()[:10]  # Limit to 10 tokens
        embeddings = []
        
        for i, word in enumerate(words):
            np.random.seed(hash(word) % 10000)
            embedding = np.random.randn(embedding_dim)
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            embeddings.append(embedding.tolist())
        
        return embeddings
    
    return mock_encoder

def test_rag_technique(pipeline, technique_name: str, queries: List[str], similarity_threshold: float = 0.3) -> Dict[str, Any]:
    """Test a RAG technique with multiple queries."""
    logger.info(f"Testing {technique_name}...")
    
    results = {
        'technique': technique_name,
        'total_queries': len(queries),
        'successful_queries': 0,
        'failed_queries': 0,
        'average_time': 0.0,
        'query_results': [],
        'errors': []
    }
    
    total_time = 0.0
    
    for i, query in enumerate(queries):
        try:
            start_time = time.time()
            
            # Execute query
            if hasattr(pipeline, 'query'):
                result = pipeline.query(query)
            else:
                # Fallback for pipelines without query method
                retrieved_docs = pipeline.retrieve_documents(query)
                answer = pipeline.generate_response(query, retrieved_docs)
                result = {
                    'query': query,
                    'answer': answer,
                    'retrieved_documents': retrieved_docs
                }
            
            query_time = time.time() - start_time
            total_time += query_time
            
            # Validate result
            if result and 'answer' in result and result['answer']:
                results['successful_queries'] += 1
                logger.info(f"  Query {i+1}/{len(queries)} successful ({query_time:.3f}s)")
            else:
                results['failed_queries'] += 1
                logger.warning(f"  Query {i+1}/{len(queries)} returned empty result")
            
            results['query_results'].append({
                'query': query,
                'success': bool(result and result.get('answer')),
                'time': query_time,
                'num_documents': len(result.get('retrieved_documents', [])) if result else 0
            })
            
        except Exception as e:
            query_time = time.time() - start_time
            total_time += query_time
            results['failed_queries'] += 1
            error_msg = str(e)
            results['errors'].append(f"Query {i+1}: {error_msg}")
            logger.error(f"  Query {i+1}/{len(queries)} failed: {error_msg}")
            
            results['query_results'].append({
                'query': query,
                'success': False,
                'time': query_time,
                'error': error_msg,
                'num_documents': 0
            })
    
    results['average_time'] = total_time / len(queries) if queries else 0.0
    results['success_rate'] = results['successful_queries'] / len(queries) if queries else 0.0
    
    logger.info(f"{technique_name} completed: {results['successful_queries']}/{len(queries)} successful "
                f"({results['success_rate']:.1%}), avg time: {results['average_time']:.0f}ms")
    
    return results

def create_mock_llm_func():
    """Create a mock LLM function for testing."""
    def mock_llm(prompt: str) -> str:
        return f"Mock response based on the provided context. Query appears to be about: {prompt[:100]}..."
    return mock_llm

def main():
    """Main validation function."""
    logger.info("üöÄ Enterprise RAG Validation with Hybrid iFind RAG")
    logger.info("=" * 70)
    
    # Test queries
    test_queries = [
        "What are the main applications of machine learning in healthcare?",
        "How does deep learning differ from traditional machine learning?",
        "What are the key challenges in natural language processing?",
        "Explain the concept of transfer learning in AI",
        "What are the ethical considerations in artificial intelligence?"
    ]
    
    # Fast mode for quick testing
    fast_mode = "--fast" in sys.argv
    if fast_mode:
        test_queries = test_queries[:2]  # Use only 2 queries for fast testing
        logger.info("üèÉ Fast mode enabled - using 2 queries for quick validation")
    
    try:
        # Get IRIS connection
        logger.info("Connecting to IRIS database...")
        iris_connection = get_iris_connection(use_mock=True)
        
        # Get functions
        embedding_func = get_embedding_func()
        llm_func = create_mock_llm_func()  # Use mock LLM to avoid dependency issues
        
        # Initialize all RAG techniques
        techniques = {}
        
        # 1. Basic RAG
        if "--skip-basic" not in sys.argv:
            try:
                techniques['BasicRAG'] = BasicRAGPipeline(
                    iris_connector=iris_connection,
                    embedding_func=embedding_func,
                    llm_func=llm_func
                )
                logger.info("‚úÖ BasicRAG pipeline initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize BasicRAG: {e}")
        
        # 2. HyDE
        if "--skip-hyde" not in sys.argv:
            try:
                techniques['HyDE'] = HyDERAGPipeline(
                    iris_connector=iris_connection,
                    embedding_func=embedding_func,
                    llm_func=llm_func
                )
                logger.info("‚úÖ HyDE pipeline initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize HyDE: {e}")
        
        # 3. CRAG
        if "--skip-crag" not in sys.argv:
            try:
                techniques['CRAG'] = CRAGPipeline(
                    iris_connector=iris_connection,
                    embedding_func=embedding_func,
                    llm_func=llm_func,
                    web_search_func=lambda q: []  # Mock web search
                )
                logger.info("‚úÖ CRAG pipeline initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize CRAG: {e}")
        
        # 4. ColBERT (Optimized)
        if "--skip-colbert" not in sys.argv:
            try:
                mock_query_encoder = create_mock_colbert_encoder()
                mock_doc_encoder = create_mock_colbert_encoder()
                
                techniques['ColBERT'] = ColBERTRAGPipeline(
                    iris_connector=iris_connection,
                    query_encoder=mock_query_encoder,
                    doc_encoder=mock_doc_encoder,
                    llm_func=llm_func
                )
                logger.info("‚úÖ ColBERT (Optimized) pipeline initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize ColBERT: {e}")
        
        # 5. NodeRAG
        if "--skip-noderag" not in sys.argv:
            try:
                techniques['NodeRAG'] = NodeRAGPipeline(
                    iris_connector=iris_connection,
                    embedding_func=embedding_func,
                    llm_func=llm_func
                )
                logger.info("‚úÖ NodeRAG pipeline initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize NodeRAG: {e}")
        
        # 6. GraphRAG
        if "--skip-graphrag" not in sys.argv:
            try:
                techniques['GraphRAG'] = GraphRAGPipeline(
                    iris_connector=iris_connection,
                    embedding_func=embedding_func,
                    llm_func=llm_func
                )
                logger.info("‚úÖ GraphRAG pipeline initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize GraphRAG: {e}")
        
        # 7. Hybrid iFind+Graph+Vector RAG
        if "--skip-hybrid" not in sys.argv:
            try:
                techniques['Hybrid iFind RAG'] = HybridIFindRAGPipeline(
                    iris_connector=iris_connection,
                    embedding_func=embedding_func,
                    llm_func=llm_func
                )
                logger.info("‚úÖ Hybrid iFind+Graph+Vector RAG pipeline initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize Hybrid iFind RAG: {e}")
        
        logger.info(f"\nüîß Initialized {len(techniques)} RAG techniques")
        
        # Test all techniques
        all_results = {}
        start_time = time.time()
        
        for technique_name, pipeline in techniques.items():
            try:
                result = test_rag_technique(pipeline, technique_name, test_queries)
                all_results[technique_name] = result
            except Exception as e:
                logger.error(f"‚ùå Error testing {technique_name}: {e}")
                all_results[technique_name] = {
                    'technique': technique_name,
                    'error': str(e),
                    'success_rate': 0.0
                }
        
        total_time = time.time() - start_time
        
        # Generate summary
        logger.info("\n" + "=" * 70)
        logger.info("üìä ENTERPRISE VALIDATION SUMMARY")
        logger.info("=" * 70)
        
        successful_techniques = 0
        total_success_rate = 0.0
        
        for technique_name, result in all_results.items():
            if 'error' not in result:
                success_rate = result['success_rate']
                avg_time = result['average_time']
                successful_techniques += 1
                total_success_rate += success_rate
                
                status = "‚úÖ" if success_rate >= 0.8 else "‚ö†Ô∏è" if success_rate >= 0.5 else "‚ùå"
                logger.info(f"{status} {technique_name}: {success_rate:.1%} success, {avg_time:.0f}ms avg")
            else:
                logger.info(f"‚ùå {technique_name}: FAILED - {result['error']}")
        
        overall_success_rate = total_success_rate / successful_techniques if successful_techniques > 0 else 0.0
        
        logger.info(f"\nüéØ Overall Results:")
        logger.info(f"   ‚Ä¢ Techniques tested: {len(techniques)}")
        logger.info(f"   ‚Ä¢ Successful techniques: {successful_techniques}")
        logger.info(f"   ‚Ä¢ Overall success rate: {overall_success_rate:.1%}")
        logger.info(f"   ‚Ä¢ Total validation time: {total_time:.1f}s")
        
        # Save detailed results
        timestamp = int(time.time())
        results_file = f"enterprise_validation_with_hybrid_ifind_{timestamp}.json"
        
        detailed_results = {
            'timestamp': timestamp,
            'validation_time': total_time,
            'fast_mode': fast_mode,
            'test_queries': test_queries,
            'techniques_tested': len(techniques),
            'successful_techniques': successful_techniques,
            'overall_success_rate': overall_success_rate,
            'results': all_results
        }
        
        with open(results_file, 'w') as f:
            json.dump(detailed_results, f, indent=2, default=str)
        
        logger.info(f"\nüíæ Detailed results saved to: {results_file}")
        
        # Final status
        if successful_techniques == len(techniques) and overall_success_rate >= 0.8:
            logger.info("\nüéâ ENTERPRISE VALIDATION SUCCESSFUL!")
            logger.info("All RAG techniques are working correctly including Hybrid iFind RAG")
            return 0
        else:
            logger.warning(f"\n‚ö†Ô∏è ENTERPRISE VALIDATION PARTIAL SUCCESS")
            logger.warning(f"Some techniques failed or have low success rates")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Enterprise validation failed: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())