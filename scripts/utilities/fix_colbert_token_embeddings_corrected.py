#!/usr/bin/env python3
"""
Fix ColBERT Token Embeddings - Corrected Version
Populate the DocumentTokenEmbeddings table for ColBERT functionality
"""

import os
import sys
import logging
import json
import numpy as np
from typing import List, Dict, Any

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from common.iris_connector import get_iris_connection

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def simple_tokenize(text: str) -> List[str]:
    """Simple tokenization for ColBERT."""
    import re
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens[:50]  # Limit to 50 tokens for performance

def generate_mock_token_embedding(token: str, dim: int = 128) -> List[float]:
    """Generate a mock token embedding based on token hash."""
    hash_val = hash(token) % (2**31)
    np.random.seed(hash_val)
    embedding = np.random.normal(0, 0.1, dim).tolist()
    return embedding

def populate_token_embeddings_for_document(iris_connector, doc_id: str, text_content: str) -> int:
    """Populate token embeddings for a single document."""
    try:
        # Tokenize the text
        tokens = simple_tokenize(text_content)
        if not tokens:
            return 0
        
        cursor = iris_connector.cursor()
        
        # Check if embeddings already exist for this document
        cursor.execute("SELECT COUNT(*) FROM RAG.DocumentTokenEmbeddings WHERE doc_id = ?", (doc_id,))
        if cursor.fetchone()[0] > 0:
            cursor.close()
            return 0
        
        # Generate and insert token embeddings
        tokens_inserted = 0
        for i, token in enumerate(tokens):
            # Generate mock embedding
            embedding = generate_mock_token_embedding(token)
            embedding_str = ','.join(map(str, embedding))
            
            # Insert token embedding using correct column names
            cursor.execute("""
                INSERT INTO RAG.DocumentTokenEmbeddings 
                (doc_id, token_index, token_text, embedding)
                VALUES (?, ?, ?, ?)
            """, (doc_id, i, token, embedding_str))
            
            tokens_inserted += 1
        
        cursor.close()
        return tokens_inserted
        
    except Exception as e:
        logger.error(f"Error populating token embeddings for {doc_id}: {e}")
        return 0

def populate_all_token_embeddings(iris_connector, max_docs: int = 1000):
    """Populate token embeddings for all documents."""
    try:
        cursor = iris_connector.cursor()
        
        # Get documents that need token embeddings
        cursor.execute(f"""
            SELECT TOP {max_docs} doc_id, text_content 
            FROM RAG.SourceDocuments 
            WHERE doc_id NOT IN (
                SELECT DISTINCT doc_id FROM RAG.DocumentTokenEmbeddings
            )
            AND text_content IS NOT NULL
        """)
        
        documents = cursor.fetchall()
        cursor.close()
        
        logger.info(f"Found {len(documents)} documents needing token embeddings")
        
        total_tokens = 0
        processed_docs = 0
        
        for doc_id, text_content in documents:
            try:
                # Limit text length for performance
                text_content = text_content[:2000] if text_content else ""
                
                if len(text_content.strip()) < 10:
                    continue
                
                tokens_created = populate_token_embeddings_for_document(
                    iris_connector, doc_id, text_content
                )
                
                if tokens_created > 0:
                    total_tokens += tokens_created
                    processed_docs += 1
                    
                    if processed_docs % 10 == 0:
                        logger.info(f"Processed {processed_docs} documents, created {total_tokens} token embeddings")
                
            except Exception as e:
                logger.error(f"Error processing document {doc_id}: {e}")
                continue
        
        logger.info(f"‚úÖ Token embeddings population complete:")
        logger.info(f"   Documents processed: {processed_docs}")
        logger.info(f"   Total tokens created: {total_tokens}")
        
        return processed_docs, total_tokens
        
    except Exception as e:
        logger.error(f"Error in populate_all_token_embeddings: {e}")
        return 0, 0

def verify_token_embeddings(iris_connector):
    """Verify token embeddings were created successfully."""
    try:
        cursor = iris_connector.cursor()
        
        # Count total tokens
        cursor.execute("SELECT COUNT(*) FROM RAG.DocumentTokenEmbeddings")
        total_tokens = cursor.fetchone()[0]
        
        # Count documents with tokens
        cursor.execute("SELECT COUNT(DISTINCT doc_id) FROM RAG.DocumentTokenEmbeddings")
        docs_with_tokens = cursor.fetchone()[0]
        
        # Get sample embedding
        cursor.execute("SELECT TOP 1 embedding FROM RAG.DocumentTokenEmbeddings")
        sample_result = cursor.fetchone()
        sample_embedding = sample_result[0] if sample_result else None
        
        cursor.close()
        
        result = {
            "total_tokens": total_tokens,
            "documents_with_tokens": docs_with_tokens,
            "sample_embedding_length": len(sample_embedding.split(',')) if sample_embedding else 0
        }
        
        logger.info(f"Token embeddings verification: {result}")
        return result
        
    except Exception as e:
        logger.error(f"Error verifying token embeddings: {e}")
        return {"error": str(e)}

def main():
    """Main function."""
    logger.info("üöÄ Starting ColBERT Token Embeddings Fix (Corrected)...")
    
    try:
        # Get database connection
        iris_connector = get_iris_connection()
        
        # Check current state
        initial_state = verify_token_embeddings(iris_connector)
        
        if initial_state.get("total_tokens", 0) > 0:
            logger.info(f"Found {initial_state['total_tokens']} existing token embeddings")
        
        # Populate token embeddings
        processed, tokens_created = populate_all_token_embeddings(iris_connector, max_docs=1000)
        
        # Final verification
        final_state = verify_token_embeddings(iris_connector)
        
        logger.info("\n" + "="*60)
        logger.info("COLBERT TOKEN EMBEDDINGS FIX SUMMARY")
        logger.info("="*60)
        logger.info(f"Documents processed: {processed}")
        logger.info(f"Tokens created: {tokens_created}")
        logger.info(f"Total token embeddings: {final_state.get('total_tokens', 0)}")
        logger.info(f"Documents with tokens: {final_state.get('documents_with_tokens', 0)}")
        
        if final_state.get("total_tokens", 0) > 0:
            logger.info("‚úÖ ColBERT token embeddings fix successful!")
        else:
            logger.warning("‚ö†Ô∏è No token embeddings created")
        
        iris_connector.close()
        
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}", exc_info=True)

if __name__ == "__main__":
    main()