#!/usr/bin/env python3
"""
Populate ColBERT Token Embeddings - VECTOR Format
Properly populate the DocumentTokenEmbeddings table with VECTOR data type
"""

import os
import sys
import logging
import numpy as np
from typing import List

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from common.iris_connector import get_iris_connection

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def simple_tokenize(text: str) -> List[str]:
    """Simple tokenization for ColBERT."""
    import re
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens[:30]  # Limit to 30 tokens for performance

def generate_mock_token_embedding(token: str, dim: int = 128) -> List[float]:
    """Generate a mock token embedding based on token hash."""
    hash_val = hash(token) % (2**31)
    np.random.seed(hash_val)
    embedding = np.random.normal(0, 0.1, dim).tolist()
    return embedding

def populate_token_embeddings_for_document(iris_connector, doc_id: str, text_content: str) -> int:
    """Populate token embeddings for a single document using VECTOR format."""
    try:
        # Tokenize the text
        tokens = simple_tokenize(text_content)
        if not tokens:
            return 0
        
        cursor = iris_connector.cursor()
        
        # Check if embeddings already exist for this document
        cursor.execute("SELECT COUNT(*) FROM RAG.DocumentTokenEmbeddings WHERE doc_id = ?", (doc_id,))
        if cursor.fetchone()[0] > 0:
            cursor.close()
            return 0
        
        # Generate and insert token embeddings using TO_VECTOR
        tokens_inserted = 0
        for i, token in enumerate(tokens):
            # Generate mock embedding
            embedding = generate_mock_token_embedding(token)
            embedding_str = ','.join(map(str, embedding))
            
            # Insert token embedding using TO_VECTOR for proper VECTOR format
            cursor.execute("""
                INSERT INTO RAG.DocumentTokenEmbeddings 
                (doc_id, token_index, token_text, embedding)
                VALUES (?, ?, ?, TO_VECTOR(?, DOUBLE))
            """, (doc_id, i, token, embedding_str))
            
            tokens_inserted += 1
        
        cursor.close()
        return tokens_inserted
        
    except Exception as e:
        logger.error(f"Error populating token embeddings for {doc_id}: {e}")
        return 0

def populate_all_token_embeddings(iris_connector, max_docs: int = 1000):
    """Populate token embeddings for all documents."""
    try:
        cursor = iris_connector.cursor()
        
        # Get documents that need token embeddings
        cursor.execute(f"""
            SELECT TOP {max_docs} doc_id, text_content 
            FROM RAG.SourceDocuments 
            WHERE doc_id NOT IN (
                SELECT DISTINCT doc_id FROM RAG.DocumentTokenEmbeddings
            )
            AND text_content IS NOT NULL
        """)
        
        documents = cursor.fetchall()
        cursor.close()
        
        logger.info(f"Found {len(documents)} documents needing token embeddings")
        
        total_tokens = 0
        processed_docs = 0
        
        for doc_id, text_content in documents:
            try:
                # Limit text length for performance
                text_content = text_content[:2000] if text_content else ""
                
                if len(text_content.strip()) < 10:
                    continue
                
                tokens_created = populate_token_embeddings_for_document(
                    iris_connector, doc_id, text_content
                )
                
                if tokens_created > 0:
                    total_tokens += tokens_created
                    processed_docs += 1
                    
                    if processed_docs % 10 == 0:
                        logger.info(f"Processed {processed_docs} documents, created {total_tokens} token embeddings")
                
            except Exception as e:
                logger.error(f"Error processing document {doc_id}: {e}")
                continue
        
        logger.info(f"‚úÖ Token embeddings population complete:")
        logger.info(f"   Documents processed: {processed_docs}")
        logger.info(f"   Total tokens created: {total_tokens}")
        
        return processed_docs, total_tokens
        
    except Exception as e:
        logger.error(f"Error in populate_all_token_embeddings: {e}")
        return 0, 0

def verify_token_embeddings(iris_connector):
    """Verify token embeddings were created successfully."""
    try:
        cursor = iris_connector.cursor()
        
        # Count total tokens
        cursor.execute("SELECT COUNT(*) FROM RAG.DocumentTokenEmbeddings")
        total_tokens = cursor.fetchone()[0]
        
        # Count documents with tokens
        cursor.execute("SELECT COUNT(DISTINCT doc_id) FROM RAG.DocumentTokenEmbeddings")
        docs_with_tokens = cursor.fetchone()[0]
        
        # Test vector operations
        cursor.execute("SELECT TOP 1 embedding FROM RAG.DocumentTokenEmbeddings")
        sample_result = cursor.fetchone()
        
        vector_test_success = False
        if sample_result:
            try:
                # Test that we can use VECTOR_COSINE on the embedding
                cursor.execute("""
                    SELECT TOP 1 VECTOR_COSINE(embedding, embedding) as test_score 
                    FROM RAG.DocumentTokenEmbeddings
                """)
                test_result = cursor.fetchone()
                if test_result and test_result[0] is not None:
                    vector_test_success = True
            except Exception as e:
                logger.warning(f"Vector operation test failed: {e}")
        
        cursor.close()
        
        result = {
            "total_tokens": total_tokens,
            "documents_with_tokens": docs_with_tokens,
            "vector_operations_working": vector_test_success
        }
        
        logger.info(f"Token embeddings verification: {result}")
        return result
        
    except Exception as e:
        logger.error(f"Error verifying token embeddings: {e}")
        return {"error": str(e)}

def main():
    """Main function."""
    logger.info("üöÄ Starting ColBERT Token Embeddings Population (VECTOR Format)...")
    
    try:
        # Get database connection
        iris_connector = get_iris_connection()
        
        # Check current state
        initial_state = verify_token_embeddings(iris_connector)
        
        if initial_state.get("total_tokens", 0) > 0:
            logger.info(f"Found {initial_state['total_tokens']} existing token embeddings")
            user_input = input("Continue adding more token embeddings? (y/N): ")
            if user_input.lower() != 'y':
                logger.info("Skipping token embeddings population")
                iris_connector.close()
                return
        
        # Populate token embeddings
        processed, tokens_created = populate_all_token_embeddings(iris_connector, max_docs=1000)
        
        # Final verification
        final_state = verify_token_embeddings(iris_connector)
        
        logger.info("\n" + "="*60)
        logger.info("COLBERT TOKEN EMBEDDINGS POPULATION SUMMARY")
        logger.info("="*60)
        logger.info(f"Documents processed: {processed}")
        logger.info(f"Tokens created: {tokens_created}")
        logger.info(f"Total token embeddings: {final_state.get('total_tokens', 0)}")
        logger.info(f"Documents with tokens: {final_state.get('documents_with_tokens', 0)}")
        logger.info(f"Vector operations working: {final_state.get('vector_operations_working', False)}")
        
        if final_state.get("total_tokens", 0) > 0 and final_state.get("vector_operations_working", False):
            logger.info("‚úÖ ColBERT token embeddings population successful!")
            logger.info("üéØ ColBERT pipeline is now ready for enterprise-scale evaluation!")
        else:
            logger.warning("‚ö†Ô∏è Token embeddings created but vector operations may not be working")
        
        iris_connector.close()
        
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}", exc_info=True)

if __name__ == "__main__":
    main()