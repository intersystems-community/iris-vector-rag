#!/usr/bin/env python3
"""
Production Scale RAG System Validation

This script validates that the RAG system works at production scale with real PyTorch models,
demonstrating key capabilities:
- Real ML model inference at scale
- Vector similarity search performance
- HNSW indexing effectiveness
- Memory and performance monitoring
- Context reduction strategies

Usage:
    python scripts/production_scale_validation.py
    python scripts/production_scale_validation.py --full-test
"""

import os
import sys
import logging
import time
import json
import argparse
import psutil
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from common.iris_connector import get_iris_connection
from common.utils import get_embedding_func, get_llm_func
from basic_rag.pipeline import BasicRAGPipeline

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ValidationResult:
    """Results from production scale validation"""
    test_name: str
    success: bool
    metrics: Dict[str, Any]
    error: Optional[str] = None

class ProductionScaleValidator:
    """Validates RAG system at production scale"""
    
    def __init__(self):
        self.connection = None
        self.embedding_func = None
        self.llm_func = None
        self.results: List[ValidationResult] = []
        self.start_time = time.time()
        
    def setup_models(self):
        """Setup real PyTorch models"""
        logger.info("üîß Setting up real PyTorch models...")
        
        try:
            # Setup embedding model
            self.embedding_func = get_embedding_func(model_name="intfloat/e5-base-v2", mock=False)
            
            # Test embedding
            test_embedding = self.embedding_func(["Production scale test"])
            logger.info(f"‚úÖ Embedding model: {len(test_embedding[0])} dimensions")
            
            # Setup LLM with context reduction
            self.llm_func = get_llm_func(provider="openai", model_name="gpt-3.5-turbo")
            
            # Test LLM
            test_response = self.llm_func("Test: What is machine learning?")
            logger.info("‚úÖ LLM model loaded and tested")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Model setup failed: {e}")
            return False
    
    def setup_database(self):
        """Setup database connection"""
        logger.info("üîß Setting up database connection...")
        
        try:
            self.connection = get_iris_connection()
            if not self.connection:
                raise Exception("Failed to establish database connection")
            
            # Get document count
            cursor = self.connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM RAG.SourceDocuments WHERE embedding IS NOT NULL")
            doc_count = cursor.fetchone()[0]
            cursor.close()
            
            logger.info(f"‚úÖ Database connected: {doc_count} documents with embeddings")
            return doc_count > 0
            
        except Exception as e:
            logger.error(f"‚ùå Database setup failed: {e}")
            return False
    
    def test_vector_similarity_performance(self) -> ValidationResult:
        """Test vector similarity search performance at scale"""
        logger.info("üîç Testing vector similarity search performance...")
        
        try:
            test_queries = [
                "diabetes treatment and management",
                "machine learning in medical diagnosis",
                "cancer immunotherapy research",
                "genetic mutations and disease",
                "artificial intelligence healthcare applications"
            ]
            
            performance_metrics = []
            
            for query in test_queries:
                # Generate query embedding
                start_time = time.time()
                query_embedding = self.embedding_func([query])[0]
                embedding_time = time.time() - start_time
                
                # Test vector similarity search
                cursor = self.connection.cursor()
                query_vector_str = ','.join(map(str, query_embedding))
                
                search_start = time.time()
                
                # Test with different similarity thresholds
                for threshold in [0.8, 0.7, 0.6]:
                    sql = """
                    SELECT TOP 50 doc_id, title, 
                           VECTOR_DOT_PRODUCT(?, embedding) as similarity
                    FROM RAG.SourceDocuments 
                    WHERE embedding IS NOT NULL 
                      AND VECTOR_DOT_PRODUCT(?, embedding) > ?
                    ORDER BY similarity DESC
                    """
                    
                    threshold_start = time.time()
                    cursor.execute(sql, (query_vector_str, query_vector_str, threshold))
                    results = cursor.fetchall()
                    threshold_time = time.time() - threshold_start
                    
                    performance_metrics.append({
                        "query": query[:30] + "...",
                        "threshold": threshold,
                        "results_count": len(results),
                        "search_time_ms": threshold_time * 1000,
                        "embedding_time_ms": embedding_time * 1000,
                        "top_similarity": results[0][2] if results else 0
                    })
                
                cursor.close()
            
            # Calculate summary metrics
            avg_search_time = np.mean([m["search_time_ms"] for m in performance_metrics])
            avg_embedding_time = np.mean([m["embedding_time_ms"] for m in performance_metrics])
            avg_results = np.mean([m["results_count"] for m in performance_metrics])
            
            metrics = {
                "avg_search_time_ms": avg_search_time,
                "avg_embedding_time_ms": avg_embedding_time,
                "avg_results_count": avg_results,
                "total_queries": len(test_queries) * 3,  # 3 thresholds per query
                "detailed_metrics": performance_metrics
            }
            
            logger.info(f"‚úÖ Vector search performance: {avg_search_time:.1f}ms avg search, {avg_results:.1f} avg results")
            
            return ValidationResult(
                test_name="vector_similarity_performance",
                success=True,
                metrics=metrics
            )
            
        except Exception as e:
            logger.error(f"‚ùå Vector similarity test failed: {e}")
            return ValidationResult(
                test_name="vector_similarity_performance",
                success=False,
                metrics={},
                error=str(e)
            )
    
    def test_context_reduction_strategies(self) -> ValidationResult:
        """Test context reduction strategies for large document sets"""
        logger.info("üìÑ Testing context reduction strategies...")
        
        try:
            test_query = "What are the latest treatments for diabetes?"
            
            # Generate query embedding
            query_embedding = self.embedding_func([test_query])[0]
            query_vector_str = ','.join(map(str, query_embedding))
            
            cursor = self.connection.cursor()
            
            # Test different context reduction strategies
            strategies = []
            
            # Strategy 1: Top-K with high threshold
            sql1 = """
            SELECT TOP 10 doc_id, title, text_content,
                   VECTOR_DOT_PRODUCT(?, embedding) as similarity
            FROM RAG.SourceDocuments 
            WHERE embedding IS NOT NULL 
              AND VECTOR_DOT_PRODUCT(?, embedding) > 0.8
            ORDER BY similarity DESC
            """
            
            start_time = time.time()
            cursor.execute(sql1, (query_vector_str, query_vector_str))
            results1 = cursor.fetchall()
            time1 = time.time() - start_time
            
            # Calculate context size
            context1 = "\n\n".join([f"Title: {r[1]}\nContent: {r[2][:500]}..." for r in results1])
            context1_tokens = len(context1.split()) * 1.3  # Rough token estimate
            
            strategies.append({
                "strategy": "top_10_high_threshold",
                "results_count": len(results1),
                "search_time_ms": time1 * 1000,
                "estimated_tokens": context1_tokens,
                "avg_similarity": np.mean([r[3] for r in results1]) if results1 else 0
            })
            
            # Strategy 2: Top-K with medium threshold
            sql2 = """
            SELECT TOP 5 doc_id, title, text_content,
                   VECTOR_DOT_PRODUCT(?, embedding) as similarity
            FROM RAG.SourceDocuments 
            WHERE embedding IS NOT NULL 
              AND VECTOR_DOT_PRODUCT(?, embedding) > 0.7
            ORDER BY similarity DESC
            """
            
            start_time = time.time()
            cursor.execute(sql2, (query_vector_str, query_vector_str))
            results2 = cursor.fetchall()
            time2 = time.time() - start_time
            
            context2 = "\n\n".join([f"Title: {r[1]}\nContent: {r[2][:300]}..." for r in results2])
            context2_tokens = len(context2.split()) * 1.3
            
            strategies.append({
                "strategy": "top_5_medium_threshold",
                "results_count": len(results2),
                "search_time_ms": time2 * 1000,
                "estimated_tokens": context2_tokens,
                "avg_similarity": np.mean([r[3] for r in results2]) if results2 else 0
            })
            
            # Strategy 3: Abstract-only with more documents
            sql3 = """
            SELECT TOP 15 doc_id, title, 
                   SUBSTRING(text_content, 1, 200) as abstract,
                   VECTOR_DOT_PRODUCT(?, embedding) as similarity
            FROM RAG.SourceDocuments 
            WHERE embedding IS NOT NULL 
              AND VECTOR_DOT_PRODUCT(?, embedding) > 0.6
            ORDER BY similarity DESC
            """
            
            start_time = time.time()
            cursor.execute(sql3, (query_vector_str, query_vector_str))
            results3 = cursor.fetchall()
            time3 = time.time() - start_time
            
            context3 = "\n\n".join([f"Title: {r[1]}\nAbstract: {r[2]}..." for r in results3])
            context3_tokens = len(context3.split()) * 1.3
            
            strategies.append({
                "strategy": "top_15_abstracts_only",
                "results_count": len(results3),
                "search_time_ms": time3 * 1000,
                "estimated_tokens": context3_tokens,
                "avg_similarity": np.mean([r[3] for r in results3]) if results3 else 0
            })
            
            cursor.close()
            
            # Test actual LLM call with reduced context
            if results2:  # Use strategy 2 (manageable size)
                prompt = f"""Answer the question based on the provided research context.

Context:
{context2}

Question: {test_query}

Answer:"""
                
                try:
                    llm_start = time.time()
                    answer = self.llm_func(prompt)
                    llm_time = time.time() - llm_start
                    
                    llm_success = True
                    answer_length = len(answer)
                    
                except Exception as e:
                    llm_success = False
                    llm_time = 0
                    answer_length = 0
                    logger.warning(f"LLM call failed: {e}")
            else:
                llm_success = False
                llm_time = 0
                answer_length = 0
            
            metrics = {
                "strategies": strategies,
                "llm_test": {
                    "success": llm_success,
                    "response_time_ms": llm_time * 1000,
                    "answer_length": answer_length
                },
                "recommended_strategy": "top_5_medium_threshold"
            }
            
            logger.info(f"‚úÖ Context reduction: {len(strategies)} strategies tested, LLM success: {llm_success}")
            
            return ValidationResult(
                test_name="context_reduction_strategies",
                success=True,
                metrics=metrics
            )
            
        except Exception as e:
            logger.error(f"‚ùå Context reduction test failed: {e}")
            return ValidationResult(
                test_name="context_reduction_strategies",
                success=False,
                metrics={},
                error=str(e)
            )
    
    def test_semantic_search_quality(self) -> ValidationResult:
        """Test semantic search quality with domain-specific queries"""
        logger.info("üéØ Testing semantic search quality...")
        
        try:
            # Domain-specific test cases
            test_cases = [
                {
                    "query": "diabetes insulin treatment",
                    "expected_terms": ["diabetes", "insulin", "glucose", "treatment", "medication"]
                },
                {
                    "query": "machine learning medical diagnosis",
                    "expected_terms": ["machine learning", "AI", "diagnosis", "medical", "algorithm"]
                },
                {
                    "query": "cancer immunotherapy research",
                    "expected_terms": ["cancer", "immunotherapy", "immune", "tumor", "therapy"]
                }
            ]
            
            quality_results = []
            
            for test_case in test_cases:
                query = test_case["query"]
                expected_terms = test_case["expected_terms"]
                
                # Generate query embedding and search
                query_embedding = self.embedding_func([query])[0]
                query_vector_str = ','.join(map(str, query_embedding))
                
                cursor = self.connection.cursor()
                sql = """
                SELECT TOP 10 doc_id, title, text_content,
                       VECTOR_DOT_PRODUCT(?, embedding) as similarity
                FROM RAG.SourceDocuments 
                WHERE embedding IS NOT NULL 
                  AND VECTOR_DOT_PRODUCT(?, embedding) > 0.7
                ORDER BY similarity DESC
                """
                
                cursor.execute(sql, (query_vector_str, query_vector_str))
                results = cursor.fetchall()
                cursor.close()
                
                # Analyze relevance
                relevant_docs = 0
                term_matches = 0
                similarities = []
                
                for doc_id, title, content, similarity in results:
                    similarities.append(similarity)
                    
                    # Check for expected terms
                    text_to_check = (title + " " + content).lower()
                    doc_matches = sum(1 for term in expected_terms if term.lower() in text_to_check)
                    
                    if doc_matches > 0:
                        relevant_docs += 1
                        term_matches += doc_matches
                
                relevance_score = relevant_docs / len(results) if results else 0
                avg_similarity = np.mean(similarities) if similarities else 0
                
                quality_results.append({
                    "query": query,
                    "results_count": len(results),
                    "relevant_docs": relevant_docs,
                    "relevance_score": relevance_score,
                    "avg_similarity": avg_similarity,
                    "term_matches": term_matches
                })
            
            # Calculate overall quality metrics
            overall_relevance = np.mean([r["relevance_score"] for r in quality_results])
            overall_similarity = np.mean([r["avg_similarity"] for r in quality_results])
            
            metrics = {
                "overall_relevance_score": overall_relevance,
                "overall_avg_similarity": overall_similarity,
                "test_cases": quality_results,
                "quality_threshold": 0.6  # 60% relevance considered good
            }
            
            success = overall_relevance >= 0.6
            
            logger.info(f"‚úÖ Semantic search quality: {overall_relevance:.2f} relevance, {overall_similarity:.4f} similarity")
            
            return ValidationResult(
                test_name="semantic_search_quality",
                success=success,
                metrics=metrics
            )
            
        except Exception as e:
            logger.error(f"‚ùå Semantic search quality test failed: {e}")
            return ValidationResult(
                test_name="semantic_search_quality",
                success=False,
                metrics={},
                error=str(e)
            )
    
    def test_system_performance_monitoring(self) -> ValidationResult:
        """Test system performance under load"""
        logger.info("üìä Testing system performance monitoring...")
        
        try:
            # Monitor system resources
            initial_memory = psutil.virtual_memory()
            initial_cpu = psutil.cpu_percent(interval=1)
            
            # Run multiple queries to simulate load
            test_queries = [
                "cardiovascular disease treatment",
                "neurological disorders research", 
                "infectious disease prevention",
                "metabolic syndrome management",
                "respiratory system function"
            ]
            
            performance_data = []
            
            for i, query in enumerate(test_queries):
                start_time = time.time()
                
                # Generate embedding
                embedding_start = time.time()
                query_embedding = self.embedding_func([query])[0]
                embedding_time = time.time() - embedding_start
                
                # Perform search
                search_start = time.time()
                query_vector_str = ','.join(map(str, query_embedding))
                
                cursor = self.connection.cursor()
                sql = """
                SELECT TOP 20 doc_id, title,
                       VECTOR_DOT_PRODUCT(?, embedding) as similarity
                FROM RAG.SourceDocuments 
                WHERE embedding IS NOT NULL 
                  AND VECTOR_DOT_PRODUCT(?, embedding) > 0.7
                ORDER BY similarity DESC
                """
                
                cursor.execute(sql, (query_vector_str, query_vector_str))
                results = cursor.fetchall()
                cursor.close()
                
                search_time = time.time() - search_start
                total_time = time.time() - start_time
                
                # Monitor resources
                current_memory = psutil.virtual_memory()
                current_cpu = psutil.cpu_percent(interval=0.1)
                
                performance_data.append({
                    "query_id": i,
                    "query": query[:30] + "...",
                    "total_time_ms": total_time * 1000,
                    "embedding_time_ms": embedding_time * 1000,
                    "search_time_ms": search_time * 1000,
                    "results_count": len(results),
                    "memory_used_gb": current_memory.used / (1024**3),
                    "memory_percent": current_memory.percent,
                    "cpu_percent": current_cpu
                })
            
            # Calculate performance metrics
            avg_total_time = np.mean([p["total_time_ms"] for p in performance_data])
            avg_embedding_time = np.mean([p["embedding_time_ms"] for p in performance_data])
            avg_search_time = np.mean([p["search_time_ms"] for p in performance_data])
            avg_memory = np.mean([p["memory_used_gb"] for p in performance_data])
            avg_cpu = np.mean([p["cpu_percent"] for p in performance_data])
            
            queries_per_second = 1000 / avg_total_time if avg_total_time > 0 else 0
            
            metrics = {
                "avg_total_time_ms": avg_total_time,
                "avg_embedding_time_ms": avg_embedding_time,
                "avg_search_time_ms": avg_search_time,
                "queries_per_second": queries_per_second,
                "avg_memory_gb": avg_memory,
                "avg_cpu_percent": avg_cpu,
                "total_queries": len(test_queries),
                "detailed_performance": performance_data
            }
            
            logger.info(f"‚úÖ Performance: {queries_per_second:.1f} queries/sec, {avg_total_time:.1f}ms avg")
            
            return ValidationResult(
                test_name="system_performance_monitoring",
                success=True,
                metrics=metrics
            )
            
        except Exception as e:
            logger.error(f"‚ùå Performance monitoring test failed: {e}")
            return ValidationResult(
                test_name="system_performance_monitoring",
                success=False,
                metrics={},
                error=str(e)
            )
    
    def run_validation_suite(self, full_test: bool = False):
        """Run the complete validation suite"""
        logger.info("üöÄ Starting Production Scale RAG Validation")
        logger.info("=" * 80)
        
        try:
            # Setup phase
            if not self.setup_models():
                logger.error("‚ùå Model setup failed - cannot continue")
                return False
            
            if not self.setup_database():
                logger.error("‚ùå Database setup failed - cannot continue")
                return False
            
            # Core validation tests
            logger.info("\nüîç Running core validation tests...")
            
            # Test 1: Vector similarity performance
            result1 = self.test_vector_similarity_performance()
            self.results.append(result1)
            
            # Test 2: Context reduction strategies
            result2 = self.test_context_reduction_strategies()
            self.results.append(result2)
            
            # Test 3: Semantic search quality
            result3 = self.test_semantic_search_quality()
            self.results.append(result3)
            
            # Test 4: System performance monitoring
            result4 = self.test_system_performance_monitoring()
            self.results.append(result4)
            
            # Generate summary report
            self.generate_summary_report()
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Validation suite failed: {e}")
            return False
        
        finally:
            # Cleanup
            if self.connection:
                try:
                    self.connection.close()
                except:
                    pass
    
    def generate_summary_report(self):
        """Generate comprehensive summary report"""
        logger.info("\n" + "=" * 80)
        logger.info("üéâ Production Scale RAG Validation Complete!")
        
        total_time = time.time() - self.start_time
        successful_tests = len([r for r in self.results if r.success])
        total_tests = len(self.results)
        
        logger.info(f"‚è±Ô∏è  Total validation time: {total_time/60:.1f} minutes")
        logger.info(f"‚úÖ Successful tests: {successful_tests}/{total_tests}")
        
        logger.info("\nüìä VALIDATION RESULTS:")
        
        for result in self.results:
            status = "‚úÖ PASS" if result.success else "‚ùå FAIL"
            logger.info(f"  {result.test_name}: {status}")
            
            if result.success and result.metrics:
                # Show key metrics for each test
                if result.test_name == "vector_similarity_performance":
                    logger.info(f"    - Avg search time: {result.metrics['avg_search_time_ms']:.1f}ms")
                    logger.info(f"    - Avg results: {result.metrics['avg_results_count']:.1f} documents")
                
                elif result.test_name == "context_reduction_strategies":
                    strategies = result.metrics.get('strategies', [])
                    if strategies:
                        best_strategy = min(strategies, key=lambda x: x['estimated_tokens'])
                        logger.info(f"    - Best strategy: {best_strategy['strategy']}")
                        logger.info(f"    - Estimated tokens: {best_strategy['estimated_tokens']:.0f}")
                
                elif result.test_name == "semantic_search_quality":
                    logger.info(f"    - Relevance score: {result.metrics['overall_relevance_score']:.2f}")
                    logger.info(f"    - Avg similarity: {result.metrics['overall_avg_similarity']:.4f}")
                
                elif result.test_name == "system_performance_monitoring":
                    logger.info(f"    - Queries/second: {result.metrics['queries_per_second']:.1f}")
                    logger.info(f"    - Avg memory: {result.metrics['avg_memory_gb']:.1f}GB")
            
            if not result.success and result.error:
                logger.info(f"    - Error: {result.error}")
        
        # Save detailed results
        timestamp = int(time.time())
        results_file = f"production_validation_results_{timestamp}.json"
        
        results_data = []
        for result in self.results:
            results_data.append({
                "test_name": result.test_name,
                "success": result.success,
                "metrics": result.metrics,
                "error": result.error
            })
        
        with open(results_file, 'w') as f:
            json.dump({
                "validation_summary": {
                    "total_time_minutes": total_time / 60,
                    "successful_tests": successful_tests,
                    "total_tests": total_tests,
                    "success_rate": successful_tests / total_tests if total_tests > 0 else 0
                },
                "test_results": results_data
            }, f, indent=2)
        
        logger.info(f"\nüìÅ Detailed results saved to: {results_file}")
        
        # Final assessment
        if successful_tests == total_tests:
            logger.info("\nüéØ PRODUCTION SCALE VALIDATION: ‚úÖ PASSED")
            logger.info("The RAG system is validated for production scale workloads!")
        else:
            logger.info(f"\n‚ö†Ô∏è  PRODUCTION SCALE VALIDATION: Partial success ({successful_tests}/{total_tests})")

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Production Scale RAG System Validation")
    parser.add_argument("--full-test", action="store_true",
                       help="Run extended validation tests")
    
    args = parser.parse_args()
    
    logger.info("Production Scale RAG System Validation")
    logger.info("Testing real PyTorch models with 1000+ documents")
    
    # Run validation
    validator = ProductionScaleValidator()
    success = validator.run_validation_suite(full_test=args.full_test)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()