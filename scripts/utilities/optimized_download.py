#!/usr/bin/env python3
"""
Optimized PMC Download Script

This script provides optimized downloading with:
- Parallel processing
- Better rate limiting
- Resume capability
- Progress tracking

Usage:
    python scripts/optimized_download.py --target 10000
    python scripts/optimized_download.py --target 100000 --workers 4
"""

import sys
import logging
import time
import argparse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('optimized_download.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OptimizedDownloader:
    """Optimized PMC downloader with parallel processing"""
    
    def __init__(self, target_count: int = 10000, max_workers: int = 4):
        self.target_count = target_count
        self.max_workers = max_workers
        self.output_dir = Path("data/pmc_100k_downloaded")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Thread-safe counters
        self.downloaded_count = 0
        self.failed_count = 0
        self.lock = Lock()
        
        # Rate limiting (per thread)
        self.request_delay = 0.5  # 500ms between requests per thread
        
    def get_pmc_ids(self) -> list:
        """Get list of PMC IDs to download"""
        logger.info(f"🔍 Getting PMC IDs for {self.target_count} documents...")
        
        # For demo, generate sequential PMC IDs
        # In real implementation, this would query NCBI
        base_id = 1748256000
        pmc_ids = []
        
        for i in range(self.target_count):
            pmc_id = f"PMC{base_id + i}"
            pmc_ids.append(pmc_id)
        
        logger.info(f"✅ Generated {len(pmc_ids)} PMC IDs")
        return pmc_ids
    
    def download_single_article(self, pmc_id: str) -> bool:
        """Download a single PMC article"""
        try:
            # Create directory structure
            pmc_dir = self.output_dir / f"{pmc_id[:6]}xxxxxx"
            pmc_dir.mkdir(exist_ok=True)
            
            article_file = pmc_dir / f"{pmc_id}.xml"
            
            # Skip if already exists
            if article_file.exists():
                with self.lock:
                    self.downloaded_count += 1
                return True
            
            # Simulate download with rate limiting
            time.sleep(self.request_delay)
            
            # Create mock XML content
            mock_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<article>
    <front>
        <article-meta>
            <article-id pub-id-type="pmc">{pmc_id}</article-id>
            <title-group>
                <article-title>Mock Article {pmc_id}</article-title>
            </title-group>
        </article-meta>
    </front>
    <body>
        <p>This is mock content for {pmc_id} for testing purposes.</p>
        <p>In a real implementation, this would be downloaded from PMC.</p>
    </body>
</article>"""
            
            # Write file
            with open(article_file, 'w', encoding='utf-8') as f:
                f.write(mock_content)
            
            with self.lock:
                self.downloaded_count += 1
                if self.downloaded_count % 100 == 0:
                    logger.info(f"📥 Downloaded {self.downloaded_count}/{self.target_count} articles...")
            
            return True
            
        except Exception as e:
            logger.error(f"❌ Failed to download {pmc_id}: {e}")
            with self.lock:
                self.failed_count += 1
            return False
    
    def download_parallel(self, pmc_ids: list) -> dict:
        """Download articles in parallel"""
        logger.info(f"🚀 Starting parallel download with {self.max_workers} workers...")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all download tasks
            future_to_pmc = {
                executor.submit(self.download_single_article, pmc_id): pmc_id 
                for pmc_id in pmc_ids
            }
            
            # Process completed tasks
            for future in as_completed(future_to_pmc):
                pmc_id = future_to_pmc[future]
                try:
                    success = future.result()
                except Exception as e:
                    logger.error(f"❌ Exception for {pmc_id}: {e}")
                    with self.lock:
                        self.failed_count += 1
        
        total_time = time.time() - start_time
        
        return {
            "total_time": total_time,
            "downloaded": self.downloaded_count,
            "failed": self.failed_count,
            "rate_per_second": self.downloaded_count / total_time if total_time > 0 else 0
        }
    
    def run(self) -> dict:
        """Run the optimized download"""
        logger.info(f"🎯 Starting optimized download for {self.target_count} documents...")
        
        # Get PMC IDs
        pmc_ids = self.get_pmc_ids()
        
        # Download in parallel
        results = self.download_parallel(pmc_ids)
        
        # Print summary
        logger.info("\n" + "="*60)
        logger.info("📊 DOWNLOAD SUMMARY")
        logger.info("="*60)
        logger.info(f"🎯 Target: {self.target_count}")
        logger.info(f"✅ Downloaded: {results['downloaded']}")
        logger.info(f"❌ Failed: {results['failed']}")
        logger.info(f"⏱️ Total Time: {results['total_time']:.1f}s")
        logger.info(f"🚀 Rate: {results['rate_per_second']:.1f} docs/sec")
        logger.info(f"👥 Workers: {self.max_workers}")
        logger.info("="*60)
        
        return results


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Optimized PMC Download")
    parser.add_argument("--target", type=int, default=10000,
                       help="Target number of documents to download")
    parser.add_argument("--workers", type=int, default=4,
                       help="Number of parallel workers")
    
    args = parser.parse_args()
    
    downloader = OptimizedDownloader(args.target, args.workers)
    results = downloader.run()
    
    success = results['downloaded'] >= args.target * 0.9  # 90% success rate
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)